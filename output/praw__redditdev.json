[{"author": "allthefoxes", "created_utc": 1453874939, "gilded": 0, "name": "t3_42w4l7", "num_comments": 8, "score": 7, "selftext": "Using PRAW", "title": "With the OAuth-mageddon approaching, what is a quick and easy way to migrate already existing scripts/bots to use it?", "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "brain_emesis", "created_utc": 1453736718, "gilded": 0, "name": "t3_42lzes", "num_comments": 4, "score": 2, "selftext": "I've been trying to make an app that requires that I get the top X posts in a subreddit. I thought the get_top_from_week request would work for me (documented [here](https://praw.readthedocs.org/en/stable/pages/code_overview.html?highlight=get_top_from_week#praw.objects.Subreddit.get_top_from_week)) but for some reason it is only returning posts that have occurred before Jan 21 (when executing it today, Jan 25). This is true even if I pass a limit of None. The documentation for it is very short so I might be misunderstanding how to use it. Does anyone know if this is correct behaviour?", "title": "[PRAW] subreddit.get_top_from_week isn't returning anything after Jan 21", "url": "https://www.reddit.com/r/redditdev/comments/42lzes/praw_subredditget_top_from_week_isnt_returning/"}, {"author": "theonefoster", "created_utc": 1453586885, "gilded": 0, "name": "t3_42d03i", "num_comments": 6, "score": 1, "selftext": "try: user = r.get_redditor(\"skldjhfgksdjhfg\", fetch=True) except praw.errors.HTTPException as e: if e.response.status_code == 404: print \"404\" elif e.response.status_code == 503: print \"503 else: print \"Other HTTPError\" I've tried the above, as suggested by previous posts, but it doesn't work. I can catch a generat HTTPException fine, but I want to distinguish between a 404 exception and a 503 exception and take different action. However the above line crashes with \"AttributeError: 'NotFound' object has no attribute 'response'\". What should I do?", "title": "How can I access the specific HTTP code in an HTTPException in praw?", "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "Joshjayk", "created_utc": 1453522440, "gilded": 0, "name": "t3_429bps", "num_comments": 10, "score": 3, "selftext": "Using praw, if i get a comment from the comment stream, is there a way for me to obtain the parent comment? I want to use this for those \"complete sentence\" replies. For example: Comment: I Reply: LOVE Reply to reply: CAKE. If I see 'CAKE.' on the comment stream, is there a way I could reconstruct the tree back up to the parent?", "title": "[PRAW] Getting parent comments?", "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "spookyyz", "created_utc": 1453333839, "gilded": 0, "name": "t3_41x45c", "num_comments": 9, "score": 1, "selftext": "So, I feel like a complete and utter idiot, but I'm really struggling to set a post I'm making with a bot to be the sticky for that sub with PRAW. I am posting via this line: GAME_THREAD = REDDIT.submit(SUBREDDIT, submission_title, text=submission_text) How do I set that submission to be the sticky for the sub when it is posted? I'm sorry I know this question is stupid as hell, but I'm really struggling with it sadly. PS. REDDIT is a PRAW object.", "title": "[PRAW] Stickying a submission posted", "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "bAndkAllDay", "created_utc": 1453249677, "gilded": 0, "name": "t3_41roji", "num_comments": 4, "score": 2, "selftext": "Hi. I'm quite new to PRAW and so far using the Python IDE writing the code line by line and it was working great. I was able to retrieve the top 5 posts for a subreddit. However, when I try and save the code into a .py and execute it, the code works but it doesn't retrieve the subreddit information, and the fields are blank. The code is grabbing the top 5 posts from a subreddit and saving the information to a text file. Anyone else run into this issue? Thanks http://pastebin.com/qkbusiaZ", "title": "PRAW Python not retrieving info when executed", "url": "https://www.reddit.com/r/redditdev/comments/41roji/praw_python_not_retrieving_info_when_executed/"}, {"author": "elihusmails", "created_utc": 1453212780, "gilded": 0, "name": "t3_41otyb", "num_comments": 13, "score": 8, "selftext": "I am interested in getting into writing bots. I have been a full time developer of Java since the 1.1 days. I have messed around with Python, but I am certainly no expert. From what I can tell, the reddit bot community is strongly slanted towards Python. So I have a couple questions: 1. Is the Java Reddit bot community active? 2. Are there API's for Java that are as good as PRAW? If No, then it'll be a good opportunity to learn Python. If that is the case, should I use Python 2.x or 3.x ? Thanks all.", "title": "Python or Java for bot development", "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "haigaguy", "created_utc": 1452916842, "gilded": 0, "name": "t3_416r43", "num_comments": 2, "score": 2, "selftext": "I am trying to fetch the most downvoted comments from the top 15 submissions of the day of a particular subreddit. PRAW has get_comments() but I don't see any way to check for votes. Thank you!", "title": "[PRAW] How to get the most downvoted comments from a submission?", "url": "https://www.reddit.com/r/redditdev/comments/416r43/praw_how_to_get_the_most_downvoted_comments_from/"}, {"author": "PieCrafted", "created_utc": 1452694719, "gilded": 0, "name": "t3_40s7gb", "num_comments": 9, "score": 1, "selftext": "I have this script that I found on GitHub, and the top few lines state: import praw, time, datetime, re, urllib, urllib2, pickle, pyimgur, os, traceback, wikipedia, string, socket, sys, collections from nsfw import getnsfw from util import success, warn, log, fail, special, bluelog from bs4 import BeautifulSoup from HTMLParser import HTMLParser But, the error log states: Traceback (most recent call last): File \"bot.py\", line 4, in from nsfw import getnsfw ImportError: No module named nsfw The requirements.txt only downloads praw, pyimgur, beautifulsoup4, wikipedia modules. I attempted to manually install via pip install nsfw but it states the package does not exist. Help?", "title": "Help with reddit bot (ImportError: No module named nsfw)", "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "tusing", "created_utc": 1452645596, "gilded": 0, "name": "t3_40pehq", "num_comments": 10, "score": 2, "selftext": "[Relevant file on GitHub](https://github.com/tusing/reddit-ffn-bot/blob/master/ffn_bot/reddit/queues.py) I'm running a bot that parses comments for requests to fanfiction stories and replies to the comments with a fully-formatted description of the fanfiction. Unfortunately, our latest update is having a few errors when people call the refresh function for our bot (which will make it delete its old comment and reply again). Basically, what happens is that people call the refresh function, when the bot parses it, that error gets thrown. After the reply is made, the bot stops querying Reddit for new comments as it's supposed to do [in this line](https://github.com/tusing/reddit-ffn-bot/blob/master/ffn_bot/reddit/queues.py#L91). Here is the error ([GitHub Gist in case formatting broke](https://gist.github.com/tusing/834f8ddaa9ebae95ff3d)): ``` Exception in thread Thread-2: Traceback (most recent call last): File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner self.run() File \"/home/ubuntu/reddit-ffn-bot/ffn_bot/reddit/queues.py\", line 93, in run self._fetch(fetcher) File \"/home/ubuntu/reddit-ffn-bot/ffn_bot/reddit/queues.py\", line 79, in _fetch fetcher(self) File \"/home/ubuntu/reddit-ffn-bot/ffn_bot/reddit/queues.py\", line 156, in _run queue.add(*func(limit=self.limit, params=params)) File \"\", line 2, in _sorted File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 247, in wrap assert not obj._use_oauth # pylint: disable=W0212 AssertionError ``` The code was updated by a contributor, so I'm not 100% sure what is going on here, which means I'm not sure how to deal with the error.", "title": "Bot freezing due to Oauth-related AssertionError", "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "Joshjayk", "created_utc": 1452553024, "gilded": 0, "name": "t3_40jfcf", "num_comments": 5, "score": 0, "selftext": "Hello! I need some help with matching a specified word in the helpers.comment_stream. Here is a snippet of what I have thus far: word = \"random\" comments = praw.helpers.comment_stream(wordbot, 'all') for comment in comments: if findWholeWord(word)(str(comment)) is not None: print(\"Word found: %s\" % str(comment)) For findWholeWord(word), I have a simple regex search: def findWholeWord(w): return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search Basically, it searches for a word in the comments, and if it finds a comment that uses that word, it'll tell me. The problem is, it doesn't seem to be working. In many tests I've tried on random subreddits, I can't get it to properly work. If I change the 'all' to a specific subreddit it'll work, but I'd like for it to work for every subreddit. Thanks!", "title": "[PRAW] Matching a keyword in comment stream.", "url": "https://www.reddit.com/r/redditdev/comments/40jfcf/praw_matching_a_keyword_in_comment_stream/"}, {"author": "kamac496", "created_utc": 1452544211, "gilded": 0, "name": "t3_40ipg4", "num_comments": 1, "score": 2, "selftext": "Hey. So far I've seen that I cannot get more than 1000 comments when using praw.objects.MoreComments, so I'd like to read the latest comments *in a submission* instead. The only problem I have is that I don't know how, because I can't seem to find that in the documentation. Anyone?", "title": "[PRAW] Get recent comments from a submission", "url": "https://www.reddit.com/r/redditdev/comments/40ipg4/praw_get_recent_comments_from_a_submission/"}, {"author": "SandyRegolith", "created_utc": 1452119393, "gilded": 0, "name": "t3_3zs8un", "num_comments": 2, "score": 1, "selftext": "What's the URL I should construct to fetch posts between timestamp A and timestamp B? Not using PRAW, just regular HTTP requests.", "title": "[not PRAW] What's the query to get all posts between two timestamps?", "url": "https://www.reddit.com/r/redditdev/comments/3zs8un/not_praw_whats_the_query_to_get_all_posts_between/"}, {"author": "Almenon", "created_utc": 1452115389, "gilded": 0, "name": "t3_3zrwso", "num_comments": 12, "score": 1, "selftext": "\\_\\_getattr__ in objects.py appears to call itself repeatedly. My code: http://pastebin.com/BudzyX3T Where the error happens: https://github.com/praw-dev/praw/blob/master/praw/objects.py#L81 This is probably a bug in PRAW but I want to make sure I'm not doing something wrong on my end first. EDIT: The error happens when I pass in a subreddit with a question mark at the end of the name.", "title": "Recursion error when using praw to iterate through subreddits. help?", "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "NotCalledBill", "created_utc": 1452114564, "gilded": 0, "name": "t3_3zrucp", "num_comments": 2, "score": 1, "selftext": "**The Context** Hey redditdev, so I've got some working praw code and now want to add in exception handling to gracefully handle when reddit is down or when a request goes wrong. My problem is that after reading up on the lazy objects documentation I'm still pretty lost as to when my code is actually making requests. I've searched around with google for similar questions but couldn't find any so I'm asking here in the hopes that somebody else can point me in the right direction. Here is the relevant function, it's simply scraping data from the first 100 hot posts in a subreddit: if name: sub = r.get_subreddit(name) else: sub = r.get_random_subreddit() name = sub.display_name submissions = list(sub.get_hot(limit=100)) database.cache([x.title for x in submissions], name, \"r/title\") for submission in submissions: if submission.is_self: database.cache(submission.selftext.split(\"\\n\\n\"), name, \"r/self\") if submission.num_comments >= 20: comments = submission.comments # Don't flatten tree to cut down on amount of overhead database.cache([x.body for x in comments if getattr(x, \"body\", None)], name, \"r/comment\") database.cache([x.author.name for x in comments if getattr(x, \"author\", None)], name, \"r/user\") **** **The Questions** So initially I had a play around with the `.has_fetched` attribute to see if I could figure out when the request was made to get information on the `sub` object. I figured it would be true after the line where I'm fetching the hot posts because surely data has to be retrieved there, but that was an incorrect assumption (presumably it just prepares an empty generator or something along those lines). My thinking now is that it must be when I'm iterating through the posts, however I'm unsure how that works exactly, it should only make a single request right? I'm also unsure if further requests are being made to get information on the comments, or if that is all included in the earlier requests (my thinking is that since I'm not expanding the `MoreComments` objects there aren't any further requests being made after the posts are retrieved). If anybody could clarify these things (or correct me) that'd be great!", "title": "[PRAW] Having trouble figuring out when requests are made", "url": "https://www.reddit.com/r/redditdev/comments/3zrucp/praw_having_trouble_figuring_out_when_requests/"}, {"author": "Developx", "created_utc": 1451928098, "gilded": 0, "name": "t3_3zfnsf", "num_comments": 15, "score": 7, "selftext": "Hi everyone. I'm new to Reddit's API and PRAW. This problem came up in a more complex script I'm writing for private subreddits, but I was able to simplify it down to these calls. I am using Python 3.4.4. I have tried PRAW 3.3.0 and the newest GitHub version. The account I'm using is a moderator of . It has granted all OAuth scopes to the script. I have authenticated manually with `r.refresh_access_information()` ([after following this tutorial](https://www.reddit.com/r/GoldTesting/comments/3cm1p8/how_to_make_your_bot_use_oauth2/) by /u/GoldenSights) and with OAuth2Util but it doesn't change anything. So I don't think it's a problem with my authentication. All normal PRAW methods work as expected (`r.get_subreddit()`, etc.). `request()` and `request_json()` seem like edge cases. ---- `mybot.py` is a setup script, using OAuth2Util for simplicity. # mybot.py import praw import OAuth2Util def login(): r = praw.Reddit(, log_requests=2) o = OAuth2Util.OAuth2Util(r) # o.refresh(force=True) # Makes no difference to this problem return r ---- **1:** Here are some examples from the Python Interpreter (with `log_requests=2`) to show the problem: >>> import mybot >>> r=mybot.login() # Login works fine substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.request(\"https://www.reddit.com\") # Works fine GET: https://www.reddit.com status: 200 >>> r.request(\"https://www.reddit.com/r/AskReddit\") # Public subreddit, works fine GET: https://www.reddit.com/r/AskReddit status: 200 >>> r.request(\"https://www.reddit.com/r/\") # Private subreddit moderated by account, does not work, returns 403 GET: https://www.reddit.com/r/ status: 403 Traceback (most recent call last): File \"\", line 1, in File \"\", line 2, in request File \"/python3.4/site-packages/praw/decorators.py\", line 116, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/python3.4/site-packages/praw/__init__.py\", line 599, in request retry_on_error=retry_on_error, method=method) File \"/python3.4/site-packages/praw/__init__.py\", line 451, in _request _raise_response_exceptions(response) File \"/python3.4/site-packages/praw/internal.py\", line 208, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden: HTTP error >>> r.request(\"https://www.reddit.com/r/\") # Exact same request, this time it works fine substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 >>> r.request(\"https://www.reddit.com/r/\") # Exact same request, still works fine substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 >>> r=mybot.login() # Authenticate again substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.request(\"https://www.reddit.com/r/\") # Exact same request after authenticating again, returns 403 GET: https://www.reddit.com/r/ status: 403 Traceback (most recent call last): File \"\", line 1, in File \"\", line 2, in request File \"/python3.4/site-packages/praw/decorators.py\", line 116, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/python3.4/site-packages/praw/__init__.py\", line 599, in request retry_on_error=retry_on_error, method=method) File \"/python3.4/site-packages/praw/__init__.py\", line 451, in _request _raise_response_exceptions(response) File \"/python3.4/site-packages/praw/internal.py\", line 208, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden: HTTP error >>> r.request(\"https://www.reddit.com/r/\") # Exact same request, working fine again substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 >>> r.request(\"https://www.reddit.com/r/\") # Exact same request, still working fine substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 ---- **2:** Here is another Python Interpreter session showing the same problem (I think), without needing to use a private subreddit: >>> import mybot >>> r=mybot.login() substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.request(\"https://oauth.reddit.com\") GET: https://oauth.reddit.com status: 403 Traceback (most recent call last): File \"\", line 1, in File \"\", line 2, in request File \"/python3.4/site-packages/praw/decorators.py\", line 116, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/python3.4/site-packages/praw/__init__.py\", line 599, in request retry_on_error=retry_on_error, method=method) File \"/python3.4/site-packages/praw/__init__.py\", line 451, in _request _raise_response_exceptions(response) File \"/python3.4/site-packages/praw/internal.py\", line 208, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden: HTTP error >>> r.request(\"https://oauth.reddit.com\") GET: https://oauth.reddit.com status: 200 >>> r.request(\"https://oauth.reddit.com\") GET: https://oauth.reddit.com status: 200 ---- **3:** The first 403 does not need to be on the exact same URL as the subsequent requests. Just the first request needing OAuth of the authenticated session: >>> import mybot >>> r=mybot.login() substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.request(\"https://oauth.reddit.com\") # First request needing OAuth returns 403 GET: https://oauth.reddit.com status: 403 Traceback (most recent call last): File \"\", line 1, in File \"\", line 2, in request File \"/python3.4/site-packages/praw/decorators.py\", line 116, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/python3.4/site-packages/praw/__init__.py\", line 599, in request retry_on_error=retry_on_error, method=method) File \"/python3.4/site-packages/praw/__init__.py\", line 451, in _request _raise_response_exceptions(response) File \"/python3.4/site-packages/praw/internal.py\", line 208, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden: HTTP error >>> r.request(\"https://www.reddit.com/r/\") # Second request needing OAuth works fine, even though it's a different URL substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 ---- Substitute `request_json()` for `request()` in these examples and it has the same problem. ---- I could use a try/except like this: try: r.request(\"https://www.reddit.com/r/\") # Will always raise 403 except: r.request(\"https://www.reddit.com/r/\") # Will work fine But I don't want to cause a lot of API errors, and I would rather fix the root of the problem. From the above examples, it seems like the problem is: The **first** `request()` or `request_json()` call **that needs to use OAuth** returns a 403 error, all other calls are fine. [PRAW issue #497 on GitHub](https://github.com/praw-dev/praw/issues/497) could possibly be related. Would really appreciate any help with this. Thanks. Edit: Added example 3.", "title": "403 Forbidden on first PRAW request() or request_json() call that needs to use OAuth. All subsequent calls work fine.", "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "askLubich", "created_utc": 1451744618, "gilded": 0, "name": "t3_3z57b4", "num_comments": 9, "score": 2, "selftext": "My bot calls the 'get_sticky' function on a subreddit and it has been working flawlessly for a couple of weeks now. However, recently it crashed with the following message: File \"C:\\Users\\...\\site-packages\\praw\\internal.py\", line 184, in _raise_redirect_exceptions if 'reddits/search' in new_url: # Handle non-existent subreddit UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 78: ordinal not in range(128) It appears that praw encouters some encoding error trying to parse the actual error message. At the time of the crash, a link and not a self-post was sticky. I did a couple of tests and it appears that the 'get_sticky' function works fine as long as a self-post is sticked. However, it crashes if a link is sticked. My questions are: * Is this a known bug? Can someone reproduce it? * Is there any known workaround? Any other way to get the sticked post? Update: As it turned out, unlike my first thought, the problem was likely not caused by a sticked link, but rather by an umlaut (\u00e4\u00f6\u00fc) in the sticked post's title.", "title": "[PRAW] Using 'get_sticky', if a link is sticked causes crash", "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "sufficiency", "created_utc": 1451704531, "gilded": 0, "name": "t3_3z3ff3", "num_comments": 5, "score": 9, "selftext": "Just out of curiosity since Google has seemingly failed me. I am thinking running two different Reddit bots (both via PRAW, in case it matters) from the same machine - they are completely different bots and will be using different accounts. Do these bots have different caps on the number of requests, or do they have to share the bandwidth?", "title": "Two bots, one IP?", "url": "https://www.reddit.com/r/redditdev/comments/3z3ff3/two_bots_one_ip/"}, {"author": "RemindMeBotWrangler", "created_utc": 1451692213, "gilded": 0, "name": "t3_3z2pgs", "num_comments": 4, "score": 3, "selftext": "Anytime there are special characters in the send message parameters httpException is raised. TEXT: \"Hol\u00e0!\" Traceback (most recent call last): File \"\", line 2, in send_message File \"/usr/local/lib/python2.7/site-packages/praw/decorators.py\", line 268, in wrap return function(*args, **kwargs) File \"\", line 2, in send_message File \"/usr/local/lib/python2.7/site-packages/praw/decorators.py\", line 174, in require_captcha return function(*args, **kwargs) File \"/usr/local/lib/python2.7/site-packages/praw/__init__.py\", line 2492, in send_message retry_on_error=False) File \"\", line 2, in request_json File \"/usr/local/lib/python2.7/site-packages/praw/decorators.py\", line 113, in raise_api_exception s return_value = function(*args, **kwargs) File \"/usr/local/lib/python2.7/site-packages/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/site-packages/praw/__init__.py\", line 445, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python2.7/site-packages/praw/internal.py\", line 212, in _raise_response_excep tions raise HTTPException(_raw=exc.response) Is there something specifically I have to do with the text before it's sent to make sure it stays intact? The text is coming from mysql saved by doing `.encode('utf-8')` The HTTPException is a 500 response.", "title": "PRAW send_message special characters", "url": "https://www.reddit.com/r/redditdev/comments/3z2pgs/praw_send_message_special_characters/"}, {"author": "Albuyeh", "created_utc": 1451594475, "gilded": 0, "name": "t3_3yy8ai", "num_comments": 5, "score": 2, "selftext": "I have a Reddit Bot made in PRAW that runs every 5m. A typical output is: Searching SUBREDDIT at 2015-12-31 10:25:13 Adding 3yxq7y to DB Adding 3yxq9p to DB ... But recently I have sometimes been getting the following error: > sys:1: ResourceWarning: unclosed I added a try/except block for my Reddit login and had it print exceptions and sometime the exception is shown as: > Error EOF occurred in violation of protocol (_ssl.c:598) What could my issue be? I think it is related to SSL? Is there a better way to debug my code to see exactly what is unclosed?", "title": "Error in PRAW: ResourceWarning: unclosed", "url": "https://www.reddit.com/r/redditdev/comments/3yy8ai/error_in_praw_resourcewarning_unclosed/"}, {"author": "BearlyBreathing", "created_utc": 1451514251, "gilded": 0, "name": "t3_3yu6nj", "num_comments": 6, "score": 6, "selftext": "I'm trying to write a script that pulls down lots of reddit data (submissions, comments, and user info; basically everything), and I want to do this for entire sub-reddits for their entire history so obviously speed is an important factor. PRAW wants to break things up into multiple API requests because it assumes you don't want that much info, but is there a way to force it to pull more data at once? For example, I can get PRAW to pull all the comments from a submission like so: sub = r.get_submission(some_url) cms = praw.helpers.flatten_tree(s.comments) But if I want to know the author of those posts I have to do something like: users = [i.author.id for i in cms] Suddenly a handful of API calls (depending on the number of comments) becomes dozens, hundreds, or even thousands as PRAW fetches the authors one-by-one. At 1-2 seconds per-call this is obviously way too slow for my purposes. Is there a way to force PRAW to fetch all related data in a more efficient way? Alternatively, searching this sub for the answer has lead me to a few posts recommending r.request_json as a means for pulling a lot more data at once (1000 lines) and then parsing that instead of working within PRAW's object model. However, the documentation on r.request_json appears scant, and I'm not sure how to get it to do anything useful. It appears to simply be spitting out JSON formatted PRAW objects, which is not at all what I need. Ex: reddit.request_json('https://www.reddit.com/r/movies/comments/3ysj7z/leonardo_di_caprio_reveals_he_turned_down_the/') [{'data': {'after': None, 'before': None, 'children': [], 'modhash': ''}, 'kind': 'Listing'}, {'data': {'after': None, 'before': None, 'children': [, , , , , , , , , , , , , , , , , ], 'modhash': ''}, 'kind': 'Listing'}] Can anyone give me a hint or direct me to a good tutorial on using request_json or similar tools? Might it be easier to drop PRAW entirely and just parse /.json pages directly?", "title": "[PRAW] Need to speed up big data collection. PRAW, r.request_json, or something else?", "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "cutety", "created_utc": 1451175217, "gilded": 0, "name": "t3_3ycb4r", "num_comments": 2, "score": 1, "selftext": "Is there a way to get praw to not write the HTTP requests it makes to the log? Or at least to a different log? I'm not a pro when it comes to python's logging module, so I hope this isn't some stupid noob question. My logging basicConfig: logging.basicConfig(filename='postlimit.log', level=logging.DEBUG, format='%(asctime)s %(message)s')", "title": "[PRAW] Stop praw from writing requests to .log", "url": "https://www.reddit.com/r/redditdev/comments/3ycb4r/praw_stop_praw_from_writing_requests_to_log/"}, {"author": "Squid__", "created_utc": 1450940215, "gilded": 0, "name": "t3_3y24fg", "num_comments": 7, "score": 5, "selftext": "I used PRAW to get all the usernames from a secret santa thread and a couple users messaged me saying they didn't show up on the list. The only thing I know is that the two users who asked me about it when clicking 'context' on the comment they made on that thread it says \"There doesn't seem to be anything here\". I just want to know if this is an issue with my script or if this is a Reddit issue. See for yourself: https://www.reddit.com/user/avisbaseball https://www.reddit.com/user/_FinanciallyFucked_ The 'HHH Secret Santa Mixtape Thread' is the one in question. Here's the code I use to grab usernames from every parent level comment. import praw def get_usernames(thread_id): user_agent = '/r/HipHopHeads Secret Santa Mixtape Messages 2k15 by /u/Squid__' r = praw.Reddit(user_agent=user_agent) r.login(disable_warning=True) print('Script is running, depending on the number of comments this could take awhile') submission = r.get_submission(submission_id=thread_id) # replaces \"MoreComments\" with all comments while type(submission.comments[-1]) == praw.objects.MoreComments: submission.replace_more_comments(limit=None, threshold=0) # adds the usernames to a set to avoid repeats usernames = set() for comment in submission.comments: usernames.add(comment.author.name) return list(usernames)", "title": "Issue with getting all usernames from a thread?", "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "washerdreier", "created_utc": 1450666993, "gilded": 0, "name": "t3_3xnl2o", "num_comments": 6, "score": 2, "selftext": "I'm trying to fetch a user's comments or submissions (or if logged in their saves, likes, etc) and have no issue with fetching all or a limit from the most current, but I can't figure out how to get a subset before/after/between timestamps or before/after a placeholder post. I'm thinking of pretty much the same method [clockstalker](https://github.com/ClockStalker/clockstalker/blob/master/responder.py#L182) uses but that was back when the module was reddit verses praw. Is there still a way to get a slice of submissions/comments/etc or is that no longer supported? I've seen some comments about using search to get around the 1000 post limit, but that is usually for a whole subreddit and not focusing on a single user's content. If this isn't supported, should I be querying for the data in another way?", "title": "[PRAW] How to get submissions, comments, saves, etc with a placeholder or date filter?", "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "godlikesme", "created_utc": 1450481652, "gilded": 0, "name": "t3_3xem6j", "num_comments": 2, "score": 7, "selftext": "There is this hack that allows you to download all submissions from a given subreddit [by doing multiple search requests using cloudsearch syntax with timestamp:XXXXXXXXXX..YYYYYYYYYY query. I used this hack/feature multiple times, so after my fifth time using it I made [a pull request to PRAW](https://github.com/praw-dev/praw/pull/554). It took a few weeks and 57 comments to merge it, but now it is available in the master branch. Which means that if you'd like to try it out, you can simply do: # pip install git+https://github.com/praw-dev/praw.git and then do: from praw.helpers import submissions_between r = praw.Reddit(\"\") for s in submissions_between(r, 'redditdev'): print s # or your own great code The helper additionally takes lowest_timestamp and highest_timestamp for filtering using, well, timestamps. Also, there is extra_cloudsearch_fields which gives you a way to filter by parameters like \"self\", \"author\", \"title\", etc: [full list of (mostly working) fields](https://www.reddit.com/wiki/search) I figured I could make this announcement because why not and because many people use PRAW and maybe they'll find this helper useful(also because I like bragging about my work). PS: If any reddit developers are reading this, please go and fix several bugs I found while developing this helper: [bug1](https://www.reddit.com/r/bugs/comments/3x9hgd/nsfw1_includes_nonnsfw_results_at_least_when/), [bug2](https://www.reddit.com/r/bugs/comments/3vkzts/cloudsearch_with_timestamp_for_rmod_returns_503/), [bug3](https://www.reddit.com/r/bugs/comments/3uxc09/api_sorting_by_new_is_broken/) (at least you could've changed the flairs to \"confirmed\")", "title": "PSA: I've just added a PRAW helper that allows you to easily download all submissions from a subreddit(or all submissions between two timestamps)", "url": "https://www.reddit.com/r/redditdev/comments/3xem6j/psa_ive_just_added_a_praw_helper_that_allows_you/"}, {"author": "TriggerHappyMoron", "created_utc": 1450273195, "gilded": 0, "name": "t3_3x2sy7", "num_comments": 3, "score": 3, "selftext": "import praw import time import datetime userA = \"WarfRPScanner Vr0.1 /u/TriggerHappyBot\" userN = \"TriggerHappyBot\" passW = \"[password]\" thread = \"test\" upVoteThread = \"freekarma\" r = praw.Reddit(user_agent = userA) def get_date(sub): time = sub.created return datetime.datetime.fromtimestamp(time) def dsh(stringToSearch, stringToFind): if(str(stringToSearch).find(stringToFind)!=-1): return True else: return False def concatListRed(lisT): tempString = \"\" for item in lisT: tempString = tempString + \" /u/\" + item return tempString while True: print(\"--------------------\") print(\"Checking /\"+thread+\"/ for RP subreddits\") r.login(userN,passW,disable_warning=True) subreddit = None subreddit = r.get_subreddit(thread) subredditHot = subreddit.get_hot(limit=5) for submission in subredditHot: if(not dsh(submission, \"[IC]\")): continue redditors = [] print(submission) print(\"Submitted: \" + str(get_date(submission))) submission.replace_more_comments(limit=None, threshold=0) flatComments = praw.helpers.flatten_tree(submission.comments) print(\"Comments: \" + str(len(flatComments))) lateTime = -1 lateComment = None for comment in flatComments: if(comment.author == None): continue if(redditors.count(str(comment.author)) == 0): redditors.append(str(comment.author)) if(lateTime == -1 or comment.created > lateTime): lateTime = comment.created lateComment = comment if(lateComment == None): print(\"Empty thread\") else: print(\"Latest Comment by \" + str(lateComment.author).lower()) print(lateComment.body) redditors = [\"TriggerHappyMoron\", \"TriggerHappyBot\"] if(not dsh(lateComment.body, \"[PING]\")): print(\"Last comment was not a ping, pinging\") lateComment.reply(\"[PING]\" + concatListRed(redditors)) time.sleep(2) print(\"--------------------\") print(\"upvoting posts in /\"+upVoteThread+\"/\") subreddit = r.get_subreddit(upVoteThread) for sub in subreddit.get_hot(limit=5): if(sub.author != userN): print(sub) sub.upvote() time.sleep(5)", "title": "Keep getting RateLimitExceeded when my bot comments twice, using praw", "url": "https://www.reddit.com/r/redditdev/comments/3x2sy7/keep_getting_ratelimitexceeded_when_my_bot/"}, {"author": "sunbolts", "created_utc": 1450058421, "gilded": 0, "name": "t3_3wq68t", "num_comments": 2, "score": 2, "selftext": "[The documentation](http://praw.readthedocs.org/en/stable/pages/code_overview.html) lists the methods and classes, but doesn't list any of the attributes, and it would be very helpful to know these and what information they contain. Are there any plans to include the attributes of each class in the docs? Thanks.", "title": "Documentation doesn't list attributes of Redditor, Submission, etc. classes", "url": "https://www.reddit.com/r/redditdev/comments/3wq68t/documentation_doesnt_list_attributes_of_redditor/"}, {"author": "souldeux", "created_utc": 1450050243, "gilded": 0, "name": "t3_3wpn6e", "num_comments": 3, "score": 18, "selftext": "I recently built a bot in response to [this post](https://www.reddit.com/r/RequestABot/comments/3we9u1/a_bot_that_creates_a_topic_and_increases_a/) in /u/requestabot and documented the experience in my blog. I thought maybe someone here might find the tutorial useful. If anyone's interested, here's a quick writeup (one of many, I know) that takes you through building a Reddit bot with Python: http://souldeux.com/blog/build-reddit-bot-with-praw/", "title": "Quick \"build-a-bot\" tutorial", "url": "https://www.reddit.com/r/redditdev/comments/3wpn6e/quick_buildabot_tutorial/"}, {"author": "BlackFayah", "created_utc": 1449743550, "gilded": 0, "name": "t3_3w7h97", "num_comments": 4, "score": 1, "selftext": "I am running Ubuntu 14.04 LTS and I'm trying to use PRAW in Python 3. Unfortunately, it keeps giving me this known error: Traceback (most recent call last): File \"timerbot.py\", line 2, in import praw ImportError: No module named 'praw' However, when I changed the default Python version to Python 3, it worked, but then other applications started crashing. How can I solve this problem? ***** **Edit:** I solved it by doing the following: sudo -rm /usr/local/bin/python3 sudo ln -s /usr/bin/python3 /usr/local/bin/python3", "title": "[PRAW] Can't get it to work with Python 3", "url": "https://www.reddit.com/r/redditdev/comments/3w7h97/praw_cant_get_it_to_work_with_python_3/"}, {"author": "chizdippler", "created_utc": 1449726474, "gilded": 0, "name": "t3_3w6n7z", "num_comments": 0, "score": 2, "selftext": "I'm having trouble figuring out what's causing this problem, but at this point I'm 99% sure it's a conflict with OAuth2 and `content_md`. I'm also using prawoauth2, but since I've tested my code using the basic `login(user, pass)` function, there's no way it's causing a problem here. Here's my code with irrelevant parts removed: import praw from prawoauth2 import PrawOAuth2Mini reddit = praw.Reddit(USER_AGENT) oauth = PrawOAuth2Mini(reddit, app_key=CLIENT_ID, app_secret=SECRET, access_token=ACCESS_TOKEN, refresh_token=REFRESH_TOKEN, scopes=SCOPES) oauth.refresh() content = reddit.get_subreddit(SUBREDDIT).get_wiki_page('sidebar_edit').content_md And the console output: File \"test.py\", line 10 content = r.get_subreddit(SUBREDDIT).get_wiki_page('sidebar_edit').content_md File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\praw\\objects.py\", line 81, in __getattr__ self._has_fetched = self._populate(None, True) File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\praw\\objects.py\", line 157, in _populate json_dict = self._get_json_dict() if fetch else {} File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\praw\\objects.py\", line 150, in _get_json_dict self._info_url, params=params, as_objects=False) File \"\", line 2, in request_json File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\praw\\decorators.py\", line 123, in raise_api_exceptions raise exc praw.errors.HTTPException: HTTP error I'm trying to read the wiki and then update the sidebar so my scope is `['readwiki', 'modconfig']`. My bot is a moderator and the subreddit and its wiki are public. I double checked that the sidebar_edit wiki page exists and has content in it. I have also double checked my tokens, secret, and app key. I've done some searching and apparently there's a similar problem to this but specific to private subreddits, which is not the case here. What could be the problem?", "title": "[PRAW] content_md causes HTTPException when using OAuth2", "url": "https://www.reddit.com/r/redditdev/comments/3w6n7z/praw_content_md_causes_httpexception_when_using/"}, {"author": "Dashing_in_the_90s", "created_utc": 1449450487, "gilded": 0, "name": "t3_3vqb21", "num_comments": 6, "score": 4, "selftext": "Reddit gold has a feature that lets me sort my saved posts by subreddit. Since my main list is over 1000 posts long the only way to access the older ones is to sort by subreddit. I am trying to make a script with PRAW that will backup my saved posts. Is it possible to return the list of saved posts from a specific subreddit rather than from the main list? edit: Problem Solved Thank you /u/GoldenSights for finding the solution: s=r.user.get_saved(params={'sr':'Askreddit'})", "title": "Using PRAW can I get my saved posts from a specific subreddit rather than from the main list?", "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "reseph", "created_utc": 1449368155, "gilded": 0, "name": "t3_3vm3q6", "num_comments": 24, "score": 5, "selftext": "Does anyone use OAuth2Util? It's giving me this error: Traceback (most recent call last): File \"update_sidebar.py\", line 6, in o = OAuth2Util.OAuth2Util(r) File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 162, in __init__ self.refresh() File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 364, in refresh self._get_new_access_information() File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 254, in _get_new_access_information self._start_webserver(url) File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 229, in _start_webserver self.server = OAuth2UtilServer(server_address, OAuth2UtilRequestHandler, authorize_url) File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 58, in __init__ super().__init__(server_adress, handler_class, bind_and_activate) TypeError: super() takes at least 1 argument (0 given) Code: import praw import OAuth2Util user_agent = \"/r/ffxiv sidebar helper by /u/reseph\" r = praw.Reddit(user_agent=user_agent) o = OAuth2Util.OAuth2Util(r) Config file is already set up.", "title": "[OAuth2Util] TypeError: super() takes at least 1 argument (0 given)", "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "lumbdi", "created_utc": 1449072493, "gilded": 0, "name": "t3_3v5muu", "num_comments": 11, "score": 5, "selftext": "I'm using PRAW library. E.g. we have this link: [https://www.reddit.com/r/**DotA2**/comments/**3sx4su**/**put_camera_yaw_into_option_menu_allow_us_to**/cx13thd](https://www.reddit.com/r/DotA2/comments/3sx4su/put_camera_yaw_into_option_menu_allow_us_to/cx13thd) I do: SUBREDDIT = \"AnalyzeLast100Games+Dota2+LearnDota2\" subreddit = r.get_subreddit(SUBREDDIT) posts = subreddit.get_comments(limit=100) and my bot finds that comment. (Let's assume the post was recently made.) I have the post id of that comment via: for post in posts: pid = post.id How do I get the \"thread id\"? And how do I get the thread title? How do I get the subreddit name (if relevant)? edit: #solution: thread title and subreddit name is irrelevant. \"thread id\" or link id is retrieved via: post.link_id constructing the link: https://www.reddit.com//comments/// [Some text](/comments///)", "title": "How to generate a link to the comment?", "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "helpmewebbit", "created_utc": 1449006006, "gilded": 0, "name": "t3_3v1yuv", "num_comments": 7, "score": 3, "selftext": "Setting `reddit.config.store_json_result = True` and writing a standalone comment from a flattened tree to a json file works fine. But whenever the comment has replies, I get this error: TypeError: is not JSON serializable I know this happens because the `replies` field cannot be serialised and it would be redundant to serialise it since I'm using a flattened tree anyway. But I'm at a loss to figure out a consistent way to handle this type of error. Can praw be set to simply ignore the replies field (which I don't need)? Or do I have to create an entirely new `dict` object sans \"replies\", which feels cumbersome?", "title": "How to serialize praw.objects.Comment in a consistent way?", "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "theonefoster", "created_utc": 1448568185, "gilded": 0, "name": "t3_3udusd", "num_comments": 6, "score": 7, "selftext": "I paused execution to [check the attributes of a submission object](https://imgur.com/8anwxE2), but nowhere was the full URL provided. If you go to the [submission in question](https://www.reddit.com/r/food/comments/3udre2/honey_roasted/), you'll see that the link has a \".jpg\" on the end, whereas in my object watch window it just has the imgur link. There are no other attributes containing the full link. [Here's the API link to that submission](https://api.reddit.com/r/food/comments/3udre2/honey_roasted/). You can see that there's no extension under \"url\", and a page search for \"jpg\" returns no results other than thumbnail links. How can I get the full link in praw? I need to get the full link, then check whether or not it ends in \".jpg\", \".png\" etc to see whether the submission is a direct image link. --- edit: [this submission seems to have the extension..](https://api.reddit.com/r/WTF/comments/3uchq3/who_needs_a_boat_when_you_got_a_car/) | [as does this one](https://api.reddit.com/r/WTF/comments/3ua3gy/a_rare_condition_called_congenital_arthrogryposis/). What's going on then? Edit2: for future people with the same problem, RES was the issue as per [this comment](https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/cxe19nc). Disabling RES displays the correct link in the browser.", "title": "How do I get the FULL submission URL from a praw submission object? submission.url seems to chop the file extension off imgur links", "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "cogsbox", "created_utc": 1448528472, "gilded": 0, "name": "t3_3ubstm", "num_comments": 5, "score": 2, "selftext": "I have read a few of the posts that have been written about deleted users, but I can't seem to handle the errors well. In my code I look for users in the title of a post then return their subreddits. Once I hit a deleted user my code crashes. This is the error: praw.errors.NotFound: HTTP error Thanks for your help!", "title": "Deleted Users", "url": "https://www.reddit.com/r/redditdev/comments/3ubstm/deleted_users/"}, {"author": "thetoethumb", "created_utc": 1448505248, "gilded": 0, "name": "t3_3uapp8", "num_comments": 4, "score": 2, "selftext": "Hey all, I'm having a first crack at interfacing with reddit using Python but can't seem to get even a basic example to work. I followed the example from the [github page](https://github.com/praw-dev/praw) pretty much exactly but no dice. Anyone have any advice? --- **MWE** import praw r = praw.Reddit(user_agent=\"/u/thetoethumb test\") submissions = r.get_subreddit('askreddit').get_hot(limit=10) print [str(x) for x in submissions] **Result/error** (full traceback in comment below) TypeError: data must be a byte string **Versions** I'm using Windows 8 64bit if that's any use, but: >>> praw.__version__ 3.3.0 >>> requests.__version__ 2.5.0 >>> sys.version '2.7.9 (default, Dec 10 2014, 12:24:55) [MSC v.1500 32 bit (Intel)]' --- I'm thinking maybe an installation error when I installed PRAW or maybe incompatibility with Python 2.7.9? I installed PRAW with ``easy_install praw`` using CMD and it didn't seem to throw any errors (still have the CMD window open if that's any use). Anyone have any advice? Thanks in advance!", "title": "PRAW error: \"data must be a byte string\"", "url": "https://www.reddit.com/r/redditdev/comments/3uapp8/praw_error_data_must_be_a_byte_string/"}, {"author": "dClauzel", "created_utc": 1448054725, "gilded": 0, "name": "t3_3tm6r6", "num_comments": 1, "score": 3, "selftext": "The previous script has been working for several months, but it stops some days ago (error 500). No idea why. Other operations (banning a user, getting the karma from a user, etc) are fine. Does someone have any clue? `test-Europe-banlist.py` #!/usr/bin/env python3 # -*- coding: utf-8 -*- import praw import OAuth2Util import datetime print(\"Connexion\u2026\", end=\" \") r = praw.Reddit(user_agent=\"posix:test-Europe-banlist:v0 (by /u/dClauzel)\", site_name=\"Reddit\") o = OAuth2Util.OAuth2Util(r, print_log=True) o.refresh() print(\"connect\u00e9.\") # liaison sur le sousjlailu de travail; ici /r/Europe sousjlailu = r.get_subreddit(\"Europe\") bannis = [i for i in sousjlailu.get_banned(limit=None,user_only=False)] print( \"Il y a {0} bannis.\".format(len(bannis)) ) for leBanni in bannis: bNom = leBanni['name'] bId = leBanni['id'] bDate = datetime.datetime.fromtimestamp(leBanni['date']) bNote = leBanni['note'] print(\"/u/{0} ({1}) : le {2}; {3}\".format( bNom, bId, bDate, bNote) ) print(\"fin Liste des bannis\") `./test-Europe-banlist.py` Connexion\u2026 connect\u00e9. Traceback (most recent call last): File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 210, in _raise_response_exceptions response.raise_for_status() # These should all be directly mapped File \"/usr/local/lib/python3.4/dist-packages/requests/models.py\", line 837, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://oauth.reddit.com/r/Europe/about/banned/.json?limit=1024&after=rb_cyrenm During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"./test-Europe-banlist.py\", line 19, in bannis = [i for i in sousjlailu.get_banned(limit=None,user_only=False)] File \"./test-Europe-banlist.py\", line 19, in bannis = [i for i in sousjlailu.get_banned(limit=None,user_only=False)] File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 1900, in _get_userlist for data in content: File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 557, in get_content page_data = self.request_json(url, params=params) File \"\", line 2, in request_json File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 445, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 212, in _raise_response_exceptions raise HTTPException(_raw=exc.response) praw.errors.HTTPException: HTTP error sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty", "title": "[praw] A script accessing the banlist is not working anymore (error 500)", "url": "https://www.reddit.com/r/redditdev/comments/3tm6r6/praw_a_script_accessing_the_banlist_is_not/"}, {"author": "AllHailTheCATS", "created_utc": 1448054491, "gilded": 0, "name": "t3_3tm67g", "num_comments": 8, "score": 0, "selftext": "Im trying to make a bot that replys to me every time I comment, right now its not doing what its supposed to, if I remove the not from the if statement I might get a reply but I also get a traceback error, if I keep it there nothing happens which makes sense as it checks if the id is in the used ids db. But if I make a new comment nothing happens still? Can someone tell me where im going wrong in my code? import praw import BotAccess import sqlite3 USERAGENT = \"A account that will reply under certain users comments \" USERNAME = BotAccess.username PASS = BotAccess.password ACCOUNT = BotAccess.account SUBREDDIT = \"test\" MAXPOSTS = 100 print(\"Setting up database..\") sql = sqlite3.connect(\"sql.db\") cur = sql.cursor() cur.execute('CREATE TABLE IF NOT EXISTS oldposts(ID TEXT)') sql.commit() print(\"Logging into reddit\") r = praw.Reddit(USERAGENT) r.login(USERNAME, PASS, disable_warning=True) def HypeManBot(): subreddit = r.get_subreddit(SUBREDDIT) comments = subreddit.get_comments(limit=MAXPOSTS) for comment in comments: cur.execute('SELECT * FROM oldposts WHERE ID=?', [comment.id]) if not cur.fetchone: try: myComment = comment.author.name if myComment == ACCOUNT: print(\"Replying..\") comment.reply(\"Yeaaaah!!\") except AttributeError: pass cur.execute('INSERT INTO oldposts VALUES(?)', [comment.id]) sql.commit() while True: HypeManBot()", "title": "My bot wont reply when its ment to.", "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "avinassh", "created_utc": 1448049203, "gilded": 0, "name": "t3_3tlt8o", "num_comments": 6, "score": 7, "selftext": "TLDR; Check the demo! - https://kekday.herokuapp.com --- Reddit gives all the user info in a handy JSON at this URL: `https://www.reddit.com/user//about.json` example: [https://www.reddit.com/user/spez/about.json](https://www.reddit.com/user/spez/about.json) The `created_utc` field in `data` is the date of user's registration aka Cake Day in [unix epoch](https://en.wikipedia.org/wiki/Unix_time) format (in UTC) and we can easily convert that to readable format: >>> import time >>> time.strftime(\"%D\", time.gmtime(1118030400)) '06/06/05' Using [Python Requests](http://python-requests.org), we can turn this into a handy function: import time import requests def get_my_cake_day(username): url = \"https://www.reddit.com/user/{}/about.json\".format(username) r = requests.get(url) created_at = r.json()['data']['created_utc'] return time.strftime(\"%D\", time.gmtime(created_at)) Though above function will work, but soon it will start throwing HTTP 429 error i.e Too Many Requests. Thing is, Reddit doesn't really like when someone tries to fetch the data like this. The requests are made directly on Reddit servers without using the API. Now if you have want to find cake day of hundreds of users, you cannot use this method. Solution? Use [Reddit's API](https://www.reddit.com/dev/api). In Python, we will use [praw](https://github.com/praw-dev/praw) and [prawoauth2](https://github.com/avinassh/prawoauth2). praw is a Python wrapper for Reddit's API and prawoauth2 helps dealing with [OAuth2](https://github.com/reddit/reddit/wiki/OAuth2). Let's start by installing praw: pip install praw Now we can convert the `get_my_cake_day` to praw version and get the user details like this: import time import praw reddit_client = praw.Reddit(user_agent='my amazing cake day bot') def get_my_cake_day(username): redditor = reddit_client.get_redditor(username) return time.strftime(\"%D\", time.gmtime(redditor.created_utc)) Above code pretty much self explanatory. What if the user doesn't exist or shadowbanned? In such cases, praw throws an exception: `praw.errors.NotFound`. Lets modify `get_my_cake_day` to catch this: def get_my_cake_day(username): try: redditor = reddit_client.get_redditor(username) return time.strftime(\"%D\", time.gmtime(redditor.created_utc)) except praw.errors.NotFound: return 'User does not exist or shadowbanned' This is better compared to earlier version and we will stop getting rate limit errors often. Also, praw will handle such cases and makes requests again to fetch the data. But what if we want to increase the limit? The above requests are not authenticated, meaning Reddit does not recognise your app. However, if we register this app in Reddit and let Reddit know, then requests limits will increase. So to authenticate our app over Oauth2, we will use prawoauth2. Lets install it first: pip install prawoauth2 Follow the simple steps [here](https://prawoauth2.readthedocs.org/usage_guide.html) to register your app on Reddit. Once done, you will get `app_token` and `app_secret`. Then you need to get `access_token` and `refresh_token`. You could use this handy [`onetime.py`](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/onetime.py) script. For detailed instructions check the documentation of [prawoauth2](https://prawoauth2.readthedocs.org). You should never make `app_token`, `app_secret`, `access_token` and `refresh_token` public and never commit them to version control. Keep them always secret. Here is the complete script using prawoauth2: import time import praw from secret import (app_key, app_secret, access_token, refresh_token, user_agent, scopes) reddit_client = praw.Reddit(user_agent='my amazing cakeday bot') oauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key, app_secret=app_secret, access_token=access_token, refresh_token=refresh_token, scopes=scopes) def get_my_cake_day(username): try: redditor = reddit_client.get_redditor(username) return time.strftime(\"%D\", time.gmtime(redditor.created_utc)) except praw.errors.NotFound: return 'User does not exists or shadowbanned' Again, pretty much self explanatory. If your tokens are correct and once `PrawOAuth2Mini` is initialized properly, there will be no issues with the app and you will have twice as many requests as compared to unauthenticated version. Want to see above app in action? Check this - [kekday](https://kekday.herokuapp.com). The [app is open source](https://github.com/avinassh/kekday) and released under MIT License.", "title": "[Tutorial] Find a Redditor's cake day using praw and prawoauth2 over OAuth", "url": "https://www.reddit.com/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/"}, {"author": "avinassh", "created_utc": 1447997338, "gilded": 0, "name": "t3_3tj7qw", "num_comments": 2, "score": 3, "selftext": "I need `created` and/or `created_utc` of praw Redditor object: user = reddit_client.get_redditor('avinassh') print(user.created) However it seems that praw is making two requests. Is it possible to fetch this in a single request? Thanks! EDIT: After /u/13steinj's comment I checked again, looks like I was wrong.", "title": "[PRAW] How to get `user` info a single request?", "url": "https://www.reddit.com/r/redditdev/comments/3tj7qw/praw_how_to_get_user_info_a_single_request/"}, {"author": "lumbdi", "created_utc": 1447943753, "gilded": 0, "name": "t3_3tfqea", "num_comments": 11, "score": 2, "selftext": "I'm new to Python PRAW and I'm sure there are a few other bots that already have this functionality. I need some example codes. I recently learned programming a bit in Python and how to use Steam API and wrote a small Python script that [displays some information](http://i.imgur.com/C4fQGXW.png). Now I want it to post to Reddit. I'm already having problems with auth.", "title": "[PRAW] Need example code for bot logging in and replying to a comment when the bot is summoned via /u/botname", "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "ffranglais", "created_utc": 1447907009, "gilded": 0, "name": "t3_3te2i6", "num_comments": 4, "score": 4, "selftext": "Shreddit: https://github.com/x89/Shreddit No I'm not a Voater, I hate those freeze peach FPH scum. Here's what I did: I was trying to get OAuth2 authorization, so I created a personal use script with the callback URL https://127.0.0.1:65010 as instructed. In Shreddit, I edited praw.ini and cross referenced the oauth_client_id, oauth_client_secret and oauth_client_uri with the script, also as instructed, and was asked to authorize said script to access my user information. So far, so good, right? Well, when I clicked accept, I got a 500 internal server error. **Why is this happening?** This error has been making my hair fall out for days, I can't figure out what the problem is. I don't know much about computer networking. I've heard that the Reddit API is finicky with HTTPS, maybe that's the problem.", "title": "I'm getting a 500 internal server error when I try to run Shreddit, and I don't know why.", "url": "https://www.reddit.com/r/redditdev/comments/3te2i6/im_getting_a_500_internal_server_error_when_i_try/"}, {"author": "alexleavitt", "created_utc": 1447737422, "gilded": 0, "name": "t3_3t4cax", "num_comments": 1, "score": 7, "selftext": "According to the API, you can query for \"related\" posts given a submission_id. https://www.reddit.com/dev/api#GET_related_{article} But as far as I could tell, you can't do this in PRAW... or am I incorrect?", "title": "Can you query PRAW for 'related' posts?", "url": "https://www.reddit.com/r/redditdev/comments/3t4cax/can_you_query_praw_for_related_posts/"}, {"author": "tobiasvl", "created_utc": 1447696234, "gilded": 0, "name": "t3_3t1mum", "num_comments": 3, "score": 1, "selftext": "Hey, I get this error when authing with Praw: https://gist.github.com/tobiasvl/71ded8d6940b357c9235 Looks like it might be an issue deeper down in my Python installation than Praw, but if you have an inkling and can nudge me in the right direction, I'd be grateful! Edit: Fixed it myself, posted solution in comment", "title": "Error when logging in with Praw", "url": "https://www.reddit.com/r/redditdev/comments/3t1mum/error_when_logging_in_with_praw/"}, {"author": "Frenchiie", "created_utc": 1447564295, "gilded": 0, "name": "t3_3sv72o", "num_comments": 6, "score": 6, "selftext": "when i do the authorization part, do i need to specify a scope if i dont actually need any permission granting user information? In non oauth mode i'm currently fetching people's publicly view-able comments with PRAW: > reddit_user = r.get_redditor(user) > for comment in reddit_user.get_comments(limit=100): My reason for switching to oauth is that 2 seconds per request is just too slow. I dont actually need anything that requires oauth except for the benefit of being able to request every second. Also for Step 4 of the [PRAW(OAuth) tutorial](http://praw.readthedocs.org/en/stable/pages/oauth.html) how does reddit know that the \"code\" from the url belongs to 1 specific user? What stops a program from re-using the code to automatically get authorization from any user?", "title": "OAuth(with PRAW) if i don't actually need permission granting information?", "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "IAPark", "created_utc": 1447352818, "gilded": 0, "name": "t3_3sk8sy", "num_comments": 12, "score": 9, "selftext": "I'm of course aware of PRAW, but it would be much more useful to me to have something using something like the template method pattern to provide more high level behavior. For example having listeners for events like: * Submission Added * Comment Added * Comment Edited * Keyword Mentioned Basically, is there something that takes the grunt work out of making a Reddit Bot? If not would there be interest from others to use such a framework if I were to develop it as part of my current project. I'd probably be coding in Python just because it has the best libraries for what I need, but what could other bot makers use?", "title": "Is There an Existing Framework for Reddit Bots?", "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "CVance1", "created_utc": 1447341810, "gilded": 0, "name": "t3_3sjioe", "num_comments": 8, "score": 2, "selftext": "Hello, I'm a bit new to the reddit API. Is there a way in PRAW to get the date something was submitted and return it, so you could potentially see when something was last posted?", "title": "[PRAW] Searching posts in a subreddit by timestamp", "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "Albuyeh", "created_utc": 1446962341, "gilded": 0, "name": "t3_3rz98x", "num_comments": 7, "score": 3, "selftext": "I am using PRAW for Python to create a bot that will detect if a users post was caught by spam filter. What is the best way to detect this and not if a post was removed by a mod.", "title": "Notify User if Post Caught By Spam Filter", "url": "https://www.reddit.com/r/redditdev/comments/3rz98x/notify_user_if_post_caught_by_spam_filter/"}, {"author": "VRCkid", "created_utc": 1446834572, "gilded": 0, "name": "t3_3rsk3k", "num_comments": 0, "score": 6, "selftext": "I've been trying to make a bot that finds a certain domain in the submission or the body of the comment doing something with that information. What I have tried is going for submissions is submissions = r.get_subreddit('all').get_new(limit=None) #iterate through and submissions = praw.helpers.submission_stream(r, \"all\", limit=10) #iterate through I don't know which one these methods would work best for getting all submissions. I've tried looking for a solution and looking at other bots and I haven't found a good answer yet. Does anyone have an idea?", "title": "[PRAW] What's the best way of checking all Submissions and all Comments?", "url": "https://www.reddit.com/r/redditdev/comments/3rsk3k/praw_whats_the_best_way_of_checking_all/"}, {"author": "hullcrush", "created_utc": 1446410643, "gilded": 0, "name": "t3_3r4fv9", "num_comments": 6, "score": 4, "selftext": "There's at least 20 posts on how to do this, yet I'm unable to complete this basic task. Several days of research later, I'm still stuck. import praw r = praw.Reddit('Comment parser example by u/_Daimon_') subreddit = r.get_subreddit(\"python\") comments = subreddit.get_comments() The subreddit is https://www.reddit.com/r/ifyoulikeblank/ I want the title of the thread and the comments within. That's it. Or get_comments from posts with only 10 comments or above would be preferred. So then what happens? Python does this... >> I'm missing something fundamental here, I apologize, as I'd like the result dumped somewhere, even if it's in hieroglyphics. I've tried print and a convoluted dump to a .txt file but I still get >> Thanks in advance, this highlights my limited code understanding.", "title": "Using PRAW to scrape first page comments of a subreddit", "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "SecretAg3nt", "created_utc": 1446159525, "gilded": 0, "name": "t3_3qroun", "num_comments": 3, "score": 1, "selftext": "So I've been playing around with praw the last two days and decided to write a bot that would message my account when ever someone mentioned a given keyword in their comments. It's worked pretty well so far, except I keep getting this type of error after a seeming random amount of time: TypeError: getresponse() got an unexpected keyword argument 'buffering' Which then stops the bot and I have to start it again. Here is the full code I have written: import praw from time import ctime from time import sleep import obot from time import time commDone = set() NUMCOMMENTS = 0 SETPHRASES = [The keywords I'm interested in] COMMLIMIT = 100 r = obot.login() print(\"Logged in\") for comment in praw.helpers.comment_stream(r, 'all', limit=COMMLIMIT): if comment.id not in commDone: cbody = comment.body.lower() cwords = cbody.split() commDone.add(comment.id) for word in cwords: for key in SETPHRASES: if word == key: NUMCOMMENTS = NUMCOMMENTS + 1 msg = str(NUMCOMMENTS) + \") *\" + ctime() + \"* Someone is talking about \\\"\" + key + \"\\\": \" + comment.author.name + \" \" + comment.permalink print(msg) subject = \"Someone is talking about \\\"\" + key r.send_message('SecretAg3nt', subject, msg) print(\"Message Sent\") and here is the full error I am getting: Traceback (most recent call last): File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 376, in _make_request httplib_response = conn.getresponse(buffering=True) TypeError: getresponse() got an unexpected keyword argument 'buffering' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 378, in _make_request httplib_response = conn.getresponse() File \"/usr/lib/python3.4/http/client.py\", line 1147, in getresponse response.begin() File \"/usr/lib/python3.4/http/client.py\", line 351, in begin version, status, reason = self._read_status() File \"/usr/lib/python3.4/http/client.py\", line 313, in _read_status line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\") File \"/usr/lib/python3.4/socket.py\", line 371, in readinto return self._sock.recv_into(b) File \"/usr/lib/python3.4/ssl.py\", line 746, in recv_into return self.read(nbytes, buffer) File \"/usr/lib/python3.4/ssl.py\", line 618, in read v = self._sslobj.read(len, buffer) socket.timeout: The read operation timed out During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/adapters.py\", line 370, in send timeout=timeout File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 609, in urlopen _stacktrace=sys.exc_info()[2]) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/util/retry.py\", line 245, in increment raise six.reraise(type(error), error, _stacktrace) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/packages/six.py\", line 310, in reraise raise value File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 380, in _make_request self._raise_timeout(err=e, url=url, timeout_value=read_timeout) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 308, in _raise_timeout raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value) requests.packages.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='oauth.reddit.com', port=443): Read timed out. (read timeout=45.0) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"redditbotcommentstream.py\", line 15, in for comment in praw.helpers.comment_stream(r, 'all', limit=COMMLIMIT): File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/helpers.py\", line 138, in _stream_generator for i, item in gen: File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/__init__.py\", line 557, in get_content page_data = self.request_json(url, params=params) File \"\", line 2, in request_json File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/__init__.py\", line 444, in _request response = handle_redirect() File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/sessions.py\", line 576, in send r = adapter.send(request, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/adapters.py\", line 435, in send raise ReadTimeout(e, request=request) requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='oauth.reddit.com', port=443): Read timed out. (read timeout=45.0) sys:1: ResourceWarning: unclosed /home/nitrous/code/redditbot/lib/python3.4/importlib/_bootstrap.py:2127: ImportWarning: sys.meta_path is empty", "title": "Problem keeping praw with OAuth2 running", "url": "https://www.reddit.com/r/redditdev/comments/3qroun/problem_keeping_praw_with_oauth2_running/"}, {"author": "StormZero1", "created_utc": 1446071753, "gilded": 0, "name": "t3_3qmign", "num_comments": 6, "score": 2, "selftext": "import praw user_agent = \"rketchupsoup\" r= praw.Reddit(user_agent = user_agent) submissions = r.get_subreddit('worldnews').get_hot(limit=5) print submissions this always prints \"\" but has never seemed to do that before, have I left something out? (edit) managed to get it to be a bit less broken. Using this code: def scrape(): subreddit = r.get_subreddit('4chan') for submission in subreddit.get_hot(limit=10): print submission I've managed to get it to work, well at least in shows in the python interpreter. However, the purpose of this script is to email me the top x posts on a subreddit, and when it arrives in my email inbox, it simply prints as \"none\" rather than showing the posts, even though it has done this in the interpreter. No idea what could be causing this.", "title": "[PRAW]Instead of displaying posts, my script is printing \"<generator object get_content at 0x02F44F80>\"", "url": "https://www.reddit.com/r/redditdev/comments/3qmign/prawinstead_of_displaying_posts_my_script_is/"}, {"author": "theonefoster", "created_utc": 1445801626, "gilded": 0, "name": "t3_3q6mhj", "num_comments": 4, "score": 2, "selftext": "subreddit = r.get_subreddit('all', fetch=True) comments = subreddit.get_comments(limit=100) That's the code I'm using. The first line is throwing a HTTP error , and I've no idea why. If I replace the first line with **either** of the following, it works fine: subreddit = r.get_subreddit('test', fetch=True) subreddit = r.get_subreddit('all') The trouble is that PRAW caches the comments for 30 seconds, so when I query the new comments every 5 seconds I just get the same list back, even if there are new comments. My bot then recognises the comment IDs and refuses to process them (intended). So I can only access the /r/all comments every 30 seconds, but I can access any individual subreddit's as often as I like. Does anyone know how to fix the HTTP error? I think what it's doing is passing the fetch=True parameter to Reddit instead of cutting it off internally. The full traceback is below, although I don't know how useful it is. Traceback (most recent call last): File \"D:\\Stuff\\Visual Studio\\SlashBotFinal\\SlashBot.py\", line 69, in checkComments() File \"D:\\Stuff\\Visual Studio\\SlashBotFinal\\SlashBot.py\", line 28, in checkComments subreddit = r.get_subreddit('all', fetch=True) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 1078, in get_subreddit return objects.Subreddit(self, subreddit_name, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 1528, in __init__ info_url, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 71, in __init__ self._has_fetched = self._populate(json_dict, fetch) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 157, in _populate json_dict = self._get_json_dict() if fetch else {} File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 150, in _get_json_dict self._info_url, params=params, as_objects=False) File \"\", line 2, in request_json File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 445, in _request_raise_response_exceptions(response) File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 207, in _raise_response_exceptions raise NotFound(_raw=response) praw.errors.NotFound: HTTP error", "title": "I'm getting a HTTP Error on \"r.get_subreddit('all', Fetch=True)\" but I'm sure the code is correct. Can anyone help?", "url": "https://www.reddit.com/r/redditdev/comments/3q6mhj/im_getting_a_http_error_on_rget_subredditall/"}, {"author": "habnpam", "created_utc": 1445738518, "gilded": 0, "name": "t3_3q3q5i", "num_comments": 3, "score": 3, "selftext": "Sort of similar to how this in PRAW: `subreddit.get_top_from_day(limit=10)` I'm able to create a Subreddit object, but I don't know how to fetch submissions(like above) from it. --- These are links to the subreddit and redditclient objects. [subreddit.java](https://github.com/thatJavaNerd/JRAW/blob/master/src/main/java/net/dean/jraw/models/Subreddit.java) [redditclient.java](https://github.com/thatJavaNerd/JRAW/blob/master/src/main/java/net/dean/jraw/RedditClient.java) From looking at the files, it doesn't seem possible atm? --- edit. found solution. An example would be this: RedditClient r = new RedditClient(new UserAgent(\"...\")); SubredditPaginator sp = new SubredditPaginator(r); sp.setLimit(100); sp.setSorting(Sorting.TOP); sp.setTimePeriod(TimePeriod.DAY); sp.setSubreddit(\"pics\"); sp.next(true); Listing list = sp.getCurrentListing(); System.out.println(list.get(0).getAuthor());", "title": "[JRAW] Get top submissions from subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/3q3q5i/jraw_get_top_submissions_from_subreddit/"}, {"author": "EndeRDIT", "created_utc": 1445546725, "gilded": 0, "name": "t3_3ptjiy", "num_comments": 5, "score": 2, "selftext": "I'm currently working on a project where I'm setting up multiple submission_streams to monitor subreddits. Each stream is running in its own thread, but sharing a reddit_session, which is authenticated using OAuth. My code right now works fine with a single thread. However, whenever I try to up the thread count >= 2, PRAW throws an AssertionError in the decorators.py file [here](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L247). Each thread uses the following as its target: def submission_loop(self, subreddit): stream = praw.helpers.submission_stream(self.reddit_session, subreddit, limit = 15) try: for submission in stream: self.logger.debug('{}: SUBMISSION: {}'.format(subreddit, submission.title)) self.act_submission(submission) except Exception as e: self.logger.error('something went wrong') exit(-1) Am I missing something here with regards to PRAW? Can anyone point me in the right direction? *EDIT: Do I need to have a single reddit_session per thread in addition to the MultiprocessHandler (already in use)?*", "title": "[PRAW] submission_stream AssertionError", "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "Naurgul", "created_utc": 1445033595, "gilded": 0, "name": "t3_3p1nu1", "num_comments": 2, "score": 7, "selftext": "I just tried the following with PRAW: for s in r.get_subreddit(\"all\").get_top_from_day(limit=None): words = s.title.split() if getDates(words): print s.score, s.title and I get way less results if I change it to limit=1000 Did anyone else notice anything similar?", "title": "Was the 1000 limit removed?", "url": "https://www.reddit.com/r/redditdev/comments/3p1nu1/was_the_1000_limit_removed/"}, {"author": "DanielGibbs", "created_utc": 1444556893, "gilded": 0, "name": "t3_3obe5g", "num_comments": 2, "score": 2, "selftext": "I have a script which runs weekly to create some weekly discussion threads and then update the sidebar and submission text with links to them, yet after upgrading to use PRAW 3.3.0 and OAuth, it doesn't seem to be able to update the sidebar anymore (although the submission text is updated). # Prior to this point, weekly_thread has been successfully submitted. sidebar_text = my_subreddit.description sidebar_text = re.sub(\"(?", "title": "Are there restrictions on changing a subreddit's sidebar?", "url": "https://www.reddit.com/r/redditdev/comments/3obe5g/are_there_restrictions_on_changing_a_subreddits/"}, {"author": "haiguise1", "created_utc": 1444515434, "gilded": 0, "name": "t3_3o9mt2", "num_comments": 8, "score": 2, "selftext": "I have been getting the following error when trying to authenticate reddit. I then tried to get new access details but this gave the same error. The error started when I updated praw to 3.3.0 with pip. I tried to downgrade back to 3.2.1 but the error persists. I'm using python 2.7. Now, whenever I try to do something which gets data from reddit the error occurs. The following code caused the error: import praw reddit = praw.Reddit(\"testing praw, by haiguise1\") a = reddit.get_subreddit(\"askscience\") b = a.get_top_from_day() c = [x for x in b] # Traceback (most recent call last): File \"test.py\", line 6, in c = [x for x in b] File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 557, in get_content page_data = self.request_json(url, params=params) File \"\", line 2, in request_json File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 444, in _request response = handle_redirect() File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 579, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 369, in send timeout=timeout File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\", line 353, in _make_request conn.request(method, url, **httplib_request_kw) File \"/usr/lib/python2.7/httplib.py\", line 1001, in request self._send_request(method, url, body, headers) File \"/usr/lib/python2.7/httplib.py\", line 1035, in _send_request self.endheaders(body) File \"/usr/lib/python2.7/httplib.py\", line 997, in endheaders self._send_output(message_body) File \"/usr/lib/python2.7/httplib.py\", line 850, in _send_output self.send(msg) File \"/usr/lib/python2.7/httplib.py\", line 826, in send self.sock.sendall(data) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 216, in sendall data = memoryview(data) TypeError: cannot make memory view because object does not have the buffer interface", "title": "Praw crashes when trying to do anything.", "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "phenomist", "created_utc": 1444457953, "gilded": 0, "name": "t3_3o6y4x", "num_comments": 7, "score": 1, "selftext": "I'm writing a bot that fetches comments on a ~20 second delay using PRAW. However, upon looking at the timestamps, it appears that it sometimes misses comments until about 2 minutes later. Is this an inherent limitation with PRAW / the reddit API or am I doing something wrong?", "title": "Is there a delay in getting comments/posts?", "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "schmodd", "created_utc": 1444400423, "gilded": 0, "name": "t3_3o3qtc", "num_comments": 11, "score": 1, "selftext": "So I have something like this code below. Is there any better solution to filter the user submitted output for a specific subreddit? Does get_submitted() feature some subreddit filter option? I am using Python 3.5. ... users = ('somenames', 'evenmorenames') handler = MultiprocessHandler() r = praw.Reddit(user_agent='fubar', handler=handler) for user in users: current_user = r.get_redditor(user) submissions = current_user.get_submitted() for submission in submissions: if submission.subreddit.display_name != targetSubreddit: ...", "title": "how to get user submitted posts filtered by specific subreddit", "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "cogsbox", "created_utc": 1444349426, "gilded": 0, "name": "t3_3o1fu2", "num_comments": 4, "score": 1, "selftext": "I have been trying to get author names of posts in this sub, but all I get returned it 'Loanbot' for the author. Is their a way around this? Here is my code import praw user_agent = \"test authorName 1.0 by /u/user\" r = praw.Reddit(user_agent=user_agent) submission = r.get_submission( \"https://www.reddit.com/r/borrow/comments/3o05n0/late_ujgohmart87_200_interest_107/\" ) comment = submission.comments[0] author = comment.author print(author.item)", "title": "Return author name on r/borrow", "url": "https://www.reddit.com/r/redditdev/comments/3o1fu2/return_author_name_on_rborrow/"}, {"author": "TeroTheTerror", "created_utc": 1444347223, "gilded": 0, "name": "t3_3o1bae", "num_comments": 0, "score": 0, "selftext": "Using the helpers function of PRAW, specifically the `comment_stream` function to gather all comments posted in a sub. Basic code looks like: import praw from time import sleep r = praw.Reddit(userAgent) h = praw.helpers Bot is logged in via OAuth. for comment in h.comment_stream(r,subreddit): try: things occur here....(including a check for OAuth key refreshes every 45 min) sleep(10) except Exception as e: print e sleep(180) The error that keeps occurring is: ('Connection aborted.', error(10054, 'An existing connection was forcibly closed by the remote host')) At first I thought the sleep time between comments wasn't enough, but now it's still doing that while sleeping 10 seconds in between each new comment. What am I missing?", "title": "Connection keeps being forcibly closed with comment-scanning PRAW bot", "url": "https://www.reddit.com/r/redditdev/comments/3o1bae/connection_keeps_being_forcibly_closed_with/"}, {"author": "Fireislander", "created_utc": 1444277845, "gilded": 0, "name": "t3_3nxjnq", "num_comments": 5, "score": 3, "selftext": "Hi all, I am trying to create a self post and then edit the info in that post a few mins later all using praw. I am submitting the post using the following code: submission = r.submit(whichSub, myThreadTitle, text=threadText) A few mins later I need the script to update the text in this thread. My script was initially set up to take in a URL so extracting the URL from this would be the easiest solution for me, but if there is a more efficient way to update the text in a thread I would love to hear it. Thank you for all of your help in advance", "title": "Getting the URL of a post I just submitted", "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "rbevans", "created_utc": 1444245192, "gilded": 0, "name": "t3_3nvjri", "num_comments": 14, "score": 0, "selftext": "I'm new to bot writing and could use some help to understand why i'm getting the errors below. Traceback (most recent call last): File \"C:\\scripts\\RecipesTopTen.py\", line 55, in r.login(USERNAME, PASSWORD) File \"\", line 2, in login File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\decorators.py\", line 75, in wrap return function(*args, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 1418, in login self.request_json(self.config['login'], data=data) File \"\", line 2, in request_json File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 444, in _request response = handle_redirect() File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\requests\\sessions.py\", line 579, in send r = adapter.send(request, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\requests\\adapters.py\", line 369, in send timeout=timeout File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 353, in _make_request conn.request(method, url, **httplib_request_kw) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\http\\client.py\", line 1083, in request self._send_request(method, url, body, headers) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\http\\client.py\", line 1123, in _send_request self.putheader(hdr, value) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\http\\client.py\", line 1060, in putheader raise ValueError('Invalid header value %r' % (values[i],)) ValueError: Invalid header value b\"Weekly Top-Ten bot for /r/recipes. Makes a weekly sticky thread with the top ten posts for the week\\n PRAW/3.3.0 Python/3.5.0 b'Windows-10.0.10240'\"", "title": "I could use some help to understand why I'm getting the following errors when running my bot.", "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "sWeeX2", "created_utc": 1443439439, "gilded": 0, "name": "t3_3morip", "num_comments": 4, "score": 1, "selftext": "Hey guys, So I'm trying to scrape a list of subreddits for a project I'm working on. In my head the best way to do this was using the reddit.com/reddits.json because from what I can see with PRAW there's no clear way of getting a list of subreddits. My problem is that I keep running into an { \"error\": 429} which is given back when I'm making too many requests I believe. Has anyone come up with a good way of getting a list of subreddits already? Any help is greatly appreciated.", "title": "Trying to scrape a list of subreddits", "url": "https://www.reddit.com/r/redditdev/comments/3morip/trying_to_scrape_a_list_of_subreddits/"}, {"author": "13steinj", "created_utc": 1443396196, "gilded": 0, "name": "t3_3mmt0t", "num_comments": 5, "score": 3, "selftext": "I'm fixing a small issue regarding the `refresh()` method on PRAW that affects specially deleted and removed comments (these have to be removed / deleted with no replies to be \"special\"), but I don't know if DMCA'd comments are affected too, so I wanted to check. How would I DMCA a comment on a local install to test what I need to check?", "title": "How would I DMCA a comment on a local install?", "url": "https://www.reddit.com/r/redditdev/comments/3mmt0t/how_would_i_dmca_a_comment_on_a_local_install/"}, {"author": "boib", "created_utc": 1443128502, "gilded": 0, "name": "t3_3m8ya2", "num_comments": 2, "score": 3, "selftext": "Traceback (most recent call last): File \"\", line 3, in File \"/usr/local/lib/python3.4/dist-packages/praw/objects.py\", line 377, in unhide return self.hide(_unhide=True) File \"\", line 2, in hide File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 268, in wrap return function(*args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/objects.py\", line 369, in hide return self.reddit_session.hide(self.fullname, _unhide=_unhide) File \"\", line 2, in hide File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 247, in wrap assert not obj._use_oauth # pylint: disable=W0212 AssertionError", "title": "PRAW v3.2.1 crash on .unhide() using oauth2", "url": "https://www.reddit.com/r/redditdev/comments/3m8ya2/praw_v321_crash_on_unhide_using_oauth2/"}, {"author": "13steinj", "created_utc": 1443062240, "gilded": 0, "name": "t3_3m5fvi", "num_comments": 0, "score": 1, "selftext": "I've been working on a bit of a personal project, and I found adding moderators with specific permissions is needed. This is currently unavailable in PRAW, (I'd link the issue number but github sucks on mobile), so I was going to add it in myself in the process of getting to the 50% marker of completion. However, I've encountered a bit of a problem. When looking at [/api](/api), if you ctrl-f for \"permissions\", the various endpoints that allow the use of choice permissions don't have info as to what those permissions are supposed to be (string, list, etc). I'd love to play around, but I'm afraid of fucking something up.", "title": "What are valid values for \"permissions\" in the api?", "url": "https://www.reddit.com/r/redditdev/comments/3m5fvi/what_are_valid_values_for_permissions_in_the_api/"}, {"author": "pyskell", "created_utc": 1442847318, "gilded": 0, "name": "t3_3lt7vv", "num_comments": 7, "score": 2, "selftext": "So I'm writing a reddit bot that responds to username mentions with PRAW. I can do something like: r = praw.Reddit(\"example\") mentions = r.get_mentions() mentions = list(mentions) mention = mentions[0] mention.mark_as_read() However if I call r.get_mentions() again I will get that old mention back. I do plan to track mentions on my end, however, it seems odd (and wasteful) to get back all my old mentions whenever I make a new request. Is there a way in the reddit API to only get mentions marked as unread? I can't seem to find anything. Alternatively is there a way to stop retrieving mentions made before a certain date, or before a certain comment id?", "title": "Ignoring \"read\" mentions", "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "MaxwellSalmon", "created_utc": 1442773958, "gilded": 0, "name": "t3_3lpbsc", "num_comments": 3, "score": 1, "selftext": "I successfully installed pip on my raspberry pi, but I can't install PRAW. I read that I should write \"pip-3.2 install \" But I get this message: \"pip-3.2 command not found\" - any more ideas? Thank you!", "title": "Trying to install PRAW on Raspberry Pi", "url": "https://www.reddit.com/r/redditdev/comments/3lpbsc/trying_to_install_praw_on_raspberry_pi/"}, {"author": "avinassh", "created_utc": 1442766661, "gilded": 0, "name": "t3_3lotxj", "num_comments": 17, "score": 17, "selftext": "Hey folks, I am the author of `prawoauth2`, a library which makes writing Reddit bots/apps using OAuth2 super easy and simple. Lately I have been receiving private messages, seeking help in migration, so I thought I would write a tutorial and redirect them here next time. Most of the text below is copied from the documentation. - Link to [Github](https://github.com/avinassh/prawoauth2) - Link to [Documentation](http://prawoauth2.readthedocs.org/) --- TLDR version: Remove every references of `username`, `password` and `praw.login` in your code. [Register](https://www.reddit.com/prefs/apps/) your bot/app in Reddit. Create an instance of `PrawOAuth2Mini` with valid params. So you have to remove one line and add another in your main code. That's all ;) --- Installation: pip install prawoauth2 --- 1. Stop using `praw.login`. Your current code probably uses Reddit account (your's or your bot's) username and password with `praw.login` and you have to remove that. With OAuth2, there should be NO references to Reddit username and password in your code: reddit_client = praw.Reddit(user_agent=user_agent) # you gotta remove the following line reddit_client.login(reddit_username, reddit_password) 2. Figure out what all `scopes` you need. Scopes specify what all permissions your app (or bot script) needs from user's Reddit account(or your bot account), like read private messages, spend gold credits etc. You can read about different scopes on praw's [official documentation](https://praw.readthedocs.org/en/stable/pages/oauth.html#oauth-scopes). For example, if your bot replies to comments and also responds to private messages, then it will need atleast these scopes: scopes = ['identity', 'read', 'submit', 'privatemessages'] 3. You need to register your bot/app on Reddit. The praw documentation already has a nice overview about [how](https://praw.readthedocs.org/en/stable/pages/oauth.html#a-step-by-step-oauth-guide). Go [here](https://www.reddit.com/prefs/apps/) and here's what I recommend for a bot: [registering](http://i.imgur.com/qiBMIl1.png) Just make sure you are setting `redirect uri` to `http://127.0.0.1:65010/authorize_callback`. Rest doesn't matter. Once you have created the app, you will get `app_key` and `app_secret`: [tokens](http://i.imgur.com/qxEKyOe.png) 4. `prawoauth2` comes with two components, `PrawOAuth2Mini` and `PrawOAuth2Server`. `PrawOAuth2Server` authorizes your app/script with the Reddit account and gives you access token. This is one time only operation. Let's call this script as `onetime.py`. `PrawOAuth2Mini` uses these tokens for all next transactions with Reddit. Remember, for a bot, you only need a valid `refresh_token` so it can *refresh* the expired `access_token`. 5. So lets first build `onetime.py`. As name suggests, you need to run this script only once for the first time. You should run this script locally, on your computer since it requires browser access. Import the required modules and create a `praw` instance: import praw from prawoauth2 import PrawOAuth2Server user_agent = 'some string that uniquely identifies my bot' reddit_client = praw.Reddit(user_agent=user_agent) 6. Pass the `app_key` and `app_secret` of your app, along with the praw instance to the `PrawOAuth2Server` oauthserver = PrawOAuth2Server(reddit_client, app_key, app_secret, state=user_agent, scopes=scopes) 7. Now, you need to start the oauth client server, which runs internally. oauthserver.start() The moment you start it, it opens the default web browser. If you are not logged in, log in with your bot account credentials and authorize the script (i.e. clicking on `accept`). 8. Once it is successful, you can get the tokens by calling `get_access_codes`. tokens = oauthserver.get_access_codes() The `tokens` is a `dict` type: >>> tokens {'access_token': '2...U', 'scope': set(['identity', 'read', 'submit']), 'refresh_token': u'2...s'} 9. Now in your main script, create an instance of `PrawOAuth2Mini` with all the required parameters: reddit_client = praw.Reddit(user_agent=user_agent) oauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key, app_secret=app_secret, access_token=access_token, refresh_token=refresh_token, scopes=scopes) That's all! Now rest of your code would require no changes and it will work as usual. --- - Here's a working example of `onetime.py` - [link](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/onetime.py) - And here's an example of bot which uses PRAW OAuth2 - [link](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/bot.py) - Detailed user guide of `prawoauth2` - [link](http://prawoauth2.readthedocs.org/usage_guide.html) - When to use refresh - [link](http://prawoauth2.readthedocs.org/tips_and_more.html#when-to-use-refresh).", "title": "[Tutorial] How to migrate your existing bots from HTTP to OAuth2 (Python-PRAW)", "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "lapropriu", "created_utc": 1442752225, "gilded": 0, "name": "t3_3lo4gn", "num_comments": 2, "score": 1, "selftext": "I'd like to get all submissions in a certain subreddit over a period of a few months. This goes over the 1,000 limit, so I'm trying to limit the query using timestamps and iterate, as per the Reddit wiki ([cloudsearch syntax](https://www.reddit.com/wiki/search#wiki_cloudsearch_syntax)). But I can't seem to get this to work with PRAW. My code: r = praw.Reddit(user_agent) posts = r.search('',subreddit='europe',sort='new',limit=None,syntax='cloudsearch',params={'timestamp':'1420027200..1420070400'}) I'm expecting this to return submissions between 1420027200 (Wed, 31 Dec 2014 12:00:00 GMT) and 1420070400 (Thu, 01 Jan 2015 00:00:00 GMT), so a 12hr interval. But what I get is submissions between 1442041040 (Sat, 12 Sep 2015 06:57:20 GMT) and 1442777039 (Sun, 20 Sep 2015 19:23:59 GMT). I've also tried using get_new(), with similar results. r = praw.Reddit(user_agent) sub = r.get_subreddit('europe') posts = sub.get_new(limit=None,syntax='cloudsearch',params={'timestamp':'1420027200..1420070400'}) Super grateful for any help!", "title": "PRAW and timestamp searches", "url": "https://www.reddit.com/r/redditdev/comments/3lo4gn/praw_and_timestamp_searches/"}, {"author": "EchoLogic", "created_utc": 1442630676, "gilded": 0, "name": "t3_3ligwq", "num_comments": 21, "score": 7, "selftext": "I want to build the world's simplest application. When someone on my website pastes in a permalink to a comment, I want my site to retrieve that comment and return it. The person doing the pasting does not even need to log in. **I just want to retrieve a comment.** Yet, it seems like I'm being forced to jump through 1000 hoops of fire to get everything to work. Why is Reddit's API so complicated to use compared to Twitter, AWS, or any other API? Based off what I read, it seemed like I needed an API wrapper, so I went and downloaded https://github.com/jcleblanc/reddit-php-sdk. And so, the problems began. 1. Oh, my project uses composer. This API wrapper doesn't. I tried for hours to try and get class autoloading set up. It didn't work. I then had to fork it, reorganize the project, add PSR-0 support, and then add it to packagist. That took up all my time last night. 2. Cool, now I'm using an semi-abandoned API wrapper to try and interact with a poorly documented API. 3. Now I can't get OAuth2 to work correctly. Why do I need to even use OAuth2 for this?! 4. Spend some more hours wasting my time looking at other API wrappers, PRAW, Guzzle, OAuth-2 repositories. I've spend the last 2 days of my time doing this, with no results to show for it. The simplest task ever, and there's roadblocks everywhere. Sorry for the rant, but I feel like I'm doing something fundamentally wrong here. Is it really this difficult to retrieve a comment from Reddit? I feel like I'm not even using the correct architecture here, should I just be using a bot? How does that work and how does it differ from an \"app\"?", "title": "How does a reddit bot differ from an application? Are they the same thing? When do I need OAuth2?", "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "sWeeX2", "created_utc": 1442428151, "gilded": 0, "name": "t3_3l7as5", "num_comments": 7, "score": 3, "selftext": "Hey guys, So I'm working on a project right now that involves using PRAW. I'm quite new to it and I'm running into a bit of a brick wall. For my project I need to get lots of comments for lots of threads. When I get the comments off a thread however it'll never quite be a complete list, it'll always be off by a bit, say there might be 500 comments and I might get 480 for example. It is also really slow to retrieve those comments. If anyone could help me out suggesting the most optimal way to do this it would be greatly appreciated.", "title": "[PRAW] Downloading Comments From Threads", "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "Flewloon", "created_utc": 1442318011, "gilded": 0, "name": "t3_3l15u0", "num_comments": 3, "score": 4, "selftext": "I have a couple questions about PRAW and using it to post stuff to a subreddit I control. Is there a way to setup the settings so that my bot doesn't have to be approved to have its post shown? I set the bot up as a moderator to the subreddit but still have to approve its posts. Also is there a way to not have to do CAPTCHA when posting to an owned subreddit. I assume no on this, but felt it couldn't hurt to ask.", "title": "PRAW no CAPTCHA posting and approving posts on owned subreddit", "url": "https://www.reddit.com/r/redditdev/comments/3l15u0/praw_no_captcha_posting_and_approving_posts_on/"}, {"author": "-SuicideThrowaway-", "created_utc": 1442258745, "gilded": 0, "name": "t3_3ky4l4", "num_comments": 2, "score": 2, "selftext": "If I remember right I should be allowed 1 request per second, right? And I mean I shouldn't request a page too often and praw limits that but if I'm just getting my unread messages I don't think it should be a big deal, right? Is it ok to poll for unread messages every second?", "title": "How often can I request my inbox?", "url": "https://www.reddit.com/r/redditdev/comments/3ky4l4/how_often_can_i_request_my_inbox/"}, {"author": "atlusio", "created_utc": 1442254076, "gilded": 0, "name": "t3_3kxsbb", "num_comments": 5, "score": 1, "selftext": "Hey guys, I am making a simple script to get the content a certain Redditor has upvoted. import praw redd = praw.Reddit(user_agent='upvoted post counter') redd.login('account', 'pwd') upvoted_threads = redd.user.get_upvoted print(upvoted_threads) which gives me: bound method LoggedInRedditor.get_upvoted of Redditor(user_name='account') When I do get subreddits, however, I get: >>> new_threads = redd.get_subreddit('subreddit').get_new(limit=10) >>> print(new_threads) I can iterate through the new_threads, but I cannot iterate through the upvoted threads... clearly because you cannot iterate on a method like you can a generator object. Any help with what I may be doing wrong would be lovely.", "title": "PRAW get_upvoted returning a bound method rather than a get_content generator?", "url": "https://www.reddit.com/r/redditdev/comments/3kxsbb/praw_get_upvoted_returning_a_bound_method_rather/"}, {"author": "NotoriousHakk0r4chan", "created_utc": 1442189886, "gilded": 0, "name": "t3_3kuj2j", "num_comments": 7, "score": 1, "selftext": "I get this mess of errors: Traceback (most recent call last): File \"/home/zibot/BeetusBot/subscription.py\", line 2, in from beetusbot import bot File \"/home/zibot/BeetusBot/beetusbot/bot.py\", line 13, in reddit.login(config.USERNAME.encode('utf-8'), config.PASSWORD.encode('utf-8')) File \"\", line 2, in login File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/decorators.py\", line 75, in wrap return function(*args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/__init__.py\", line 1416, in login self.request_json(self.config['login'], data=data) File \"\", line 2, in request_json File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/__init__.py\", line 444, in _request response = handle_redirect() File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 569, in send r = adapter.send(request, **kwargs) File \"/usr/lib/python2.7/dist-packages/requests/adapters.py\", line 362, in send timeout=timeout File \"/usr/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 516, in urlopen body=body, headers=headers) File \"/usr/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 308, in _make_request conn.request(method, url, **httplib_request_kw) File \"/usr/lib/python2.7/httplib.py\", line 1001, in request self._send_request(method, url, body, headers) File \"/usr/lib/python2.7/httplib.py\", line 1035, in _send_request self.endheaders(body) File \"/usr/lib/python2.7/httplib.py\", line 997, in endheaders self._send_output(message_body) File \"/usr/lib/python2.7/httplib.py\", line 850, in _send_output self.send(msg) File \"/usr/lib/python2.7/httplib.py\", line 826, in send self.sock.sendall(data) File \"/usr/lib/python2.7/dist-packages/urllib3/contrib/pyopenssl.py\", line 208, in sendall return self.connection.sendall(data) File \"/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py\", line 969, in sendall raise TypeError(\"buf must be a byte string\") TypeError: buf must be a byte string -------------- Whats going on? Someone else had the problem [here](https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/), but never listed any kind of fix for it.", "title": "TypeError: buf must be a byte string in PRAW", "url": "https://www.reddit.com/r/redditdev/comments/3kuj2j/typeerror_buf_must_be_a_byte_string_in_praw/"}, {"author": "Frenchiie", "created_utc": 1442015110, "gilded": 0, "name": "t3_3klvrj", "num_comments": 3, "score": 2, "selftext": "I know you can get a list of a users comments but what about what subreddits all of those comments came from? Does the API let you do something like that? If it makes a difference I am looking at PRAW.", "title": "Can the API get a list of subreddits where a users posts come from?", "url": "https://www.reddit.com/r/redditdev/comments/3klvrj/can_the_api_get_a_list_of_subreddits_where_a/"}, {"author": "DarkMio", "created_utc": 1441856815, "gilded": 0, "name": "t3_3kci9s", "num_comments": 4, "score": 1, "selftext": "There is little to no documentation on the handlers, but how can I keep up the rate-limit with the DefaultHandler instead of praw-multiprocess when I run a multithreaded bot in Python3+? I can't seem to get that worked out, using a new Handler or giving all Reddit Sessions the same handler don't seem to limit the rate: Something like this: import praw handler = praw.handler.DefaultHandler() r_1 = Reddit('somethingsomething', handler=handler) r_2 = Reddit('somethingdifferent', handler=handler) def c_stream(): for comments in praw.helpers.comment_stream(r_1, 'all') #something cool pass def s_stream(): for submission in praw.helpers.comment_stream(r_2, 'all') # something more cool pass and this is set into two threads. Looking at the request-log (a patched DefaultHandler found here: https://github.com/DarkMio/RedditRover/blob/master/core/PRAWHandler.py#L12) reveals multiple requests per second - here's a snippet: 07:10:17 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=88&after=t3_3k7i20&limit=1000&before=t3_3k7i1x 07:10:23 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=89&limit=1000&before=t3_3k7i2b 07:10:23 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=6&after=t1_cuvdbzb&limit=1024&before=t1_cuvdbza 07:10:25 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=7&limit=1024&before=t1_cuvdc29 07:10:26 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=89&after=t3_3k7i2f&limit=1000&before=t3_3k7i2b 07:10:26 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=90&limit=1000&before=t3_3k7i2o 07:10:26 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=7&after=t1_cuvdc2a&limit=1024&before=t1_cuvdc29 07:10:28 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=8&limit=1024&before=t1_cuvdc53 07:10:29 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=90&after=t3_3k7i2q&limit=1000&before=t3_3k7i2o 07:10:29 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=8&after=t1_cuvdc54&limit=1024&before=t1_cuvdc53 07:10:32 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=91&limit=1000&before=t3_3k7i2w 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json", "title": "How does tze DefaultHandler work in praw?", "url": "https://www.reddit.com/r/redditdev/comments/3kci9s/how_does_tze_defaulthandler_work_in_praw/"}, {"author": "metaranha", "created_utc": 1441466485, "gilded": 0, "name": "t3_3jr2qc", "num_comments": 17, "score": 1, "selftext": "I'm a beginner with PRAW and i'm trying to write a simple script to pull the hot list in IAMA, but whenever I print the variable holding the results, I end up getting this output: Why do I keep getting this output? I can't seem to find an explanation I understand for why I'm getting this rather than the actual output. Here's my code: #!/usr/bin/python import praw import time r = praw.Reddit('AMA request compiler /u/metaranha') r.login() count = 0 while(count", "title": "printing search results: <generator object search at (mem location)>", "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "green_flash", "created_utc": 1441320750, "gilded": 0, "name": "t3_3jjqlq", "num_comments": 6, "score": 12, "selftext": "If you have a script based on PRAW and get an error like this when logging in: ERROR: Unexpected redirect from http://www.reddit.com/user//about/.json to https://www.reddit.com/user//about/.json that is likely because you're running a very old version of PRAW that doesn't support https. The switch seems to have been made today.", "title": "Reddit login is https only now it seems, be sure to use an up-to-date PRAW version", "url": "https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/"}, {"author": "codsane", "created_utc": 1440913405, "gilded": 0, "name": "t3_3ixgq2", "num_comments": 5, "score": 2, "selftext": "I was looking to use PRAW get all comments of a submission, but found that that was not feasible. for comment in praw.helpers.comment_stream(r, 'subreddit_containing_thread'): if comment.link_id == \"thread_i_want_to_monitor\": # Save comment The above code simply grabs a comment stream of the entire subreddit, but only saves the comments that are within the thread I would like to monitor. Is there any more efficient way to get this done, perhaps it is possible to get a comment stream of a single submission?", "title": "Comment stream of submission?", "url": "https://www.reddit.com/r/redditdev/comments/3ixgq2/comment_stream_of_submission/"}, {"author": "natos20", "created_utc": 1440887744, "gilded": 0, "name": "t3_3iw7ij", "num_comments": 2, "score": 2, "selftext": "Noob here. I'm learning how to write reddit bots in python, and I always get that error. If it's any more useful, here's the whole traceback: Traceback (most recent call last): File \"C:/Users/User/Desktop/myprogram.py\", line 4, in r = praw.reddit(user_agent = \"blahblahblah\") AttributeError: 'module' object has no attribute 'reddit' Thanks in advance for any help.", "title": "'module' object has no attribute 'reddit'?", "url": "https://www.reddit.com/r/redditdev/comments/3iw7ij/module_object_has_no_attribute_reddit/"}, {"author": "SeriousBug", "created_utc": 1440756150, "gilded": 0, "name": "t3_3ipjs6", "num_comments": 1, "score": 6, "selftext": "I have an [open-source application](https://github.com/SeriousBug/redditcurl) that depends on the Reddit API. There is one thing I don't understand about the switch to OAuth2, looking at the [documentation for PRAW](https://praw.readthedocs.org/en/v3.1.0/pages/oauth.html#step-2-setting-up-praw), I see that I need to keep `client_secret` as a secret. However, how can I do that when the application is open source? Am I supposed to ask all the users to register their application? Edit: Reading the [reddit's documentation](https://github.com/reddit/reddit/wiki/OAuth2) > Installed app: Runs on devices you don't control, such as the user's mobile phone. Cannot keep a secret, and therefore, does not receive one. The correct solution is to pick an installed app.", "title": "OAuth2 with open source apps", "url": "https://www.reddit.com/r/redditdev/comments/3ipjs6/oauth2_with_open_source_apps/"}, {"author": "boib", "created_utc": 1440555172, "gilded": 0, "name": "t3_3iey8r", "num_comments": 2, "score": 1, "selftext": "I don't see that in the API but it must be possible, right? /r/sub/about/flair gives that list, right? Is that possible in PRAW?", "title": "PRAW: How do I get a list of all the users in my that have set user flair?", "url": "https://www.reddit.com/r/redditdev/comments/3iey8r/praw_how_do_i_get_a_list_of_all_the_users_in_my/"}, {"author": "PoodleWorkout", "created_utc": 1440470294, "gilded": 0, "name": "t3_3ia8m4", "num_comments": 3, "score": 1, "selftext": "Hi all! I'm new to PRAW, and I'm trying to get a list of all saved links from a logged-in user. However, it seems that I'm only getting the most recent 25 links for my own account. I'm pasting a code snippet below, and any insight would be greatly appreciated! saved_links = reddit_user.get_saved(sort=\"new\", time='all') for value in saved_links: print (value.title + \" \" + value.short_link)", "title": "Getting ALL saved links?", "url": "https://www.reddit.com/r/redditdev/comments/3ia8m4/getting_all_saved_links/"}, {"author": "TomSparkLabs", "created_utc": 1440425991, "gilded": 0, "name": "t3_3i7dcc", "num_comments": 4, "score": 4, "selftext": "It seems very complex for a simple script, that only needs to access my account. However, if I use import praw r = praw.Reddit('just a test - /u/TomSparkLabs') r.login(username='TomSparkLabs', password='notreallymypass') I get this error thrown at me: reddit intends to disable password-based authentication of API clients sometime in the near future. As a result this method will be removed in a future major version of PRAW. For more information please see: * Original reddit deprecation notice: https://www.reddit.com/comments/2ujhkr/ * Updated delayed deprecation notice: https://www.reddit.com/comments/37e2mv/ Pass ``disable_warning=True`` to ``login`` to disable this warning. What do I do now? If I need to upgrade, how? The guide is very confusing.", "title": "How do I upgrade to OAuth2 authentication with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/3i7dcc/how_do_i_upgrade_to_oauth2_authentication_with/"}, {"author": "D0cR3d", "created_utc": 1440262326, "gilded": 0, "name": "t3_3hzg8r", "num_comments": 1, "score": 1, "selftext": "Is there a way to get the ID of a object in the search list from a praw search? So far all I'm seeing is the number of votes, and the title. Thanks.", "title": "Get Submission ID from the results of a praw search", "url": "https://www.reddit.com/r/redditdev/comments/3hzg8r/get_submission_id_from_the_results_of_a_praw/"}, {"author": "cartoonii", "created_utc": 1440180870, "gilded": 0, "name": "t3_3hvli0", "num_comments": 11, "score": 2, "selftext": "So, this is what happens when I go to my Python33\\Scripts file and type in \"pip install --allow-external praw-oauth2util\" and then I get this error: Could not find a version that satisfies the requirement install (from versions : ) Some externally hosted files were ignored as access to them may be unreliable (use --allow-external install to allow). No matching distribution found for install Any help? Solution: * Changed PATH System Variable to the version I was using..", "title": "Installing PRAW OAuth2Util", "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "D0cR3d", "created_utc": 1440001213, "gilded": 0, "name": "t3_3hlmtc", "num_comments": 7, "score": 2, "selftext": "For instance, if you visit [/r/mod/about/edited](https://www.reddit.com/r/mod/about/edited/) you see any comments or submissions edited. Does that exist in PRAW itself?", "title": "Is there a Get_Edited() function in PRAW to be able to get any things that have been edited?", "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "wlhlm", "created_utc": 1439980332, "gilded": 0, "name": "t3_3hkgou", "num_comments": 5, "score": 1, "selftext": "I'm working on a script that modifies the sidebar. For this purpose, PRAW has `set_settings()`, which allows to update the subreddit settings including the sidebar. Here's the [documentation](https://praw.readthedocs.org/en/v3.1.0/pages/code_overview.html#praw.__init__.ModConfigMixin.set_settings) for it: > **set_settings(**subreddit, **title**, **, \\*\\*kwargs**)** > Set the settings for the given subreddit. > **Parameters**: *subreddit* \u2013 Must be a subreddit object. > **Returns:** The json response from the server. > Requires the modconfig oauth scope oruser/password authentication as a mod of the subreddit. What struck me as odd is, that `title` is a mandatory argument. Why do I have to set it every time I change settings? It looks to me like any other subreddit setting and I don't think it can be used as identifier.", "title": "[PRAW] Why is 'title' a required argument for set_settings()?", "url": "https://www.reddit.com/r/redditdev/comments/3hkgou/praw_why_is_title_a_required_argument_for_set/"}, {"author": "Bromskloss", "created_utc": 1439947399, "gilded": 0, "name": "t3_3hiwod", "num_comments": 10, "score": 0, "selftext": "It was [declared](https://www.reddit.com/r/videos/comments/3hgons/how_oldschool_graphics_worked/cu7jise) that anyone who replies to a particular comment will get a notification when the next part of a certain interesting video becomes available. I'd like to automate this. So far, I can extract the list of users that are to receive the notification: #!/usr/bin/env python3 # coding=utf-8 import praw r = praw.Reddit(user_agent='generic:one-off-reminder-thing:v0 (by /u/Bromskloss)) c = r.get_submission(url='https://www.reddit.com/r/videos/comments/3hgons/how_oldschool_graphics_worked/cu7jise') c.replace_more_comments() authors = {ch.author.name for ch in c.comments[0]._replies} print(\"\\n/u/\".join(authors)) What remains is to send a message to each user. I count on getting stuck in a spam filter if I go ahead and just try to do it. What is the solution? How do other bots that sends many PMs do it? (This is the first time I'm doing anything with the API. Suggestions for improvements are welcome.)", "title": "How to send a message to a few hundred users without being stopped by a spam filter?", "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "solaceinsleep", "created_utc": 1439942261, "gilded": 0, "name": "t3_3hil7j", "num_comments": 4, "score": 2, "selftext": "Here is post on stackoverflow: [http://stackoverflow.com/questions/32083842/how-to-add-reddit-overload-support-to-my-script-that-is-using-the-praw-module](http://stackoverflow.com/questions/32083842/how-to-add-reddit-overload-support-to-my-script-that-is-using-the-praw-module). Thank you.", "title": "How to add reddit overload support to my script that is using the praw module?", "url": "https://www.reddit.com/r/redditdev/comments/3hil7j/how_to_add_reddit_overload_support_to_my_script/"}, {"author": "EliteMasterEric", "created_utc": 1439864422, "gilded": 0, "name": "t3_3he8ev", "num_comments": 8, "score": 1, "selftext": "Since I updated to PRAW 3.2.0, when I run this code: r.get_flair_choices(subreddit) I only receive the first 20 possible flairs for the subreddit. Previously, running this command would retrieve all possible flairs. Does anyone have a fix for this?", "title": "[PRAW] Not getting all flair choices after PRAW 3.2.0", "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "KidKrule", "created_utc": 1439766821, "gilded": 0, "name": "t3_3h91fe", "num_comments": 10, "score": 5, "selftext": "I'm making a bot that will take a tweet, reformat it a little and then post it to a dedicated sub. I'm using PRAW and login in through OAuth. My useragent string follows the guidelines. I'm posting my tests in /r/reddit_api_test and after only 4 links posted, I'm asked to wait one hour. Before that, I couldn't post 2 links in a row, I had to wait something like 10 minutes (using praw.errors.RateLimitExceeded to get the time remaining before I can post again). Am I doing anything wrong ? I find those limits a little bit too hard. Especially when testing... **[UPDATE]** I was actually running into the \"new account limit\", and not the API's limit. How I got over it ? I got someone to make the bot moderator of a test subreddit, and it can post there freely (as long as it follows the API's guidelines of course).", "title": "Trying to make a bot but running into drastic API limits", "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "dClauzel", "created_utc": 1439658068, "gilded": 0, "name": "t3_3h40nx", "num_comments": 4, "score": 2, "selftext": "Using `get_banned()` from praw v3.2.0 (*wink wink \ud83d\ude09*), I can get the following data about bans: * 'name': the banned account; * 'id': the banned account\u2019s id; * 'date': timestamp of when the ban was set; * 'note': the private (moderators only) comment associated to the ban. What is missing is: - the time remaining for a ban to expire; - the name of the moderator who placed the ban. Is there a way to get those informations?", "title": "[PRAW] Getting the expiration time of a ban and the person who placed it?", "url": "https://www.reddit.com/r/redditdev/comments/3h40nx/praw_getting_the_expiration_time_of_a_ban_and_the/"}, {"author": "spawnofyanni", "created_utc": 1439653834, "gilded": 0, "name": "t3_3h3s34", "num_comments": 10, "score": 3, "selftext": "Hi all. I've had a bot running for a little over a year, and recently moved it over to OAuth what with the reddit HTTPS switch. A big part of the bot is the ability to reply to private messages. The core functionality has been working fine both before the switch and for a few weeks after it; recently, though, the bot has been entirely unable to reply to PMs, coming up with the following error: Traceback (most recent call last): File \"<pyshell#41>\", line 1, in <module> msg.reply('test') File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 407, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 2524, in _add_comment retval = decorator(add_comment_helper)(self, thing_id, text) File \"<string>\", line 2, in add_comment_helper File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 255, in wrap return function(*args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 2517, in add_comment_helper retry_on_error=False) File \"<string>\", line 2, in request_json File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 604, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 437, in _request _raise_response_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 200, in _raise_response_exceptions raise Forbidden(_raw=response) Forbidden If I switch to the bot account just using a browser I can send and reply to private messages fine. If I manually login in a python shell using the OAuth details I can recreate the error above - I can start a new PM thread but I can't reply to any messages. I'm not sure if it's a scope thing (I found [this recent thread](https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/) which seems sort of relevant, but I'm not sure) or something else entirely, though I'm certain the OAuth scope includes 'privatemessages' (and 'submit'). Again, this is a pretty recent thing, private messaging was working fine before I started using OAuth and for a while after I switched to it, too. All other functionality - submitting threads, editing, deleting, etc - also still works as expected. This was happening with both PRAW 3.1 and 3.2 after I upgraded this morning. I'm neither a python nor an OAuth savant, but I've been able to get by. I'm completely lost here, though, so I was hoping someone could help out.", "title": "Bot gets \"Forbidden\" error when trying to reply to private messages", "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "soymilkbot", "created_utc": 1439647799, "gilded": 0, "name": "t3_3h3hmm", "num_comments": 4, "score": 5, "selftext": "Hello everyone, I'm new to making bots and python in general and I have this problem that's been bugging me for a while. I can't figure out how to catch a specific exception, in particular a 503 HTTPError. Here is the relevant piece of code: try: # some code except KeyboardInterrupt: print(\"Shutting down.\") break except requests.exceptions.HTTPError as e: #second block print(\"Some thing bad happened! HTTPError\", str(e.response.status_code)) if e.response.status_code == 503: print(\"Let's wait til reddit comes back! Sleeping 60 seconds.\") time.sleep(60) except Exception as e: #third block print(\"Some thing bad happened!\", e) traceback.print_exc() The thing is that last night while the bot was running a 503 occurred but instead of the second except block running, the third one did. Here is the traceback: Some thing bad happened! Traceback (most recent call last): File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 201, in _raise_response_exceptions response.raise_for_status() # These should all be directly mapped File \"C:\\Python34\\lib\\site-packages\\requests\\models.py\", line 851, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 503 Server Error: Service Unavailable During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"soymilk.py\", line 19, in for comment in test_comments: File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 524, in get_content page_data = self.request_json(url, params=params) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 424, in _request _raise_response_exceptions(response) File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 203, in _raise_response_exceptions raise HTTPException(_raw=exc.response) praw.errors.HTTPException As you can see even though the traceback says it's a requests.exceptions.HTTPError, my try except still went to the third block. Why? I've googled the whole day for an answer and I still can't figure it out. Also, since it went into the third block I was expecting it to print a descriptive string of the error next to \"Something bad happened!\" because my print statement is print(\"Some thing bad happened!\", e) But somehow it doesn't print anything for the e. So summarizing I have two questions: 1. Why does my try except block not detect that the exception is a HTTPError? 2. Why is nothing printed when I print the exception as e? Thank you for the help in advance!", "title": "How do you catch a specific exception?", "url": "https://www.reddit.com/r/redditdev/comments/3h3hmm/how_do_you_catch_a_specific_exception/"}, {"author": "dClauzel", "created_utc": 1439370514, "gilded": 0, "name": "t3_3gpbiu", "num_comments": 21, "score": 0, "selftext": "# solved Problem solved with this: /r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/cu1b1nc ---- I am a moderator of /r/Europe, and I am trying to get a list of the banned accounts on this subreddit. I am using PRAW and praw-OAuth2Util with python3.4 on Debian, from pip: # pip3 search praw praw-oauth2util - OAuth2 wrapper for PRAW INSTALLED: 0.2.2 (latest) prawtools - A collection of utilities that utilize the reddit API. INSTALLED: 0.19 (latest) prawoauth2 - Library to make your life easier using OAuth2 for PRAW praw - PRAW, an acronym for `Python Reddit API Wrapper`, is a python package that allows for simple access to reddit's API. INSTALLED: 3.1.0 (latest) For Reddit\u2019s oauth, the app is declared as a script, and all scopes are given to it. I fail to see where my mistake is. Help? ---- demo.py #!/usr/bin/env /usr/bin/python3.4 # -*- coding: utf-8 -*- import praw import OAuth2Util ### # connexion print(\"connexion\u2026\", end=\" \") r = praw.Reddit(user_agent=\"posix:eu.clauzel.couteau-suisse:v0 (by /u/dClauzel)\", log_requests=1, api_request_delay=4.0, timeout=300.0, site_name=\"dClauzel\") o = OAuth2Util.OAuth2Util(r, print_log=True) o.refresh() print(\"connect\u00e9.\") ### # informations utilisateur print(\"Je suis {0} et j\u2019ai un karma de {1} pour mes commentaires.\".format(r.get_me().name, r.get_me().comment_karma) ) ### # top des 5 soumissions populaires print(\"Top 5\") sousjlailu = r.get_subreddit(\"Europe\", fetch=True) for soumission in sousjlailu.get_hot(limit=5): print(\" - {0} \u2014 {1} \u2014 {2}\".format( soumission.title, soumission.author, soumission.url) ) ### # bannissements print(\"Liste des bannis\") bannis = sousjlailu.get_banned() bannis = [x for x in bannis] print(bannis) ### # nettoyage print(\"fin\") ---- oauth.txt # Config scope=identity,account,edit,flair,history,livemanage,modconfig,modflair,modlog,modothers,modposts,modself,modwiki,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote,wikiedit,wikiread refreshable=True # Appinfo app_key=*redacted* app_secret=*redacted* # Token token=*redacted* refresh_token=*redacted* ---- praw.ini # -*- coding: utf-8 -*- [dClauzel] check_for_updates: True ---- Running the program: $ ./demo.py substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json connexion\u2026 connect\u00e9. substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json Je suis dClauzel et j\u2019ai un karma de 15936 pour mes commentaires. Top 5 substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/Europe/about/.json substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/europe/.json - Immigration Megathread - Part VI \u2014 ModeratorsOfEurope \u2014 http://www.reddit.com/r/europe/comments/3frno2/immigration_megathread_part_vi/ - Sweden boosts security for asylum seekers after IKEA knife attack; two Eritrean suspects detained \u2014 Chunkeeguy \u2014 http://www.abc.net.au/news/2015-08-12/sweden-boosts-security-for-asylum-seekers-after-ikea-attack/6690180 - A reminder that there is an active war going on in Eastern Europe. Pro-Russian separatists film one of their unsuccessful attacks on Ukrainian positions. \u2014 RabbitOfCaerbanog \u2014 https://www.youtube.com/watch?v=0N0rplcH32g&t=241 - A creeping occupation in action: Russian forces again move the border with Georgia, this time a further 800 meters into the Georgian territory. \u2014 RabbitOfCaerbanog \u2014 https://www.youtube.com/watch?v=l1HUk2LEJxU - The European Union wastes about 22 million tonnes of food a year and Britain wastes the most, according to a study by European Commission-backed researchers. \u2014 Libertatea \u2014 http://www.reuters.com/article/2015/08/11/europe-food-waste-idINKCN0QG2DB20150811?feedType=RSS&feedName=worldNews Liste des bannis GET: https://api.reddit.com/r/europe/about/banned/.json Traceback (most recent call last): File \"./demo.py\", line 43, in bannis = [x for x in bannis] File \"./demo.py\", line 43, in bannis = [x for x in bannis] File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 1811, in _get_userlist for data in content: File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 524, in get_content page_data = self.request_json(url, params=params) File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 424, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 196, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty sys:1: ResourceWarning: unclosed ----", "title": "[PRAW][OAuth2Util] Problem using get_banned() : raise Forbidden(_raw=response)", "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "sufficiency", "created_utc": 1439260818, "gilded": 0, "name": "t3_3gjm58", "num_comments": 4, "score": 1, "selftext": "Hi, I am currently hosting a bot which, in a nutshell, posts stuff from another website as a comment on Reddit. This in itself works fine, but occasionally the comment to be made is more than 10k characters long, making it impossible to be posted on Reddit. I believe the best way to overcome this problem is to make a \"comment chain\". Basically, if the bot wants to post a 15k characters essay, it will make an initial comment with around 10k characters, then it will reply to the first comment with the remaining 5k. My question is how can I achieve this in a way that's reliable. I COULD do the following: 1. Make initial comment 2. Load the bot's own comments, newest first. Presumably the initial comment will be on top 3. Reply to the initial comment. But the problem here is that the Reddit API (as far as I can tell) has some lags. Additionally, the initial comment could potentially get deleted due to automod, etc. So I am hoping to see if there are better approaches. EDIT: I am using PRAW", "title": "Making a comment chain to overcome Reddit comment size limit", "url": "https://www.reddit.com/r/redditdev/comments/3gjm58/making_a_comment_chain_to_overcome_reddit_comment/"}, {"author": "Aton_Five", "created_utc": 1438767035, "gilded": 0, "name": "t3_3fuv52", "num_comments": 8, "score": 2, "selftext": "Using OAuth2Util for OAuth2 handling with the following permissions (and several variations for testing): scope=identity,account,read,report,wikiedit,wikiread,modposts,modcontributors The issue is with: sub.add_ban(comment2.author.name) Error: Traceback (most recent call last): File \"F:\\XXXX\\Bots\\test2.py\", line 42, in sub.add_ban(comment2.author.name) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 346, in wrapped raise errors.LoginRequired(function.__name__) praw.errors.LoginRequired: `do_relationship` requires a logged in session` Using the old form of logging in r.login('username', 'password') causes the code to work and the user in question to be banned as expected. I'm not sure what to try next to get this to work, I am pretty sure the permissions are right.", "title": "PRAW: Issues banning with local script using OAuth2", "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "EmperorSofa", "created_utc": 1438757957, "gilded": 0, "name": "t3_3fujpg", "num_comments": 9, "score": 2, "selftext": "So I thought it would be fun to create a bot to gather some meta data for me. My idea is I give my bot a time frame to work from say from now to five days ago, I give it a few keywords to filter titles based on, and I specify if I want self posts or liink posts or both. Then it will go off on its merry way and return all the posts that have been made between those times that fit my criteria. The issue is I want to have the option for my bot to return information on all the posts made since the subreddit's creation. Using subreddit.get_new(limit=100) might not cut it. My question is does the API have any kind of function that would allow me to sort a subreddit's posts based on time of creation. Would that function allow me to get all the posts ever made on a particular subreddit? I understand the API has built in limits and PRAW enforces them for me. If a subreddit has only existed for a few months and only has a few hundred posts that's not that many API calls but if I pointed this bot at a default subreddit that's existed for the better part of a decade I might be in for a long wait. Speed isn't my concern so much as automating an otherwise mind numbing task.", "title": "[PRAW] Iterating through subreddit posts based on time of creation.", "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "RemindMeBotWrangler", "created_utc": 1438664122, "gilded": 0, "name": "t3_3fpmx8", "num_comments": 5, "score": 1, "selftext": "#!/usr/bin/env python import praw import OAuth2Util import traceback r = praw.Reddit(\"Testing 123\") o = OAuth2Util.OAuth2Util(r, print_log=True) try: print str(r.get_submission(\"http://www.reddit.com/r/videos/comments/38ae9m/construct/crtswre\").comments[0]) except Exception as err: print traceback.print_exc() print err._raw When I run the above with oauth2, I get 403s with that URL. But other URLs work and there are other URLs that don't. There is no ambiguity like this when using the old username/password login. The traceback: Traceback (most recent call last): File \"testing.py\", line 12, in print str(r.get_submission(\"http://www.reddit.com/r/videos/comments/38ae9m/construct/crtswre\").comments[0]) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 985, in get_submission params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 348, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 1034, in from_url response = reddit_session.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 424, in _request _raise_response_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 196, in _raise_response_exceptions raise Forbidden(_raw=response) Forbidden None ---- Does the same happen for others? The scope I'm using for oAuth2Util: scope=identity,account,edit,flair,history,livemanage,modconfig,modflair,modlog,modothers,modposts,modself,modwiki,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote,wikiedit,wikiread", "title": "PRAW - get_submission() works only \"sometimes\" with oAuth2 - 403s", "url": "https://www.reddit.com/r/redditdev/comments/3fpmx8/praw_get_submission_works_only_sometimes_with/"}, {"author": "NotSinceYesterday", "created_utc": 1438596159, "gilded": 0, "name": "t3_3flqqh", "num_comments": 6, "score": 4, "selftext": "Hi all, I have a bot that I'm updating to use Oauth, and it appears that the scopes listed [here](http://praw.readthedocs.org/en/latest/pages/oauth.html#oauth-scopes) aren't fully correct. Replying to a private message using .reply() appears to need the 'Submit' scope instead of the 'privatemessages' scope. File \"C:\\Users\\x\\Dropbox\\Pokemontrades\\Oauth Scripts\\Porygon2-Bot.py\", line 96, in messagesuccess message.reply(\"Flair updated.\") File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 376, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 342, in wrapped raise errors.LoginOrScopeRequired(function.__name__, scope) LoginOrScopeRequired: ``_add_comment` requires a logged in session or the OAuth2 scope `submit`` requires a logged in session", "title": "PRAW - Incorrect Scopes?", "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "Phteven_j", "created_utc": 1438476283, "gilded": 0, "name": "t3_3fgo6t", "num_comments": 8, "score": 4, "selftext": "EDIT: RESOLVED via: $ pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip --- So I got my OAuth working for the most part, but for the life of me I cannot get banning working. Every other mod-type thing works for me: removing, distinguishing, flairing, etc. I am using the modcontributors scope to enable banning and if I print my _authorization variable, I get: set([u'wikiedit', u'save', u'wikiread', u'subscribe', u'edit', u'modcontributors', u'mysubreddits', u'privatemessages', u'modconfig', u'read', u'modlog', u'modposts', u'modflair', u'vote', u'modwiki', u'submit', u'identity', u'flair']) And here is the error I get when I attempt a ban (yes I'm a mod of the sub): File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py\", line 346, in wrapped raise errors.LoginRequired(function.__name__) praw.errors.LoginRequired: `do_relationship` requires a logged in session I appreciate any help and let me know if I can provide more info.", "title": "[PRAW] Getting \"LoginRequired\" error when attempting to ban", "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "Phteven_j", "created_utc": 1438370238, "gilded": 0, "name": "t3_3fbqfw", "num_comments": 20, "score": 2, "selftext": "Hey guys, I have a series of simple python reddit bots that need to be logged in to perform their tasks. I've been using r.login() but I'm converting over to OAuth like I should. The problem is that when I stop and restart a script, the authorization needs to be redone and I don't know a good way to do it. I followed the guide here https://praw.readthedocs.org/en/v3.1.0/pages/oauth.html. Here's the relevant code: r.set_oauth_app_info(client_id,client_secret,redirect_uri) url = r.get_authorize_url('hcebot', 'identity', True) import webbrowser webbrowser.open(url) access_information=r.get_access_information(\"\") r.set_access_credentials(**access_information) authenticated_user=r.get_me() So I run the bot, grab the code, put it into my script, and run the bot. It runs fine, but if I need to restart it or it crashes/restarts, I need to update the code. Is there a way to do this automatically? This is the error: praw.errors.OAuthInvalidGrant: invalid_grant on url https://api.reddit.com/api/v1/access_token/ Thanks for your help and let me know if I left anything out.", "title": "OAuth help for simple python bots: getting invalid_grant on subsequent runs", "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "DontKillTheMedic", "created_utc": 1438264279, "gilded": 0, "name": "t3_3f5rzi", "num_comments": 28, "score": 10, "selftext": "I know that very soon, PRAW will be transitioning to another authentication protocol for logging into reddit accounts. Does anybody know exactly when this change will occur? And I assume the simple fix is to switch to the OAuth protocol mentioned in the documents. Am I correct in doing so? Thanks!", "title": "[PRAW] Reddit.login(), when is it leaving?", "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "jian142857", "created_utc": 1438222242, "gilded": 0, "name": "t3_3f3ydz", "num_comments": 6, "score": 2, "selftext": "I'd like to grab some latest users for research purpose. One method might be possible is increasing the user id and query if it exists or not. but I found that PRAW doesn't provide one API to get user by id but only by the user name. Is there any method I could get the latest user or get the user by id?", "title": "How to get the latest registered users?", "url": "https://www.reddit.com/r/redditdev/comments/3f3ydz/how_to_get_the_latest_registered_users/"}, {"author": "OtakuSRL", "created_utc": 1438122781, "gilded": 0, "name": "t3_3eyfz9", "num_comments": 3, "score": 2, "selftext": "I updated PRAW accordingly as well (via pip) with no change. Am I still on an outdated build somehow? I am looking to use the following functions which was added in a recent build/release of PRAW. `newpost.sticky(bottom=True)` returns an error that it doesn't recognize the \"bottom\" part, and the error obviously terminates the program. The rest of the program works fine and posts the thread, it just lacks any sort of sticky effect as the code terminates before it could finish properly. I am thinking this is a version issue but I am not sure. Is pip not the best route to take when upgrading PRAW? I believe this was pushed to the release or whatever. Sorry for the lack of a screenshot of the error but hopefully this is simple enough to understand.", "title": "[PRAW] Sticky bottom=True function not being recognized, even after updating PRAW", "url": "https://www.reddit.com/r/redditdev/comments/3eyfz9/praw_sticky_bottomtrue_function_not_being/"}, {"author": "iceph03nix", "created_utc": 1438043150, "gilded": 0, "name": "t3_3eu705", "num_comments": 8, "score": 2, "selftext": "So I'm reading through the [PRAW guide](https://praw.readthedocs.org/en/v3.1.0/pages/comment_parsing.html) and I'm trying to just grab the top comments from a specific submission. I found the option to load all comments... >>> submission.replace_more_comments(limit=None, threshold=0) >>> all_comments = submission.comments ...but due to the number of comments, it's taking quite a while, and I have a feeling I'm pulling a lot of extraneous info. Is there a good way to grab all the 'root' comments without pulling everything?", "title": "Loading all top level comments from a specific submission", "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "Triplanetary", "created_utc": 1437952666, "gilded": 0, "name": "t3_3epjgl", "num_comments": 1, "score": 2, "selftext": "Two questions: 1. Does PRAW use the 1 call per second rate limit when connected via OAuth or does it still wait two seconds between requests in this situation? 2. Does PRAW multiprocess take into account that the OAuth rate limit is per-user? AND/OR could I safely ignore multiprocess and run multiple scripts connected to different OAuth'd users in separate processes? (If the answer to the latter question is a resounding no, that's cool, just let me know.) Thanks.", "title": "PRAW and OAuth rate limit", "url": "https://www.reddit.com/r/redditdev/comments/3epjgl/praw_and_oauth_rate_limit/"}, {"author": "catcint0s", "created_utc": 1437856459, "gilded": 0, "name": "t3_3eld2o", "num_comments": 3, "score": 1, "selftext": ">>> from praw import Reddit >>> r = Reddit(\"hey\") >>> s = r.get_submission(\"https://www.reddit.com/r/worldnews/comments/3ek0h4/ne w_horizons_finds_nitrogen_glaciers_and_hazy_air/\") >>> s.title \"New Horizons Finds Nitrogen Glaciers and Hazy Air on Pluto: Astronomers astoun ded by the dwarf planet's active geology and atmosphere\" >>> s.subreddit Subreddit(subreddit_name='worldnews') >>> s.subreddit.name 't5_2qh13' >>> s.subreddit.fullname 't5_2qh13' Tried this way but it gives the id", "title": "[PRAW] How to get a Subreddit object's name", "url": "https://www.reddit.com/r/redditdev/comments/3eld2o/praw_how_to_get_a_subreddit_objects_name/"}, {"author": "codsane", "created_utc": 1437800577, "gilded": 0, "name": "t3_3ej576", "num_comments": 2, "score": 0, "selftext": "Hello. Was messing around with some bot ideas, and it seems like I am having trouble logging into Reddit. I have tried running some of my old bots, and they authenticate completely fine. Here is the little test script I am trying to run: >import praw > >redditPraw = praw.Reddit(user_agent = \"User Agent\") >redditPraw.login('Username', 'Password') >redditPraw.get_subreddit('test') After running, I am getting praw.errors.Forbidden on the login line. Any ideas as to why this is happening? All my other bots run fine. I am stuck here. Thanks! :)", "title": "Getting forbidden error, not sure why.", "url": "https://www.reddit.com/r/redditdev/comments/3ej576/getting_forbidden_error_not_sure_why/"}, {"author": "MycTyson", "created_utc": 1437776096, "gilded": 0, "name": "t3_3ehx0t", "num_comments": 3, "score": 3, "selftext": "Hello - today I fired up Python, Installed PRAW and got down with my first PRAW shenanigans. Now that I know what I am doing in terms of the basics, I wanted to ask before I set out on a quest to do it myself, if there exists something somewhere to fulfill my need. I run a small sub, and enforce the [TAG] of every post. No tag, no post. Recently, I added CSS Flair to make sorting the sub easier. I have to manually go through and assign all the flair, or count on users to do it for their posts. Relying on users to put in effort is less than ideal. Enter Python and PRAW to do the automation for me in terms of applying flair to existing posts, and enforcing it with new posts. I have both a Windows VPS and a CentOS VPS at my disposal to let this sucker run on once I get it hashed out locally, but as with most things there is probably someone who has wanted to do this before and I don't want to waste any time if I don't have to. Again, today is literally day one however I am a big fan of not reinventing the wheel if possible. TL;DR: See title.", "title": "[PRAW] Automaticallly assign flair to posts in sub, based on content of [Brackets] in the title?", "url": "https://www.reddit.com/r/redditdev/comments/3ehx0t/praw_automaticallly_assign_flair_to_posts_in_sub/"}, {"author": "MrRogersbot", "created_utc": 1437586943, "gilded": 0, "name": "t3_3e80jx", "num_comments": 3, "score": 0, "selftext": "[Automoderator can do it]( https://www.reddit.com/r/modnews/comments/36fcrd/moderators_automoderator_updates_filter_action/) **Ability to display a reason for acting in the moderation log** This is a much-requested feature that I've finally been able to add today - you can now set action_reason on any rule that has an action, and the reason will be displayed in the moderation log for approvals/removals, or used as a report reason if it's a report rule. So for example, you could define this rule: title: [\"red\", \"yellow\", \"blue\"] action: remove action_reason: primary color in title And if AutoModerator removes a post because of that rule, the entry in the moderation log would read something like: AutoModerator removed link \"DAE think red is overrated?\" by Deimorz (primary color in title) This should help with one of the biggest difficulties with AutoModerator - not being able to tell exactly why it approved or removed something (unless you used comments/modmail/flair, which all have their own issues). Note that action_reason completely replaces report_reason, but report_reason is still supported (and just acts as an alias of action_reason) so that all the existing rules using report_reason are still functional. Can this be added to praw?", "title": "[PRAW] Setting action reason for removal in mod log?", "url": "https://www.reddit.com/r/redditdev/comments/3e80jx/praw_setting_action_reason_for_removal_in_mod_log/"}, {"author": "AchillesDev", "created_utc": 1437352766, "gilded": 0, "name": "t3_3dw2y8", "num_comments": 8, "score": 2, "selftext": "I more or less understand how to use OAuth for a bot (I am working on a quick implementation of it to make sure I get it), however I am working on a utility that will commit specific actions on two given (and existing) user accounts. I know this is done successfully by other apps, but I'm not sure if I am thinking about structuring my utility the right way. How would the app get the correct information for a given user and commit those actions on their behalf using OAuth? I am more familiar with using PRAW's login method and config files than I am with using OAuth, so maybe I am thinking about the logging in concept the wrong way? As I understand it, the OAuth information is tied to a bot's account for a bot implementation, but the utility I am using will be using existing accounts that others own. Currently, I'm monkeying around with OAuth2Util.", "title": "[PRAW/OAuth] Using OAuth to automate user actions on existing accounts", "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "Ouiski", "created_utc": 1436965538, "gilded": 0, "name": "t3_3ddlcv", "num_comments": 2, "score": 2, "selftext": "I've developed a python crawler (not using praw) that collects comments using an existing list of submissions that I've already collected. It uses OAuth. When I try to run it for extended periods of time after about 1800 submissions worth of comments, the call doesn't return the data correctly. I am using a timer to prevent over calling, (I'm definitely doing less than 60 a minute) , but does anyone know of any other rule that prevents calling after a certain amount of calls in a row? Or has anyone had any similar problems? Thanks!", "title": "Crawling issue", "url": "https://www.reddit.com/r/redditdev/comments/3ddlcv/crawling_issue/"}, {"author": "pcjonathan", "created_utc": 1436910346, "gilded": 0, "name": "t3_3db07x", "num_comments": 3, "score": 3, "selftext": "I'm running a script to collect content from a few subreddits I moderate (which then gets emailed and dumped). Every now and then, it comes across a post containing characters that aren't normal. It's a very rare occurrence (I've just added a new sub to it hence why the example link is very old). The problem occurs with praw-multiprocess so there's not much I can do. This is the line that crashes: if r.get_submission(url=sub.perma).approved_by != None: An example sub.perma is: https://www.reddit.com/r/gallifreyan/comments/1uerl8/i_tried_doing_a_name_with_accents_b%C5%91g%C3%A9r_eszter/ (sub is a peewee database object) It gets me the following output and error when I run my script: Lost connection with multiprocess server during read. Trying again. Lost connection with multiprocess server during read. Trying again. Lost connection with multiprocess server during read. Trying again. Traceback (most recent call last): File \"/home/pcj/py3/lib/python3.4/site-packages/praw/handlers.py\", line 209, in _relay retval = cPickle.load(sock_fp) EOFError: Ran out of input and this: praw.errors.ClientException: Successive failures reading from the multiprocess server. The praw-multiprocess output is as follows: GET https://www.reddit.com/r/gallifreyan/comments/1uerl8/i_tried_doing_a_name_with_accents_b%C5%91g%C3%A9r_eszter/.json Exception in thread Thread-69877: Traceback (most recent call last): File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner self.run() File \"/usr/lib/python3.4/threading.py\", line 868, in run self._target(*self._args, **self._kwargs) File \"/usr/lib/python3.4/socketserver.py\", line 612, in process_request_thread self.handle_error(request, client_address) File \"/usr/lib/python3.4/socketserver.py\", line 609, in process_request_thread self.finish_request(request, client_address) File \"/usr/lib/python3.4/socketserver.py\", line 344, in finish_request self.RequestHandlerClass(request, client_address, self) File \"/usr/lib/python3.4/socketserver.py\", line 665, in __init__ self.handle() File \"/usr/local/lib/python3.4/dist-packages/praw/multiprocess.py\", line 77, in handle cPickle.dump(retval, self.wfile, # pylint: disable=E1101 UnboundLocalError: local variable 'retval' referenced before assignment Does anyone have any advice or a possible fix for PRAW? Sidenote: Is there anyway to increase PRAW's caching for everything but override that increase for certain pages (or visa versa)? (e.g. 5 minutes on submissions, 30 seconds on /r/sub/new and /r/sub/comments).", "title": "[PRAW] Crashing on Non-Standard Characters", "url": "https://www.reddit.com/r/redditdev/comments/3db07x/praw_crashing_on_nonstandard_characters/"}, {"author": "zzpza", "created_utc": 1436862377, "gilded": 0, "name": "t3_3d8csx", "num_comments": 7, "score": 1, "selftext": "Now we have two stickies, roughly how long before this functionality will be available in PRAW? I have a sticky rotation script I need to update... (In case you missed the announcement: https://www.reddit.com/r/modnews/comments/3d7i0q/moderators_you_can_now_have_two_stickies_in_your/) Also, where should I be watching for new release announcements for PRAW? I tried looking at /r/praw but that is private. TIA. :D", "title": "PRAW support for two stickies?", "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "austin_18", "created_utc": 1436829123, "gilded": 0, "name": "t3_3d6p0y", "num_comments": 5, "score": 0, "selftext": "**Problem Solved** So I have a bot that I've recently coded in Python 3.4 and ran on and off for the past week. Throughout the week I've noticed an error or warning that keeps interrupting my bot. Traceback (most recent call last): * File \"C:\\Users\\Austin\\Dropbox\\DongerBot-Counter.py\", line 99, in comment.reply(\"\u30fd\u0f3c\u0e88\u0644\u035c\u0e88\u0f3d\uff89 \\n\\n ^Now ^With ^**Donger** ^**Facts!**: \\n\\n ^Dongers ^Supplied: ^**%s** \\n\\n ^^That ^^Is ^^**%s** ^^Upvote(s) ^^Per ^^Donger!\" %(x, avgKarma)) * File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 399, in reply response = self.reddit_session._add_comment(self.fullname, text) * File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 348, in wrapped return function(cls, *args, **kwargs) * File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 2407, in _add_comment retry_on_error=False) * File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) * File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 579, in request_json retry_on_error=retry_on_error) * File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 424, in _request _raise_response_exceptions(response) * File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 196, in _raise_response_exceptions raise Forbidden(_raw=response) * praw.errors.Forbidden I am completely lost at why this is being displayed. I am using comment_stream, if that is any help. I can post my code if needed. Thanks! Edit: formatting", "title": "My bot code is returning a rather lengthy error/warning/Traceback, and I'm not sure why. Any help is appreciated!", "url": "https://www.reddit.com/r/redditdev/comments/3d6p0y/my_bot_code_is_returning_a_rather_lengthy/"}, {"author": "teaearlgraycold", "created_utc": 1436581240, "gilded": 0, "name": "t3_3cvdof", "num_comments": 13, "score": 4, "selftext": "praw.errors.RedirectException: Unexpected redirect from https://api.reddit.com/api/wiki/edit/.json to https://www.reddit.com/r/explainlikeimfive/login.json?dest=https%3A%2F%2Fapi.reddit.com%2Fapi%2Fwiki%2Fedit%2F.json The bot can still use the API (make modmails etc.) but throws this when it tries to edit a wiki page. I gave it both `wikiedit` and `edit` OAuth scopes. **Edit:** Title should have been \"strange error\" :Pb", "title": "String error after implementing OAuth", "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "asdfusernameasdf", "created_utc": 1436545345, "gilded": 0, "name": "t3_3ct9m8", "num_comments": 7, "score": 14, "selftext": "^(Sorry for probably being the 50th person to post this but I couldn't find any useful answers.) I'm asking because 1) there are a metric ton of comments per second and I have no idea how to keep up and 2) I haven't found an API function that gives me /r/all/comments. I know PRAW can do it but I'd prefer C# over Python because everything else I need is already written in it.", "title": "How do Reddit bots read every single comment?", "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "Bitani", "created_utc": 1436385325, "gilded": 0, "name": "t3_3cl8oq", "num_comments": 3, "score": 1, "selftext": "I'm currently trying to create a program that retrieves as many comments as it can from a user's page and then searches the first-child replies to those comments for certain keywords. Problem is, I can't find an easy way to load the replies to each comment. It seems like the only viable method I can find is to load the entire submission each comment is a child of. Whether through PRAW or the raw API, what would be the best way to load the replies to a comment just having its ID?", "title": "Load replies to a comment fetched from user page?", "url": "https://www.reddit.com/r/redditdev/comments/3cl8oq/load_replies_to_a_comment_fetched_from_user_page/"}, {"author": "avinassh", "created_utc": 1436377770, "gilded": 0, "name": "t3_3ckq5a", "num_comments": 27, "score": 26, "selftext": "**TLDR;** [Github](https://github.com/avinassh/prawoauth2) | [Example](https://github.com/avinassh/prawoauth2/tree/master/examples/halflife3-bot) | [pypi](https://pypi.python.org/pypi/prawoauth2/) --- **What it does?** - It makes your life super easy for handling OAuth with Praw - It can get you new `access_token` and `refresh_token` - It can 'refresh' your praw instance to use new tokens - You don't really need to worry about token expiry - Best used for bots, which run on Heroku, AWS etc. It will help you run your bot forever! - Comes with nice documentation and a working example you can play with! - It's open source and released under MIT License! (: **Installation:** pip install prawoauth2 **Info:** `prawoauth2` comes with two components, `PrawOAuth2Mini` and `PrawOAuth2Server`. `PrawOAuth2Server` authorizes your app/script with the Reddit account and gives you access token. `PrawOAuth2Mini` uses these tokens for all next transactions with Reddit. Remember, for a bot, you only need `access_token`. Why it is written like that? If you are writing a bot and running it on a headless server, something like Amazon AWS or Openshift, you cannot authorize your script with the Reddit account, as it tries to open the browser for authorization. This is one time only operation(if you pass the parameter `permanent`). So, I decided to break this into two parts. Run the `PrawOAuth2Server` locally on your computer, get the `access_token`, `refresh_token` and save them somewhere. Later, `PrawOAuth2Mini` can make use of these tokens for further transactions. And it does not require browser at all, so that it can run in a headless server without any hiccups. **TLDR;** `PrawOAuth2Server` meant to be run only once locally on your main machine to fetch the first `access_token`, `refresh_token` and `PrawOAuth2Mini` later and for everything.", "title": "Hi, I released a helper module for Praw, which makes writing OAuth bots super easy and fun", "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "Nouv", "created_utc": 1436334368, "gilded": 0, "name": "t3_3ciqeq", "num_comments": 0, "score": 1, "selftext": "Hey, Is there any way to use SnooCore with a comment stream, PRAW style? Checked docs but may have been missing something.", "title": "Comment stream using SnooCore?", "url": "https://www.reddit.com/r/redditdev/comments/3ciqeq/comment_stream_using_snoocore/"}, {"author": "umop_aplsdn", "created_utc": 1436313600, "gilded": 0, "name": "t3_3chntm", "num_comments": 4, "score": 2, "selftext": "Hi, I'm trying to mark my messages as read and unread. Here is my code: messages = [message for message in r.get_unread(limit=None) if message.subject == ''] messages.sort(key=lambda m: m.created) for message in messages: print type(message) message.mark_as_read() Here is the error: Traceback (most recent call last): File \"/Users/tyler/Documents/code/ModInviterBot/main.py\", line 49, in message.mark_as_unread() File \"/usr/local/lib/python2.7/site-packages/praw/objects.py\", line 367, in mark_as_unread return self.reddit_session.user.mark_as_read(self, unread=True) AttributeError: 'NoneType' object has no attribute 'mark_as_read' It seems like self.reddit_session.user.mark_as_read isn't being initialized. My Python version is 2.7.9 on Mac OSX, installed using Homebrew.", "title": "[PRAW] Message.mark_as_read() and mark_as_unread() broken?", "url": "https://www.reddit.com/r/redditdev/comments/3chntm/praw_messagemark_as_read_and_mark_as_unread_broken/"}, {"author": "swim1929", "created_utc": 1436236566, "gilded": 0, "name": "t3_3cdo5a", "num_comments": 2, "score": 2, "selftext": "Hi. I just want to make sure I'm not exceeding the request limit or anything, since I don't completely understand how comment_stream works yet. If anybody could explain, I'd really appreciate it. Anyways, here's the relevant code: word_to_match = ['example'] cache = [] def corrector_bot(): comments = praw.helpers.comment_stream(r, 'all', limit=None, verbosity=0) for comment in comments: comment_text = comment.body.lower() isMatch = any(string in comment_text for string in word_to_match) if comment.id not in cache and isMatch: #stuff here while True: try: corrector_bot() time.sleep(2) except: traceback.print_exc() print('Resuming in 1 minute...') time.sleep(60)", "title": "[PRAW] Am I breaking any API rules with this?", "url": "https://www.reddit.com/r/redditdev/comments/3cdo5a/praw_am_i_breaking_any_api_rules_with_this/"}, {"author": "gschizas", "created_utc": 1436017701, "gilded": 0, "name": "t3_3c3phr", "num_comments": 10, "score": 5, "selftext": "My goal was to work with OAuth2, and to make the end code as simple as possible. State is kept in a config file, with a very simple structure. The end script should look something like this: from bot import reddit_agent r = reddit_agent(user_agent='Some (optional) user agent', ini_section='optional_section_defaults_to_DEFAULT') print(r.user) The ini file looks something like this: [DEFAULT] client = client_id_from_/prefs/apps secret = secret_key_from_/prefs/apps The bot.py (the bulk of the work) is the following: # coding: utf-8 import urllib.parse import praw import configparser import datetime import http.server import webbrowser from dateutil.parser import parse as dateparser def start_web_server(port): \"\"\" :rtype : string \"\"\" server = http.server.HTTPServer(('', port), ScriptCallbackWebServer) print('Started httpserver on port:', port) server.now_serving = True server.callback_code = None # Wait for incoming http requests # until a proper result is found while server.now_serving: server.handle_request() server.server_close() return server.callback_code class ScriptCallbackWebServer(http.server.BaseHTTPRequestHandler): def do_GET(self): url = urllib.parse.urlparse(self.path) query = urllib.parse.parse_qs(url.query) if url.path != '/authorize_callback' or 'code' not in query: self.send_response(404) return self.send_response(200) self.send_header(\"Content-type\", \"text/html\") self.end_headers() self.wfile.write(\"Simple Bot Helper\".encode('utf-8')) self.wfile.write(\"This is the authorise callback page.\".encode('utf-8')) self.wfile.write(\"You accessed path: {}\".format(self.path).encode('utf-8')) self.wfile.write(\"You can close your browser\".encode('utf-8')) self.wfile.write(\"\".encode('utf-8')) self.server.callback_code = query['code'][0] self.server.now_serving = False def reddit_agent(user_agent=None, ini_section='DEFAULT'): if user_agent is None: user_agent = 'Reddit Temporary Script by /u/gschizas version ' + datetime.date.today().isoformat() scope = {'identity', 'flair', 'read', 'modflair', 'modlog', 'modposts', 'mysubreddits'} result = praw.Reddit(user_agent=user_agent) result.config.decode_html_entities = True # result.config.log_requests = 1 cfg = configparser.ConfigParser() with open('bot.ini') as f: cfg.read_file(f) client = cfg[ini_section]['client'] secret = cfg[ini_section]['secret'] access_token = cfg[ini_section].get('access_token', '') refresh_token = cfg[ini_section].get('refresh_token', '') redirect_url = 'http://localhost:65281/authorize_callback' result.set_oauth_app_info(client, secret, redirect_url) # first time running if access_token == '' or refresh_token == '': url = result.get_authorize_url('reddit_scratch', scope, True) webbrowser.open(url) callback_code = start_web_server(65281) access_information = result.get_access_information(callback_code) access_token = access_information['access_token'] refresh_token = access_information['refresh_token'] save_state(cfg, ini_section, access_token, refresh_token) last_refresh = dateparser(cfg[ini_section]['last_refresh']) minutes = (datetime.datetime.now() - last_refresh).total_seconds() / 60 print(minutes) if minutes", "title": "[praw] The simplest bot I could make (I hope someone can make it simpler)", "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "swim1929", "created_utc": 1435982561, "gilded": 0, "name": "t3_3c2jeu", "num_comments": 6, "score": 3, "selftext": "Hi, I'm having trouble with the following code. The bot runs fine when I define a word such as \"fish\" as the words_to_match string. However, as soon as I put spaces (such as defining the string as 'I could care less'), the bot can no longer find the comment even if it's actually there. I tried removing the brackets around the string (I just put 'I could care less') and it started replying to every comment. Thanks! import praw import time r = praw.Reddit(user_agent = \"TestBot by Ali /u/BlessYouBot_\") words_to_match = ['I could care less'] cache = [] def run_bot(): subreddit = r.get_subreddit(\"test\") comments = subreddit.get_comments(limit=25) for comment in comments: comment_text = comment.body.lower() isMatch = any(string in comment_text for string in words_to_match) if comment.id not in cache and isMatch: comment.reply(\"\"\"I think you meant to say, \"I **couldn't** care less\".\\n\\n*____________^I ^am ^a ^bot, ^and ^this ^action ^was ^performed ^automatically. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate.*\"\"\") cache.append(comment.id) print(\"Comments loop finished, time to sleep v.v\") while True: run_bot() time.sleep(10)", "title": "[Simple Noob Question] Bot not finding comments containing string.", "url": "https://www.reddit.com/r/redditdev/comments/3c2jeu/simple_noob_question_bot_not_finding_comments/"}, {"author": "armandg", "created_utc": 1435835105, "gilded": 0, "name": "t3_3buzu2", "num_comments": 2, "score": 1, "selftext": "I'm taking the risk of asking a possibly stupid question, but here goes: [The PRAW code](https://github.com/praw-dev/praw/blob/c64e3f71841e8f0c996d42eb5dc9a91fc0c25dcb/praw/__init__.py#L2416-L2420) states that the \"send_replies\"-function is a Gold Only-feature. So, naturally nothing happens when I try to deactivate the *\"send_replies\"*-feature. Here's my code: r.submit(subreddit, title, output, captcha=None, send_replies=None, resubmit=None) I am, however, able to deactivate this myself manually when submitting and even after submission has been done. Is only the automation of this feature a \"Gold Only Feature\" or am I missing something?", "title": "PRAW: send_replies a \"Gold Only Feature\"?", "url": "https://www.reddit.com/r/redditdev/comments/3buzu2/praw_send_replies_a_gold_only_feature/"}, {"author": "thomasbomb45", "created_utc": 1435628933, "gilded": 0, "name": "t3_3bky0m", "num_comments": 2, "score": 1, "selftext": "I am trying to write a program that, given a URL, will find a comment containing the URL and search the replies to *that* comment for other URLs. If my understanding of PRAW and the API is correct, there is no way for me to do this, other than to scrape and cache the entire website. Is there a better way to do this, or a public copy of the database I can download?", "title": "Search for links in comments?", "url": "https://www.reddit.com/r/redditdev/comments/3bky0m/search_for_links_in_comments/"}, {"author": "EliteMasterEric", "created_utc": 1435593738, "gilded": 0, "name": "t3_3bit3y", "num_comments": 18, "score": 2, "selftext": "**NOTE:** I just realized I wrote the title wrong, I know how to write a bot, just not how to integrate it with OAuth. I've read the tutorial on [the PRAW website](https://praw.readthedocs.org/en/v3.0.0/pages/oauth.html), but it only raises more questions for me. In particular, it says it requires the user to grant authorization by accessing a URL in their browser. This is, of course, impossible on a Python script running on a schedule through cron on an Amazon EC2 server. I don't think there even IS a web growser on that computer! Regardless, when I run my PRAW script, I get a message in the console reading: DeprecationWarning: Password-based authentication will stop working on 2015/08/03 and as a result will be removed from PRAW4. For more information please see: https://www.reddit.com/comments/2ujhkr/ Does this change make it impossible to create automated bots, or can I somehow give a program \"permanent\" permission to access my bot?", "title": "[PRAW][OAuth] How do I make an automated bot?", "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "Liudvikam", "created_utc": 1435592308, "gilded": 0, "name": "t3_3bipzt", "num_comments": 10, "score": 1, "selftext": "So because cookie authentication is being deprecated, OAuth will have to be used for authentication. But since I'm quite inexperienced when it comes to the Reddit API, PRAW, and coding in general, I just *can't* fully understand the [guide on how to use OAuth with PRAW on PRAW's website](https://praw.readthedocs.org/en/v3.0.0/pages/oauth.html). So could someone ELI5 how OAuth works and how to use it with PRAW?", "title": "Help with OAuth", "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "TerminalCase", "created_utc": 1435545230, "gilded": 0, "name": "t3_3bgtlp", "num_comments": 6, "score": 1, "selftext": "Using PRAW, is there a way to search all subreddits in a given multireddit? E.g., if I want to search for every post made within a given time period on all subs in the [nationalphotosubs multi](http://www.reddit.com/user/I_AM_STILL_A_IDIOT/m/nationalphotosubs), is there a way to do it?", "title": "How can I search a multi with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "dado3212", "created_utc": 1435512286, "gilded": 0, "name": "t3_3bf2qz", "num_comments": 2, "score": 1, "selftext": "I'm trying to extract some data from my subreddit /r/mathriddles, but I don't know if it's possible. The way the flairing is set up, problems can be flaired with a difficulty setting, and then can be changed to solved, so you can easily see what problems are still open. I was curious if there was a way to see when the flair was changed, or a list of the flairs (and dates) that it's had. Any ideas if this is possible in general with Reddit, or more specifically with PRAW?", "title": "Getting History of Link Flair", "url": "https://www.reddit.com/r/redditdev/comments/3bf2qz/getting_history_of_link_flair/"}, {"author": "_thelichking_", "created_utc": 1435507243, "gilded": 0, "name": "t3_3betip", "num_comments": 6, "score": 1, "selftext": "Python. So I have the following code: submissions = r.get_subreddit(targetSubreddit).get_top_from_all(limit=10) for submission in submissions: if contname in ???: here contname is a keyword which the user gives as an input, and I would like to check for that word in the name of the submission,is there any object( which i can put in place of ???) in PRAW which does this? Like submission.url checks the url of the submission,so something like that? I already checked the docs but couldn't find it.", "title": "[PRAW] [PYTHON] Reading keywords in submission name using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/3betip/praw_python_reading_keywords_in_submission_name/"}, {"author": "avinassh", "created_utc": 1435419943, "gilded": 0, "name": "t3_3bb8tw", "num_comments": 18, "score": 1, "selftext": "I am using [Praw Oauth wrapper](https://www.reddit.com/r/botwatch/comments/38k30h/oauth2util_a_wrapper_around_praws_oauth2/). Now problem is, the script asks access to my bot account everytime I run it or whenever `access token` is expired (I guess) So, how do I make it permanent setting? The app appears in https://www.reddit.com/prefs/apps/ but why it does asks me to authorize everytime?", "title": "How do I give access of my bot account to my script, permanently (OAuth)?", "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "codsane", "created_utc": 1435210819, "gilded": 0, "name": "t3_3b1jry", "num_comments": 2, "score": 3, "selftext": "I was wondering if this possible? I am posting news articles to my own subreddit and sometimes users post before me, I would like to check and see if the link has already been submitted. I tried adding an exception for the praw already submitted error but it does not accomplish what I am looking for. Thanks!", "title": "Check if link already submitted to a subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/3b1jry/check_if_link_already_submitted_to_a_subreddit/"}, {"author": "BananaGranola", "created_utc": 1435154760, "gilded": 0, "name": "t3_3aybk8", "num_comments": 9, "score": 3, "selftext": "I'm getting this. The same code has worked fine until I updated praw yesterday. Does anyone know what I'm doing wrong? Does it have to do with the recent HTTPS requirement? Thanks! > Traceback (most recent call last): File \"./linkrav_bot.py\", line 128, in main() File \"./linkrav_bot.py\", line 83, in main reddit.login(reddit_username, reddit_password) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 88, in wrapped return function(self, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 1321, in login self.request_json(self.config['login'], data=data) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 170, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 569, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 413, in _request response = handle_redirect() File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 383, in handle_redirect timeout=timeout, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 147, in wrapped result = function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 102, in request allow_redirects=False) File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 573, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 370, in send timeout=timeout File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\", line 349, in _make_request conn.request(method, url, **httplib_request_kw) File \"/usr/lib/python2.7/httplib.py\", line 1001, in request self._send_request(method, url, body, headers) File \"/usr/lib/python2.7/httplib.py\", line 1035, in _send_request self.endheaders(body) File \"/usr/lib/python2.7/httplib.py\", line 997, in endheaders self._send_output(message_body) File \"/usr/lib/python2.7/httplib.py\", line 850, in _send_output self.send(msg) File \"/usr/lib/python2.7/httplib.py\", line 826, in send self.sock.sendall(data) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 208, in sendall sent = self._send_until_done(data) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 198, in _send_until_done return self.connection.send(data) File \"/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py\", line 947, in send raise TypeError(\"data must be a byte string\") TypeError: data must be a byte string", "title": "Login error", "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "Spedwards", "created_utc": 1435148809, "gilded": 0, "name": "t3_3ay1c9", "num_comments": 4, "score": 3, "selftext": "So I'm trying to get my scripts completely converted to a OAuth login however I have some questions. Currently, I have set the `client_id`, `client_secret`, and `redirect_url` and then in IDLE, used that to get the access key which is requested from the script. I'm looking at the docs and they have the following as an example config file: [bboe] domain: www.reddit.com ssl_domain: ssl.reddit.com user: bboe pswd: this_isn't_my_password [reddit_dev] domain: www.reddit.com ssl_domain: ssl.reddit.com user: someuser pswd: somepass [local_dev1] domain: reddit.local:8000 user: someuser pswd: somepass [local_wacky_dev] domain: reddit.local:8000 user: someuser pswd: somepass api_request_delay: 5.0 default_content_limit: 2 So I looked in mine and found their test oauth login which I'll use in this as well: [reddit_oauth_test] oauth_client_id: stJlUSUbPQe5lQ oauth_client_secret: iU-LsOzyJH7BDVoq-qOWNEq2zuI oauth_redirect_uri: https://127.0.0.1:65010/authorize_callback So I copied that below and changed the information for my script but I have no idea how to actually use it. Also, what does it change exactly? I noticed that a `oauth_refresh_token` field was added in the latest version but I don't even know how to get that! My current script: # prompted for ACCESS_KEY above r = praw.Reddit('My UserAgent') r.set_oauth_app_info(CLIENT_ID,CLIENT_SECRET,REDIRECT) access_information = r.get_access_information(ACCESS_KEY) r.set_access_credentials(**access_information) import time while True: try: r.refresh_access_information(access_information['refresh_token']) # do stuff time.sleep(300) except KeyboardInterrupt as e: print('Exiting...') sys.exit(-1) Is it possible to modify it so I don't have to get the access token every time I reboot the script? This whole thing is giving me the shits.", "title": "[PRAW] OAuth Login", "url": "https://www.reddit.com/r/redditdev/comments/3ay1c9/praw_oauth_login/"}, {"author": "BnMcGi", "created_utc": 1434917754, "gilded": 0, "name": "t3_3amxbp", "num_comments": 2, "score": 0, "selftext": "I've followed the tutorial at the Praw docs (http://praw.readthedocs.org/en/latest/pages/oauth.html) to get an initial access token and refresh token. I've then taken this refresh token and added it to my script, adding the praw configuration values for oauth_client_id, oauth_client_secret and oauth_redirect_uri. I'm lead to believe I should just be able to call \"self.reddit.refresh_access_information(self.oauth_refresh_token)\" to get a new access token to use, but this is raising a \"requests.exceptions.HTTPError: 400 Client Error: Bad Request\" exception... Any ideas what I'm doing wrong?", "title": "PRAW and refresh tokens", "url": "https://www.reddit.com/r/redditdev/comments/3amxbp/praw_and_refresh_tokens/"}, {"author": "TeroTheTerror", "created_utc": 1434744914, "gilded": 0, "name": "t3_3afuui", "num_comments": 4, "score": 1, "selftext": "Using praw, uploading 80K+ flairs and it errors out most times, it seems to work fine in smaller batches and I was wondering if anyone knows the upper limit? I tried checking readthedocs for praw and didn't see any number mentioned.", "title": "Limit on r.set_flair_csv?", "url": "https://www.reddit.com/r/redditdev/comments/3afuui/limit_on_rset_flair_csv/"}, {"author": "GoldenSights", "created_utc": 1434685736, "gilded": 0, "name": "t3_3ad6j0", "num_comments": 10, "score": 4, "selftext": "In my attempts to determine the character limit for wiki pages, I've found that they do not have a hard ceiling like submissions, comments, messages, etc. My subreddit is /r/GoldTesting. I've found that on the wiki page called [75000](/r/goldtesting/wiki/75000), I can submit a maximum of 511,873 characters. On the page [7500](/r/goldtesting/wiki/7500), I can submit a maximum of 511,874 characters. For page [7](/r/goldtesting/wiki/7), that limit goes up to 511,877... but that's only when I'm in the browser. When I use PRAW, the limits are 511,891, 511,892, and 511,895 respectively. For every byte that comes off of the page name, I can include one more in the text. This tells me that the limit is based on the **total size of the HTTP request**, because Chrome obviously sends more headers. This means that different pages on different subreddits, edited using different clients will experience a different character cap. I understand that Rule #1 of reddit's API is that it's inconsistent, but this is a little extreme. I tried to follow the source code to see if there's a number anywhere, but the closest I got was [here](https://github.com/reddit/reddit/blob/c6f959504466333c0d7d51c131240473aaf78b04/r2/r2/models/wiki.py#L375). The special_length_restriction_bytes dict doesn't have a number for this, so it falls back on [pylons.g.wiki_max_page_length_bytes](https://github.com/reddit/reddit/blob/c6f959504466333c0d7d51c131240473aaf78b04/r2/r2/models/wiki.py#L47) but I don't know what pylons is or where it comes from. Can wiki pages please receive a standard character limit? And perhaps an error message when editing over the limit in the browser (Currently the saving wheel just spins around, then stops)? It's not common for wiki pages to become this large, but there is definitely some information missing for those who need it.", "title": "Why do wiki pages have an arbitrary character ilmit?", "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "ErrorX", "created_utc": 1434548965, "gilded": 0, "name": "t3_3a5v11", "num_comments": 2, "score": 5, "selftext": "So, I have been using PRAW and the r.Login() function for my bot, which I see is being depreciated. Trying to figure out how to use OAuth to do the same thing. I got the token and I can get the comments and parse the data I need, but I'm confused at how to reply to the comments. I see that you need to make a call to get submit scope, but I'm not sure how to go about that. Once you get the scope, how do you actually reply?", "title": "OAuth2 Authorization for Python", "url": "https://www.reddit.com/r/redditdev/comments/3a5v11/oauth2_authorization_for_python/"}, {"author": "norbornyl", "created_utc": 1434349849, "gilded": 0, "name": "t3_39vzmt", "num_comments": 2, "score": 4, "selftext": "The school gives us some space on their Apache servers. I'm using mine to host my website and a Python script that processes some text, and I'd like to be able to feed this script some reddit comments. I'm just unsure how to install custom Python modules. I tried pasting the praw directory into my folder but obviously that didn't work because of the various other modules praw imported. Thanks for reading.", "title": "How to install PRAW on my personal webspace on my University's servers?", "url": "https://www.reddit.com/r/redditdev/comments/39vzmt/how_to_install_praw_on_my_personal_webspace_on_my/"}, {"author": "MrJohz", "created_utc": 1434207581, "gilded": 0, "name": "t3_39pmbl", "num_comments": 13, "score": 2, "selftext": "Are there any instructions on how to use the script-base OAuth system? Is PRAW set up to allow that, or do I still need to use the older login API?", "title": "[PRAW] Guide to Script-base OAuth", "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "godlikesme", "created_utc": 1433880189, "gilded": 0, "name": "t3_397dng", "num_comments": 1, "score": 1, "selftext": "Hi, I'm using praw to automatically approve submissions of certain shadowbanned users(those submissions are great and very relevant to the subreddit). The code stopped working a few weeks ago. IIRC my code stayed the same. Was there any changes in api regarding approving submissions? Relevant issue on praw github: https://github.com/praw-dev/praw/issues/422", "title": "Approving submissions of shadowbanned users doesn't work using praw/API", "url": "https://www.reddit.com/r/redditdev/comments/397dng/approving_submissions_of_shadowbanned_users/"}, {"author": "raymestalez", "created_utc": 1433784256, "gilded": 0, "name": "t3_391vlc", "num_comments": 6, "score": 5, "selftext": "Hi! I want to make some sort of interface for /r/writingprompts, so that users of my website could write a story(on my website) and it would automatically get submitted as a comment(from the user's account) to a thread on /r/writingprompts. Is it possible to do that with praw? And is it okay with reddit's TOS? Edit: This is what I was looking for - http://praw.readthedocs.org/en/latest/pages/oauth.html", "title": "Can I automatically submit comments to reddit using praw?", "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "Xeran_", "created_utc": 1433767559, "gilded": 0, "name": "t3_390uf4", "num_comments": 2, "score": 2, "selftext": "Is there a way to get the sidebar contents of a subreddit to which you don't have moderator access to it using praw? I know one can retrieve the sidebar contents (if you're a mod of the specific sub) using: r.get_settings(SUBREDDITNAME)['description'] I suppose the problem can be worked around it by scraping the http response of the subreddits about page. However, this adds at least one unnecessary dependency for a python module as well as there is most likely a better way of doing it. If it is not possible to retrieve the sidebar as a non-moderator in a normal way using the praw reddit api, then my question would be: why not? Is there some reason behind this?", "title": "PRAW retrieve sidebar contents as non-moderator", "url": "https://www.reddit.com/r/redditdev/comments/390uf4/praw_retrieve_sidebar_contents_as_nonmoderator/"}, {"author": "haiguise1", "created_utc": 1433465402, "gilded": 0, "name": "t3_38lo61", "num_comments": 5, "score": 6, "selftext": "I've been having some trouble getting OAuth to work without using a browser to get the code (step 3 in the [tutorial](http://praw.readthedocs.org/en/latest/pages/oauth.html)). What I want to do get the url code (for r.get_access_information(url code)) which will then allow me to get an access and refresh token for my script through praw so that it uses OAuth instead of user/password. I have been trying to write some code using the requests library and I can get an access token, but I can't seem to get praw to work with it, and I can't get the url code with which I can get a refresh token and so hopefully let praw use the refresh token to set itself up correctly but I haven't gotten that far yet. So in short, I want to either get a url code or a refresh token without having to open the Allow page in a browser.", "title": "PRAW and OAuth", "url": "https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/"}, {"author": "picflute", "created_utc": 1433398237, "gilded": 0, "name": "t3_38hgnu", "num_comments": 9, "score": 3, "selftext": "Context: I'm looking to archive as much modmail as I can get. Currently with PRAW I am able to get 6,668 ModMail Links using modmail = r.get_mod_mail('leagueoflegends ,params = None,limit = None) Problem is that it's not enough. There's information I wish to access farther back but do not know how to get there. Any advice on how to do it with PRAW or maybe even a basic demo of how to do it in the reddit API?", "title": "How to deep dive into ModMail Archives?", "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "i-am-you", "created_utc": 1433384271, "gilded": 0, "name": "t3_38gokh", "num_comments": 8, "score": 3, "selftext": "I want to submit a self post with an empty \"\" body, but praw seems to complain. Here is the specific error: if bool(text) == bool(url): raise TypeError('One (and only one) of text or url is required!')", "title": "[PRAW] How to submit empty self post?", "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "obamabot9000", "created_utc": 1433091124, "gilded": 0, "name": "t3_37z2ql", "num_comments": 5, "score": 1, "selftext": "I have created the ObamaBot (See my post in /r/botwatch), and he is made with Python in PRAW. He is set to search all subreddits for \"Thanks obama,\" (and respond with \"You're welcome!\")but he doesn't respond to all comments. Sometimes he will reply to one, but other times, he won't. He responded to one comment in a random thread, but I never saw him respond to anything else. If I tell him to search /r/botwatch or any other independent subreddit, he works fine. Why is this, and how can I fix it? I am relatively noonish, so bear with me. The full code is available upon request.", "title": "ObamaBot not responding to all posts", "url": "https://www.reddit.com/r/redditdev/comments/37z2ql/obamabot_not_responding_to_all_posts/"}, {"author": "habnpam", "created_utc": 1432941079, "gilded": 0, "name": "t3_37s4or", "num_comments": 8, "score": 2, "selftext": "I am able to edit the wiki page just fine if I use the old login `r.login(\"name\", \"pass\")`. But when I try to edit the Wiki page using OAuth, I get this error: `requests.exceptions.HTTPError: 403 Client Error: Forbidden`. I allowed the full scope [from the OAuth tutorial page](http://praw.readthedocs.org/en/latest/pages/oauth.html), and also [from this post](http://www.reddit.com/r/redditdev/comments/1wxkbb/oauth2_wiki_scopes/). When I go to `/prefs/apps/`, it does show that I gave the script permission to edit the wiki. The subreddit I am trying to edit is set to private, but anyone with access should be able to edit the wiki. Here is some of the code(edit: I put in the log-in stuff): r = praw.Reddit(user_agent=\"************* /u/habnpam\") r.set_oauth_app_info(client_id='[client_id]', client_secret='[client_secret]', redirect_uri='http://127.0.0.1:65010/authorize_callback') # the function will return a new 'access_token' refreshed = r.refresh_access_information('[refresh_token]') #\"log in\" with the credentials. r.set_access_credentials(access_token=refreshed['access_token'], refresh_token=refreshed['refresh_token'], scope=refreshed['scope']) wiki = r.get_wiki_page(\"habnpam\", 'index')#This causes the error. new_content = wiki.content_md.encode('utf-8') #this changes the unicode to string...I think. print(new_content) #check point. new_content = new_content + \"\\n\\nOauth initialized GOOD TO GO\" #updating the content. json = r.edit_wiki_page(\"habnpam\", page='', content=new_content, reason='') #making the edit. print(\"Oauth Successful.\")", "title": "How can I edit wiki while using OAuth?", "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "gimunu", "created_utc": 1432931292, "gilded": 0, "name": "t3_37riu0", "num_comments": 4, "score": 4, "selftext": "Hi guys, I am a teaching assistant in Maths with a good interest in data science. I have been thinking about an illustration for my students and for that I would need data from reddit, basically sampling in a non-biased way the comment karma distribution. I coded a small script yesterday, not using PRAW but primitively reading the comment karma score on the user page and then jumping to a new user taken from a queue. The queue is itself fed on the user page as I extract every possible user name from there. I have a doubt about biases in this sampling, but I can already tell that it is quite inefficient: in a day I only got 30 000 user inputs. Could there be a smarter way of getting access to such information? ps: sorry if this is not the right place to ask this", "title": "Scraping comment karma data in an efficient way", "url": "https://www.reddit.com/r/redditdev/comments/37riu0/scraping_comment_karma_data_in_an_efficient_way/"}, {"author": "Day_Old_Pizza", "created_utc": 1432905222, "gilded": 0, "name": "t3_37psu6", "num_comments": 2, "score": 1, "selftext": "I get the unexpected redirect from http to https error when I try to login to reddit through my python praw script. I tried to run the following from [this post from /u/bboe](https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/): pip install git+git://github.com/praw-dev/praw.git@master but it gave me an error too: Error [WinError 2] The system cannot find the file specified while executing command git clone -q git://github.com/pra w-dev/praw.git C:\\Users\\<username>\\AppData\\Local\\Temp\\pip-uxo4pebj-build Cannot find command 'git' Please help. Thanks.", "title": "New to using praw and pip", "url": "https://www.reddit.com/r/redditdev/comments/37psu6/new_to_using_praw_and_pip/"}, {"author": "1ama", "created_utc": 1432880388, "gilded": 0, "name": "t3_37ow7j", "num_comments": 4, "score": 1, "selftext": "Hello, I want to get submissions from the domain - *https://www.reddit.com/domain/imgur.com* with Python using PRAW. When I want to get subreddit I use this code: submissions = r.get_subreddit('ampoll').get_hot(limit=5) Is there method to get domain?", "title": "How to get domain instead of get subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/37ow7j/how_to_get_domain_instead_of_get_subreddit/"}, {"author": "bboe", "created_utc": 1432713690, "gilded": 0, "name": "t3_37foul", "num_comments": 1, "score": 4, "selftext": "It would appear that maybe the field could be useful for using a more appealing title than what isn't available in a URL. However, at the moment it appears there is no way through reddit's website to actually make these differ, and when doing it manually through the API there appears to be no visual use of `display_name`? Example where `name` and `display_name` differ: https://api.reddit.com/api/multi/user/PyAPITestUser2/m/praw_32dm76iiu0/", "title": "Why do multireddit's have a separate `display_name` field?", "url": "https://www.reddit.com/r/redditdev/comments/37foul/why_do_multireddits_have_a_separate_display_name/"}, {"author": "thomasbomb45", "created_utc": 1432696823, "gilded": 0, "name": "t3_37eyjb", "num_comments": 2, "score": 2, "selftext": "Currently, when you initialize a comment or submission stream (using praw.helpers), it must load up to the limit set or the earliest item is reached before any results are yielded. This is a problem if you don't set a limit and use multiple subreddits in one stream, because it loads 1000 per subreddit. Is there a setting that allows me to access them as they are loaded? I'm guessing this results from those helper functions returning oldest first.", "title": "Praw comment/submission streams", "url": "https://www.reddit.com/r/redditdev/comments/37eyjb/praw_commentsubmission_streams/"}, {"author": "jbw976", "created_utc": 1432614878, "gilded": 0, "name": "t3_37ah9n", "num_comments": 7, "score": 2, "selftext": "Using praw and inspecting comment objects from the comment_stream function, I see a subreddit_id field for comments such as: 'subreddit_id': u't5_2r8lo' how can i convert that to a subreddit name, such as /r/dragonage? I could not figure this out after a few searches of this subreddit and the internet at large. thanks!", "title": "convert subreddit_id to subreddit name?", "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "AnneBancroftsGhost", "created_utc": 1432588747, "gilded": 0, "name": "t3_378zz9", "num_comments": 4, "score": 3, "selftext": "I'm not new to coding, but I am new to PRAW/python/php. Please bear with me. :) I've been wanting to learn how to code/run a reddit bot for fun and for experience. I've been spending this afternoon in that rabbit hole trying to figure it out. So I've been following the instructions here (https://www.reddit.com/r/redditdev/comments/2bc04n/how_to_build_a_reddit_bot_noob_friendly/) to try and make the bot. The basic premise is that I would like someone to be able to call the bot (using the karmalytics service to search and report the call statement to the bot) and then send the standard reply to the parent comment. And also send a second reply to the keyworded comment, letting them know a reply has been sent to the parent comment (though I'm pretty sure I have a handle on this part). I went looking for a bank of keywords so that I could figure it out myself. But I'm stuck. Maybe I'm just asking the wrong questions. Anyway, how would you go about it? Here's the sample code that will reply to the keyworded comment. \"; if(!isset($_POST['redditAlertJson'])){ die('Missing redditAlertJson'); } $redditAlertJson = $_POST['redditAlertJson']; $redditAlert = json_decode($redditAlertJson); if($redditAlert->webhookKey!=$webhookKey){ die('Wrong webhookKey'); } require 'reddit.php'; $reddit = new reddit($redditUsername,$redditPassword); $response = $reddit->addComment($redditAlert->reddit->name,'\u30fd\u0f3c\u0e88\u0644\u035c\u0e88\u0f3d\uff89'); var_dump($response); exit;", "title": "Reply to parent comment.", "url": "https://www.reddit.com/r/redditdev/comments/378zz9/reply_to_parent_comment/"}, {"author": "BananaGranola", "created_utc": 1432357758, "gilded": 0, "name": "t3_36yk6c", "num_comments": 3, "score": 1, "selftext": "My bot suddenly stopped seeing child comments. I check the authors of the child comments to see whether my bot has already responded to a given comment. Given a valid **comment** object, **comment.replies** used to return a list of child comments. As of this morning, > prettyprint(vars(comment)) says, among other things, > '_replies': [] when I know there are child comments. A check of the [json](http://api.reddit.com/r/knitting/comments/36qke2/looking_for_suggestions_for_a_quickish_a_50s/crg8gda) reveals that the replies are showing up in the json returns. They're just not showing up in praw. Did something change? Am I doing something wrong? Code is [here](https://github.com/bananagranola/LinkRav_Bot/blob/master/linkrav_bot.py).", "title": "comment.replies returns empty list", "url": "https://www.reddit.com/r/redditdev/comments/36yk6c/commentreplies_returns_empty_list/"}, {"author": "Fi8_CoC", "created_utc": 1432335536, "gilded": 0, "name": "t3_36xioo", "num_comments": 3, "score": 2, "selftext": "I have a script which I run weekly with cron. It makes a post to a subreddit every week, and I would like that post distinguished (yes the bot does have mod permissions). The post is made successfully each time, but I get an error with the distinguish line below. I have tried my best to look at the docs as well as other examples. #!/usr/bin/env python import praw r = praw.Reddit(user_agent=\"My user agent\") r.login(\"USER\", \"PASS\") sub = r.get_subreddit(\"SUBREDDIT\") sub.submit(\"Title of post\", text='''Long winded body of the post''') ## Here is what I thought should make the post distinguished sub.distinguish(as_made_by=u'mod') Any help would be greatly appreciated. Thanks.", "title": "[PRAW] How can my weekly autopost script distinguish the post?", "url": "https://www.reddit.com/r/redditdev/comments/36xioo/praw_how_can_my_weekly_autopost_script/"}, {"author": "DarkMio", "created_utc": 1432278565, "gilded": 0, "name": "t3_36unvz", "num_comments": 2, "score": 1, "selftext": "Currently I am developing a nice plugin-based bot framework, which I wanted to feed some test-data. Naive as I am, I thought: Great, lets simply pickle some stuff from praw, without any success. I am hitting the recursion limit (setting it higher doesn't resolve the problem ^(or crashes python)). This is what I have done and tried so far: \"\"\"Looks like PRAW doesn't want to get pickle'd. Both single submissions and generators are indeed broken.\"\"\" from praw import Reddit import pickle from sys import setrecursionlimit from time import time from os import listdir def get_session(): return Reddit(user_agent=\"Data Pickler for data training.\") def get_submissions(r, subreddit): return r.get_subreddit(subreddit).get_hot(limit=1) def get_single_submission(generator): for sub in generator: return sub def write_reddit(obj, path): with open(path, \"wb\") as f: pickle.dump(obj, f) def load_reddit(path): with open(path, \"rb\") as f: return pickle.load(f) def single_submission(r): return r.get_submission(url='http://www.reddit.com/r/thebutton/comments/36salf/') if __name__ == \"__main__\": r = get_session() # Get hot: submissions = get_submissions(r, \"dota2\") # Return first of hot: submission = get_single_submission(submissions) # Get a singular submission via URL: single_submission = single_submission(r) carelist = [submissions, submission, single_submission] for pls_pickle in carelist: try: write_reddit(pls_pickle, \"../t_data/{}.pi\".format(int(time()))) except Exception as e: print(\"Writing error: {}\".format(e)) setrecursionlimit(10000) for file in listdir(\"../t_data/\"): try: print(load_reddit(\"../t_data/{}\".format(file))) except Exception as e: print(\"Loading error: {}\".format(e)) yields: Writing error: Can't pickle : attribute lookup generator on builtins failed Loading error: maximum recursion depth exceeded Loading error: maximum recursion depth exceeded This is in Python 3.4.3, I'll edit all errors once it's done executing again (as trying pickling a single loaded submission takes a ton of time. Edit: Just to be clear: I actually mostly care about having some training data to develop offline, while having no internet whatsoever. If there is an alternative way (storing some json and saving it - then reading it with praw, or importing a single stored api-request would work for me too.", "title": "Unable to pickle single submissions / comment with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/36unvz/unable_to_pickle_single_submissions_comment_with/"}, {"author": "Agent_HK-47", "created_utc": 1432173585, "gilded": 0, "name": "t3_36pfcm", "num_comments": 2, "score": 2, "selftext": "In the praw docs there is a section [here](https://praw.readthedocs.org/en/v2.1.21/pages/configuration_files.html#the-sites) on configuring praw. At the end it has this code saying that they can define multiple \"sites\" in the config. However there is no explanation of how the proper \"site\" is selected. Does anyone know more about this topic? How is [bboe] chosen over [reddit_dev]. [bboe] domain: www.reddit.com ssl_domain: ssl.reddit.com user: bboe pswd: this_isn't_my_password [reddit_dev] domain: www.reddit.com ssl_domain: ssl.reddit.com user: someuser pswd: somepass [local_dev1] domain: reddit.local:8000 user: someuser pswd: somepass [local_wacky_dev] domain: reddit.local:8000 user: someuser pswd: somepass api_request_delay: 5.0 default_content_limit: 2", "title": "[PRAW] How is the correct site selected in praw.ini", "url": "https://www.reddit.com/r/redditdev/comments/36pfcm/praw_how_is_the_correct_site_selected_in_prawini/"}, {"author": "Agent_HK-47", "created_utc": 1432092392, "gilded": 0, "name": "t3_36kwbq", "num_comments": 2, "score": 1, "selftext": "I've looked through the docs and I even grepped through the open source code and I couldn't find the `praw.Reddit()` method. Could anyone tell me where the documentation for this method is?", "title": "[PRAW] Where is the praw.Reddit() method defined?", "url": "https://www.reddit.com/r/redditdev/comments/36kwbq/praw_where_is_the_prawreddit_method_defined/"}, {"author": "ibbignerd", "created_utc": 1432011470, "gilded": 0, "name": "t3_36ggaz", "num_comments": 3, "score": 2, "selftext": "Hey guys. Very basically I'm trying to check if my bot already replied to a comment. I thought the easiest way to do this would be to check the authors of it's children. Is there a better way to do this? If not, how would I easily get the children authors. I'm using Praw on Python 2.7!", "title": "Getting child comments from parent comment object", "url": "https://www.reddit.com/r/redditdev/comments/36ggaz/getting_child_comments_from_parent_comment_object/"}, {"author": "lizardsrock4", "created_utc": 1431823713, "gilded": 0, "name": "t3_367w2j", "num_comments": 2, "score": 1, "selftext": "Is there any way to add a ban reason and length in praw using add_ban(), I only one param for the username Thanks!", "title": "[PRAW] Ban reason and length?", "url": "https://www.reddit.com/r/redditdev/comments/367w2j/praw_ban_reason_and_length/"}, {"author": "Arnoyo12", "created_utc": 1431514020, "gilded": 0, "name": "t3_35tfe9", "num_comments": 5, "score": 2, "selftext": "Hi guys, Sorry to bother you with such a basic issue but I'm a beginner. I'm basically trying to retrieve the max nr (200) of comments per threads I choose. From what I understand you can simply do so by specifying your submission details and associating it with the actual post. However, when I try to print the ouput after iterating over the comments, I get the following: for every comment. this is the code I used: import praw import pprint user_agent = (\"Comment breakdown 1.0 by /u/Arnoyo12\") r = praw.Reddit(user_agent=user_agent) submission = r.get_submission(submission_id='34pp8j') thing_limit = 100 user_name = \"Arnoyo12\" comment = {} for thing in submission.comments: pprint.pprint(thing) Thanks in advance!", "title": "[PRAW] Noob issue while retrieving comments from a thread", "url": "https://www.reddit.com/r/redditdev/comments/35tfe9/praw_noob_issue_while_retrieving_comments_from_a/"}, {"author": "treacherous_tim", "created_utc": 1431453550, "gilded": 0, "name": "t3_35qi47", "num_comments": 1, "score": 1, "selftext": "So this is for a command. If someone types the command, I need to take the text of the comment above that command. Any ideas how to do this? I'm pretty new to PRAW. Thanks!", "title": "[PRAW] How to reference a comment above", "url": "https://www.reddit.com/r/redditdev/comments/35qi47/praw_how_to_reference_a_comment_above/"}, {"author": "Toofifty", "created_utc": 1431410798, "gilded": 0, "name": "t3_35okwm", "num_comments": 2, "score": 2, "selftext": "I'm not too experienced with PRAW, but I know there's a few ways to go about this. I just need to get new comments in one particular thread as fast as possible (within a few seconds). I could use `comment_stream`, `get_comments`, or `submission.comments` but I'm unsure which is the best for my case. Thanks :)", "title": "[PRAW] What's the best way to get comments as fast as possible in a single thread/submission?", "url": "https://www.reddit.com/r/redditdev/comments/35okwm/praw_whats_the_best_way_to_get_comments_as_fast/"}, {"author": "opsoyo", "created_utc": 1431231984, "gilded": 0, "name": "t3_35got2", "num_comments": 8, "score": 1, "selftext": "I'm trying my hardest to capture the replies for a comment, process the given replies, then request more replies for that comment as the MoreComments object appears, but I can't for the life of me figure out how to do it. **What I have:** submission = r.get_submission(submission_id=XXXXX) ('XXXXX' is defined elsewhere in file) for c in submission.comments: # Iterate comments within specified post if isinstance(c, praw.objects.MoreComments): # Bypass for prototype continue if comment.body == X: # Iterate replies for specified comment ('X' is defined elsewhere in file) replies = comment.replies # Gives list, which doesn't respond to 'replace_more_comments' replies.replace_more_comments(limit=None, threshold=0) # Returns error of no attribute **What I'm looking to accomplish:** submission = r.get_submission(submission_id='XXXXX') for c in submission.comments: # Iterate comments within specified post if isinstance(c, praw.objects.MoreComments): # Bypass for prototype continue if comment.body == X: # Iterate replies for specified comment # for reply in replies # try: # Mess with first reply # Mess with second reply # Mess with hundredth reply # except AttributeError: # Manipulate MoreComments when done with already given replies And, I'm really trying to not use 'replace_more_comments' where I don't *have* to. I'd like to keep it isolated to running it on that particular comment to get its replies when it comes time. Possible? Am I asking too much? Am I flat wrong? Please, any help is appreciated! :)", "title": "How to use replace_more_comments with replies in PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "hizinfiz", "created_utc": 1431052945, "gilded": 0, "name": "t3_358zpt", "num_comments": 4, "score": 3, "selftext": "Hello, I have python, pip, praw, and six installed on my Macbook but when I try to `import praw` in Python 3.4.3, I get Traceback (most recent call last): File \"\", line 1, in ImportError: No module named 'praw' Interestingly enough, I'm able to import it just fine in Python 2.7.7 (OS X default install of Python) Software: * OS X 10.10.3 * Python 2.7.7 * Python 3.4.3 * pip 6.1.1 * praw 2.1.21 * six 1.9.0", "title": "Having trouble getting PRAW to work with Python 3", "url": "https://www.reddit.com/r/redditdev/comments/358zpt/having_trouble_getting_praw_to_work_with_python_3/"}, {"author": "SpotiList", "created_utc": 1431047037, "gilded": 0, "name": "t3_358osw", "num_comments": 3, "score": 3, "selftext": "I probably have a skewed view of oauth, as I'm pretty new to all this, but is there a reason I shouldn't put my refresh token in praw.ini? I have a bot that isn't on 24/7 and only ever uses one reddit account. I wanted it to be able to start up and just do: import praw r = praw.Reddit('UA-string') r.refresh_access_information() and go about my business. It was just a couple of lines changed in \\_\\_init__.py to make this work, but I'm wondering if there's a reason the functionality isn't in there to begin with. Thanks.", "title": "refresh_token and praw.ini", "url": "https://www.reddit.com/r/redditdev/comments/358osw/refresh_token_and_prawini/"}, {"author": "redalastor", "created_utc": 1431045517, "gilded": 0, "name": "t3_358lu6", "num_comments": 3, "score": 1, "selftext": "I'd like to know which praw or requests exception I can get when reddit is busy or down so I can have those requests tried again a few seconds later.", "title": "What exception may I expect when reddit is \"busy\"", "url": "https://www.reddit.com/r/redditdev/comments/358lu6/what_exception_may_i_expect_when_reddit_is_busy/"}, {"author": "WheelsOnPavement", "created_utc": 1431039640, "gilded": 0, "name": "t3_358amz", "num_comments": 7, "score": 2, "selftext": "Not sure why this has suddenly started happening... I'm fairly new to bot creation, and am, quite frankly, stumped. It was working totally fine, but now every time I try to post a reply it gives me a 403. I don't have the word \"bot\" inside of the user_agent, and I've been grabbing posts at a very low rate. The code is a mishmash of some tutorials and (mostly) my own writing. I am 90% positive it's a 403- all other bots that I've tried making return the same result as of now. Similarly, there was no difference when attempting to run the script on another machine. I'm hoping somebody can shed some light on the situation... I'd hate to think I was put on some sort of a spam blacklist. Maybe I made a mistake and am missing something? Here's the bot that --was-- working: import praw import time import os import sqlite3 sql = sqlite3.connect('sql.db') cur = sql.cursor() print (\"Logging in...\") r = praw.Reddit(user_agent=\"BB by /u/WheelsOnPavement\") r.login(\"-----\", \"-----\") print (\"Sucessfully logged in!\") time.sleep(2) print(\"Writing SQL...\") cur.execute('CREATE TABLE IF NOT EXISTS oldposts(id TEXT)') sql.commit() print(\"SQL written successfully!\") time.sleep(2) os.system('cls') words_to_match = ['bagels', 'bagel', 'cream cheese', 'cream-cheese'] postLimit = 50 def run_bot(): subreddit = r.get_subreddit(\"-----\", \"bottest3000\") print(\"Fetching comments...\") comments = subreddit.get_comments(limit=postLimit) print(\"Successfully fetched comments!\") print(\"\") for comment in comments: comment_text = comment.body.lower() cur.execute('SELECT * FROM oldposts WHERE ID=?', [comment.id]) try: comment.id = comment.author.name except AttributeError: print('Comment has been previously deleted') continue if cur.fetchone(): print(\"Comment already in database\") continue elif comment.author.name == '-----': print(\"Not replying to posts already written by -------\") continue elif any(string in comment_text for string in words_to_match): try: print('Replying to %s by %s' % (comment.id, comment.author)) comment.reply('You have summoned the bagel gods! Bask in their glory!') cur.execute('INSERT INTO oldposts VALUES(?)', [comment.id]) except praw.requests.exceptions.HTTPError as e: if e.response.status_code == 403: print('403 FORBIDDEN - is the bot banned from %s?' % comment.subreddit.display_name) continue elif e.response.status_code == 503: print('503 Service Unavailable') continue else: print('Unknown service error %s' % e.response.status.code) continue while True: run_bot() time.sleep(10)", "title": "Suddenly getting a 403 when attempting to use PRAW to reply to posts?", "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "GoldenSights", "created_utc": 1430870319, "gilded": 0, "name": "t3_3502mg", "num_comments": 2, "score": 3, "selftext": "According to the api docs, the [get unread](http://www.reddit.com/dev/api#GET_message_unread) url has a parameter called \"mark\" that should clear my orangereds, on [this line](https://github.com/reddit/reddit/blob/98ddd8cd065290e6448f1ae74c7092e28644f613/r2/r2/controllers/listingcontroller.py#L1164) I think. However, the PRAW parameter in get_unread that's supposed to trigger this doesn't seem to work. I tried it using the request client Postman and [it didn't work either](http://i.imgur.com/ZuXR4mP.png) (I also tried using GET with url parameters instead of post data). I toggled the \"mark messages as read when I open my inbox\" preference multiple times, but the unreads never go away until I visit them in my browser. Am I missing something, or is this a bug? I'm probably missing something. Thanks!", "title": "/message/unread/ does not mark my mail as read even with the \"mark\" parameter.", "url": "https://www.reddit.com/r/redditdev/comments/3502mg/messageunread_does_not_mark_my_mail_as_read_even/"}, {"author": "jordanzzz", "created_utc": 1430813040, "gilded": 0, "name": "t3_34x4ri", "num_comments": 3, "score": 1, "selftext": "I'm working with PRAW right now in Python trying to find comments that match a search term so what I'm looking for is any comment that has @Search: searchterm So right now its looking for any comment with @Search: in it, however I want to strip the comment so it removes anything before the @Search: and anything after searchterm. Any clues to help me here? Perhaps this is more of a python issue, but maybe there is soemthing within PRAW that can help. def findSummoner(): print(\"Fetching subreddit\") subreddit = r.get_subreddit(SUBREDDIT) print(\"Fetching comments\") comments = subreddit.get_comments(limit=MAXPOSTS) for comment in comments: try: cbody = comment.body.lower() if any(key.lower() in cbody for key in SETPHRASES): print(\"replying to someone\") comment.reply(cbody) else: print(\"nothing found...\") except AttributeError: pass findSummoner() This is working as expected as far as replying, but I need to only get the first word after @Search appears in the comment. =/", "title": "Parsing comments?", "url": "https://www.reddit.com/r/redditdev/comments/34x4ri/parsing_comments/"}, {"author": "demonlordghirahim", "created_utc": 1430794789, "gilded": 0, "name": "t3_34wfsa", "num_comments": 18, "score": 1, "selftext": "I got pip via \"sudo easy_install pip\" and now I'm trying to install praw but get thrown an exception and it doesn't work. This is the error I'm getting > Exception: Traceback (most recent call last): File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/basecommand.py\", line 246, in main status = self.run(options, args) File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/commands/install.py\", line 352, in run root=options.root_path, File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/req/req_set.py\", line 693, in install **kwargs File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/req/req_install.py\", line 817, in install self.move_wheel_files(self.source_dir, root=root) File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/req/req_install.py\", line 1018, in move_wheel_files isolated=self.isolated, File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/wheel.py\", line 237, in move_wheel_files clobber(source, lib_dir, True) File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/wheel.py\", line 208, in clobber os.makedirs(destdir) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 157, in makedirs mkdir(name, mode) OSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/requests' I think it's because I'm running python 3.4 and pip doesn't work with that I guess? Is there a version of pip that runs with python 3.4 or is there another problem?' edit: this is the last thing written to the terminal before it errors out > Requirement already satisfied (use --upgrade to upgrade): six>=1.4 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from praw) Installing collected packages: requests, update-checker, praw ----------------- NEVERMIND. apparently uninstalling it and using the file get-pip.py on [this page](https://pip.pypa.io/en/latest/installing.html) worked better. Although I'm now a littler nervous I fucked up my computer somehow using sudo since I know that's kind of a bad thing to do am I okay?", "title": "Trouble installing PRAW via pip", "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "BullpenInc", "created_utc": 1430729632, "gilded": 0, "name": "t3_34t0df", "num_comments": 9, "score": 1, "selftext": "Is it possible to create an account through the API? Searching I found that PRAW seems to support create_redditor but I couldn't find anything in the API documentation (pretty important to be able to do that to streamline the experience for non-experienced new users of reddit)", "title": "Creating account through API?", "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "hargikas", "created_utc": 1430636733, "gilded": 0, "name": "t3_34p5i0", "num_comments": 0, "score": 3, "selftext": "I am trying to write a small program with PRAW, and I am searching and posting to reddit greek content. I have a function: def find_if_posted(usr, title, url): for content in usr.search(title): return True for content in usr.search(url): return True return False which tries to search if there is a post with the same title or url. When it starts to search with the url I am getting the error: Traceback (most recent call last): File \"C:\\Users\\hargikas\\Dropbox\\Private\\src\\reddit\\auto_poster\\leftist.py\", line 133, in while (key is None) or find_if_posted(usr, key, content[key]): File \"C:\\Users\\hargikas\\Dropbox\\Private\\src\\reddit\\auto_poster\\leftist.py\", line 86, in find_if_posted for content in usr.search(url): File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 1108, in search **kwargs): File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 515, in get_content root = page_data.get(root_field, page_data) AttributeError: 'str' object has no attribute 'get' Also when I am trying to post something (using the submit function) I get this error: Traceback (most recent call last): File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 372, in _make_request httplib_response = conn.getresponse(buffering=True) TypeError: getresponse() got an unexpected keyword argument 'buffering' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 374, in _make_request httplib_response = conn.getresponse() File \"C:\\Python34\\lib\\http\\client.py\", line 1162, in getresponse raise ResponseNotReady(self.__state) http.client.ResponseNotReady: Request-sent During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"C:\\Python34\\lib\\site-packages\\requests\\adapters.py\", line 370, in send timeout=timeout File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 597, in urlopen _stacktrace=sys.exc_info()[2]) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\", line 245, in increment raise six.reraise(type(error), error, _stacktrace) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\packages\\six.py\", line 309, in reraise raise value.with_traceback(tb) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 374, in _make_request httplib_response = conn.getresponse() File \"C:\\Python34\\lib\\http\\client.py\", line 1162, in getresponse raise ResponseNotReady(self.__state) requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',)) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"C:\\Users\\hargikas\\Dropbox\\Private\\src\\reddit\\auto_poster\\leftist.py\", line 140, in usr.submit('greekreddit' , key, url=content[key]) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 338, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 237, in wrapped return function(obj, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 2218, in submit retry_on_error=False) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 561, in request_json retry_on_error=retry_on_error) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 402, in _request response = handle_redirect() File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 375, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\handlers.py\", line 144, in wrapped result = function(cls, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\handlers.py\", line 99, in request allow_redirects=False) File \"C:\\Python34\\lib\\site-packages\\requests\\sessions.py\", line 573, in send r = adapter.send(request, **kwargs) File \"C:\\Python34\\lib\\site-packages\\requests\\adapters.py\", line 415, in send raise ConnectionError(err, request=request) requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',)) Do you have an idea what I am doing wrong? I am using Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:43:06) [MSC v.1600 32 bit (Intel)] on win32 and PRAW Version: 2.1.21 Thank you in advance! Harry", "title": "Problems using PRAW (searching and posting)", "url": "https://www.reddit.com/r/redditdev/comments/34p5i0/problems_using_praw_searching_and_posting/"}, {"author": "manyQuestsSendHelp", "created_utc": 1430428598, "gilded": 0, "name": "t3_34g369", "num_comments": 0, "score": 1, "selftext": "I saw in the PRAW documentation that it's temporary (ref: http://praw.readthedocs.org/en/latest/pages/exceptions.html), but it's been happening all day. Anybody else getting 502 errors? Or should I worry?", "title": "Anyone else getting 502 errors with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/34g369/anyone_else_getting_502_errors_with_praw/"}, {"author": "Mackan90095", "created_utc": 1430300261, "gilded": 0, "name": "t3_349c0d", "num_comments": 2, "score": 3, "selftext": "Hi! I'm messing around with PRAW trying to get a random submission from a NSFW subreddit. The method get_random_submission(subreddit=\"aww\") works. But if I put the subreddit as \"NSFW\" it doesn't work, it just errors like so; Traceback (most recent call last): File \"redd.py\", line 4, in link = r.get_random_submission(subreddit=sys.argv[1]) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 940, in get_random_submission raise errors.ClientException('Expected exception not raised.') praw.errors.ClientException: Expected exception not raised. Why is this? Thanks \\- Mackan", "title": "[PRAW] Can't get NSFW submission", "url": "https://www.reddit.com/r/redditdev/comments/349c0d/praw_cant_get_nsfw_submission/"}, {"author": "slack101", "created_utc": 1430235605, "gilded": 0, "name": "t3_345x51", "num_comments": 1, "score": 1, "selftext": "Hi, I've been following [praw's API documentation](https://praw.readthedocs.org/en/PRAW-1.0.9/praw.html), where it mentions that the subreddit's get_comments function should be able to take a \"sort\" argument. Whenever I try the following code, I get an exception: Code: user_agent = \"Foo bot 1.0 by /u/slack101\" r = praw.Reddit(user_agent=user_agent) subredditObj = r.get_subreddit(subreddit) comments = subredditObj.get_comments(sort='top',limit=None) Exception: TypeError: get_content() got an unexpected keyword argument 'sort' Any idea what could be going wrong?", "title": "praw: Subreddit.get_comments does not take \"sort\" keyword argument", "url": "https://www.reddit.com/r/redditdev/comments/345x51/praw_subredditget_comments_does_not_take_sort/"}, {"author": "Taph", "created_utc": 1430072378, "gilded": 0, "name": "t3_33xwym", "num_comments": 5, "score": 2, "selftext": "I've written some (very) rough code for a bot that will take a reddit URL from my clipboard, find the post on reddit, and then make a post on my own subreddit that consists of the post title and link. Basically a cross-post bot. It works, but I'm getting a *ton* of errors when I run it. The last time I ran the bot the results were this: Logging in ... Bot successfully loged in. Captcha URL: http://www.reddit.com/captcha/Q16JkZEiFATLjgNvmwFucHatPqBTxN77.png Captcha: VOGKTR Traceback (most recent call last): File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 372, in _make_request httplib_response = conn.getresponse(buffering=True) TypeError: getresponse() got an unexpected keyword argument 'buffering' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 374, in _make_request httplib_response = conn.getresponse() File \"/usr/lib/python3.4/http/client.py\", line 1139, in getresponse raise ResponseNotReady(self.__state) http.client.ResponseNotReady: Request-sent During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/adapters.py\", line 370, in send timeout=timeout File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 597, in urlopen _stacktrace=sys.exc_info()[2]) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/util/retry.py\", line 245, in increment raise six.reraise(type(error), error, _stacktrace) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/packages/six.py\", line 309, in reraise raise value.with_traceback(tb) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 374, in _make_request httplib_response = conn.getresponse() File \"/usr/lib/python3.4/http/client.py\", line 1139, in getresponse raise ResponseNotReady(self.__state) requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',)) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"archive_it.py\", line 72, in main() File \"archive_it.py\", line 65, in main make_post() File \"archive_it.py\", line 59, in make_post r.submit('taphs_archive', title, url=link) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/decorators.py\", line 338, in wrapped return function(cls, *args, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/decorators.py\", line 237, in wrapped return function(obj, *args, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/__init__.py\", line 2218, in submit retry_on_error=False) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/__init__.py\", line 561, in request_json retry_on_error=retry_on_error) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/__init__.py\", line 402, in _request response = handle_redirect() File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/__init__.py\", line 375, in handle_redirect timeout=timeout, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/handlers.py\", line 144, in wrapped result = function(cls, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/handlers.py\", line 99, in request allow_redirects=False) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/sessions.py\", line 573, in send r = adapter.send(request, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/adapters.py\", line 415, in send raise ConnectionError(err, request=request) requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',)) Google hasn't turned up anything specific that I may be doing wrong to get these errors, and since the bot posts as it should (aside from not printing \"Post successful\" as it should after the post is made) I'm not really sure why I'm getting the errors. Here's the bot code: import praw, pyperclip r = praw.Reddit(user_agent = \"Reddit Archiving Manager by /u/Taph\") def reddit_login(): with open(\"credentials.txt\", \"r\") as credentials: text = credentials.read() text = text.split() username, password = text r.login(username, password) if r.is_logged_in: print(\"Bot successfully logged in.\\n\") def post_data(): post_data = [] permalink = pyperclip.paste() post = r.get_submission(permalink) title = post.title link = post.permalink post_data = title, link return post_data def make_post(): title, link = post_data() r.submit('taphs_archive', title, url=link) def main(): print(\"\\nLogging in ...\") reddit_login() make_post() print(\"Post successful.\") if __name__ == \"__main__\": main() I'm sure this is something simple, but I don't have a clue.", "title": "PRAW post submit problem", "url": "https://www.reddit.com/r/redditdev/comments/33xwym/praw_post_submit_problem/"}, {"author": "sunbolts", "created_utc": 1430017396, "gilded": 0, "name": "t3_33vuok", "num_comments": 9, "score": 5, "selftext": "I'm using PRAW to make a bot which I run from command prompt, and I want to make sure the rate limit usage isn't going over the maximum allowed amount. It says at [this link](https://github.com/reddit/reddit/wiki/API): Make no more than thirty requests per minute. This allows some burstiness to your requests, but keep it sane. On average, we should see no more than one request every two seconds from you. Monitor the following response headers to ensure that you're not exceeding the limits: - X-Ratelimit-Used: Approximate number of requests used in this period - X-Ratelimit-Remaining: Approximate number of requests left to use - X-Ratelimit-Reset: Approximate number of seconds to end of period How can I view these Ratelimit response headers?", "title": "How do you check the RateLimit usage?", "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "JustLTU", "created_utc": 1429961299, "gilded": 0, "name": "t3_33t71g", "num_comments": 4, "score": 2, "selftext": "I'm a very new coder, both with praw and python in general. I tried to make a very simple bot, just for a learning experience. The bot simply scans through the comment stream, and if it finds a keyword it sends a certain reply. The problem is that it started replying to itself, is there any way to make it ignore certain users (like itself)?", "title": "[PRAW] How to make a bot ignore certain user's comments?", "url": "https://www.reddit.com/r/redditdev/comments/33t71g/praw_how_to_make_a_bot_ignore_certain_users/"}, {"author": "GoldenSights", "created_utc": 1429925483, "gilded": 0, "name": "t3_33rz9o", "num_comments": 2, "score": 6, "selftext": "I'm working on a pull request for PRAW that adds multireddits, or at least the basics. However, all of my DELETE requests are giving me 403 \"please sign in\" errors. http://i.imgur.com/pp4tg9B.png http://i.imgur.com/bGPrgrb.png http://www.reddit.com/dev/api/oauth#DELETE_api_multi_{multipath}_r_{srname} Clearly, post and put both work just fine, but delete fails even though it has the same modhash as the others. I tried watching what Chrome does when deleting a subreddit, but it doesn't seem to have any special headers. Overall, I've found the Multireddit api to be kinda inconsistent and disjointed from everything else. Multireddits and /api/v1/me/friends/ are the only things to use a Delete method, everything else uses a Post to a specific deletion url. Is there any reason for this, and is it related to my problem? Any help is appreciated. Edit: [Pull request Part 1](https://github.com/praw-dev/praw/pull/404).", "title": "All DELETE requests are returning 403 \"Please sign in to do that\"", "url": "https://www.reddit.com/r/redditdev/comments/33rz9o/all_delete_requests_are_returning_403_please_sign/"}, {"author": "heyitzaustin", "created_utc": 1429830457, "gilded": 0, "name": "t3_33nfe0", "num_comments": 2, "score": 1, "selftext": "I'm trying to make a script that parses through reddit comments in a subreddit. This requires me to get a comment tree for each submissions, which contains instances of both comment and more_comment objects. I want to be able to flatten the tree so I can just get the comments (i don't care if their replies or not), but when I use (submission.replace_more_comments), it slows the script down like crazy(probably since according to the documentation each replacement requires one API request.) Is there away to avoid using this API call for what I want to do? It could be the difference for my script running for 15 minutes vs it running for 40 minutes. Here's the part of my code: for x in submissions: submission = r.get_submission(submission_id=x.id) submission.replace_more_comments(limit=16, threshold=10) flat_comments = praw.helpers.flatten_tree(submission.comments) for comments in flat_comments: #blah blah blah", "title": "[PRAW]replace_more_comments is bottlenecking my script!", "url": "https://www.reddit.com/r/redditdev/comments/33nfe0/prawreplace_more_comments_is_bottlenecking_my/"}, {"author": "seriouslulz", "created_utc": 1429812839, "gilded": 0, "name": "t3_33mcom", "num_comments": 0, "score": 1, "selftext": "I keep getting 429 errors even though I'm using praw. I tried to set the delay to 10s but no luck.", "title": "How exactly does rate limiting work for GAE apps?", "url": "https://www.reddit.com/r/redditdev/comments/33mcom/how_exactly_does_rate_limiting_work_for_gae_apps/"}, {"author": "Lark_vi_Britannia", "created_utc": 1429791412, "gilded": 0, "name": "t3_33l4jt", "num_comments": 30, "score": 4, "selftext": "I'm running this script: https://github.com/SwedishBotMafia/RScanBot.Gen/ And it won't allow me to log-on. I've tried over scripts as well and they all return the same thing: a JSON error. Here is the error I get for this bot: http://i.imgur.com/Fr886Um.png I've also tried this bot as well: http://www.reddit.com/r/botrequests/comments/1qqbmu/a_bot_that_says_if_you_know_what_i_mean_every/cdwf6z5 And this is the error I get: http://i.imgur.com/LIbj6Vm.png I've spent the last 3-4 hours googling trying to figure out how to solve this, but no solutions have worked for me so far. I'm using Python2.7 and all of my pip stuff is updated to the latest versions. I'm running the script inside of the Kivy extracted folder. When running out of the Python2.7 Scripts folder, the second script gives me the same error. I can't figure out how to get Kivy installed in the original Python2.7 folder, so I can't run the first one. Anyone have any ideas on how to fix this? TL;DR: Basically, it won't allow me to login to reddit via PRAW, any requests are returned with \"No JSON object could be decoded\"", "title": "Using PRAW and getting a JSON error: \"No JSON Object could be decoded\"", "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "pandoraparadox", "created_utc": 1429652539, "gilded": 0, "name": "t3_33eill", "num_comments": 4, "score": 1, "selftext": "For an analysis project I want to analyse the comments of posts that contain specific keyword(s). In specific I would need the comment: text, time, up down (ratio). And of the post I would need the comment count, and up & down vote ratio. Is this possible with Python Praw? I know there is a cache limit of 1000. Would this imply that topics over 1000 comments need to be split up? Thank You Very Much!", "title": "PRAW need comments' text, time, upvotes for topics of a certain keyword", "url": "https://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/"}, {"author": "TheEnigmaBlade", "created_utc": 1429600500, "gilded": 0, "name": "t3_33bx3j", "num_comments": 0, "score": 4, "selftext": "I'm trying to access a mod-only wiki page (bot config) from a praw-based bot using oauth, but the API always returns a 403. The bot's account has proper permissions on reddit. To summarize everything I've tried: * Can access page from the web client with the bot's account * Can access page using praw and old auth method * Can access moderated subreddits * Can access public wiki pages + page list * Can't access public wiki pages in private subreddit (to which it has approved submitter access) * Can't access any mod-only wiki pages (403) * Using every single oauth scope doesn't work I'm out of ideas.", "title": "Access mod-only wiki pages through oauth", "url": "https://www.reddit.com/r/redditdev/comments/33bx3j/access_modonly_wiki_pages_through_oauth/"}, {"author": "Amablue", "created_utc": 1429418306, "gilded": 0, "name": "t3_333k7r", "num_comments": 6, "score": 1, "selftext": "I'm trying to scan over all the comments in a thread, and according to various threads and documentation I'm just supposed to need to call `replace_more_comments()` on the submission I'm scanning, but that appears to do nothing at all. Here's a test script to demonstrate: r = praw.Reddit('Test script by /u/amablue') permalink = 'http://www.reddit.com/comments/333jjs/' def print_comments(comment): print comment.body for child in comment.replies: print_comments(child) post = r.get_submission(permalink) post.replace_more_comments(limit=None, threshold=0) for comments in post.comments: print_comments(comments) The output of that is the following: one two three four five six seven eight nine ten If you look at the [test thread](http://www.reddit.com/r/amablue/comments/333jjs/test_post_please_ignore/) though I counted all the way up to thirteen, not ten. Why isn't it printing the whole thing?", "title": "replace_more_comments doesn't seem to do anything", "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "habnpam", "created_utc": 1429263348, "gilded": 0, "name": "t3_32wnhw", "num_comments": 10, "score": 2, "selftext": "So this is the code I ran: r = praw.Reddit(\"/u/habnpam sflkajsfowifjsdlkfj test test test\") for c in praw.helpers.comment_stream(reddit_session=r, subreddit=\"helpmefind\", limit=500, verbosity=1): print(c.author) --- From what I understand, comment_stream() gets the most recent comments. So if we specify the limit to be 100, it will initially get the 100 newest comment, and then constantly update to get new comments. It seems to works appropriately for every subreddit except /r/helpmefind. For /r/helpmefind, it fetches around 30 comments, regardless of the limit.", "title": "[PRAW] comment_stream() messes up when getting comments from a certain subreddit.", "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "scarface1993", "created_utc": 1429150742, "gilded": 0, "name": "t3_32rc86", "num_comments": 3, "score": 4, "selftext": "I just want to know how to get all the comments to a post using PRAW. I looked at the PRAW documentation, but it just showed how to do it with the comment id number. Also is it possible to get the comment id number, the time stamp and the author for the comments? Thank you", "title": "[PRAW] How do I get the comments to a particular post?", "url": "https://www.reddit.com/r/redditdev/comments/32rc86/praw_how_do_i_get_the_comments_to_a_particular/"}, {"author": "gschizas", "created_utc": 1428584696, "gilded": 0, "name": "t3_31zs3a", "num_comments": 3, "score": 1, "selftext": "I'm converting a semi-personal mod helper site to use OAuth (mostly to stop it from being semi-personal and make it semi-public), and I seem to be unable to use the modqueue function with OAuth2 (it was definitely working with username and password login). I've been using the praw3 branch (praw-3.0a1), because I was using the username and password login system, and my account is SSL-only, so I was getting an \"unexpected redirect\" on login, but this happens in 2.1.21 (the version on PyPI) as well. I think that the required scope ('read', according to [reddit's API documentation](https://www.reddit.com/dev/api/oauth#scope_read)) is missing somewhere (in PRAW), but I have no idea where.", "title": "PRAW: get_mod_queue doesn't work with OAuth2?", "url": "https://www.reddit.com/r/redditdev/comments/31zs3a/praw_get_mod_queue_doesnt_work_with_oauth2/"}, {"author": "zzpza", "created_utc": 1428569385, "gilded": 0, "name": "t3_31z8g6", "num_comments": 4, "score": 3, "selftext": "To set expectations from the beginning, I'm a n00b at python, PRAW, and have never used OAuth before. Is the migration to OAuth a necessity for me? All my scripts are mod assistance scripts that all run from the same account. I have no web apps that use other user logins. I've read the guide here: http://praw.readthedocs.org/en/latest/pages/oauth.html My PRAW environment is on a linux server on the net somewhere. It's a text only environment. I could setup an X server for the browser section, but it looks like you just need part of the URL. Can't I just `print url` instead of opening it in a browser and copying the necessary parts? With regards to scopes, my scripts cover several scopes within each script. Is there a super user scope that does everything? Or can I request multiple scopes at the same time? Or do I have to switch scopes for each command? If so, how? Would changing scope be the same as changing user? TIA! :)", "title": "Questions regarding converting from user/pass to OAuth with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/31z8g6/questions_regarding_converting_from_userpass_to/"}, {"author": "MrFanzyPanz", "created_utc": 1428520893, "gilded": 0, "name": "t3_31wpmp", "num_comments": 5, "score": 7, "selftext": "Hey everybody! I'm part of a research team studying Reddit. We've been collecting and analyzing data for the past 2 years using PRAW. We have a server that is constantly collecting data from 2013, and it's been hitting server errors for the past week or so. The error is requests.exceptions.HTTPError: 500 Server Error: Server Error We've tried using a VPN to see if this was caused by our IP, but it seems that it's not IP related. I tried using PRAW in the console to manually pull just one submission to see if it's a problem with our script, and even that one pull didn't work. The code I used was: import praw r = praw.Reddit(user_agent='MrFanzyPanz miscellanious data collector for research') r.get_info(thing_id='t3_1km4e2') I have the latest version of PRAW and am operating in Python 2.7. Can anybody help me figure this out? It seems like this is an internal problem with PRAW.", "title": "PRAW suddenly returning endless HTTP errors", "url": "https://www.reddit.com/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/"}, {"author": "TomSparkLabs", "created_utc": 1428450084, "gilded": 0, "name": "t3_31t6py", "num_comments": 5, "score": 1, "selftext": "Hi! I'm making a comment reply bot, (for my own subreddit) but I don't want to have it get downvotes, so if a comment is incorrect, that the bot's karma won't be affected a lot. Here's what I have so far: from time import sleep import praw r = praw.Reddit(user_agent='Downvoted comments remover 0.1 by /u/TomSparkLabs') r.login(username='Bot', password=' ') user = r.get_redditor('Bot') while True: for comment in user.get_comments(limit=20): if comment.score", "title": "Auto-delete downvoted bot's comments", "url": "https://www.reddit.com/r/redditdev/comments/31t6py/autodelete_downvoted_bots_comments/"}, {"author": "gswhoops22", "created_utc": 1428386549, "gilded": 0, "name": "t3_31pxl5", "num_comments": 2, "score": 1, "selftext": "Hey everyone this is my first time using PRAW and I was looking for a little bit of help. I'm doing some basic comment scraping using: submission = r.get_submission(submission_id=2ul5vi, comment_limit=100 ,comment_sort='new') and then accessing comments through submission.comments my only issue is that while printing these comments all of the longer comments just get cut off and get replaced by \"...\" for example one of the comments printed as : \"Kidd using similar schemes from last year's playoffs - going small with Dudle...\" is there any parameter I can set that refers to the character length of comments? Thanks for any help and sorry if my formatting is crappy lol", "title": "PRAW: Get comments without character limit", "url": "https://www.reddit.com/r/redditdev/comments/31pxl5/praw_get_comments_without_character_limit/"}, {"author": "jar_jar_binks", "created_utc": 1428378594, "gilded": 0, "name": "t3_31pkst", "num_comments": 0, "score": 1, "selftext": "I'm trying to return all top-level comments in order: import praw r = praw.Reddit('my subreddit comment listing') r.login('xxxxxxxx', 'xxxxxxxxx') submission = r.get_submission(submission_id='xxxxxxx/yaddayadda/?sort=new&limit=1500') submission.replace_more_comments(limit=None, threshold=0) forest_comments = submission.comments i=0 for comment in forest_comments: print(str(i) + '|' + str(comment.id) + '|' + str(comment.author)) i += 1 This seems to grab all top-level comments, but the order is all messed up after the first few paginations. The final (2300 or so) printed comment is not the oldest comment. Any thoughts?", "title": "PRAW: return all comments in order?", "url": "https://www.reddit.com/r/redditdev/comments/31pkst/praw_return_all_comments_in_order/"}, {"author": "Hook3d", "created_utc": 1427862689, "gilded": 0, "name": "t3_310mji", "num_comments": 2, "score": 0, "selftext": "Hi everyone, here is the most recent version of a script/bot I have written to check a common grammatical error: BOT_CONSTANTS.py: __author__ = '/u/Hook3d' #login credentials LOGIN_NAME = \"GrammarBotv1\" LOGIN_PW = \"redacted\" #API rule constants THING_LIMIT = 25 #(sub)reddit constants CURR_SUBREDDIT = \"funny\" #Posting constants MESSAGE_TO_POST = \"One common English error is the incorrect usage of the preposition \\'of\\' in the place of the verb \\'have\\'. For instance:\\n\\n \" \\ \"\\'I could of\\' should be \\'I could have\\', which contracts to \\'I could've.\\'\\n\\n\" \\ \"\\'I should of\\' should be \\'I should have\\', which contracts to \\'I should've.\\'\\n\\n\" \\ \"\\'I would of\\' should be \\'I would have\\', which contracts to \\'I would've.\\'\\n\\n\" \\ \"bot by /u/Hook3d\" SUBS_TO_CRAWL = [\"nba\", \"sports\", \"soccer\", \"videos\", \"gaming\", \"askreddit\", \"news\", \"test\", \"all\", \"funny\", \"pics\", \"gaming\", \"ProgrammerHumor\", \"aww\"] GrammarBot.py: __author__ = '/u/Hook3d' import praw, time from BOT_CONSTANTS import * from pprint import pprint user_agent = \"Simple grammar script v1 by /u/Hook3d\" common_errors = [\"would of \", \"could of \", \"should of \"] completed_submissions = [] reddit = praw.Reddit(user_agent = user_agent) #create reddit object with praw call reddit.login(LOGIN_NAME, LOGIN_PW) #login with bot credentials def has_error(errors, comment): for error in errors: if comment.find(error) != -1: return True return False #main loop while True: for curr_sub in SUBS_TO_CRAWL: subreddit = reddit.get_subreddit(curr_sub) #grab the sub i = 1 for submission in subreddit.get_hot(): #loop over the submissions if submission in completed_submissions: continue completed_submissions.append(submission) print(\"on the \" + str(i) + \" submission\") i = i + 1 #submission.replace_more_comments() flat_comments = praw.helpers.flatten_tree(submission.comments) #grab the comments for comment in flat_comments: if isinstance(comment, praw.objects.Comment): comment_author = comment.author comment_text = comment.body.lower() #grab submission text print(comment_text) if has_error(common_errors, comment_text) and comment_author.name != LOGIN_NAME: #don't reply to own comments comment.reply(MESSAGE_TO_POST) time.sleep(60) Does anyone have any suggestions for improving this script/bot? Edit: Edited GrammarBot.py to use a binary tree. The linear search was getting a bit strong. __author__ = '/u/Hook3d' import praw, time from bintrees import BinaryTree from BOT_CONSTANTS import * from pprint import pprint user_agent = \"Simple grammar script v1 by /u/Hook3d\" common_errors = [\"would of \", \"could of \", \"should of \"] completed_submissions = BinaryTree() reddit = praw.Reddit(user_agent = user_agent) #create reddit object with praw call reddit.login(LOGIN_NAME, LOGIN_PW) #login with bot credentials def has_error(errors, comment): for error in errors: if comment.find(error) != -1: return True return False #main loop while True: for curr_sub in SUBS_TO_CRAWL: subreddit = reddit.get_subreddit(curr_sub) #grab the sub i = 1 for submission in subreddit.get_hot(): #loop over the submissions #pprint(dir(submission)) if completed_submissions.__contains__(submission.id): continue completed_submissions.__setitem__(submission.id, submission) print(\"on the \" + str(i) + \" submission\") i = i + 1 #submission.replace_more_comments() flat_comments = praw.helpers.flatten_tree(submission.comments) #grab the comments for comment in flat_comments: if isinstance(comment, praw.objects.Comment): comment_author = comment.author #pprint(dir(comment)) comment_text = comment.body.lower() #grab submission text print(comment_text) if has_error(common_errors, comment_text) and comment_author.name != LOGIN_NAME: #don't reply to own comments comment.reply(MESSAGE_TO_POST) time.sleep(60)", "title": "Suggestions for grammar bot?", "url": "https://www.reddit.com/r/redditdev/comments/310mji/suggestions_for_grammar_bot/"}, {"author": "IAMA_YOU_AMA", "created_utc": 1427767647, "gilded": 0, "name": "t3_30vp0e", "num_comments": 6, "score": 9, "selftext": "Version 2.1.21 of praw now gives me a warning if my user_agent string contains the word 'bot' Is there something going on with reddit that I should know about? Are they cracking down on bots?", "title": "Warning in praw about using the keyword 'bot'", "url": "https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/"}, {"author": "howbigis1gb", "created_utc": 1427622033, "gilded": 0, "name": "t3_30oqky", "num_comments": 7, "score": 2, "selftext": "I've been trying my hand at a project to analyse some data. And the data set is moderately large, so I want to download the thread/post data and perform some analytics with it. The problem I'm facing is that sometimes there's an error and the comment or post (as the case may be) isn't downloaded. This is an issue which I'm not sure how to handle, and is imperative that I do as I would rather not run multiple passes to get the data because it often takes many hours to do that. Here is the code I'm using for the same: # Post # creation_utc, prettytime, edited_time, pretty_edited_time, upvotes, upvote_ratio, banned_by, report_reasons, body, permalink, author, gold, time_to_get, error, depth, parent_id, controversial, distinguished, title, num_comments # Comment # creation_utc, prettytime, edited_time, pretty_edited_time, upvotes, upvote_ratio, banned_by, report_reasons, body, permalink, author, gold, time_to_get , error, depth, parent_id, controversial, distinguished, title, num_comments import praw import csv import datetime import time import fileinput # global starttime # starttime = time.time() # global subtime # subtime = starttime def checkComments(bigfile, smallfile, comments, depth): for comment in comments: try: # time2 = time.time() - time - starttime created_time = comment.created_utc pretty_created_time = datetime.datetime.fromtimestamp(created_time).strftime('%Y-%m-%d %H:%M:%S') edited_time = comment.edited if edited_time==\"FALSE\": pretty_edited_time = datetime.datetime.fromtimestamp(edited_time).strftime('%Y-%m-%d %H:%M:%S') else: pretty_edited_time = \"FALSE\" author = comment.author controversial = comment.controversiality distinguished = comment.distinguished report_reasons = comment.report_reasons upvotes = comment.ups # upvote_ratio = comment.upvote_ratio permalink = comment.permalink parent_id = comment.parent_id banned_by = comment.banned_by comment_body = comment.body.encode('utf-8') gold = comment.gilded # time2 = time.time() - time bigfile.writerow([created_time, pretty_created_time, edited_time, pretty_edited_time, upvotes, \"NA\", banned_by, report_reasons, comment_body, permalink, author, gold, \"not implemented\", \"success\", depth, parent_id, controversial, distinguished, \"NA\", \"NA\"]) smallfile.writerow([created_time, pretty_created_time, edited_time, pretty_edited_time, upvotes, \"NA\", banned_by, report_reasons, comment_body, permalink, author, gold, \"not implemented\", \"success\", depth, parent_id, controversial, distinguished, \"NA\", \"NA\"]) # subtime = subtime - time.time() checkComments(bigfile, smallfile,comment.replies, depth+1) except Exception as e: print e print \"recurse\" # time2 = time.time()-time bigfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", e, \"\", \"\", \"\", \"\", \"\", \"\"]) smallfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", e, \"\", \"\", \"\", \"\", \"\", \"\"]) def processSub(sub, bigfile, smallfile): sub.replace_more_comments(limit=None, threshold=0) checkComments(bigfile, smallfile, sub.comments, 0) #replace Comment Scraping Bot with your own user agent ID - be unique r = praw.Reddit('') r.login('', '') bigfile = csv.writer(open(\"bigfile.csv\", \"w\",0), dialect='excel') bigfile.writerow([\"creation_utc\", \"prettytime\", \"edited_time\", \"pretty_edited_time\", \"upvotes\", \"upvote_ratio\", \"banned_by\", \"report_reasons\", \"body\", \"permalink\", \"author\", \"gold\", \"time_to_get\", \"error\", \"depth\", \"parent_id\", \"controversial\", \"distinguished\", \"title\", \"num_comments\"]) for sub_id in fileinput.input([\"1-2500.txt\"]): # list of file names as strings try: starttime = time.time() submission = r.get_submission(submission_id = sub_id) created_time = submission.created_utc pretty_created_time = datetime.datetime.fromtimestamp(created_time).strftime('%Y-%m-%d %H:%M:%S') prettytime_file = datetime.datetime.fromtimestamp(created_time).strftime('%Y-%m-%d %Hh%Mm%Ss') smallfile = csv.writer(open(prettytime_file+\" \"+sub_id.rstrip()+\".csv\", \"w\",0), dialect='excel') smallfile.writerow([\"creation_utc\", \"prettytime\", \"edited_time\", \"pretty_edited_time\", \"upvotes\", \"upvote_ratio\", \"banned_by\", \"report_reasons\", \"body\", \"permalink\", \"author\", \"gold\", \"time_to_get\", \"error\", \"depth\", \"parent_id\", \"controversial\", \"distinguished\", \"title\", \"num_comments\"]) edited_time = submission.edited if edited_time==\"FALSE\": pretty_edited_time = datetime.datetime.fromtimestamp(edited_time).strftime('%Y-%m-%d %H:%M:%S') else: pretty_edited_time = \"FALSE\" upvotes = submission.ups upvote_ratio = submission.upvote_ratio banned_by = submission.banned_by report_reasons = submission.report_reasons num_comments = submission.num_comments author = submission.author title = submission.title.encode('utf-8') body = submission.selftext.encode('utf-8') user_reports = submission.user_reports ups = submission.ups permalink = submission.permalink.encode('utf-8') gold = submission.gilded distinguished = submission.distinguished bigfile.writerow([created_time, pretty_created_time, edited_time, pretty_edited_time, upvotes, upvote_ratio, banned_by, report_reasons, body, permalink, author, gold, \"not implemented\", \"success\", \"0\", \"NA\", \"NA\", distinguished, title, num_comments]) smallfile.writerow([created_time, pretty_created_time, edited_time, pretty_edited_time, upvotes, upvote_ratio, banned_by, report_reasons, body, permalink, author, gold, \"not implemented\", \"success\", \"0\", \"NA\", \"NA\", distinguished, title, num_comments]) processSub(submission, bigfile, smallfile) #terminator line print \"Finished processing id:\"+ sub_id.rstrip() + \" with \"+ str(num_comments) +\" comments\" endtime = time.time()-starttime bigfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", endtime, \"success\", \"\", \"\", \"\", \"\", \"\", \"\"]) smallfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", endtime, \"success\", \"\", \"\", \"\", \"\", \"\", \"\"]) except Exception as e: print \"sub\" print e endtime = time.time()-starttime bigfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", endtime, e, \"\", \"\", \"\", \"\", \"\"]) smallfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", endtime, e, \"\", \"\", \"\", \"\", \"\"]) I recognise that this is probably not the best way to do things, and I should probably look into storing things in a heirarchical structure, but my question was pertaining more to error handling Any inputs would be greatly appreciated. Thanks", "title": "What is the best way to handle errors and failures, so I can retry the same post/comment?", "url": "https://www.reddit.com/r/redditdev/comments/30oqky/what_is_the_best_way_to_handle_errors_and/"}, {"author": "Phteven_j", "created_utc": 1427220491, "gilded": 0, "name": "t3_305o0j", "num_comments": 7, "score": 1, "selftext": "I run multiple bots and a small website on a Digital Ocean server. For some reason, one particular bot (pretty simple python script) keeps throwing the HTTP 429 when I execute it server-side (locally is fine) any time it attempts to login. I think this is deeper than just the particular code, but here it is anyway: import praw user=\"\" pass=\"\" r=praw.Reddit(\"description of this particular bot here\") r.login(user,pass) etc. And the error: Traceback (most recent call last): File \"edcbot.py\", line 8, in r.login(username,password) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 1333, in login self.user = self.get_redditor(user) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 948, in get_redditor return objects.Redditor(self, user_name, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 741, in __init__ fetch, info_url) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 73, in __init__ self.has_fetched = self._populate(json_dict, fetch) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 133, in _populate json_dict = self._get_json_dict() if fetch else {} File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 126, in _get_json_dict as_objects=False) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 557, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 399, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python2.7/dist-packages/praw/internal.py\", line 178, in _raise_response_exceptions response.raise_for_status() File \"/usr/local/lib/python2.7/dist-packages/requests/models.py\", line 831, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 429 Client Error: Too Many Requests Let me know what other info I can provide to help troubleshoot this. Thanks so much!", "title": "[PRAW] Getting HTTPError 429: Too many client requests when attempting r.login()", "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "5loon", "created_utc": 1426999264, "gilded": 0, "name": "t3_2zvl1x", "num_comments": 13, "score": 6, "selftext": "I know it sounds like I'm basically asking you guys to write me a script, but I have a reasonably good knowledge of PRAW and Python and I'm just having trouble making a script that could do this efficiently. Where should I begin? It would theoretically grab a post, delete its flair, and move to the next one (chronologically back in time). Thanks in advance!", "title": "How could I go about removing every flair on every post in an entire subreddit's history?", "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "Isolder", "created_utc": 1426642373, "gilded": 0, "name": "t3_2zf5b1", "num_comments": 8, "score": 4, "selftext": "I'm using Perl instead of PRAW. I want to monitor /r/all/comments.json and basically do something with every new comment posted to reddit. Surely /r/all/comments.json isn't going to expose every comment as there's probably thousands of comments a minute. Just want to know the cleanest way to both call it and get an updated listing. Thanks", "title": "What's the proper way to call /r/all/comments.json ? What interval? It doesn't look like it's always updating", "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "wonrah", "created_utc": 1426470965, "gilded": 0, "name": "t3_2z6rzn", "num_comments": 1, "score": 1, "selftext": "For example, I want to try this: subreddit = r.get_subreddit(\"my_sub\") submissions = subreddit.get_new(limit=30, period='today', sort='new') --- Now [the documentations](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Subreddit) have a few key word arguments. Are those all the key words arguments that every function in that object can take in?", "title": "[PRAW] Where do we find the list of **kwargs for a specific object ?", "url": "https://www.reddit.com/r/redditdev/comments/2z6rzn/praw_where_do_we_find_the_list_of_kwargs_for_a/"}, {"author": "Kaphox", "created_utc": 1426289685, "gilded": 0, "name": "t3_2yys8k", "num_comments": 1, "score": 1, "selftext": "See screenshot: http://i.gyazo.com/248a3ec5a6946d79087d28933e797b86.png I want a PRAW reddit bot to be able to change wiki page permissions to \"only mods may edit and view\" - I've looked absolutely everywhere and not found a response - is this possible to do? Thanks.", "title": "PRAW: making a bot change the \"who can edit this page?\" settings on wiki pages in PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/2yys8k/praw_making_a_bot_change_the_who_can_edit_this/"}, {"author": "ibbignerd", "created_utc": 1426263821, "gilded": 0, "name": "t3_2yx982", "num_comments": 4, "score": 1, "selftext": "Whenever I try to run my bot, I get the following traceback: Traceback (most recent call last): File \"/home/jailbreakflairbot/RoundUp/RoundUp2.7.py\", line 2, in import praw File \"/home/jailbreakflairbot/.local/lib/python2.7/site-packages/praw/__init__.py\", line 43, in from six.moves.urllib.parse import parse_qs, urlparse, urlunparse ImportError: No module named urllib.parse | Package | Version | |---|---| |six|1.8.0| |praw|2.1.20| |python|2.7| I can't figure out what is causing this issue.", "title": "Praw not working anymore", "url": "https://www.reddit.com/r/redditdev/comments/2yx982/praw_not_working_anymore/"}, {"author": "zathegfx", "created_utc": 1426263295, "gilded": 0, "name": "t3_2yx828", "num_comments": 1, "score": 3, "selftext": "I am trying to find the link flair on a submission. for submission in praw.helpers.submission_stream(reddit, 'technology', limit=1, verbosity=1): print \"---------------------------------------\" print submission.get_flair_choices() This returns (in my opinion) a very strange JSON object: { u'current':{ u'flair_css_class':u'netneutrality', u'flair_template_id':u'59948c02-705a-11e4-aee5-12313b0e81b6', u'flair_position':u'left', u'flair_text':u'Net Neutrality' }, u'choices':[ { u'flair_css_class':u'politics', u'flair_text_editable':True, u'flair_template_id':u'ce7632c4-cef6-11e3-a9a4-12313b0a74a7', u'flair_text':u'Politics', u'flair_position':u'left' }, { u'flair_css_class':u'business', u'flair_text_editable':True, u'flair_template_id':u'd9e9442a-cef6-11e3-8dbf-12313d18e464', u'flair_text':u'Business', u'flair_position':u'left' }, { u'flair_css_class':u'puretech', u'flair_text_editable':True, u'flair_template_id':u'e60ffeea-e2fd-11e3-abc6-12313d18e5cd', u'flair_text':u'Pure Tech', u'flair_position':u'left' }, { u'flair_css_class':u'netneutrality', u'flair_text_editable':True, u'flair_template_id':u'59948c02-705a-11e4-aee5-12313b0e81b6', u'flair_text':u'Net Neutrality', u'flair_position':u'left' }, { u'flair_css_class':u'comcast', u'flair_text_editable':True, u'flair_template_id':u'31e0da86-70fe-11e4-bf7d-12313b0e92a7', u'flair_text':u'Comcast', u'flair_position':u'left' } ] } I am looking to grab `flair_css_class` from `current`, however I dont understand how to pass the object through `json.load` ... this is my botched version I tried: k = submission.get_flair_choices str(k); j = json.loads(k) print j[\"u'current'\"] Am I doing something wrong? Thanks -Z", "title": "PRAW: find the current flair for a specific link / submission", "url": "https://www.reddit.com/r/redditdev/comments/2yx828/praw_find_the_current_flair_for_a_specific_link/"}, {"author": "NewArithmetic", "created_utc": 1426178715, "gilded": 0, "name": "t3_2yt6lb", "num_comments": 3, "score": 0, "selftext": "I'm sure that there is a way to do this, however I have yet to find it. In my limited experience with praw, I could get the comments, but I think there would be a limit set at 200. I'm pretty sure I could do this with beautifulsoup, however I would prefer to do this the praw way. Any advice?", "title": "Can praw get the last 2000 comments in a sub?", "url": "https://www.reddit.com/r/redditdev/comments/2yt6lb/can_praw_get_the_last_2000_comments_in_a_sub/"}, {"author": "tooproudtopose", "created_utc": 1426043556, "gilded": 0, "name": "t3_2yn0dd", "num_comments": 10, "score": 0, "selftext": "I'm trying to make a scraper that will download all of the top comments from a particular subreddit and put the result into a csv (I used /r/Portland as an example). The problem I'm getting is that if I set limit=4 or above in my code, then I get the error `AttributeError: '' has no attribute 'body'`, but I don't get any errors at all when limit=3 (or less). What's going wrong? Here's my code: import praw import csv target_subreddit = 'portland' r = praw.Reddit(user_agent=\"u/tooproudtopose reddit-scraper\") sub = r.get_subreddit(target_subreddit) post_generator = sub.get_top(limit=4) fname = '%s.comments.csv' %target_subreddit f = open(fname, \"w\") writer = csv.writer(f, lineterminator = \"\\n\") for submission in post_generator: comments = praw.helpers.flatten_tree(submission.comments) for comment in comments: writer.writerow((comment.body, comment.author))", "title": "Comment scraper stops working once the limiting number of posts gets too high", "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "cyanoge", "created_utc": 1425936136, "gilded": 0, "name": "t3_2yhhm3", "num_comments": 3, "score": 6, "selftext": "E.g., if I use PRAW to fetch a user's inbox, is that data passed over the wire encrypted or is it in plaintext?", "title": "[PRAW][python]Are requests encrypted?", "url": "https://www.reddit.com/r/redditdev/comments/2yhhm3/prawpythonare_requests_encrypted/"}, {"author": "JBHUTT09", "created_utc": 1425870745, "gilded": 0, "name": "t3_2yekdx", "num_comments": 5, "score": 1, "selftext": "I'm currently trying to rewrite my bot to use OAuth2 because of the upcoming change. I've spent the past couple of hours stuck on this one problem. How do I get a refresh token? I've found [this piece of documentation](https://github.com/reddit/reddit/wiki/OAuth2#authorization) but I can't for the life of me figure out how to work it into my code. This is what I have: class FlairBot( object ): def __init__( self, client_info_file ): self.token_expiration = 0 self.client_info = { } with open( client_info_file ) as file: for line in file: ( key, value ) = line.split( ':' ) self.client_info[ key ] = value.strip() print( self.client_info ) self.reddit = praw.Reddit( user_agent = self.client_info[ 'user_agent' ] ) self.client_auth = requests.auth.HTTPBasicAuth( self.client_info[ 'client_id' ], self.client_info[ 'client_secret' ] ) self.get_new_token( ) def get_new_token( self ): headers = { 'user-agent': self.client_info[ 'user_agent' ] } post_data = { \"grant_type\": \"password\", \"username\": self.client_info[ \"username\" ], \"password\": self.client_info[ \"password\" ] } response = requests.post( \"https://www.reddit.com/api/v1/access_token\", auth = self.client_auth, data = post_data, headers = headers ) token_data = response.json( ) self.token_expiration = time.time( ) + token_data[ 'expires_in' ] #self.reddit.set_access_credentials( token_data[ 'scope' ], token_data[ 'access_token' ] ) #headers = { \"Authorization\": token_data[ 'token_type' ] + ' ' + token_data[ 'access_token' ], \"User-Agent\": self.client_info[ 'user_agent' ] } print( token_data ) How do I work in that duration parameter? Also, am I doing anything glaringly wrong? I'm kind of worried that the OAuth2 stuff won't work with PRAW in the way I'm doing things. Thanks.", "title": "How do I get an OAuth2 refresh token for a python script?", "url": "https://www.reddit.com/r/redditdev/comments/2yekdx/how_do_i_get_an_oauth2_refresh_token_for_a_python/"}, {"author": "iforgotmylegs", "created_utc": 1425752120, "gilded": 0, "name": "t3_2y9cs3", "num_comments": 7, "score": 2, "selftext": "I have been using Notepad++ but I have recently switched to Spyder since it came with another module and I just liked it better. Spyder doesn't recognize praw and I found out that I have to load it in using the PYTHONPATH manager. I can't find where to actually point this thing, I found a folder in Python34/Lib/site-packages called \"praw\" but there's nothing to point it to. Can somebody help me out?", "title": "Where is the praw module stored?", "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "lizardsrock4", "created_utc": 1425597040, "gilded": 0, "name": "t3_2y2rvj", "num_comments": 3, "score": 0, "selftext": "Is there anyway with praw to have it only check edited comments using the subbreddit/about/edited url? Thanks for the help!", "title": "[PRAW] Comment stream search subreddit/about/edited?", "url": "https://www.reddit.com/r/redditdev/comments/2y2rvj/praw_comment_stream_search_subredditaboutedited/"}, {"author": "Mekhami", "created_utc": 1425587750, "gilded": 0, "name": "t3_2y26ty", "num_comments": 9, "score": 1, "selftext": "I can't find the 'likes' attribute anywhere in the PRAW docs. Was this removed from PRAW or never implemented? This is still available in the API, right?", "title": "[PRAW] [Python] What happened to the 'likes' attribute?", "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "RuleIV", "created_utc": 1425444451, "gilded": 0, "name": "t3_2xvhp4", "num_comments": 8, "score": 3, "selftext": "I've written a script that gets the mod log and find users that have more than X post removals in Y days. It then messages the mod mail with that information for scrutiny and discussion, and sends a message to each user warning them. For example: \\#.Removals|\\#.Still.Up|User|Posts|B|SB :-:|:-:|:-|:-|:-:|:-: 31|47|/u/Mutt1223|[1](http://redd.it/2wzjn2), [2](http://redd.it/2x3yjv), [3](http://redd.it/2xcshx), [4](http://redd.it/2wzjq6), [5](http://redd.it/2wva8p), [6](http://redd.it/2wjs8p), [7](http://redd.it/2wjigk), [8](http://redd.it/2whgxo), [9](http://redd.it/2whh3n), [10](http://redd.it/2whf0d), [11](http://redd.it/2wh55n), [12](http://redd.it/2wh49k), [13](http://redd.it/2w3fou), [14](http://redd.it/2vno1n), [15](http://redd.it/2vnq1f), [16](http://redd.it/2vnoex), [17](http://redd.it/2vjkl3), [18](http://redd.it/2vlk25), [19](http://redd.it/2veypu), [20](http://redd.it/2vf62e), [21](http://redd.it/2vf3sa), [22](http://redd.it/2vb1tc), [23](http://redd.it/2v6z9m), [24](http://redd.it/2v70le), [25](http://redd.it/2v0hsw), [26](http://redd.it/2v18e7), [27](http://redd.it/2v0e7y), [28](http://redd.it/2uxiur), [29](http://redd.it/2upelo), [30](http://redd.it/2upduy)|No|No I want to have columns indicating if they are banned (B) and if they are shadowed banned (SB) and also not message the user if they are already banned from the subreddit or shadow banned from Reddit. Using PRAW, how can I tell if as user is shadow banned, and also how can I tell if a specific user is banned from a subreddit? I could pull the entire ban list and look for their name, but that wouldn't work if there were too many bans. Thank you. The subreddit I'm writing this for has approximately 500 bans so I can use `subreddit_bans = r.get_banned(subreddit_name, user_only=True, limit=None)` but I'd still like to know if it's possible to check a single name in case I want to use this on a subreddit with more than 1,000 bans.", "title": "[PRAW] How to tell if a user shadow banned, or is a user banned from a subreddit.", "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "PixelDJ", "created_utc": 1425155759, "gilded": 0, "name": "t3_2xhrkc", "num_comments": 4, "score": 2, "selftext": "Hello! I'm scratching my head because I seem to be having issues iterating through my bot's unread messages. I'm probably missing something simple here. I searched, but didn't quite come up with anything. Here's the relevant code I'm using: import praw def length(iterable): try: return len(iterable) except: i = 0 for x in iterable: i += 1 return i def scanMessages(): ''' Check for new messages and handle them appropriately ''' unread_messages = 0 messages = r.get_unread(limit = None) unread_messages = length(messages) if unread_messages > 0: print \"You have %d unread message(s).\" % unread_messages for message in messages: print \"Message: \", print message.body r = praw.Reddit(user_agent=\"hardyharharrr\") r.login(bot_user, bot_pass) scanMessages() The code enters the IF statement fine, but once it gets to the FOR, it just finishes without printing anything. What am I missing here?", "title": "Iterating through unread messages isn't working as expected.", "url": "https://www.reddit.com/r/redditdev/comments/2xhrkc/iterating_through_unread_messages_isnt_working_as/"}, {"author": "PixelOrange", "created_utc": 1424651762, "gilded": 0, "name": "t3_2wtg1n", "num_comments": 2, "score": 1, "selftext": "Normally, I use PRAW for everything bot related, but PRAW has not updated their wiki functions to allow for setting a wiki page as unlisted yet (or that I've found, anyway). We have [one of the largest wikis](http://www.reddit.com/r/changemyview/wiki/pages) on reddit and a great deal of that is redundant from previous versions of code that I desperately want to get rid of but it's not feasible without running a script. I've spent all day figuring out how to make direct API calls in Python and I've gotten to the point where I can oauth and get the wiki pages or even get the wiki page settings but when I try to POST I either get a 404, 403, or 500. output = requests.get(\"https://oauth.reddit.com/r/changemyview/wiki/settings/aahdin/.json\",headers=headers) Gives me an appropriate output. hide_req = requests.post(\"https://oauth.reddit.com/r/changemyview/wiki/settings/aahdin/.json\",headers=headers) Obviously doesn't do anything... because I don't know how to pass listed=False. I've tried throwing simply \"listed=False\" before headers=headers but that just gives me an unexpected argument. I know it's listed in json but I don't know how to tell it to accept my new json. Both the API documentation and the PRAW documentation are listed in a way that shows you what the options are but after about an hour of searching I couldn't find any reliable examples of HOW you use those options. Please help... or bboe, if you read this, please update PRAW with the listed page option.", "title": "[API] How do you pass variables with an API POST?", "url": "https://www.reddit.com/r/redditdev/comments/2wtg1n/api_how_do_you_pass_variables_with_an_api_post/"}, {"author": "iforgotmylegs", "created_utc": 1424564222, "gilded": 0, "name": "t3_2wpkzd", "num_comments": 6, "score": 1, "selftext": "I notice in [the docs](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html) that there are two ways to call get_comments, one is as a method of Redditor and another is as a method of Subreddit. I also notice that the Redditor version has a parameter to indicate the time period from which the retrieve the comments, but I don't understand how it is formatted? Googling it only turns up [this](http://stackoverflow.com/questions/27768132/how-to-get-comments-from-last-week-using-praw) (rather unhelpful) Stackoverflow thread. Is the time parameter passed a string consisting of only \"minute\", \"hour\", etc., and further to that, is it possible to call it within a timeframe, say from Feb. 19th at 00:00:00 GMT to Feb. 19th at 17:00:00 GMT? And even further to that, can this parameter also be passed using Subbreddit's get_comments method? Or only with Redditor's?", "title": "How does time work for get_comments, and can it be done when called by a Subreddit instead of a Redditor?", "url": "https://www.reddit.com/r/redditdev/comments/2wpkzd/how_does_time_work_for_get_comments_and_can_it_be/"}, {"author": "iforgotmylegs", "created_utc": 1424548355, "gilded": 0, "name": "t3_2woo12", "num_comments": 9, "score": 5, "selftext": "I am following the guide [here](https://praw.readthedocs.org/en/v2.1.20/index.html#main-page) and it says to install pip. Fine, I found a link here with easy setup links for pip and distribute, which I ran. Now apparently I have to type in \"pip install praw\"? Where do I type that in? Into cmd? Into My Python interpreter? Don't I have to actually download the praw module first? I'm using Python 2.7.", "title": "I don't understand how you are supposed to install praw", "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "SeaCowVengeance", "created_utc": 1424544283, "gilded": 0, "name": "t3_2wofnb", "num_comments": 0, "score": 2, "selftext": "I'm trying to get a users saved submissions via the [history API](https://www.reddit.com/dev/api/oauth#GET_user_{username}_saved). However, I'm not just trying to get _all_ of their saved submissions, but just the saved submissions for a given category. I see in PRAW there's the method [get_saved](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html?highlight=saved#praw.objects.LoggedInRedditor.get_saved), but that returns all saved submissions for a user, not by category, and there's no parameter to specify category. It looks like if you use `get_content` and specify the category as a subpath it does what I want: r.get_content( \"https://oauth.reddit.com/user//saved/\", params=None, limit=4, place_holder=None, root_field='data', thing_field='children', after_field='after', use_oauth=True, object_filter=None)` Note the category in the path. That will return all the saved posts for a user in a given category. Is this there a better way to do this? If not, maybe `get_saved` should have a category parameter added to it?", "title": "[PRAW] Built in method for getting a user's saved submission sorted by category", "url": "https://www.reddit.com/r/redditdev/comments/2wofnb/praw_built_in_method_for_getting_a_users_saved/"}, {"author": "RudyH246", "created_utc": 1424500880, "gilded": 0, "name": "t3_2wmvah", "num_comments": 8, "score": 1, "selftext": "Hey /r/redditdev. I downloaded Python and started using PRAW less than an hour ago, so I'm pretty new at this. I'm trying to build a simple auto-responder that only responds to a certain user when I have an unread message from them in my inbox. Here's what I currently have: import praw r = praw.Reddit(user_agent='RudyH246 User Agent Auto Responder Thing') r.login('my_username', 'my_password') for comment in r.get_unread(): if comment.author == r.get_redditor('username') comment.reply('Reply.') comment.mark_as_read() I feel like this should accomplish this simple goal, but I figured I'd run it by here to see if I made any glaring errors in the logic. I've never written in Python before, so I'm also wary of syntactical errors, but I think I got the gist of it.", "title": "[PRAW] A Simple Auto-Responder Based on User Name.", "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "Seventytvvo", "created_utc": 1423491786, "gilded": 0, "name": "t3_2vaun5", "num_comments": 4, "score": 1, "selftext": "Hey guys, I'm just learning about APIs and PRAW with python, and after doing a few tutorials, I started wondering if it would be possible to create some kind of upvote-circle sniffer bot. The ultimate goal would be to sniff out pairs or groups of usernames who upvote each other so that a human can go take a closer look. Ultimately, I think it would be cool to try to use this as an vote manipulation / astroturfing-protection tool. Given PRAW's limitations, I came up with a way this might work: It will grab get_liked data for a seed user, UserA. It will then find the user, UserB, whom UserA has upvoted the most number of times. The script will move on to UserB and repeat the process, crawling through reddit. The script will keep track of what users it has examined and will look for instances where it starts to repeat a pattern of names. The idea is that if two usernames are upvoting one another, the program will get stuck alternating between these two. The idea of seeing repeating sets of usernames will hopefully allow this to scale up to larger groups of upvote circles. For example, if 4 users all upvote each other, the program may start to cycle through these 4 names. There would probably be some filtering needed. For most users, it's likely they've only voted for other users a low number of times, so filtering out vote tallies of 1 or 2 probably makes sense. If UserA has upvoted other users once or twice, but upvoted UserB 27 times,, that's what we'd be looking for. We would then go look at UserB's voting history... This is probably a crappy way to do this, and I've already thought of a few problems: 1) Not sure if it is possible to access random users' vote history even w/ OAuth 2) Will be bad at detecting a \"funnel\"-type upvote group, where many accounts upvote one user, but that user deoes not reciprocate 3) Will be very slow, crawling through one user at a time. Perhaps it's best to use this on suspected upvote rings rather than to sniff them out? 4) Since upvote counts are changing dynamically, it's possible the program might miss something if a user happened to upvote someone not in the upvote circle at the right time **My questions for you all -** 1) General thoughts on the idea? Does it seem useful? 2) Is this even possible to do within PRAW? You guys know the barriers far better than I do at this point... 3) Any suggestions on a better approach?", "title": "Wanted to run an idea by you guys - I'm new to praw/APIs", "url": "https://www.reddit.com/r/redditdev/comments/2vaun5/wanted_to_run_an_idea_by_you_guys_im_new_to/"}, {"author": "Fireislander", "created_utc": 1423432540, "gilded": 0, "name": "t3_2v8dic", "num_comments": 2, "score": 1, "selftext": "I am trying to update praw on my mac and I keep getting the following error http://pastebin.com/3uwvCmgc Any help would be appreciated", "title": "Unable to update PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2v8dic/unable_to_update_praw/"}, {"author": "chrisarr", "created_utc": 1423367338, "gilded": 0, "name": "t3_2v5s0c", "num_comments": 6, "score": 6, "selftext": "Hey r/RedditDev community, I have been working on a project as part of an Interaction Design Graduate Thesis project utilizing Reddit AMA. The project, called AskUsAnything, is meant to be an annotated and analytic tool for reading AMA threads. I am interested in bringing a new way of viewing and understanding, even comparing and contrasting AMAs, with the intention of making the wealth of individual information and knowledge in each thread viewable in a new and unique way. The prototype is online at [http://askusanything.cc](http://askusanything.cc) I wanted to bring the prototype version to the subreddit here so that some people who have spent a lot of time working on the platform and with the API could see the project. The project is emphatically \"beta\" at this stage, and I am fast at work on the next version. I have built this on Python/Flask, using the Reddit API and PRAW, utilizing NLTK for language analysis, BokehJS for visualization, and deployed on a DigitalOcean Ubuntu droplet. If you have any feedback on how you might use something like this (as potentially a Reddit/AMA user), how I might do something differently, or if this work reminds of you anything you have seen someone doing with Reddit data, I would love to hear about it. Cheers!", "title": "Building a tool for Reddit AMA \u2013 Take a peek!", "url": "https://www.reddit.com/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/"}, {"author": "Midgetapple5", "created_utc": 1423361266, "gilded": 0, "name": "t3_2v5ho4", "num_comments": 3, "score": 2, "selftext": "hi, I'm new to praw and python in general, I was wondering what the best way is to make a bot that makes a post in a subreddit once a week. I've looked up guides, but all they tell me is how to make bots that reply to comments.", "title": "Help making a bot that posts once a week", "url": "https://www.reddit.com/r/redditdev/comments/2v5ho4/help_making_a_bot_that_posts_once_a_week/"}, {"author": "Marorin", "created_utc": 1423237417, "gilded": 0, "name": "t3_2uzqy7", "num_comments": 5, "score": 1, "selftext": "Hey there guys. First timer here as the topic indicates. I like to think I'm a bit of a programmer, recently trying to get into Python for a class. I wanted to utilize the Reddit API for a presentation on it's use for web programming. I was thinking to use it to show like the top 10 hottest reddits for a subreddit. I was wondering how feasible that would be and whether it be better to use the PRAW library instead. I do apologize for being a tad sparse, classes started pretty recently so I'm pretty much willing to take in suggestions and advice for more experienced developers.", "title": "First Timer here looking for guidance", "url": "https://www.reddit.com/r/redditdev/comments/2uzqy7/first_timer_here_looking_for_guidance/"}, {"author": "Kim___Jong___Un", "created_utc": 1422404080, "gilded": 0, "name": "t3_2twjw9", "num_comments": 4, "score": 2, "selftext": "I am trying to figure out how to keep my bot from replying to a specific sub. I know that in order to add specific subs, I would do SUBREDDIT = \"FirstSubName+SecondSubName\" but I tried doing a \"-\" instead of \"+\" and it didn't work. Does anyone know how to exclude a specific sub without having a moderator ban my bot? I looked at the PRAW docs and did not find what I seek. Cheers for any insights.", "title": "Excluding specific subs from bot replies (PRAW)", "url": "https://www.reddit.com/r/redditdev/comments/2twjw9/excluding_specific_subs_from_bot_replies_praw/"}, {"author": "SurviAvi", "created_utc": 1422225264, "gilded": 0, "name": "t3_2tnplt", "num_comments": 3, "score": 5, "selftext": "My link bot for /r/archlinux wasn't able to run for the past few days, since I kept on getting this error: Traceback (most recent call last): File \"bot/allb.py\", line 218, in for item in praw.helpers.comment_stream(r, SUBR, limit = None, verbosity = 0): File \"/usr/local/lib/python3.2/dist-packages/praw/helpers.py\", line 138, in _stream_generator for i, item in gen: File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 504, in get_content page_data = self.request_json(url, params=params) File \"/usr/local/lib/python3.2/dist-packages/praw/decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 562, in request_json delattr(self, '_request_url') AttributeError: _request_url Has anyone experienced this error? I could't find anything similar. Source: https://gist.github.com/phikal/271374cf5eee9c659c7f", "title": "'comment_stream' error", "url": "https://www.reddit.com/r/redditdev/comments/2tnplt/comment_stream_error/"}, {"author": "KnightOfDark", "created_utc": 1422191356, "gilded": 0, "name": "t3_2tlzxl", "num_comments": 8, "score": 1, "selftext": "I'm using a praw-based subreddit scraper to gather corpora for a linguistics project. Before the release of 2.1.20 the scraper worked with no issue - I have already gathered about 3.000 threads worth of comments. Unfortunately, the following code now produces assertion errors: agent = praw.Reddit(user_agent='Subreddit scraper for corpus gathering') sub = agent.get_subreddit(subreddit) content_generator = sub.get_new(limit=thread_limit) for thread in content_generator: thread.replace_more_comments(limit=None, threshold=0) I have narrowed the fault down to the replace_more_comments methods, as scraping threads with no MoreComment objects does not result in errors. Two different assertion errors are produced: Traceback (most recent call last): File \"Scraper.py\", line 194, in SubredditToCSV(sub_name, limit=thread_limit, separator='/') File \"Scraper.py\", line 175, in SubredditToCSV threads, users, comments = ScrapeSubreddit(subreddit, limit) File \"Scraper.py\", line 62, in ScrapeSubreddit thread.replace_more_comments(limit=None, threshold=0) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 1122, in replace_mo re_comments new_comments = item.comments(update=False) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 700, in comments return self._continue_comments(update) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 683, in _continue_c omments assert len(self.children) == 1 and self.id == self.children[0] AssertionError And: Traceback (most recent call last): File \"Scraper.py\", line 194, in SubredditToCSV(sub_name, limit=thread_limit, separator='/') File \"Scraper.py\", line 175, in SubredditToCSV threads, users, comments = ScrapeSubreddit(subreddit, limit) File \"Scraper.py\", line 62, in ScrapeSubreddit thread.replace_more_comments(limit=None, threshold=0) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 1122, in replace_mo re_comments new_comments = item.comments(update=False) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 700, in comments return self._continue_comments(update) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 686, in _continue_c omments assert len(self._comments) == 1 AssertionError Is this a bug in the newest release, or do I need to change something in my program?", "title": "[PRAW] Assertion errors on replace_more_comments after 2.1.20 release", "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "Isagoge", "created_utc": 1422028646, "gilded": 0, "name": "t3_2tevir", "num_comments": 3, "score": 1, "selftext": "Hello guys, I'm writing a bot on a separate account to help me get relevant posts from the subreddits i'm subscribed to on my main account. What i did was basically grab a list of my subscribed subreddit and write them in a text file, i use the script to refresh the list when i subscribe to new subreddits. After i use another script to try and subscribe to each of the subreddits that are in the text file. The script run without hassle, but when i check on the account, it didn't manage to subscribe to the subreddits. I wanted to know if you guys could help me with this. Here is the code i used to subscribe to the subreddit. The text file is basically only the name of the subreddits with one on each line. r = praw.Reddit(USERAGENT) r.login(REDDIT_USERNAME, REDDIT_PASS) print(\"Logging into Reddit\") with open(\"Subreddits.txt\", \"r\") as p: liste = p.read().split(\"\\n\") for i in range(len(liste) - 1): subreddit = r.get_subreddit(liste[i]) print(\"Obtention du subreddit: \", liste[i]) subreddit.subscribe() print(\"Suscribing to: \", liste[i])", "title": "Problems subscribing to a list of subreddits from a .txt file", "url": "https://www.reddit.com/r/redditdev/comments/2tevir/problems_subscribing_to_a_list_of_subreddits_from/"}, {"author": "letgoandflow", "created_utc": 1421942160, "gilded": 0, "name": "t3_2taq2w", "num_comments": 7, "score": 1, "selftext": "I posted a thread about this yesterday, but I think I've zeroed in on the actual problem (hopefully). I'm creating an authorization URL using PRAW: oauth_link = r.get_authorize_url( session['oauth_token'], ['identity', 'submit'], True ) This part is working just fine. PRAW creates the URL properly and the user is asked to allow permissions for both scopes. When the user returns after giving my app permission, the scope is returned in this format: set([u'identity submit']) This looks like PRAW is interpreting multiple scopes as a single string ('identity submit'), which doesn't match any of the actual scopes. So I have an authorized user with no permissions. Note that if I only ask for a single scope, it works just fine. I'm fairly confident that something has changed with the reddit API because I had two separate apps start showing this problem at the same time and no changes have been made to either of them.", "title": "[PRAW] Not able to authorize a user with multiple OAuth2 scopes", "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "themetallurgist", "created_utc": 1421893489, "gilded": 0, "name": "t3_2t8t9v", "num_comments": 7, "score": 3, "selftext": "When I use the code import praw, time r = praw.Reddit(user_agent=\"Bot experiment\") r.login('redacted', 'redacted') I get the traceback: Traceback (most recent call last): File \"redacted\", line 5, in r.login('redacted', 'redacted') File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/__init__.py\", line 1263, in login self.request_json(self.config['login'], data=data) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/__init__.py\", line 519, in request_json response = self._request(url, params, data) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/__init__.py\", line 383, in _request _raise_response_exceptions(response) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/internal.py\", line 172, in _raise_response_exceptions response.raise_for_status() File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/models.py\", line 831, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 403 Client Error: Forbidden I've tried everything I can think of. Mac and Windows, python 3.4 & 2.7, different accounts, I've run it from IDLE and the terminal. I tried not putting in my username and password and then trying to login when prompted. I just have no idea. Does anyone have any clues?", "title": "Can't login with praw", "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "zzpza", "created_utc": 1421703400, "gilded": 0, "name": "t3_2sz98i", "num_comments": 4, "score": 1, "selftext": "If I run the following script, there is a post missing from the results. ---- zzpza@linux:~/reddit/scripts/testing$ cat search.py #!/usr/bin/env python # from datetime import date, timedelta, datetime from time import strptime import calendar import praw import csv import unicodedata currentWeek = int(format(datetime.now().strftime('%V'))) print \"Current Week: %d\" % currentWeek r = praw.Reddit(user_agent=\"/r/analog sticky post rotation script v0.1 by /u/zzpza\") r.login(\"zzpza\", \"[redacted]\") #subreddit = r.get_subreddit('analog') with open('analog.csv', 'w') as fp: writer = csv.writer(fp) for submission in r.search('flair=Community',subreddit='analog',sort='new',period='week'): id = submission.id print \"ID: %s\" % id title = submission.title title_ascii = unicodedata.normalize('NFKD', title).encode('ascii','ignore') print \"Title: %s\" % title_ascii url = submission.short_link author = submission.author a=(id, title_ascii, url, author) writer.writerow(a) ---- When I run the script, I get two results: zzpza@linux:~/reddit/scripts/testing$ ./search.py Current Week: 4 ID: 2svvrx Title: Weekly 'Gear Photos & Discussion' - Week 04 ID: 2svvr0 Title: Weekly 'Ask Anything About Analog Photography' - Week 04 zzpza@linux:~/reddit/scripts/testing$ ---- If I run the search via a browser (http://www.reddit.com/r/analog/search?q=flair%3Acommunity&sort=new&restrict_sr=on&t=week) I get three results. My initial theory was that it could be that the period='week' is absolute and not relative. The missing post from the search results was posted in a different week if you go by number, but less than a week ago in time. I changed period='week' to period='month' and it still won't return the [OTW] post. How come I don't see the same from the PRAW results as I do in a browser? How do I get the missing result to show up in my PRAW script? Thanks! :) (Also, sorry for the sloppy code - I'm a beginner).", "title": "PRAW search not returning posts expected", "url": "https://www.reddit.com/r/redditdev/comments/2sz98i/praw_search_not_returning_posts_expected/"}, {"author": "letgoandflow", "created_utc": 1421698333, "gilded": 0, "name": "t3_2syxjd", "num_comments": 4, "score": 0, "selftext": "[Here is the warning](https://praw.readthedocs.org/en/v2.1.19/pages/oauth.html#step-2-setting-up-praw): > This example, like most of the PRAW examples, binds an instance of PRAW to the r varaible. While we\u2019ve made no distinction before, r (or any instance of PRAW) should not be bound to a global variable due to the fact that a single instance of PRAW cannot concurrently manage multiple distinct user-sessions. >If you want to persist instances of PRAW across multiple requests in a web application, we recommend that you create an new instance per distinct authentication. Furthermore, if your web application spawns multiple processes, it is highly recommended that you utilize PRAW\u2019s multiprocess functionality. Currently, my app is setting up the PRAW instance using the r variable when the app is initialized. Based on the warning, I'm pretty sure I shouldn't be doing this anymore, but I'm not sure how I should be doing it differently.", "title": "Not sure what to do about warning in the PRAW docs regarding binding PRAW instance to global variables.", "url": "https://www.reddit.com/r/redditdev/comments/2syxjd/not_sure_what_to_do_about_warning_in_the_praw/"}, {"author": "Hirayu", "created_utc": 1421561022, "gilded": 0, "name": "t3_2st4us", "num_comments": 2, "score": 3, "selftext": "Hello all! I recently have started to learn python and I wanted to try to build a bot using praw but I am currently stuck in this situation. I have tried following the praw tutorial and have managed to login and send messages with my bot(yay!). I want my bot to be able to pull from this website http://www.hearthpwn.com/decks/158447-fakenicks-legend-1-eu-season-10-pala and just post the deck list as a comment. Unfortuantely I am unsure on how to begin. I think I may need to use a web scraper of some sort? If someone could point me in the right direction, I would be most grateful. I know Java and C++ so I am not a complete programming noobie but any help is appreciated. Thanks!", "title": "Trying to pull data from a website?", "url": "https://www.reddit.com/r/redditdev/comments/2st4us/trying_to_pull_data_from_a_website/"}, {"author": "darkfire613", "created_utc": 1421471001, "gilded": 0, "name": "t3_2sphkf", "num_comments": 0, "score": 1, "selftext": "I'm learning to write reddit bots, and my first project using praw simply scans submissions to a subreddit's new and records the usernames of posters. I'm using place_holder to prevent getting the name from the same posts twice. However, place_holder is stopping at the provided ID inclusively. That is, I get the list of all the new posts' usernames, including the username of the post whose ID is the placeholder. This means that username gets doubled up on my list. I think I'm probably using place_holder incorrectly. If anyone has tips on a better way to get just new posts since my last request, let me know. Thank you! firstloop = True for submission in subreddit.get_new(limit=25, place_holder=placeholder): username = submission.author f.write(str(username) + '\\n') if firstloop: placeholder = submission.id firstloop = False And sample output when I let the loop run four times on /r/redditdev with a limit of 5 posts: darkfire613 Oxilic avapoet wscottsanders kemitche darkfire613 darkfire613 darkfire613 As you can see, my name gets duplicated because I have the post whose ID is being used as the placeholder.", "title": "using Praw, subreddit.get_new(place_holder) returning info for placeholder ID twice", "url": "https://www.reddit.com/r/redditdev/comments/2sphkf/using_praw_subredditget_newplace_holder_returning/"}, {"author": "wscottsanders", "created_utc": 1421360857, "gilded": 0, "name": "t3_2skb9u", "num_comments": 5, "score": 1, "selftext": "I'm teaching a class this semester that is using reddit as a forum for class discussion, Q&A, and chatting with guests. I am writing a bot that will self post an update for the students about the karma they earn in the subreddit. Currently, my biggest difficulty is bypassing the captcha. I know this must be possible based on some of the silly bots I've seen on reddit. I've [read the praw documentation](https://praw.readthedocs.org/en/v2.1.19/pages/faq.html#how-can-i-handle-captchas-myself) and I can't seem to figure out how this is done. Any hints would be greatly appreciated? This is the function I have for posting to reddit: def post_to_reddit(message): captcha ={'iden': 'EBG2KudNny2pB6o5x2VTOIeyjT9n92SH', 'captcha': 'ILBZVT'} r.submit('poitest', 'Captcha Works!', text=message, raise_captcha_exception=True, captcha=captcha) print \"Done.\"", "title": "Bypassing Captcha to Post Updates to a Reddit?", "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "Brandon23z", "created_utc": 1421291800, "gilded": 0, "name": "t3_2sh6sp", "num_comments": 5, "score": 1, "selftext": "In reddit, you can hyperlink [](), italics *_*, bold **_**, that kind of stuff. How can I do that in PRAW? Right now my bot just uses plain text, no formatting. I'd like to include hyperlinks. Here is my reply code. SETRESPONSE1 = \"What ain't no country I ever heard of.\" comment.reply(SETRESPONSE1) How can I reply with a hyperlink? Do I just use reddit formatting when replying to the comment?", "title": "How can I use reddit formatting with a bot? Using PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/2sh6sp/how_can_i_use_reddit_formatting_with_a_bot_using/"}, {"author": "hassanchug", "created_utc": 1421100163, "gilded": 0, "name": "t3_2s7mcc", "num_comments": 5, "score": 2, "selftext": "How would I go about using PRAW with Google App Engine? I've set up most of the stuff, and have the praw folder in the root directory of my app-engine folder, but when I try to deploy my project, I get the following error: `ImportError: No module named requests` If I then copy over the \"requests\" folder from the \"site-packages\" directory into the root of the app-engine folder and try to re-deploy, I get another list of errors, such as \"No module named _ssl\" and so on. Anybody have any experience with using PRAW on GAE (or generally importing any third-party libraries into GAE)?", "title": "Using PRAW with Google App Engine?", "url": "https://www.reddit.com/r/redditdev/comments/2s7mcc/using_praw_with_google_app_engine/"}, {"author": "jakethespectre", "created_utc": 1420536997, "gilded": 0, "name": "t3_2ri0r7", "num_comments": 11, "score": 3, "selftext": "I have been trying to make a script that will automatically update the stylesheet on my subreddit /r/jakethespectre. I'm a noob with python and I didn't even understand what an API was 2 days ago, so forgive silly questions. Here is my code: http://pastebin.com/SgbT2mAS (I wrote a bit on the error messages I get next to the def calls at the bottom) Questions: **1.** Why isn't https://ssl.reddit.com/api/v1/access_token in any documentation? It seems pretty important so if there's some doc I'm missing, please point me to it. (also for oauth.reddit.com?) **2.** Why the hell am I getting so many 429 errors? Is there a way to prevent that? I read it was from doing an invalid login too many times but my login info has been correct the whole time. On the documentation it says that I could get that if it's sending requests more than 30 times in 2 seconds, but even if I ran the program with all the functions uncommented that would be 5 requests and I get that even after I've only ran the program once. **3.** I also sometimes get 401, 403, and 404 errors with login() and getModhash() as well. I dont know what they mean and why I'm only getting them SOMETIMES. **4.** Even after login has returned a 200, getModhash() will still give me this: {\"json\": {\"errors\": [[\"USER_REQUIRED\", \"please sign in to do that\", null]]}} **5.** Why isn't getModhash() working? I can get the modhash by typing that address into my browser, and every time I refresh I can get a new code. This function also sometimes gives me {}. Thanks a lot, me.json :/ **6.** Am I making this much more complicated than it needs to be? Note: I'm not using praw because [apparently](https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/) something was updated and it isn't working. I think that's all the questions I had, although I'll probably remember another one approximately two seconds after I click submit on this post. **Ninja_edit:** formatting", "title": "A lot of questions about the reddit API", "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "sallurocks", "created_utc": 1420444309, "gilded": 0, "name": "t3_2rdtf1", "num_comments": 2, "score": 2, "selftext": "Has somebody tried to run a bot in qpython which uses praw? It does support pip so I installed praw but gives errno 21 when I try to import praw, here's the log Traceback (most recent call last): File \"test.py\", line 1, in import praw File \"/data/data/com.hipipal.qpyplus/files/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\", line 56, in platform_info = platform.platform(True) File \"/QPython/core/build/python-install/lib/python2.7/platform.py\", line 1575, in platform File \"/QPython/core/build/python-install/lib/python2.7/platform.py\", line 163, in libc_ver IOError: [Errno 21] Is a directory: '/storage/emulated/0/com.hipipal.qpyplus/scripts'", "title": "Praw in qpython?", "url": "https://www.reddit.com/r/redditdev/comments/2rdtf1/praw_in_qpython/"}, {"author": "jakethespectre", "created_utc": 1420430066, "gilded": 0, "name": "t3_2rd7n3", "num_comments": 9, "score": 3, "selftext": "I haven't even started to write the actual code, I'm a n00b with python and praw. I'm very confused. this is my code: r = praw.Reddit('Currently, it is trying to login to my account' 'I plant to have it auto-submit my stylesheet' 'Made by /u/jakethespectre, v1.0') r.login('username','password'); And this is the error: Traceback (most recent call last): File \"C:\\Users\\Steam\\Desktop\\sendStylesheet.py\", line 6, in r.login('username','password'); File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 1266, in login self.user = self.get_redditor(user) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 890, in get_redditor return objects.Redditor(self, user_name, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 663, in __init__ fetch, info_url) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 72, in __init__ self.has_fetched = self._populate(json_dict, fetch) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 127, in _populate json_dict = self._get_json_dict() if fetch else {} File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 120, in _get_json_dict as_objects=False) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 526, in request_json data = json.loads(response, object_hook=hook) File \"C:\\Python34\\lib\\json\\__init__.py\", line 318, in loads return _default_decoder.decode(s) File \"C:\\Python34\\lib\\json\\decoder.py\", line 343, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"C:\\Python34\\lib\\json\\decoder.py\", line 361, in raw_decode raise ValueError(errmsg(\"Expecting value\", s, err.value)) from None ValueError: Expecting value: line 1 column 1 (char 0) Edit from nath_schwarz: Check what you are copy/pasting!", "title": "Can't login with praw?", "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "armandg", "created_utc": 1420314772, "gilded": 0, "name": "t3_2r8974", "num_comments": 2, "score": 2, "selftext": "Hey guys, I'm fiddling with a bot for /r/Tippeligaen that pushes the newest submissions to the twitter-bot. I use praw for the fetching and I use this script (a simple version): done_subm = [] for submission in subreddit.get_new(limit=5): if submission.id not in done_subm: title = submission.title url = submission.short_link text = title + ': ' + url [code for publishing on twitter] done_subm.append(submission.id) The problem is that the script starts, and haven't published anything yet, it publishes the tweets in the wrong direction when compared with the subreddit, in the way that the oldest submission is the newest tweet. When the script is running, it works perfect, because it always fetches the newest submission and publishes it. Any tips on how to fix this?", "title": "Pushing submissions to twitter, but not correct sort?", "url": "https://www.reddit.com/r/redditdev/comments/2r8974/pushing_submissions_to_twitter_but_not_correct/"}, {"author": "zooparoo_skidoo", "created_utc": 1420149608, "gilded": 0, "name": "t3_2r1g5a", "num_comments": 2, "score": 1, "selftext": "I am trying to get a list of replies to a comment. I have been able to load a comment object, where I know there are replies, but when polled, I see no reply objects r = praw.Reddit(user_agent='zooparoo') comment_object = r.get_info(url=None,thing_id='t1_cnbign0',limit=None) At that point, I should have a praw.object.Comment object. I know there is a reply, but if I invoke: replies_to_comment_object = comment_object.replies I get an empty variable when I should be seeing info on the child object, t1_cnbigut. Should I be accessing comments a different way?", "title": "Proper way to access comment replies in PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2r1g5a/proper_way_to_access_comment_replies_in_praw/"}, {"author": "GAPINGCUNT", "created_utc": 1420004551, "gilded": 0, "name": "t3_2qw7gu", "num_comments": 4, "score": 2, "selftext": "On one of the subreddits I moderate, 99.9% of the posts that are caught by the modqueue are not spam. I want to have my reddit bot automatically approve these posts, but leave anything else (reported comments, etc) alone for me to sift through when I go into the queue. import praw user_agent = ('Automoderator for GAPINGCUNT') r = praw.Reddit(user_agent=user_agent) r.login(username,password) queue = r.get_mod_queue(subreddit='TestSubreddit') for item in queue: ### Need to check to see if it is a post, this line of code is not real ### if item == post: item.approve() Is this possible, or is my bot not going to discriminate against anything in the mod queue?", "title": "Check if item in Modqueue is a flagged post or a reported comment?", "url": "https://www.reddit.com/r/redditdev/comments/2qw7gu/check_if_item_in_modqueue_is_a_flagged_post_or_a/"}, {"author": "haiguise1", "created_utc": 1419962411, "gilded": 0, "name": "t3_2qu0tu", "num_comments": 6, "score": 1, "selftext": "I've been using PRAW to get comments from Reddit and a couple days ago I started getting 403 Client error: Forbidden errors. This program has been running for close to a month without spitting up any errors. I think it has something to do with reddit's 30 requests per minute rule, but I was timing the requests in my program and they all waited 2 seconds between requests. However, after 30 requests had been made the error pops up. Am I being locked out for some reason? ps. I'm using PRAW 2.1.19 edit: I figured out the problem, HistoricalWhatIf had been set to private, and it was number 30 in my list of subreddits.", "title": "403 error using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": "rhiever", "created_utc": 1419882605, "gilded": 0, "name": "t3_2qqm9j", "num_comments": 4, "score": 2, "selftext": "It's been ages since I had to log in to scrape reddit data, but I'm trying to access some mod-only data this time. Whenever I run the code: r = praw.Reddit(user_agent=\"/u/rhiever mod log scraping\") r.login(\"username\", \"password\") (obviously with my login information) I get the following error: >HTTPError: 403 Client Error: Forbidden which seems to be happening at: >1263 self.request_json(self.config['login'], data=data) Anyone else having this issue?", "title": "Can't log in with PRAW any more?", "url": "https://www.reddit.com/r/redditdev/comments/2qqm9j/cant_log_in_with_praw_any_more/"}, {"author": "RuleIV", "created_utc": 1419773624, "gilded": 0, "name": "t3_2qm6ml", "num_comments": 2, "score": 2, "selftext": "###Resolved. Thank you /u/largenocream I discovered today that get_new() will not collect low karma submissions in the same way that they are hidden from users by default. Is it possible to get a full list of submissions? Relevant section of code I'm using to get submissions. user_agent = (\"redacted\") r = praw.Reddit(user_agent = user_agent) subreddit = r.get_subreddit(SUB_NAME) r.login(CRED_USER, CRED_PASS) new_submissions = subreddit.get_new(limit = SCAN_LIMIT) Thanks.", "title": "[PRAW] Prevent get_new() hiding low karma submissions.", "url": "https://www.reddit.com/r/redditdev/comments/2qm6ml/praw_prevent_get_new_hiding_low_karma_submissions/"}, {"author": "minlite", "created_utc": 1419333213, "gilded": 0, "name": "t3_2q5zly", "num_comments": 25, "score": 8, "selftext": "Hi all. As you might know at /r/millionairemakers we are trying to run a drawing using the comments retrieved from reddit API. Our picking method is described in detail [here](http://www.reddit.com/r/millionairemakers/comments/2ournt/explanation_of_our_new_drawing_system_inspired_by/). I have wrote a python script to grab the comments and generate the winner using PRAW, but the issue is that apparently 110K comments is too much for the reddit API to handle. The code is available here: https://github.com/millionairemakers/millionairemakers More specifically, the issue rises in [this](https://github.com/millionairemakers/millionairemakers/blob/master/webserver.py) file at line 152, where the system tries to replace the \"more\" comments. submission.replace_more_comments(limit=None, threshold=0) I get the following error: Exception in thread Thread-1: Traceback (most recent call last): File \"/usr/lib64/python2.7/threading.py\", line 811, in __bootstrap_inner self.run() File \"webserver.py\", line 150, in run submission.replace_more_comments(limit=None, threshold=0) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/objects.py\", line 1029, in replace_more_comments new_comments = item.comments(update=False) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/objects.py\", line 638, in comments response = self.reddit_session.request_json(url, data=data) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\", line 519, in request_json response = self._request(url, params, data) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\", line 383, in _request _raise_response_exceptions(response) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/internal.py\", line 172, in _raise_response_exceptions response.raise_for_status() File \"/usr/lib/python2.7/site-packages/requests-2.5.0-py2.7.egg/requests/models.py\", line 829, in raise_for_status raise HTTPError(http_error_msg, response=self) HTTPError: 413 Client Error: Too Big I know this is coming from reddit and not from the client, but is there anyway to fix this? Thanks.", "title": "Error 413 : Too Big with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "picflute", "created_utc": 1419147757, "gilded": 0, "name": "t3_2pyi07", "num_comments": 2, "score": 3, "selftext": "Been a while since I've been to redditdev. Picking up developing bots again and I'm getting this [traceback](http://pastebin.com/raw.php?i=1aueQRLz) and I don't know what it means. My Code: import praw r = praw.Reddit(\"Test Bot\") r.login('user','pass') I have SSL enabled on this account as well.", "title": "Value Error when trying to login?", "url": "https://www.reddit.com/r/redditdev/comments/2pyi07/value_error_when_trying_to_login/"}, {"author": "chaz6", "created_utc": 1419098451, "gilded": 0, "name": "t3_2pwgc9", "num_comments": 6, "score": 1, "selftext": "How can I get comments for a submission ordered by highest rated first? This is my sample code which gets the comments in no particular order:- import praw r = praw.Reddit('PRAW') submissions = r.get_subreddit('AskReddit').get_top_from_month(limit=20) for submission in submissions: submission.replace_more_comments(limit=64, threshold=10) flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: print(comment)", "title": "[PRAW] Get comments for a submission ordered by votes", "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "brucemo", "created_utc": 1418434303, "gilded": 0, "name": "t3_2p4un7", "num_comments": 0, "score": 4, "selftext": "I know that the idea of removing a comment in mod mail is weird, but it happens from time to time, and I'd like to know if it is possible to detect this when writing a mod mail scanner with PRAW. I don't see any fields that pertain, and when I looked through the JSON dict there didn't seem to be anything there, as well. Thank you.", "title": "Detecting removed mod mail", "url": "https://www.reddit.com/r/redditdev/comments/2p4un7/detecting_removed_mod_mail/"}, {"author": "Sleepydragn1", "created_utc": 1418421321, "gilded": 0, "name": "t3_2p48qb", "num_comments": 4, "score": 3, "selftext": "I'm pretty new to PRAW, and I'm wondering if it's possible to grab all the submissions to a particular subreddit within a particular time frame. Subreddit.get_new() allows for a limit, but I cannot seem to find a parameter/arg that's based on date. Any ideas?", "title": "[PRAW] Getting all posts to a subreddit within a certain time frame?", "url": "https://www.reddit.com/r/redditdev/comments/2p48qb/praw_getting_all_posts_to_a_subreddit_within_a/"}, {"author": "coolstorym8", "created_utc": 1418137863, "gilded": 0, "name": "t3_2ordbo", "num_comments": 7, "score": 1, "selftext": "I'm trying to cover the case when my internet goes down. Assume I've logged in and when im just about to call get_subreddit, my internet cuts. Currently you can see that there is a silent timeout that occurs and no requests.ConnectionError is raised. I want it to appear so I can then drop my function that will wait for a reconnection. Here is my test code.. (not actual, Just for demonstration. I have covered error cases for login) USERAGENT = \"blah blah blah\" USERNAME = \"username\" PASSWORD = \"password\" r = praw.Reddit(USERAGENT) r.login(USERNAME, PASSWORD) print \"Disconnect Now\" time.sleep(5) submissions = r.get_subreddit('videos').get_hot(limit=30) You run this code and disconnect the internet when it prints to do so. Watch what happens when it calls get_subreddit. I want it to force the error instead of remaining silent.", "title": "How do I force an exception error instead of getting a silent timeout when using get_subreddit with no internet connection?", "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "hooked_dev", "created_utc": 1418054023, "gilded": 0, "name": "t3_2onkd8", "num_comments": 8, "score": 2, "selftext": "I'm trying to debug my code that rips entire threads of reddit. Everything seems to be working, but it seems like there is hard cap on the depth of a conversation in a reddit thread (specifically a depth of ten). It looks like there is a problem with the \"continue this thread\" links. For example, consider: http://www.reddit.com/r/test/comments/2onit4/test_depth_0/ How can I get PRAW to pull the entire thread?", "title": "[PRAW] Maximum depth of subtrees?", "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "SexySatan", "created_utc": 1417461439, "gilded": 0, "name": "t3_2nysxy", "num_comments": 0, "score": 2, "selftext": "My goal: Retrieve the post titles from the first page of a subreddit using Java. What I've done: Most suggestions I've seen online suggest using jReddit for Java. Their example code seems to do what I need and can be found here. https://github.com/karan/jReddit/blob/master/examples/GetPaginatedTopics.java I've downloaded this entire project and created a JAR using Maven, and added the JAR as an external JAR to the project in eclipse. This method only resolved one of the imports. import com.github.jreddit.utils.restclient.RestClient; The other 4 are still invalid. I've also tried using a premade JAR found here: http://mvnrepository.com/artifact/com.github.jreddit/jreddit/1.0.0 This will resolve the first two imports but not the others. It seems as though different versions of jReddit will allow different parts of the example code to work, but I can't find a version that will make the entire thing function. I'm beginning to think that this example code is just nonfunctional so I look online for different examples and found this reddit thread: http://www.reddit.com/r/redditdev/comments/2col0f/problems_with_jreddit/ The poster who solved the problem for the OP is using a JAR with an identical name to mine, however, I appear to have some missing functions. My User class doesn't have the .about method so I can't properly initialize UserInfo. Just to test I scrapped most of the code except for RestClient restClient = new HttpRestClient(); User user = new User(restClient, \"username\", \"password\"); user.connect(); which will compile just fine, but results in a run time error. At this point I'm frustrated because I've made much more complex things via PRAW with little effort, but I need to use Java because I'm trying to make a simple android app. Does anyone have any insight on how to achieve this in Java? What I need to do is simple enough that jReddit may not even be required, but I'd like to get it working so that I can have the option of adding more advanced features.", "title": "Java: Get top post titles from subreddit", "url": "https://www.reddit.com/r/redditdev/comments/2nysxy/java_get_top_post_titles_from_subreddit/"}, {"author": "beardgoggles", "created_utc": 1416960783, "gilded": 0, "name": "t3_2nfe4j", "num_comments": 4, "score": 2, "selftext": "I'm building my first Reddit API bot and trying to figure out how to call api/me.json. I've been doing the PRAW tutorial here: http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html But I'm not really clear on how to adapt the other parts of the Reddit API to PRAW. Does anyone have an example of how to use me.json with PRAW?", "title": "Voting by API, can't find examples of me.json with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2nfe4j/voting_by_api_cant_find_examples_of_mejson_with/"}, {"author": "minlite", "created_utc": 1416949628, "gilded": 0, "name": "t3_2nesds", "num_comments": 5, "score": 3, "selftext": "Hi. I'm a mod over at /r/millionairemakers and we just had a drawing last night. If you check out the drawing thread you will see that there are over 8000 comments. What I want to accomplish is to retrieve a list of usernames from the top level comments so we can do the drawing as fairly as possible. For that, I wrote a PRAW script that retrieves the comments and then replaces the \"more\" kind with their respective comments: print \"Getting comments...\" submission = r.get_submission(submission_id='') print \"Replacing all more comments...\" submission.replace_more_comments(limit=None, threshold=0) print \"Flatting comments\" flat_comments = praw.helpers.flatten_tree(submission.comments) f = open('usernames', 'w') for comment in flat_comments: if type(comment) is praw.objects.Comment: if type(comment.author) is praw.objects.Redditor: f.write(comment.author.name + \"\\n\") This works fine, but the issue is that it takes a lot of time because PRAW descends to the children of the top level comments as well to replace the \"more\" kind, making hundreds of unnecessary requests. Of course, I can check `is_root` when writing to my file, but I want the script to run as fast as possible and do not make useless requests. Is there anyway to limit the `replace_more_comments` method to only replace the top level comments? In my calculations, if I have 8000 comments and reddit returns 200 comments per request, I would need to make 40 requests. Taking the 30 req/minute limit into account, it shouldn't take more than 2 minutes for this script to run, but currently it takes 25 minutes which is really unacceptable.", "title": "Make PRAW only replace top level comments (not the children)", "url": "https://www.reddit.com/r/redditdev/comments/2nesds/make_praw_only_replace_top_level_comments_not_the/"}, {"author": "danielvd", "created_utc": 1416877238, "gilded": 0, "name": "t3_2nblsd", "num_comments": 1, "score": 2, "selftext": "Hi I'm currently developing an Android application for my mobile computing class and had a question concerning OAuth. Background - * Reddit application to allow users to post to reddit in an attempt to earn more karma * Allow users to verify via the application and receive an access token Where I'm currently at - * I've managed to authorize the user via client-side using AndrOAuth and have received an access token. * On top of that I have a REST API which interfaces over PRAW, and was planning to make a POST endpoint so that users can reply to a submission/comment. Problem - I'm struggling to figure out how to allow the user to actually post with PRAW and an access token. Some APIs allow you to just pass the access token and make a post on the user's behalf, however I'm seeing that PRAW doesn't allow this. I intend to keep my API stateless since it's REST, however I don't know what I can do to fix this problem. I've looked up PRAW's OAuth solution online, however it requires sessions on the backend, which would result in a stateful API. Does anyone have any suggestions on what I could do? I'm extremely stuck and anything would be helpful! Suggested solutions after discussion with my group - * Keep a session on the client-side (some sort of boolean) and store the username/password on the Android device. This is safe from what I've heard since it exists on the device only and doesn't pose too many security risks. * After storing the session and storing the credentials, pass the username/password into API calls and call r.login() every time. However I'm not too well-informed on the security risks of this.", "title": "[PRAW] OAuth and replying to comments", "url": "https://www.reddit.com/r/redditdev/comments/2nblsd/praw_oauth_and_replying_to_comments/"}, {"author": "boibtest", "created_utc": 1416420568, "gilded": 0, "name": "t3_2msi7y", "num_comments": 7, "score": 3, "selftext": "I have a few accounts and only have this problem with the account \"boibtest\". And I can log in and out of boibtest through the browser. But when using PRAW, I get this: r.login(\"boibtest\", \"password\") Traceback (most recent call last): File \"C:\\Python33\\lib\\json\\decoder.py\", line 367, in raw_decode obj, end = self.scan_once(s, idx) StopIteration During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"\", line 1, in File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 1266, in login self.user = self.get_redditor(user) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 890, in get_redditor return objects.Redditor(self, user_name, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 663, in __init__ fetch, info_url) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 72, in __init__ self.has_fetched = self._populate(json_dict, fetch) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 127, in _populate json_dict = self._get_json_dict() if fetch else {} File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 120, in _get_json_dict as_objects=False) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 526, in request_json data = json.loads(response, object_hook=hook) File \"C:\\Python33\\lib\\json\\__init__.py\", line 316, in loads return _default_decoder.decode(s) File \"C:\\Python33\\lib\\json\\decoder.py\", line 351, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"C:\\Python33\\lib\\json\\decoder.py\", line 369, in raw_decode raise ValueError(\"No JSON object could be decoded\") ValueError: No JSON object could be decoded * I don't get this error when logging in other accounts with PRAW * I don't get an error logging in as boibtest in a web browser. What am I doing wrong?", "title": "[PRAW] - \"No JSON object could be decoded\" error from r.login(u,p)", "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "letgoandflow", "created_utc": 1416366064, "gilded": 0, "name": "t3_2mqf5z", "num_comments": 7, "score": 3, "selftext": "I want to pull in some submissions from a specific subreddit and I'm having a hard time figuring out how to do so. The PRAW docs are very helpful for a few specific tasks, but once you step outside of those tasks it is hard to find the specific functions/methods that you need. There is the [Code Overview](https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html), but it's a little confusing. For example, I want to use the [get_top method](https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html#praw.objects.Subreddit.get_top) on the Subreddit class, but it doesn't really tell you how to use any parameters that would modify the request (e.g. t or limit). Am I missing something?", "title": "Having trouble figuring out how to use PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "hooked_dev", "created_utc": 1416346920, "gilded": 0, "name": "t3_2mpeqt", "num_comments": 4, "score": 1, "selftext": "I'm still new to PRAW and I'm trying to figure out if I'm doing something wrong. Here is my comment scraper code: import praw R = praw.Reddit('comment_scraper 0.1 by u/hooked_dev') submission = R.get_submission(submission_id=submission_id) submission.replace_more_comments(limit=None, threshold=0) comments = praw.helpers.flatten_tree(submission.comments) With a `submission_id=\"2e4cvj\"` this finishes in 17 seconds and there are 193 comments with a speed of 11/comments per second. Pulling a **large** submission, `submission_id=\"2e4cvj\"` which is the Bill Gates AMA (19625 comments) takes 6400 seconds giving me a speed of 3 comments per second. Is there any way to speed up the download of a large submission? It seems unreasonable to take over an hour and a half for a *single* submission.", "title": "[PRAW] Why does it take an hour to download a large submission?", "url": "https://www.reddit.com/r/redditdev/comments/2mpeqt/praw_why_does_it_take_an_hour_to_download_a_large/"}, {"author": "letgoandflow", "created_utc": 1416273243, "gilded": 0, "name": "t3_2mm9sd", "num_comments": 2, "score": 3, "selftext": "There is some great info about handling captchas in the PRAW docs ([see here](https://praw.readthedocs.org/en/v2.1.19/pages/faq.html#how-can-i-handle-captchas-myself)). It tells you how to raise the InvalidCaptcha exception and get the captcha_id, but for some reason it skips over how to actually get the URL of the corresponding captcha image. I see there is a [/captcha/ API method](http://www.reddit.com/dev/api#GET_captcha_{iden}), but I'm not sure how to use it with PRAW. **Edit:** So apparently you can just build the URL using the following format - http://www.reddit.com/captcha/insert_captcha_id_here.png. Easy enough! Hope this helps someone else that is a n00b like me.", "title": "[PRAW] How to get a captcha image URL after you have the captcha ID", "url": "https://www.reddit.com/r/redditdev/comments/2mm9sd/praw_how_to_get_a_captcha_image_url_after_you/"}, {"author": "heerensharma", "created_utc": 1416170937, "gilded": 0, "name": "t3_2mht2a", "num_comments": 4, "score": 1, "selftext": "I tried pip as well as easy_install. Yet there is something which is not going right. whenever I am trying to import, I am getting this error message: \"Traceback (most recent call last): File \"\", line 1, in ImportError: No module named praw\"", "title": "How to install PRAW in anaconda in mac?", "url": "https://www.reddit.com/r/redditdev/comments/2mht2a/how_to_install_praw_in_anaconda_in_mac/"}, {"author": "PasDeDeux", "created_utc": 1416085927, "gilded": 0, "name": "t3_2merm5", "num_comments": 5, "score": 1, "selftext": "Hi everyone, I'm having a problem with PRAW where the .get_comments() function is only returning ~1000 comments and then dying. Theoretically, get_comments(limit = None) should return all of the comments in a given subreddit, 100 comments at a time. When I do /all I get ~700 comments, /libertarian gives ~990. def downloader(subname): subreddit = r.get_subreddit(subname) corpus = [] comlist = subreddit.get_comments(limit = None) for comment in comlist: commenttext = comment.body.lower() cid = comment.id corpus.append([cid, commenttext]) print(len(corpus)) So just for clarity, comlist is actually a generator object, so it's not like I can just pass that. That wouldn't be economic either, given I'm trying to generate very large amounts of data to save later.", "title": "[PRAW] Hitting a limit with .get_comments(limit = None)", "url": "https://www.reddit.com/r/redditdev/comments/2merm5/praw_hitting_a_limit_with_get_commentslimit_none/"}, {"author": "archon_rising", "created_utc": 1416019103, "gilded": 0, "name": "t3_2mcgcm", "num_comments": 7, "score": 1, "selftext": "Hey guys, I'm trying to scrape all comments in the top 5 posts from a subreddit. After getting these comments, I want to print them on screen. What I see are of the form [] etc. How do I print these comments? I'm slightly new to PRAW. Code is here: https://gist.github.com/anonymous/9b8865b33025e2b697db Thanks!", "title": "[PRAW] Trying to use PRAW to scrape all comments", "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "hooked_dev", "created_utc": 1415822414, "gilded": 0, "name": "t3_2m3qxk", "num_comments": 1, "score": 3, "selftext": "I'm quite new to PRAW, so I'm not sure if it is hidden in the documentation somewhere, but I'm looking for all the top rated posts from a subreddit from the previous week.", "title": "[PRAW] How to pull all top rated posts from \"last\" week.", "url": "https://www.reddit.com/r/redditdev/comments/2m3qxk/praw_how_to_pull_all_top_rated_posts_from_last/"}, {"author": "BeezInTheTrap", "created_utc": 1415516401, "gilded": 0, "name": "t3_2lqtpq", "num_comments": 2, "score": 1, "selftext": "Newbie to Python and PRAW here. I'm trying to search for links in the comments of a submission. Is there any way to do so? Such as a function that checks whether a comment has links, or a string I can search for that might indicate a link? Thanks", "title": "PRAW: Any way to tell if a comment contains a link?", "url": "https://www.reddit.com/r/redditdev/comments/2lqtpq/praw_any_way_to_tell_if_a_comment_contains_a_link/"}, {"author": "mjgcfb", "created_utc": 1415038242, "gilded": 0, "name": "t3_2l6e6b", "num_comments": 7, "score": 1, "selftext": "I'm very new to programming and I can't figure out why my code below is so slow going through the loop. This is not the complete code but its the bulk of it. comments = praw.helpers.comment_stream(r, subreddit, limit=500) for comment in comments: # Key ID's comment_id = comment.id link_id = comment.link_id link_id = r.get_info(thing_id=link_id) author = comment.author defaults_post = { 'link_title' : comment.link_title, 'link_url' : comment.link_url, 'domain' : link_id.domain, 'num_comments' : link_id.num_comments, 'permalink' : link_id.permalink, 'url' : link_id.url } try: cleaned_flair = h.unescape(comment.author_flair_text).split(' / ')[0] except TypeError: try: cleaned_flair = comment.author_flair_text.split(' / ')[0] except AttributeError: cleaned_flair = 'None' # Pack non ID Fields into Defaults for Author Model defaults_author = { 'author_flair_css_class' : comment.author_flair_css_class, 'comment_karma' : author.comment_karma, 'link_karma' : author.link_karma } # Pack non ID Fields into Defaults for Comment Model defaults_comments = { # Comment Attributes 'body' : h.unescape(comment.body), 'body_html' : h.unescape(comment.body_html), 'edited' : comment.edited, 'gilded' : comment.gilded, 'parent_id' : comment.parent_id, 'subreddit_id' : comment.subreddit_id, 'subreddit' : comment.subreddit, 'distinguished' : comment.distinguished, #Votable Attributes 'ups' : comment.ups, 'downs' : comment.downs, 'comment_permalink' : comment.permalink } parent_id = comment.parent_id if 't1' in parent_id: parent_id = r.get_info(thing_id=parent_id) defaults_comments['parent_author'] = parent_id.author defaults_comments['parent_author_flair_css_class'] = parent_id.author_flair_css_class defaults_comments['parent_edited'] = parent_id.edited defaults_comments['parent_gilded'] = parent_id.gilded defaults_comments['parent_ups'] = parent_id.ups defaults_comments['parent_downs'] = parent_id.downs defaults_comments['parent_permalink'] = parent_id.permalink", "title": "Why is my for loop take around 5+ seconds for each loop using praw?", "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "powderblock", "created_utc": 1414626295, "gilded": 0, "name": "t3_2kqai0", "num_comments": 4, "score": 2, "selftext": "Writing a simple Reddit bot using PRAW that crawls through /r/all on a timer and checks if they are valid imgur posts, if they are, then the image gets piped to other functions for further processing. Problem is, it isn't possible to tell whether or not a .jpg on Imgur is animated or not. From Imgur FAQ \"The maximum animated file size (both GIF and PNG) is 2MB.\" (Section: *Is there a maximum file size I can upload?*) If a gif has a file size greater than 2MB, it gets compressed or cut down and consequently Imgur displays the URL as .jpg. This means when I am checking if a URL is of type .jpg, it will reply true regardless if it's animated or not. How can I check if a compressed .jpg is actually an animated gif file? Is there an API call for this? I found [this](https://api.imgur.com/models/image) on the subject but I don't understand how to implement it within the API. Here is an example of an animated .jpg (Not actually a .jpg, I know, but it's what the browser and PRAW see from /r/all) http://i.imgur.com/9sYwSwz.jpg Any help is appreciated", "title": "Checking if .jpg Imgur URL is Animated Gif File", "url": "https://www.reddit.com/r/redditdev/comments/2kqai0/checking_if_jpg_imgur_url_is_animated_gif_file/"}, {"author": "icedvariables", "created_utc": 1414437108, "gilded": 0, "name": "t3_2kho9h", "num_comments": 3, "score": 1, "selftext": "I've been writing a bot that searches for the word 'pug' in the title of posts in /r/all (or whatever sub you specify). It collects all the titles of these posts and sends a private message to me. The problem is that when the bot sends the message this error is thrown: Traceback (most recent call last): File \"pugs.py\", line 23, in r.user.send_message(\"icedvariables\", \"Pug posts\", pugPosts) File \"/Library/Python/2.7/site-packages/praw/decorators.py\", line 58, in wrapped return function(self.reddit_session, self, *args, **kwargs) File \"/Library/Python/2.7/site-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/Library/Python/2.7/site-packages/praw/decorators.py\", line 225, in wrapped return function(obj, *args, **kwargs) File \"/Library/Python/2.7/site-packages/praw/__init__.py\", line 2021, in send_message data.update(captcha) ValueError: dictionary update sequence element #0 has length 1; 2 is required Here is my code: import sys, time, praw r = praw.Reddit(\"pug_bot: Searches for posts contain the word 'pug'\") try: subreddit = r.get_subreddit(sys.argv[1]) except IndexError: subreddit = r.get_subreddit(\"all\") r.login(\"pug_bot\", xxx) while(True): pugPosts = \"\" for submission in subreddit.get_new(limit=50): title = submission.title if(\"pug\" in title): print \"PUG POST:\", title pugPosts += \"Pug post:\"+title+\"\\n\\n\" r.user.send_message(\"icedvariables\", \"Pug posts\", pugPosts) time.sleep(1800)", "title": "Praw error when sending PM: ValueError: dictionary update sequence element #0 has length 1; 2 is required", "url": "https://www.reddit.com/r/redditdev/comments/2kho9h/praw_error_when_sending_pm_valueerror_dictionary/"}, {"author": "teaearlgraycold", "created_utc": 1414379630, "gilded": 0, "name": "t3_2kfiou", "num_comments": 14, "score": 5, "selftext": "So I've put a bit of work into a reddit bot for ELI5, and it seems like it's not working consistently. It's coded in Python with the help of PRAW. For a while it would just look through the ELI5 new queue, keep a running list of all posts, and then remind users who hadn't flaired their post explained after 6 hours to flair it if applicable. Then I began a secondary portion of the bot, a portion that reads through the ELI5 modmail for certain commands. I currently have commands that provide user summaries and shadowbans. But it seems that very often when I move the bot into ELI5 after testing it in a private subreddit it has problems performing. Sometimes it'll log tons of errors claiming it can not send PMs (for the flair checking), or that [the connection to reddit has been dropped](http://i.imgur.com/ZjWG9Uz.png). I've tried to keep the number of API calls per minute down below 30, but still the problems persist. Here's my code (Yes, it's a mess. This is my first project in Python): http://pastebin.com/znWuvsRq I currently have the flair checking commented out and the sleep timer set to 40 (it used to be at 20 seconds back when the flair section was enabled). Is it possible my bot is flagged/banned/something else on ELI5 because it broke the API rules? **Edit:** I talked with an admin and they seem to think I'm having an issue with connection pooling/keep alive and not API limitations. Has anyone else had issues with that? I also took a look at the PRAW GitHub and it seems like there's a setting in a praw.ini file that automatically makes sure that API calls only happen every two seconds. Is this something that is automatically done? Should I remove my sleep() timers and let PRAW time things for me?", "title": "What's wrong with my bot?", "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "GoldenSights", "created_utc": 1414366911, "gilded": 0, "name": "t3_2kex5m", "num_comments": 2, "score": 2, "selftext": "http://www.reddit.com/r/redditdev/comments/2eur0l/api_update_apiinfo_supports_lists_of_fullnames/ /u/bsimpson says that the info endpoint allows for comma-separated lists of fullnames to fetch. I've been collecting subreddit info, and I only just remembered that this was even possible, but PRAW won't let me pass lists, and strings are only fetching the first item. I really don't understand how to use raw GET and POST http requests. Has PRAW wrapped this yet? If not, would anyone mind giving me the line to do this through urllib? Thank you!", "title": "Does PRAW support multiple things for get_info?", "url": "https://www.reddit.com/r/redditdev/comments/2kex5m/does_praw_support_multiple_things_for_get_info/"}, {"author": "Zapurdead", "created_utc": 1414127495, "gilded": 0, "name": "t3_2k6668", "num_comments": 2, "score": 1, "selftext": "Like the title says, I'm using the Reddit praw API, and 504 Errors have been tripping me up. I wrapped all the API calls in a try-catch statement, so it can just sleep when there's an error: from requests.exceptions import HTTPError .... while True: try: print(\"Fetching comments...\") print(\"=====\\n\") with open('completed.json', 'r') as comment_history_file: comment_history = json.load(comment_history_file) for comment in comments_by_keyword(r, 'rip mobile users', subreddit='all', print_comments=True): if is_valid(comment, comment_history): reply_with_image(r, comment, comment_history) # Reddit caches recent comments every 30 seconds, so fetch comments in intervals of a little over 30 seconds print(\"Last Successful Query (System Time): \" + strftime(\"%Y-%m-%d %I:%M:%S\\n\")) except HTTPError as e: msg = \"HTTPError(\" + str(e.errno) + \"): \" + str(e.strerror) r.send_message('Zapurdead', \"[MOBILE-WIZARD] HTTPError\", msg) pass sleep(30) However, the exception still stops the program from running. Any idea what I'm doing wrong? I've searched quite a number of threads on this subreddit to no avail.", "title": "PRAW 504 HTTPError, Exception not being caught", "url": "https://www.reddit.com/r/redditdev/comments/2k6668/praw_504_httperror_exception_not_being_caught/"}, {"author": "redditdev1", "created_utc": 1414119278, "gilded": 0, "name": "t3_2k5ui0", "num_comments": 2, "score": 1, "selftext": "At first everything seemed OK. When I manually check out the reddit.com/r/subreddit/.json and reddit.local/r/.json, I get nearly identical responses. The only suspicious value is the \"modhash.\" For my clone, the modhash is just the username, or blank. On reddit it looks like its a real hash. The problem is when I go to PRAW (with the domain changed to the appropriate value), the generator get_subreddit returns 'dicts' instead of praw.objects.Submission. Does anyone have any experience with resolving this conflict?", "title": "Figuring out the API on a reddit clone", "url": "https://www.reddit.com/r/redditdev/comments/2k5ui0/figuring_out_the_api_on_a_reddit_clone/"}, {"author": "doob10163", "created_utc": 1413854152, "gilded": 0, "name": "t3_2juell", "num_comments": 17, "score": 1, "selftext": "I have ran get-pip.py When executing `pip install praw` in powershell, I get `Import Error: No module named site` I've installed praw on another machine before and I had it run. I have no idea why this is not working on mine though.", "title": "Can't install praw on this other machine?", "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "catzhoek", "created_utc": 1413614688, "gilded": 0, "name": "t3_2jl8ks", "num_comments": 6, "score": 3, "selftext": "I'm having trouble logging in using the easiest of all approaches. I don't get it. import praw r = praw.Reddit(user_agent='User-Agent: dhvpostbot/1.0 by u/catzhoek') r.login() [traceback](http://pastebin.com/n3mVWnU1) I can successfully run stuff like r.get_redditor('catzhoek') etc. without any problems. Any wrong password cases report back with proper exceptions. Does anyone have a clue what the culprit could be? Thanks. Edit: **Solved**, it's caused by requiring HTTPS on the account you try to login with.", "title": "Can't get praw (2.1.18) to login", "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "chrisarr", "created_utc": 1413487109, "gilded": 0, "name": "t3_2jg19x", "num_comments": 3, "score": 5, "selftext": "I am wondering how best to replicate the AMA categorization implemented on the Official AMA app. Is there a categorization that is derivable from the API/PRAW, or would some form of NLP need to be used to manually process and categorize each post? Cheers", "title": "How is the categorization on the Official AMA App achieved?", "url": "https://www.reddit.com/r/redditdev/comments/2jg19x/how_is_the_categorization_on_the_official_ama_app/"}, {"author": "huegue", "created_utc": 1413306882, "gilded": 0, "name": "t3_2j8dsv", "num_comments": 4, "score": 3, "selftext": "I'm trying to put my python script (that uses PRAW) on the Google App Engine so that it will run periodically without my computer. I initially ran into problems running locally (with dev_appserver.py) because of missing libraries, so I copied the necessary libraries over to my application's lib folder (which initially only contained flask). Now I'm getting a problem with this line, but only on the live version (not when I test locally). r = praw.Reddit(user_agent=\"some_agent\") Here's the traceback: Traceback (most recent call last): File \"/base/data/home/runtimes/python27/python27_lib/versions/1/google/appengine/runtime/wsgi.py\", line 266, in Handle result = handler(dict(self._environ), self._StartResponse) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1836, in __call__ return self.wsgi_app(environ, start_response) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1820, in wsgi_app response = self.make_response(self.handle_exception(e)) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1403, in handle_exception reraise(exc_type, exc_value, tb) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1817, in wsgi_app response = self.full_dispatch_request() File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1477, in full_dispatch_request rv = self.handle_user_exception(e) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1381, in handle_user_exception reraise(exc_type, exc_value, tb) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1475, in full_dispatch_request rv = self.dispatch_request() File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1461, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/main.py\", line 32, in update_pics r = praw.Reddit(user_agent=\"fresh_pics\") File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/praw/__init__.py\", line 1067, in __init__ super(AuthenticatedReddit, self).__init__(*args, **kwargs) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/praw/__init__.py\", line 536, in __init__ super(OAuth2Reddit, self).__init__(*args, **kwargs) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/praw/__init__.py\", line 648, in __init__ super(UnauthenticatedReddit, self).__init__(*args, **kwargs) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/praw/__init__.py\", line 316, in __init__ update_check(__name__, __version__) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/update_checker.py\", line 170, in update_check result = checker.check(package_name, package_version, **extra_data) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/update_checker.py\", line 67, in wrapped retval = function(obj, package_name, package_version, **extra_data) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/update_checker.py\", line 121, in check data['platform'] = platform.platform(True) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/platform.py\", line 1603, in platform libcname,libcversion = libc_ver(sys.executable) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/platform.py\", line 163, in libc_ver f = open(executable,'rb') IOError: [Errno 2] No such file or directory: '/base/data/home/runtimes/python27/python27_dist/python'", "title": "Using PRAW with the Google App Engine: no such file or directory '/base/data/home/runtimes/python27/python27_dist/python'", "url": "https://www.reddit.com/r/redditdev/comments/2j8dsv/using_praw_with_the_google_app_engine_no_such/"}, {"author": "alexleavitt", "created_utc": 1413173615, "gilded": 0, "name": "t3_2j364y", "num_comments": 5, "score": 3, "selftext": "Related to this post \u2013 http://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/ \u2013 I'm wondering if it's possible to get the number of downvotes anymore for comments...? \"upvote_ratio\" works for posts but doesn't seem to work for comments. Also, \"ups\" seems to only report the total score of the comment now.", "title": "PRAW - Getting downvotes for comments?", "url": "https://www.reddit.com/r/redditdev/comments/2j364y/praw_getting_downvotes_for_comments/"}, {"author": "PieMan2201", "created_utc": 1413151613, "gilded": 0, "name": "t3_2j28r0", "num_comments": 0, "score": 1, "selftext": "I'm using comment_stream to get comments from /r/all, but after a while it effectively stops, and shows something like this: `Items: 558 (0.00 ips) ` Sometimes it will go for hours, other times for a few minutes. I've tried it on multiple systems with the same result. Any idea what's going on? Edit: I've also tried re-installing PRAW.", "title": "[PRAW] comment_stream not working?", "url": "https://www.reddit.com/r/redditdev/comments/2j28r0/praw_comment_stream_not_working/"}, {"author": "swissmcnoodle", "created_utc": 1412999098, "gilded": 0, "name": "t3_2ix3aj", "num_comments": 3, "score": 2, "selftext": "r = praw.Reddit(user_agent=USERAGENT) submission = r.get_submission(url='http://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/') print(vars(submission)) Very simple code, outputs the api information on a reddit url. I am new to python and am finding it quite confusing. When I print submission there are a number of fields including:'comments','created','created_utc','delete','distinguish','domain' How do I print for example, 'domain' from submission object. Thanks in advance EDIT: Actually a better question might be to ask this. The comment data you retrieve from a post submission is outputted like this in console: > I am assuming the praw.objects.Comment will contain fields containing username, post string, upvotes etc. How would I access something like that?", "title": "How do I read the fields of PRAW submission objects/dictionaries?", "url": "https://www.reddit.com/r/redditdev/comments/2ix3aj/how_do_i_read_the_fields_of_praw_submission/"}, {"author": "PixelOrange", "created_utc": 1412929356, "gilded": 0, "name": "t3_2iu8de", "num_comments": 0, "score": 1, "selftext": "I'm trying to determine if wiki pages are hidden and then hide or unhide based on the results. slyf added the ability to hide wiki pages a couple of months ago but I can't find any reference to that API in the PRAW documents. Alternatively, if this isn't possible currently, could someone walk me through how to do this via API? I've always accessed reddit through PRAW and never directly through an API call.", "title": "Is it possible to hide/unhide wiki pages through PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/2iu8de/is_it_possible_to_hideunhide_wiki_pages_through/"}, {"author": "NosajReddit", "created_utc": 1412889705, "gilded": 0, "name": "t3_2isomp", "num_comments": 2, "score": 1, "selftext": "First, I'm extremely new to Python, so I appreciate any help you can offer. I'm trying to scrape reddit posts with PRAW while logged in with oauth. The [reddit API rules](https://github.com/reddit/reddit/wiki/API#rules) say you can make 60 requests a minute if you're logged in with oauth, and that you can monitor various headers to make sure you don't go over. PRAW naturally waits 2 seconds between calls to make sure it doesn't exceed the rate limit. I'd like to make calls every 1 second. I believe the only way to do this is to (manually?) edit the [api_request_delay variable in PRAW.ini](https://github.com/praw-dev/praw/blob/master/praw/praw.ini) from 2.0 to 1.0. I want to make sure I'm following the rules, so I have the following questions before I start sending requests: 1. Is this the best way to use PRAW to exceed the non-oauth rate limit? The only way? 1. Is 'read' the right scope, or do any of them work for the higher rate limit? 1. Is there a way to use PRAW to access the response headers? If not, what's the best way to do this manually? Thanks for your help!", "title": "[PRAW] Using OAuth with reddit API ratelimits", "url": "https://www.reddit.com/r/redditdev/comments/2isomp/praw_using_oauth_with_reddit_api_ratelimits/"}, {"author": "sneaky_dragon", "created_utc": 1412830504, "gilded": 0, "name": "t3_2iqen5", "num_comments": 2, "score": 1, "selftext": "****SOLVED. SEE COMMENT BELOW**** --- I last ran my script maybe around Mar 2014 to set flairs for users in /r/rabbits as a mod. It worked then, but I tried it today with an updated PRAW package (v2.1.18), and I keep getting a RedirectException error. The mod permissions I have in the subreddit are \"access, config, flair, mail, posts, wiki.\" Could anyone help me debug this? Code: import praw r = praw.Reddit(user_agent='/r/rabbits flair script by sneaky_dragon v0.1') r.login('user', 'pwd') ... r.set_flair_csv('rabbits', flairmap); The errors I got: r.set_flair_csv('rabbits', flairmap); File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 313, in wrapped if mod and not is_mod_of_all(obj.user, subreddit): File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 275, in is_mod_of_all mod_subs = user.get_cached_moderated_reddits() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 771, in get_cached_moderated_reddits for sub in self.reddit_session.get_my_moderation(limit=None): File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 472, in get_content page_data = self.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 510, in request_json response = self._request(url, params, data) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 374, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 348, in handle_redirect url = _raise_redirect_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 158, in _raise_redirect_exceptions raise RedirectException(response.url, new_url) praw.errors.RedirectException: Unexpected redirect from http://www.reddit.com/subreddits/mine/moderator/.json?limit=1024 to http://www.reddit.com/subreddits/login.json?dest=http%3A%2F%2Fwww.reddit.com%2Fsubreddits%2Fmine%2Fmoderator%2F.json%3Flimit%3D1024", "title": "Keep getting RedirectException using set_flair_csv with PRAW...", "url": "https://www.reddit.com/r/redditdev/comments/2iqen5/keep_getting_redirectexception_using_set_flair/"}, {"author": "doob10163", "created_utc": 1412806528, "gilded": 0, "name": "t3_2ipcik", "num_comments": 4, "score": 1, "selftext": "I'm trying to develop a bot similar to /u/MTGCardFetcher for /r/hearthstone. I need to be able to iterate over a given subreddits comments and search for instances of \"[[\" and \"]]\" and then respond with the card's information that is put in between the brackets. Here's what I have so far, it's almost exactly the same as the information at praw.readthedocs.org import praw r = praw.Reddit(\"HS Card Fetcher by u/doob10163\") r.login('doob_test', 'thepassword') submission = r.get_submission(submission_id=\"2ioe5e\") #subreddit = r.get_subreddit('test') #subreddit_comments = subreddit.get_comments() flat_comments = praw.helpers.flatten_tree(submission.comments) already_done = set() for comment in flat_comments: if \"[[\" in comment.body and \"]]\" in comment.body and comment.id not in \\ already_done: comment.reply(\"\"\"This is a test response.\"\"\") already_done.add(comment.id) A few things worth mentioning that I feel like need to be addressed in some way when I develop this: My comment rate is severely limited on this bot, how long do I wait in between comments? How do I limit the bot so it only pulls the requests for the given limit (what is the limit anyways)? Should I use time.sleep() ? How does the bot know how many comments to pull from the subreddit? How do I ensure that I will call only the most recent posts and not do the same thing over again when I cancel the bot running and have it run again fresh?", "title": "Completely new to reddit API and PRAW with python. I'm trying to check all comments in a subreddit for instances of a string", "url": "https://www.reddit.com/r/redditdev/comments/2ipcik/completely_new_to_reddit_api_and_praw_with_python/"}, {"author": "cylindrical418", "created_utc": 1412739786, "gilded": 0, "name": "t3_2immwi", "num_comments": 4, "score": 1, "selftext": "I'm getting this error using `praw` after restarting a bot and making it reply to a comment Traceback (most recent call last): File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 378, in bot.fastboot() File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 91, in fastboot self.work() File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 261, in work self.scanComments(comments, submission) File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 295, in scanComments self.parseCommand(comment) File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 305, in parseCommand self.doCommand(command, comment) File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 343, in doCommand self.doFetchUnitCommand(command, comment) File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 357, in doFetchUnitCommand comment.reply(r) File \"F:\\Dev\\Python\\Python\\Python27\\lib\\site-packages\\praw\\objects.py\", line 348, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"F:\\Dev\\Python\\Python\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"F:\\Dev\\Python\\Python\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 2047, in _add_comment retry_on_error=False) File \"F:\\Dev\\Python\\Python\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 177, in wrapped raise error_list[0] APIException: (NO_TEXT) `we need something here` on field `text` What's weird is that it was working fine before. [This](http://www.reddit.com/r/test/comments/2ictuj/bfbottest/cl3gndo) is the comment that causes this error. I have no idea what this error means. Can anyone help me?", "title": "[praw] APIException: (NO_TEXT)", "url": "https://www.reddit.com/r/redditdev/comments/2immwi/praw_apiexception_no_text/"}, {"author": "F3AR3DLEGEND", "created_utc": 1412602453, "gilded": 0, "name": "t3_2ig2ci", "num_comments": 10, "score": 3, "selftext": "How can I gild users and comments, preferably with PRAW? I can't find any method for it. I know how to get the fullname of an object, but I'm still not sure how to give gold. Any help is greatly appreciated.", "title": "Gild Comments/Users", "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": "phil_s_stein", "created_utc": 1412451639, "gilded": 0, "name": "t3_2iaspn", "num_comments": 7, "score": 1, "selftext": "I'm trying to write a bot that responds to mentions. I'm using the simple loop below to test `get_mention()`. I start the loop, login as someone who is not UID, then mention them in a thread like so: `Hello /u/UID how are you?` (where UID is the reddit username). Then I login as UID and see the PM from the mention (meaning the mention worked I'm assuming), but the code below never tells me that it sees the mention. What am I doing wrong? The UID account has gold and has mentions enabled. The simple loop I'm testing with: import praw from time import sleep from BotConfig import UID, PASSWD if __name__ == '__main__': r = praw.Reddit('/u/phil_s_stein commandline praw testing: fetching username mentions with praw') r.login(username=UID, password=PASSWD) while True: for m in r.get_mentions(): print('{} mentioned: {}'.format(UID, m)) sleep(5) Does anyone have any bot code that responds to mentions that I could take a look at? Thanks. Edit: updated user agent as suggested.", "title": "How to use PRAW and get_mentions()?", "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "stats94", "created_utc": 1412334956, "gilded": 0, "name": "t3_2i6gcs", "num_comments": 3, "score": 1, "selftext": "So I'm trying to develop a Bot that posts Game Day threads to the /r/KontinentalHL subreddit, but whenever I try to login I get the following error: Traceback (most recent call last): File \"startBot.py\", line 12, in r.login(Username,Password) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 1230, in login self.request_json(self.config['login'], data=data) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 510, in request_json response = self._request(url, params, data) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 375, in _request _raise_response_exceptions(response) File \"C:\\Python33\\lib\\site-packages\\praw\\internal.py\", line 172, in _raise_response_exceptions response.raise_for_status() File \"C:\\Python33\\lib\\site-packages\\requests\\models.py\", line 808, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 403 Client Error: Forbidden I'm certain my login details are correct, I'm confused! I'm probably being an idiot, but any help? Also when I try r.login() and input my username the program freezes and never gets to the password stage...", "title": "[PRAW] HTTPError when trying to login?", "url": "https://www.reddit.com/r/redditdev/comments/2i6gcs/praw_httperror_when_trying_to_login/"}, {"author": "paulshapiro", "created_utc": 1412009714, "gilded": 0, "name": "t3_2hsvhj", "num_comments": 1, "score": 2, "selftext": "Right now, I'm playing with PRAW but am wondering if there is an easier or already existing implementation for something like this?", "title": "Easiest way to delete all posts for a particular sub?", "url": "https://www.reddit.com/r/redditdev/comments/2hsvhj/easiest_way_to_delete_all_posts_for_a_particular/"}, {"author": "F3AR3DLEGEND", "created_utc": 1411834364, "gilded": 0, "name": "t3_2hmh42", "num_comments": 5, "score": 6, "selftext": "Hey guys, I'm trying to give gold via the Reddit API. I want it to utilize the creddits already in the account. I saw the `api/v1/gold/give` method in the API docs but couldn't figure out how to use it. I can get the current user's `modhash` by logging in with PRAW. I tried this: import requests import praw reddit = praw.Reddit(user_agent = \"Gold Testing Bot\") reddit.login(my_user, my_pass) modhash = reddit.modhash client = requests.session() client.headers = {\"user-agent\": \"Gold Testing Bot\"} response = client.post(\"https://www.reddit.com/api/v1/gold/give\", {\"months\": 1, \"username\": some_user}) print response.text I am receiving a bunch of HTML, which appears to be the default Reddit home page. Is there any way to do this? I'm not sure how to utilize the modhash, so I think that's part of the issue. Thanks!", "title": "How to Give Gold with the API", "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "88rarely", "created_utc": 1411362533, "gilded": 0, "name": "t3_2h3n2u", "num_comments": 4, "score": 1, "selftext": "It's supposed to breakdown karma from each subreddit. I got it from [here](https://praw.readthedocs.org/en/v2.1.16/pages/getting_started.html) /u/_Daimon_ didn't implement a print so I don't know where it goes to actually make the program be useful. I've been trying to get it work for more than two hours with no luck. import praw user_agent = (\"Karma breakdown 1.0 by /u/_Daimon_ \" \"github.com/Damgaard/Reddit-Bots/\") r = praw.Reddit(user_agent=user_agent) thing_limit = 10 user_name = \"_Daimon_\" user = r.get_redditor(user_name) gen = user.get_submitted(limit=thing_limit) karma_by_subreddit = {} for thing in gen: subreddit = thing.subreddit.display_name karma_by_subreddit[subreddit] = (karma_by_subreddit.get(subreddit, 0) + thing.score)", "title": "How would I make this Karma Breakdown program print like it's supposed to?", "url": "https://www.reddit.com/r/redditdev/comments/2h3n2u/how_would_i_make_this_karma_breakdown_program/"}, {"author": "ReBurnInator", "created_utc": 1411350725, "gilded": 0, "name": "t3_2h35s9", "num_comments": 2, "score": 2, "selftext": "Hello everyone, I apologize in advance if this question has been asked before. I've been digging through this sub looking for an answer and I haven't had any luck. I'm using Python with PRAW, and today is the first time I've used PRAW or the reddit API. Hopefully this has a simple answer. I'm working on a script that reads the new submissions from a subreddit. The purpose of this subreddit is aggregate the most helpful submissions and comments from a group of themed subreddits. The purpose of the bot is to reply to each of the linked submissions/comments with a note informing the community that it has already been submitted. Think something like totes_meta_bot, but it only looks at the submissions in one subreddit and only replies in a handful of related subreddits instead of spamming all of reddit. The bot works by looping through the submissions in the meta subreddit, replying to each submission, and tracking what has been replied to so that it doesn't reply again. What I'm trying to figure out, without much success, is how to determine whether the URL in the submission I'm processing is a link to a comment or if it is a link to a submission. It could be either. I've been searching, and I know that you can use the get_submission function to some degree to get information when you know for sure whether your submission is a submission or a comment: import praw r = praw.Reddit('') submission = r.get_submission('http://www.reddit.com/r/redditdev/comments/10msc8/how_to_calculate_more_comments_count_not_just/') comment = r.get_submission('http://www.reddit.com/r/redditdev/comments/10msc8/how_to_calculate_more_comments_count_not_just/c6euu6b').comments[0] The problem I'm having is that I don't know for sure whether the URL is to a comment or submission without inspecting the URL. I could parse the URL and pull the thing id's out, and if there is a second thing id I would know for sure if the URL pointed to a comment. But that just seems hacky to me. I've been digging through the API docs, but I don't see anything. Is there something in PRAW that can inspect the URL and tell me whether it links to a comment or submission? Or do I need to write code to figure this out for myself? Thanks!", "title": "PRAW: Is there a way to tell whether a URL links to a reddit comment versus a reddit submission?", "url": "https://www.reddit.com/r/redditdev/comments/2h35s9/praw_is_there_a_way_to_tell_whether_a_url_links/"}, {"author": "b0wmz", "created_utc": 1411139199, "gilded": 0, "name": "t3_2gv6zy", "num_comments": 9, "score": 1, "selftext": "I'm using search to figure out how many posts a user has previously submitted to a sub. I know that I'm not able to get more than 100 search results, but I can only retrieve 25 max. How can I raise this limit using praw? Also, is there a better alternative to retrieving the amount of the users's previous posts on a certain sub?", "title": "Praw: Retrieve more than 25 search results", "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "bboe", "created_utc": 1410935671, "gilded": 1, "name": "t3_2gmzqe", "num_comments": 2, "score": 12, "selftext": "PRAW client developers, I have made a PRAW branch to test using only HTTPS over the API. This change requires some testers to see if there any issues that did not come up from our set of unit tests. This is the first of a few improvements that will (hopefully soon) be released with PRAW version 3. If you want to start using HTTPS exclusively through PRAW please update via the following: pip install git+git://github.com/praw-dev/praw.git@praw3 If you experience any issues feel free to report them here, however filing a bug on github (https://github.com/praw-dev/praw/issues) would be ideal. Thanks!", "title": "[PRAW] HTTPS enabled PRAW testing needed", "url": "https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/"}, {"author": "DAMN_it_Gary", "created_utc": 1410831323, "gilded": 0, "name": "t3_2gio0c", "num_comments": 9, "score": 1, "selftext": "using this simple code: r = praw.Reddit(user_agent='example') r.login(\"damn_it_gary\", \"hunter2\") r.send_message('zumicts', 'Subject Line', 'You are awesome!') I get this: praw.errors.NotLoggedIn: `please login to do that` on field `None`", "title": "Praw can't send messages (PM)", "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "Morgneer", "created_utc": 1410827433, "gilded": 0, "name": "t3_2gihdt", "num_comments": 2, "score": 1, "selftext": "For got to add [PRAW]", "title": "Is it possible to run a submission and comment stream in the same program?", "url": "https://www.reddit.com/r/redditdev/comments/2gihdt/is_it_possible_to_run_a_submission_and_comment/"}, {"author": "spinnelein", "created_utc": 1410759201, "gilded": 0, "name": "t3_2gfnzr", "num_comments": 2, "score": 1, "selftext": "I'm having trouble going over 15,000 characters in self post text. For subs that are configured text posts only, the limit on reddit is something like 45,000 but praw errors out. praw.errors.APIException: (TOO_LONG) `this is too long (max: 15000.0)` on field `text` Ideas?", "title": "15000 character limit on self posts in praw", "url": "https://www.reddit.com/r/redditdev/comments/2gfnzr/15000_character_limit_on_self_posts_in_praw/"}, {"author": "TheLazarbeam", "created_utc": 1410742505, "gilded": 0, "name": "t3_2gezwe", "num_comments": 3, "score": 1, "selftext": "1. I went through the PRAW tutorial and it mentioned that one of three big problems with bots is the issue of keeping it running all the time. Their remedy was seemingly to [keep the script open in my command prompt all the time and refresh it every 30 minutes.](https://praw.readthedocs.org/en/v2.1.16/pages/writing_a_bot.html) Am I missing something here, or is this what you have to do? Do people put their bots on personal servers? 2. Every time I run a PRAW script, every time no matter what, even if it encounters an exception, at the end it will print out: \"sys:1: ResourceWarning: unclosed \" \"C:\\Python34\\lib\\importlib\\_bootstrap.py:2127: ImportWarning: sys.meta_path is empty\" Should I be concerned? Am I failing to close my praw.Reddit() object? How can I get this to stop happening? I went to that spot in bootstrap.py and commented it out but to no avail. As you can see, I'm running Python 3.4, and I've heard PRAW has issues with 3.x. Could this be the issue? Thanks for reading all of that!", "title": "Two simple questions from a PRAW noob.", "url": "https://www.reddit.com/r/redditdev/comments/2gezwe/two_simple_questions_from_a_praw_noob/"}, {"author": "GoldenSights", "created_utc": 1410589791, "gilded": 0, "name": "t3_2g9rig", "num_comments": 4, "score": 1, "selftext": "I've found that I can't get users' information through their ID number, which I regularly do with submissions, comments, and even subreddits. [Comment](http://www.reddit.com/api/info.json?id=t1_ckgqal1) - t1_ckgqal1 [Submission](http://www.reddit.com/api/info.json?id=t3_2fvdyx) - t3_2fvdyx [Subreddit](http://www.reddit.com/api/info.json?id=t5_31x09) - t5_31x09 [Account?](http://www.reddit.com/api/info.json?id=t2_co7mq) - t2_co7mq Using praw, I can fetch a redditor and print his ID number, but using get_info() on that number returns nothing. I'm having a hard time finding someone else with the same problem. Can anyone point me in the right direction?", "title": "Why can't I get_info with an account fullname?", "url": "https://www.reddit.com/r/redditdev/comments/2g9rig/why_cant_i_get_info_with_an_account_fullname/"}, {"author": "ovooDE", "created_utc": 1410259273, "gilded": 0, "name": "t3_2fw9fz", "num_comments": 3, "score": 2, "selftext": "I have been fiddling around to get a comment scraper running for a project of mine, and have started running with PRAW in python. The problem i have is that comment crawling slow down significantly after the initial 1000 comments alloted per subreddit, and i wonder if i can somehow speed it up. Currently it looks like this: r = praw.Reddit(user_agent = user_agent) comments = praw.helpers.comment_stream(r, subreddit, limit=None) for comment in comments: #do something with the comment As i said, the first 1000 comments per subreddit are fetched pretty quickly (100 per second) and after that it slows down to 5-10 ips, which should not be due to rate limitations by reddit. Maybe it would be faster to just use custom urls for get_content() with the comment ids?", "title": "[PRAW] slowdown in helpers.get_comment_stream", "url": "https://www.reddit.com/r/redditdev/comments/2fw9fz/praw_slowdown_in_helpersget_comment_stream/"}, {"author": "GoldenSights", "created_utc": 1409958901, "gilded": 0, "name": "t3_2fle0e", "num_comments": 2, "score": 2, "selftext": "I am trying to pull all the items (or as many as I can) from /u/GoldenSights/liked. In the browser I can easily scroll through pages, but PRAW doesn't want to get more than 50 posts. I find that `get_liked()` does not accept the `limit=1000` argument that we usually use, nor `time='all'` What can I do to change the limit here? I'm probably missing something simple. Thank you in advance. [get_liked() documentation](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.Redditor.get_liked)", "title": "[PRAW] user.get_liked() only returns 50 items, does not support limit argument. How can I get more items?", "url": "https://www.reddit.com/r/redditdev/comments/2fle0e/praw_userget_liked_only_returns_50_items_does_not/"}, {"author": "Magzter", "created_utc": 1409825800, "gilded": 0, "name": "t3_2ffxm9", "num_comments": 1, "score": 2, "selftext": "Using the following code: https://github.com/reddit/reddit/wiki/OAuth2-PHP-Example I've authorized the app on Reddit and updated the code with the correct client_id, client_secret and redirect_uri. The app succesfully redirects me to Reddit and after approving it takes me back with a json that says \"User Agent Required\". I can find a ton of posts showing how to set the user_agent string in Python using PRAW, but nothing on PHP. Anyone able to shed some light? I tried adding the following: $_SERVER['HTTP_USER_AGENT'] = \"MyTestApp/0.1 by /u/Magzter\"; But something tells me I'm wildly off.", "title": "\"User Agent Required\" - PHP/OAuth", "url": "https://www.reddit.com/r/redditdev/comments/2ffxm9/user_agent_required_phpoauth/"}, {"author": "Leonlg", "created_utc": 1409672419, "gilded": 0, "name": "t3_2f9lyn", "num_comments": 4, "score": 3, "selftext": "I started learning PRAW a while ago and I'm making a bot now, but I can't figure this out. Can someone explain how to do this? Thank you!", "title": "[PRAW] How can I see if my bot already replied to a tread?", "url": "https://www.reddit.com/r/redditdev/comments/2f9lyn/praw_how_can_i_see_if_my_bot_already_replied_to_a/"}, {"author": "tryme1029", "created_utc": 1409517175, "gilded": 0, "name": "t3_2f3yz8", "num_comments": 1, "score": 0, "selftext": "Using PRAW/Python", "title": "How would I scan for all contents by a particular user and reply to them?", "url": "https://www.reddit.com/r/redditdev/comments/2f3yz8/how_would_i_scan_for_all_contents_by_a_particular/"}, {"author": "Lost_it", "created_utc": 1409458404, "gilded": 0, "name": "t3_2f265d", "num_comments": 2, "score": 1, "selftext": "I am interested in the position(rank) of this post in the subreddit, not the score. edit: using praw", "title": "Any way to get the rank of a submission in a subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/2f265d/any_way_to_get_the_rank_of_a_submission_in_a/"}, {"author": "kemitche", "created_utc": 1409081085, "gilded": 0, "name": "t3_2enl27", "num_comments": 1, "score": 12, "selftext": "Hello everyone! I'm here to give some quick tips on what to put in your /r/redditdev post to make it easier for us to help you. \"XYZ isn't working\", on its own, is not enough information for us to help you! We need more specifics to be able to understand what exactly isn't working for you. Please try and include the following information in your posts. If you're not sure how to get some/all of it, google around or ask for additional assistance in your post. * The exact API URL you were hitting * The request headers (but obfuscate cookies and secrets!) * The POST data you're sending, if any * Any response headers * The response **body** In other words, include as much information about the raw HTTP request as you can. If you're using a library of some kind, be aware that not everyone is going to be familiar with whatever language, library, or SDK you're working with. That means that while including the code in your post can help, it's not *nearly* as helpful as the raw HTTP information. The main exception here is PRAW, as it has wide enough use that someone may be able to help without the raw HTTP info.", "title": "Help us help you - include important debugging info!", "url": "https://www.reddit.com/r/redditdev/comments/2enl27/help_us_help_you_include_important_debugging_info/"}, {"author": "df27hswj95bdt3vr8gw2", "created_utc": 1409080911, "gilded": 0, "name": "t3_2enkr2", "num_comments": 2, "score": 2, "selftext": "[I want to pull stats from this page.](http://www.reddit.com/r/AskReddit/about/traffic/) I'm looking at PRAW and there doesn't seem to be a way, which isn't surprising because it seems to not even be available through the API. Is there a good way to do it?", "title": "PRAW/API: Is there a way to pull traffic stats from a subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/2enkr2/prawapi_is_there_a_way_to_pull_traffic_stats_from/"}, {"author": "Dobias", "created_utc": 1408537371, "gilded": 0, "name": "t3_2e2q2l", "num_comments": 7, "score": 3, "selftext": "When using the following code import praw r = praw.Reddit('downs test') while True: subreddit = r.get_subreddit('programming') for submission in subreddit.get_hot(limit=10): print submission.ups, submission.downs the output is like 129 0 1274 0 19 0 78 0 15 0 22 0 22 0 53 0 5 0 157 0 So the downvote count is always zero. This can not be right, at least I guess. Am I doing something wrong or it this a bug in PRAW?", "title": "PRAW - downvote count always zero?", "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": "hit_bot", "created_utc": 1408511791, "gilded": 0, "name": "t3_2e2191", "num_comments": 1, "score": 2, "selftext": "Ok, so I've written some code that, for all intents and purposes, should work: def checkComments(comments): for comment in comments: print comment.body checkComments(comment.replies) def processSub(sub): sub.replace_more_comments(limit=None, threshold=0) checkComments(sub.comments) #login and subreddit init stuff here subs = mysubreddit.get_hot(limit=50) for sub in subs: processSub(sub) However, given a submission with a comment that has 50 nested replies like so: root comment -> 1st reply -> 2nd reply -> 3rd reply ... -> 50th reply The above code only prints: root comment 1st reply 2nd reply 3rd reply 4th reply 5th reply 6th reply 7th reply 8th reply 9th reply Any idea how I can get the remaining 41 levels of replies? Or is this a praw limitation?", "title": "Praw - How to retrieve replies to a comment past 10 levels deep", "url": "https://www.reddit.com/r/redditdev/comments/2e2191/praw_how_to_retrieve_replies_to_a_comment_past_10/"}, {"author": "snaysler", "created_utc": 1408122660, "gilded": 0, "name": "t3_2dndq3", "num_comments": 10, "score": 3, "selftext": "Hi everyone, I am running an online course at /r/ludobots. The whole site is mediated and run by a group of bots that are always up and running on a remote server. Anyway, I've been getting more use recently and there is a huge problem. PRAW seems to no longer be able to edit the \"config/sidebar\" wiki page, which I use as a live feed for what people are doing on the site. As of about a day ago, all of the bots are no longer able to edit this wiki page. If I physically sign in to the bot accounts and navigate to the wiki page, I can edit it with no problem. In fact, I can edit it no problem on any mod account. But PRAW seems no longer capable of editing this page. I already have a connection failsafe with a try/except python loop that tries to connect to reddit 10 times with a 5 second break between attempts, then it quits the program. Because all my bots are trying to update \"config/sidebar\", somewhat regularly (every 30min or so) it seems like none of them can now. Why is this happening? Why is PRAW so damn unreliable? How can I fix this? The site is growing in popularity and I will only be getting more traffic, and I can't keep up anymore with all this live maintenance and apologies to users. Please Help! -Tayler B", "title": "Serious Issue with PRAW in my Sub (Urgent!!)", "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "irrestistable", "created_utc": 1407971685, "gilded": 0, "name": "t3_2dhhrk", "num_comments": 7, "score": 2, "selftext": "I'm writing a reddit bot using PRAW for python and I'm having trouble going through the documentation to find the right commands. I'm specifically looking to find the score of a comment and a post but I can't seem to find it.", "title": "PRAW command list?", "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "echocage", "created_utc": 1407873301, "gilded": 0, "name": "t3_2dd6n8", "num_comments": 2, "score": 3, "selftext": "I'd like to know if anyone has a decent method for this using praw", "title": "Get all new submissions, including subreddits who have excluded themselves from /r/all?", "url": "https://www.reddit.com/r/redditdev/comments/2dd6n8/get_all_new_submissions_including_subreddits_who/"}, {"author": "deletedLink", "created_utc": 1407749183, "gilded": 0, "name": "t3_2d7yqc", "num_comments": 9, "score": 2, "selftext": "Hello everyone, I'd like to make a call out for a subreddit configuration file along the same lines as the internet maintains -- [the robots.txt file](http://en.wikipedia.org/wiki/Robots_exclusion_standard). It would work pretty simply. Just set up a page called /r//wiki/robots.txt with a line in it. 1. **Disallow all bots** User-agent: * Disallow: / 2. **Except my bot** User-agent: mybot Disallow: The \"Disallow\" field may be able to be customized to different categories. - Self-posts - Link-posts - Wiki - Fun (e.g. robots that are just fun) - Informational (e.g. unit conversion, wiki lookup) - Cross-linking Something along those lines. It would be pretty easy to create the functionality in PRAW to support and standardize this feature. Thoughts and input would be welcome. Cheers! DL", "title": "Subreddit robots permission file -- analogous to robots.txt?", "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "bsturtle", "created_utc": 1407722409, "gilded": 0, "name": "t3_2d72ou", "num_comments": 6, "score": 2, "selftext": "I'm working on a bot to monitor author post frequency. The community only allows one submission per user every 7 days. I'm using sqlite to store submissions less than 7 days old, and then looking for new submissions every 30 min or so, comparing the author to the db. When creating the initial db and when checking for new submissions, I'd like to stop looking at the submissions if I hit one i've already seen, or if it is more than 7 days old. How does reddit or PRAW return the submission data? Is it in any order like oldest or newest first? Is it dependable? Thanks", "title": "[PRAW] How are submissions retrieved from a subreddit when get_new() is used?", "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "brucemo", "created_utc": 1407392559, "gilded": 0, "name": "t3_2cv1ua", "num_comments": 2, "score": 1, "selftext": "I know the ID of a mail message, so I can append \"t4_\" in front of that to get its id, and use: o = rh.get_info(thing_id = \"t4_\" + msg) ... to return the thing. But this thing has no replies, even though in reality it has replies. This does nothing: for reply in o.replies: ... do stuff ... ... because o == []. It shouldn't be. How do I get the object's replies to populate? I don't want to waste people's time here, so am I supposed to be able to answer this question myself? The doc doesn't seem to do it because the objects aren't documented and I don't appear to have access to many functions that must exist under the hood. http://www.reddit.com/r/redditdev/comments/1kxd1n/how_can_i_get_the_replies_to_a_comment_with_praw/ I did find that but it didn't get me anywhere other than to tell me why my code doesn't work. I can't find an analogous means of solving the problem.", "title": "How do I get this PRAW object to populate?", "url": "https://www.reddit.com/r/redditdev/comments/2cv1ua/how_do_i_get_this_praw_object_to_populate/"}, {"author": "mcLudobot", "created_utc": 1407175412, "gilded": 0, "name": "t3_2clwr7", "num_comments": 2, "score": 1, "selftext": "Hi, I'm a bot for an online course at /r/ludobots. I'm controlled by a python program using praw. All I have to do when my program runs is get my unread messages and reply to them. I use the get_unread function to do this. Unfortunately it seems that this method is unreliable and only works with about 60% of the messages I'm sent. For some reason, many of the messages I receive go unanswered by my program, though they are marked as read, which is strange. Does anyone know of this problem, or has anyone experienced inconsistent behavior from praw in terms of dealing with messages? The public launch of the course is this week, and we need me to be working correctly. Thanks!", "title": "PRAW get_unread() issue, please help", "url": "https://www.reddit.com/r/redditdev/comments/2clwr7/praw_get_unread_issue_please_help/"}, {"author": "tingmakpuk", "created_utc": 1407081405, "gilded": 0, "name": "t3_2ciblc", "num_comments": 2, "score": 1, "selftext": "Noob here, probably asking a stupid question. But I recently built a bot as a means of learning python, and in studying a handful of bots, they all seem to look for their trigger by just grabbing 100 comments at a time. Over and over. I can see how this is unavoidable for the wikibot, for example, because many posters would be unaware of its existence, but it still serves a useful function. But a lot of them are being called by name, or some other name related trigger. If there was an actual messaging system in the praw, the bots wouldn't have to grab comments haphazardly. I'm trying to make sense of dogetipbot now, and it almost seems like it's using such a messaging system -- but I can't be sure if it's that or a complex structure for grabbing comments. Does this exist? What's it called? Edit words.", "title": "Praw inefficiency? Or does this exist?", "url": "https://www.reddit.com/r/redditdev/comments/2ciblc/praw_inefficiency_or_does_this_exist/"}, {"author": "mathleet", "created_utc": 1407035742, "gilded": 0, "name": "t3_2ch33g", "num_comments": 3, "score": 2, "selftext": "Hey guys, I'm trying to write a function that checks to see if a specified user has already responded to a comment. This is what I have so far: def already_responded(comment, submission): \"\"\"Checks to see if user has already responded to this comment\"\"\" ## Parse replies to comment for replies in comment.replies: ## Check to see if it is a MoreComments object ## If it is, load more comments, and recurse this method if isinstance(replies, praw.objects.MoreComments): submission.replace_more_comments() already_responded(comment, submission) if not isinstance(replies, praw.objects.Comment): return False if str(replies.author) == \"fixed_username\": return True return False (FWIW, I know recursing the function probably isn't the most efficient way of getting more comments. Ideas there would be good too!) This function seems to have worked on some test cases, but when I try to use it on [this thread](http://www.reddit.com/r/videos/comments/2cfy08/a_couple_of_friends_of_mine_are_currently_on/cjf2dll) to see if haiku_robot has responded, it returns a False. The function can read that srpske is an author but nobody beyond that. Any ideas? Thanks! [EDIT] If it helps, here is a pastebin of some test code that I'm playing with to help illustrate the point. http://pastebin.com/yQQ1BbQV", "title": "Trying to use praw to see if a user has already responded to a comment. Any ideas?", "url": "https://www.reddit.com/r/redditdev/comments/2ch33g/trying_to_use_praw_to_see_if_a_user_has_already/"}, {"author": "Theemuts", "created_utc": 1406973945, "gilded": 0, "name": "t3_2cex9y", "num_comments": 8, "score": 8, "selftext": "Hi, I've written a script using PRAW to collect user data, but when I fetched the data of /u/stuff_and_crap AVG showed me this popup: http://i.imgur.com/BOLwFtt.png Is this a false positive? As far as I know, I'm only fetching text.", "title": "Virus detected in data returned by reddit", "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "haiguise1", "created_utc": 1406814555, "gilded": 0, "name": "t3_2c8lem", "num_comments": 2, "score": 2, "selftext": "Is there a way to get all comments in a thread without getting all MoreComments objects in PRAW? Or atleast all/most parent comments?", "title": "Is there a way to get all comments in a thread without getting all MoreComments objects in PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/2c8lem/is_there_a_way_to_get_all_comments_in_a_thread/"}, {"author": "FirestarterMethod", "created_utc": 1406813762, "gilded": 0, "name": "t3_2c8k7m", "num_comments": 2, "score": 1, "selftext": "So I've written a bot in Python using PRAW. If certain conditions are met, the bot will post a comment. I have a cfg file with all the user-editable vars, and I read it in with ConfigParser. Right now it grabs all of the vars from the cfg file, but I've had to keep the message in the main bot file, because I can't figure out how to import the message from the cfg file and keep the correct markdown syntax. -------------- This is what's in the main body. MESSAGE = ''' Hello world. *I have multiple lines* ''' **This way displays the markdown correctly in the comment.** But when I make it: MESSAGE = configs.get('misc','message') and this in the .cfg file: [misc] message= ''' Hello world. *I have multiple lines* ''' it displays **badly (as 'code' in the comment).** ***Is there something I can do to fix this?***", "title": "Using an external cfg file for multi-line markdown/comment?", "url": "https://www.reddit.com/r/redditdev/comments/2c8k7m/using_an_external_cfg_file_for_multiline/"}, {"author": "Dobias", "created_utc": 1406641612, "gilded": 0, "name": "t3_2c1iv5", "num_comments": 2, "score": 1, "selftext": "Hi, as a test I wrote this small python code: import praw r = praw.Reddit('comment parser') comments = r.get_comments(\"redditdev\", limit=None) print len(list(comments)) The output is: 994 But I strongly guess there are more comments here. ;-) Are these only the top-level comments? How can I get all?", "title": "How to get all comments ever written in a specific subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/2c1iv5/how_to_get_all_comments_ever_written_in_a/"}, {"author": "sworeiwouldntjoin", "created_utc": 1406370461, "gilded": 0, "name": "t3_2brrf9", "num_comments": 4, "score": 4, "selftext": "Hi all! I'm new to PRAW and I was just wondering, what's the most efficient way to process the source of a comment, and extract any links contained therein? I have a couple different ways in mind but I'm fairly certain I'm overlooking something. I've looked through the PRAW docs, and read through the comment parsing section, so I'm just wondering if I need to process the body of the comment as a string and pattern match until I get any links, or if there's a method that already exists for that? Please forgive me if this question is ignorant.", "title": "[PRAW] - How do I extract links from a comment?", "url": "https://www.reddit.com/r/redditdev/comments/2brrf9/praw_how_do_i_extract_links_from_a_comment/"}, {"author": "Eathed", "created_utc": 1406269606, "gilded": 0, "name": "t3_2bo5wq", "num_comments": 0, "score": 1, "selftext": "I'm trying to grab the list of link flair templates from a subreddit my bot moderates. I've come across [get_flair_list](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModFlairMixin.get_flair_list) but it seems like that only returns a list of who made each post and what their user flair is. Am I overlooking something here or is there not a way to grab this?", "title": "[PRAW] Getting a list of link flair templates", "url": "https://www.reddit.com/r/redditdev/comments/2bo5wq/praw_getting_a_list_of_link_flair_templates/"}, {"author": "naiyt", "created_utc": 1406268671, "gilded": 0, "name": "t3_2bo4xn", "num_comments": 4, "score": 16, "selftext": "https://github.com/naiyt/reddit-replier Some prior knowledge of Python is expected. We have PRAW which abstracts the actual HTTP interactions with the Reddit API. Why not something on top of PRAW that simplifies creating reply bots? I've read the source for a lot of Reddit bots, and most weren't really designed to lend themselves well to reuse. redditreplier is intended to simplify the \"bot\" part of the process, and let you focus on what you actually want your bot to respond to and say. It will use PRAW to continually watch whatever subreddits you specify. It will then pass any new messages to your code, which decides whether it should reply and what it should reply with. You'll want to go through the README for more details, but you basically just create a class with a `parse` method. redditreplier will pass that any new messages. You then parse that message to decide if you want to respond, and act accordingly. This means creating new bots is a simple process that doesn't require you to rewrite all of the PRAW and Reddit interactions every time. It's still in beta, so there are likely some issues. The exception handling definitely needs some work, for example. Feel free to fork the repo and send in a pull request if you have any suggestions or fixes. I wrote a proof of concept [here](https://github.com/naiyt/autogithubbot) that is basically like AutoWikiBot, but for GitHub links. I'm not actually running it live anywhere because I didn't think anybody would really find it useful. But feel free to peruse that code if you want some ideas. Also, if there is interest in this, **please use it to make interesting, non spammy bots**. Nobody really likes uncalled for bots that respond to every message that contains a specific string. My favorite bots are those that only respond when specifically summoned (with specific keywords that won't accidentally be used) and provide the user with interesting information. If anybody has any questions or comments on this, feel free to PM me or open a GitHub issue.", "title": "I wrote a Python module to make writing Reddit bots simpler (please use for good and not evil)", "url": "https://www.reddit.com/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/"}, {"author": "ThreeCorners", "created_utc": 1406235798, "gilded": 0, "name": "t3_2bmq3s", "num_comments": 3, "score": 2, "selftext": "*I apologize in advance if this is the wrong subreddit for this, but I didn't know of a better one. This arose when I was coding a project with PRAW.* So I've been tracking score over time for various reddit submissions. I found that the most popular submissions often have this very strange \"stepping\" feature, as for example here: [http://imgur.com/saKf104](http://imgur.com/saKf104). Does anybody know if this some aspect of vote fuzzing (it looks as though batches of votes being dropped periodically), or if there is some other explanation? It looks artificial, so my guess is that it is some undocumented feature of reddit.", "title": "Very bizarre submission score behavior", "url": "https://www.reddit.com/r/redditdev/comments/2bmq3s/very_bizarre_submission_score_behavior/"}, {"author": "tylerdurdenbot", "created_utc": 1406235345, "gilded": 0, "name": "t3_2bmp83", "num_comments": 0, "score": 4, "selftext": "Honestly, and imho, this is the single most annoying thing about the reddit api for bots. I haven't personally run into this error for years. I feel like there should be a way for me to authorize any bots that I create so that they can get over this **new user** limitation. But since there isn't any way around this as of yet, PRAW reply bot developers, how are you handling this error? Do you just wait out the 10 minutes or so, or do you somehow create some kind of queue and test the oldest item every cycle or so?", "title": "PRAW - \"You're Doing That too Much!\"", "url": "https://www.reddit.com/r/redditdev/comments/2bmp83/praw_youre_doing_that_too_much/"}, {"author": "DarkMio", "created_utc": 1406205446, "gilded": 0, "name": "t3_2bl8ew", "num_comments": 3, "score": 2, "selftext": "Hi, my problem has to do with the praw.helpers_comment_stream, which seems to have more data than I can access to. import praw r = praw.Reddit(user_agent='Data-Poller for several bot-logins by /u/DarkMio') r_ss = praw.Reddit(user_agent='Small Subreddit Mention Bot by /u/DarkMio') r_ss.login('Botaccount', 'Botpassword') comment_stream = praw.helpers.comment_stream(r, 'dota2', limit=1, verbosity=3) comment = next(comment_stream) This is my general setup. Since I don't want to create more instances than needed, I would like to reply with multiple accounts. (As you may be aware, some subreddits ban particular bots, some don't.) The problem is, that the vars(comment) seems to lack data, where I could point my other praw-instance at. Sample data: {'_info_url': 'http://www.reddit.com/api/info/', '_replies': None, '_submission': None, '_underscore_names': ['replies'], 'approved_by': None, 'author': Redditor(user_name='fr0z3n_'), 'author_flair_css_class': u'thrall', 'author_flair_text': u'ill disrupt you', 'banned_by': None, 'body': u'He played Dota 1 for mouz. But i was talking about the Dota 2 scene :)\\n\\n', 'body_html': u'<div class=\"md\"><p>He played Dota 1 for mouz. But i was talking about the Dota 2 scene :)</p>\\n</div>', 'controversiality': 0, 'created': 1406233756.0, 'created_utc': 1406204956.0, 'distinguished': None, 'downs': 0, 'edited': False, 'gilded': 0, 'has_fetched': True, 'id': u'cj6dzxq', 'json_dict': None, 'likes': None, 'link_author': u'hoang_iee2000', 'link_id': u't3_2bl1t2', 'link_title': u'Puppey - The one-man army', 'link_url': u'http://imgur.com/EDD8o6j', 'name': u't1_cj6dzxq', 'num_reports': None, 'parent_id': u't1_cj6dw60', 'reddit_session': , 'saved': False, 'score': 1, 'score_hidden': True, 'subreddit': Subreddit(display_name='DotA2'), 'subreddit_id': u't5_2s580', 'ups': 1} Either I am completely missing here something (like generating permalinks out of the comment-id - or it isn't possible at all. Thanks for you interest. Edit: Appearantly I've found the most nastiest way to do that in PRAW: import praw from pprint import pprint r = praw.Reddit(user_agent='Data-Poller for several bot-logins by /u/DarkMio') r_ss = praw.Reddit(user_agent='SmallSubredditBot by /u/DarkMio') found = 0 comment_stream = praw.helpers.comment_stream(r, 'all', limit=1, verbosity=3) comment = next(comment_stream) #pprint(vars(comment)) #print comment.link_id thread = str(comment.link_id).replace('t3_', '') submission = r_ss.get_submission(submission_id=thread) submission.replace_more_comments(limit=None, threshold=0) flat_comments = praw.helpers.flatten_tree(submission.comments) print comment.name target_comment = comment.name.replace('t1_', '') for comments in flat_comments: if comments.id == target_comment: print comment.body.encode('utf-8') comment.reply(\"Thanks.\") the big problem with this example is, that I don't target the comment, I just point at the thread where the target can be found and try to load all comments if possible. This fails if there are many stacked comments in a row. How stated, this doesn't look nice nor do I think is it good or anything. A really nasty way to get from one instace the data - and reply with another instance.", "title": "PRAW - loading with one instance - replying with another instance", "url": "https://www.reddit.com/r/redditdev/comments/2bl8ew/praw_loading_with_one_instance_replying_with/"}, {"author": "rreyv", "created_utc": 1406147247, "gilded": 0, "name": "t3_2bj2h3", "num_comments": 2, "score": 2, "selftext": "Reddit Live is awesome! Are there any plans to support creating live threads in PRAW?", "title": "PRAW and creating live threads?", "url": "https://www.reddit.com/r/redditdev/comments/2bj2h3/praw_and_creating_live_threads/"}, {"author": "DarkMio", "created_utc": 1405785947, "gilded": 0, "name": "t3_2b50uq", "num_comments": 0, "score": 1, "selftext": "G'day, I run into an inconsitency with PRAW when using def submission_stream(self): \"\"\"Checks all submissions. Either they're link.posts or self.posts. Either way, we catch both.\"\"\" while True: try: submission_stream = praw.helpers.submission_stream(self.r, self.subreddit, limit=None, verbosity=0) log.info(\"Opened submission stream successfully.\") while self.running == True: submission = next(submission_stream) # retrieve the next submission self.subreddit is 'all' in this case. It is common that I get this error every few minutes lately: Traceback (most recent call last): File \"massdrop_linkfix.py\", line 173, in submission_stream submission = next(submission_stream) # retrieve the next submission File \"/usr/local/lib/python2.7/dist-packages/praw/helpers.py\", line 118, in _stream_generator for i, item in gen: File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 471, in get_content page_data = self.request_json(url, params=params) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 517, in request_json delattr(self, '_request_url') AttributeError: _request_url I am running this on an Raspberry Pi with a Debian + Python 2.7.7 - what's happening here?", "title": "PRAW submission helper is running inconsistently?", "url": "https://www.reddit.com/r/redditdev/comments/2b50uq/praw_submission_helper_is_running_inconsistently/"}, {"author": "WilliamNyeTho", "created_utc": 1405736490, "gilded": 0, "name": "t3_2b3nua", "num_comments": 9, "score": 2, "selftext": "I understand that this is a very simple question, and I have been to the recommended sites to download it. I currently have pip installed. The next step says to type: $ pip install praw But it doesn't say where to type this (I'm running windows 8). Any quick advice is appreciated. Thanks!", "title": "How do I install PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "brucemo", "created_utc": 1405649379, "gilded": 0, "name": "t3_2b0f51", "num_comments": 3, "score": 2, "selftext": "The following is a minimal program that is supposed to go through your mod mail and print out the authors of each of your messages. The real program does more than this, but I removed a bunch of code in order to eliminate other explanations for the error I received. This code worked in previous versions of python and/or PRAW. I reinstalled Windows 8.1 recently so I got new everything. Python 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 10:38:22) [MSC v.1600 32 bit (Intel)] on win32 .. and what appears to be praw-2.1.17 I do not know what I had previously. Other PRAW scripts run, but this one does not. import praw, sys def main(argv): rh = praw.Reddit(\"whatever\") rh.login() for obj in rh.get_mod_mail(limit = 1000): if obj.author: name = obj.author.name else: name = \"[Deleted]\" print(name) if __name__ == \"__main__\": main(sys.argv[1:]) I am a mod of stuff and have mod mail. The output of the above program is nothing. It blows up as follows on my planet: Traceback (most recent call last): File \"testm.py\", line 15, in main(sys.argv[1:]) File \"testm.py\", line 7, in main for obj in rh.get_mod_mail(limit = 1000): File \"C:\\Python\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 295, in wrapped function.func_defaults[0]) AttributeError: 'function' object has no attribute 'func_defaults' sys:1: ResourceWarning: unclosed C:\\Python\\Python34\\lib\\importlib\\_bootstrap.py:2150: ImportWarning: sys.meta_path is empty", "title": "PRAW: Is get_mod_mail broken?", "url": "https://www.reddit.com/r/redditdev/comments/2b0f51/praw_is_get_mod_mail_broken/"}, {"author": "elihusmails", "created_utc": 1405478149, "gilded": 0, "name": "t3_2atnid", "num_comments": 3, "score": 1, "selftext": "I was running into a SSL \"Certificate verify failed\" error in PRAW when logging in to Reddit. I found that if I use r = Reddit(user_agent=username) r.config._ssl_url = None r.login(username, password) things work, but when I log in, my username/password is sent in the clear. So how can I install the proper cert(s) into my Python environment to prevent this from happening?", "title": "How to install Reddit SSL Cert?", "url": "https://www.reddit.com/r/redditdev/comments/2atnid/how_to_install_reddit_ssl_cert/"}, {"author": "elihusmails", "created_utc": 1405334094, "gilded": 0, "name": "t3_2anne2", "num_comments": 3, "score": 2, "selftext": "I have a python bot that goes through my liked and saved links and parses comments. All was working fine until I noticed that when I call \"login\" from PRAW, I now get SSL certificate verify failed error. if __name__ == '__main__': username = sys.argv[1] password = sys.argv[2] r = Reddit(user_agent=username) r.login(username, password) print 'LOGIN SUCCESS' There are situations where I am re-posting data from comments to another subreddit, so I do need to login for the bot to be successful. Did something either change in PRAW or Reddit that would cause this and how do I fix it? EDIT: I'm going to try and verify this to get things working - http://stackoverflow.com/questions/21655478/sslerror-with-praw EDIT2: The link above did work, for anyone who may have run into the same problem.", "title": "PRAW 2.1.17 - certificate verify failed", "url": "https://www.reddit.com/r/redditdev/comments/2anne2/praw_2117_certificate_verify_failed/"}, {"author": "honeyplease", "created_utc": 1405326760, "gilded": 0, "name": "t3_2anhlc", "num_comments": 4, "score": 1, "selftext": "I am building a bot in Python and I'm having trouble with the final stage in which it replies to certain comments. Suppose I know a comment in the 'all' subreddit says \"Apples are green.\", and I want the bot to reply to it, but I don't know its id. I am able to find the comment in the list of flattened comments: submissions = r.get_subreddit('all').get_hot(limit=100) submission = next(submissions) flat_comments = praw.helpers.flatten_tree(submission.comments) print(flat_comments[0]) # Apples are green But how could I retrieve the comment id from this? My goal is to reply to get the id for the comment that matches 'Apples are green', so I can reply to this comment specifically in the last step. I guess I should add that I've been extracting the comments directly from the json dictionary using regex, not using PRAW, so I ended up having a comment without an identifier in PRAW. I am able to see the id of all parent comments using vars(comments) as in the tutorial, but I haven't figured out how to see the vars for the child comments, maybe this is what would help me solve my problem. The tutorials and man pages online aren't too helpful for me because they assume more skills than I have. The nested nature of the json dict makes it kind of a pain to find the comment id using regex, that's why I turned to PRAW for this, but I'm stuck.", "title": "PRAW Retrieve comment id from body, then reply to it.", "url": "https://www.reddit.com/r/redditdev/comments/2anhlc/praw_retrieve_comment_id_from_body_then_reply_to/"}, {"author": "linpressionism", "created_utc": 1404847975, "gilded": 0, "name": "t3_2a65gw", "num_comments": 0, "score": 2, "selftext": "I'm setting up bots to moniter/update a set of wikis for a subreddit, but can't find a way to 1) set wikis to editable only by mods via praw (instead of by hand) or 2) restore a previous version of a wiki via praw. Does anyone know if/how these things are possible? Thanks!", "title": "Wiki restoration and editor control through praw?", "url": "https://www.reddit.com/r/redditdev/comments/2a65gw/wiki_restoration_and_editor_control_through_praw/"}, {"author": "theroflcoptr", "created_utc": 1404175198, "gilded": 0, "name": "t3_29isgn", "num_comments": 1, "score": 0, "selftext": "Some subreddits allow normal users to set submission flair. Is there any way to get PRAW to do this? Seems like it immediately throws a ModeratorOrScopeRequired error without actually trying to set the flair.", "title": "Using PRAW to set submission flair without moderator permissions", "url": "https://www.reddit.com/r/redditdev/comments/29isgn/using_praw_to_set_submission_flair_without/"}, {"author": "Cuisinier_Microsoft", "created_utc": 1404101128, "gilded": 0, "name": "t3_29g2sd", "num_comments": 5, "score": 2, "selftext": "Hello /r/redditdev, I've been looking around the doc and the internet but I can't seem to find how to get the date of a submission with praw. Is it even possible?", "title": "Getting the date of a submission", "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "DarkMio", "created_utc": 1403989462, "gilded": 0, "name": "t3_29chn0", "num_comments": 0, "score": 1, "selftext": "##NVM - I was retarded. next(comment_stream) simply exhausted and stopped the iteration of the stream, since dota2moddingtesting has only very few comments. That stopped the further execution. --- ##Original Post: Running this: import praw r = praw.Reddit(user_agent='MASSDROP guest link poster') comment_stream = praw.helpers.comment_stream(r, 'dota2moddingtesting') submission_stream = praw.helpers.submission_stream(r, 'dota2moddingtesting') while True: comment = next(comment_stream) # Retrieve the next comment submission = next(submission_stream) # retrieve the next submission print submission returns this: Items: 32 (107.74 ips) Items: 36 (18.53 ips) 4 :: Custom Smell Powe r 2 :: Drow Ranger Nekkid 2 :: [Custom Gamemodes] Index for all relevant topics. 3 :: w3x-to-vmf now gets you VAC banned 3 :: Beep bop beep I AMZ A ROBOTZ beep 2 :: aaaa 2 :: Is this real life? 2 :: No pun intended 2 :: Spam 3 :: darkmio peen size confirmed 2 :: modding apply directly to the bobbin 6 :: Alchemist realism mod 2 :: maximum amounts of letters are hard to reach, but when you do, you know,... 2 :: Stylesheet Test for Stylesheet Kids 2 :: Custom Gamemodes are scientifically proven to make karma 2 :: Kappa Kappa & the Chameleons 2 :: Making Titanfall ... er ... what? 2 :: The \"oh the mods broke the subreddit\" thread. 2 :: The new design is now online! 1 :: this update test 0 :: Lorem Ipsum Dolor 2 :: lore dispum 2 :: need more brainless content 2 :: That's a bot message. 2 :: 05/22/14 02:59 PM 2 :: Dota 2 main client update for 05/22/14 03:14 PM 2 :: Dota 2 main client update for 05/22/14 03:18 PM 1 :: Dota 2 main client update for 05/24/14 10:02 AM 0 :: Dota 2 main client update for 05/24/14 11:07 AM 2 :: Stretch Goal #20 unlocked: New Upgraded Creeps 2 :: Stretch Goal #20 unlocked: New Upgraded Creeps 2 :: Stretch Goal #21 unlocked: A->Z Challenge Support Looking at /r/dota2moddingtesting, you'll see, that there are 3 posts missing: 2 about massdrop and one about Stretch Goal #22. Am I missing something here? It's pretty odd that 3 submissions are entirely missing. Edit: To give more detail, what I want to do, is to check recent comments and submissions for massdrop-links. Running something like comment_stream = praw.helpers.comment_stream(r, 'dota2moddingtesting') submission_stream = praw.helpers.submission_stream(r, 'dota2moddingtesting') for comments, submissions in zip(comment_stream, submission_stream): # do work seems to break the iterator-process entirely, ending in never running the for-loop itself.", "title": "PRAW praw.helpers.submission_stream() works, but is missing some entries", "url": "https://www.reddit.com/r/redditdev/comments/29chn0/praw_prawhelperssubmission_stream_works_but_is/"}, {"author": "labtec901", "created_utc": 1403886291, "gilded": 0, "name": "t3_2991wo", "num_comments": 1, "score": 1, "selftext": "I've been following this tutorial https://praw.readthedocs.org/en/v2.1.16/pages/writing_a_bot.html to create the question discover bot with /u/_Daimon_ 's tutorial, but now I'd like to do the same thing, but with the comments in a particular subeddit. How would I go about this?", "title": "[PRAW] Writing a Question-Discover program for comments rather than self posts.", "url": "https://www.reddit.com/r/redditdev/comments/2991wo/praw_writing_a_questiondiscover_program_for/"}, {"author": "BadgerBalls", "created_utc": 1403655826, "gilded": 0, "name": "t3_290jf7", "num_comments": 2, "score": 1, "selftext": "I'm using PRAW to delete and re-upload an image to my sub. It doesn't look like the image takes effect until I manually go in to about/stylesheet and click the \"save\" button. Is there a way to do this via PRAW?", "title": "PRAW equivalent of clicking the \"Save\" button on the Stylesheet page?", "url": "https://www.reddit.com/r/redditdev/comments/290jf7/praw_equivalent_of_clicking_the_save_button_on/"}, {"author": "ThreeCorners", "created_utc": 1403590581, "gilded": 0, "name": "t3_28y0c8", "num_comments": 11, "score": 3, "selftext": "I want to add up a given user's link karma and comment karma. Let's take u/unidan, for example. This gives numbers that are quite different from the numbers reported at www.reddit.com/u/unidan. Here's a snippet: import praw r = praw.Reddit('Testing PRAW.') u = r.get_redditor('unidan') link_karma = 0 for p in u.get_submitted(limit = None): link_karma += p.score print link_karma His profile gives 158,784 link karma, 2,235,856 comment karma. PRAW gives 219,160 link karma, 85,198 comment karma.", "title": "Very inaccurate karma via PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "chocolatecloud12", "created_utc": 1403477378, "gilded": 0, "name": "t3_28tqod", "num_comments": 5, "score": 2, "selftext": "When I type in 'pip install praw' in the terminal, I get a traceback. File \"(stdin)\", line 1 pip install praw ^ SyntaxError: invalid syntax I'm using python 3.3.5, and I have pip installed. Please explain to me 'noob friendly' as possible. Thanks.", "title": "Having problems with installing PRAW using pip", "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": "baloneyeagle123", "created_utc": 1403127692, "gilded": 0, "name": "t3_28hwaz", "num_comments": 2, "score": 0, "selftext": "Here is the code I'm trying to run: def scan_submission(submission): for comment in submission.comments: #What throws the error scan_comment(submission, comment) Which is exactly how every place I've found has said to get comments from a submission. But when I try this to get the following error: AttributeError: '' has no attribute 'comments' Which is extremely confusing to me as the official PRAW documentation shows that the Submission object does contain a comments attribute.", "title": "Error trying to get comments from a submission in PRAW", "url": "https://www.reddit.com/r/redditdev/comments/28hwaz/error_trying_to_get_comments_from_a_submission_in/"}, {"author": "stickytruth", "created_utc": 1403125099, "gilded": 0, "name": "t3_28hrir", "num_comments": 3, "score": 0, "selftext": "I'm suddenly getting inconsistent results using Submission.comments in praw where it is throwing exceptions. Python 3.3.0 Praw 2.1.16 and 2.1.15 Demo script: import sys, praw print('Python version:',sys.version_info) print('Praw Version:', praw.__version__) print('------------') reddit = praw.Reddit('/u/stickytruth bug hunt', log_requests=1) items = ['t3_28holl', # comments = [] # http://www.reddit.com/r/photoshopbattles/comments/28holl/this_parrot_skateboarding/ 't3_28hlqq', # comments = '' has no attribute 'comments' # http://www.reddit.com/r/photoshopbattles/comments/28hlqq/amazons_jeff_bezos/ 't3_28hl19', # comments = '' has no attribute 'comments' # http://www.reddit.com/r/photoshopbattles/comments/28hl19/italian_football_player_marchisio_levitating_in/ 't3_28hb7u' # comments = [] # http://www.reddit.com/r/photoshopbattles/comments/28hb7u/a_buddy_of_mine_midcatch_during_an_ultimate/ ] for thing in reddit.get_submissions(items): print('\\nLoading', thing.title) try: print('# Comments', thing.num_comments) print(thing.comments) except Exception as e: print(e) --- Output [2.1.16]: Python version: sys.version_info(major=3, minor=3, micro=0, releaselevel='final', serial=0) Praw Version: 2.1.16 ------------ retrieving: http://www.reddit.com/by_id/t3_28holl,t3_28hlqq,t3_28hl19,t3_28hb7u.json Loading This Parrot Skateboarding # Comments 0 retrieving: http://www.reddit.com/r/photoshopbattles/comments/28holl/this_parrot_skateboarding/.json [] Loading Amazon's Jeff Bezos # Comments 1 retrieving: http://www.reddit.com/r/photoshopbattles/comments/28hlqq/amazons_jeff_bezos/.json '' has no attribute 'comments' Loading Italian football player Marchisio levitating in training # Comments 2 retrieving: http://www.reddit.com/r/photoshopbattles/comments/28hl19/italian_football_player_marchisio_levitating_in/.json '' has no attribute 'comments' Loading A buddy of mine mid-catch during an Ultimate Frisbee league game # Comments 1 retrieving: http://www.reddit.com/r/photoshopbattles/comments/28hb7u/a_buddy_of_mine_midcatch_during_an_ultimate/.json [] --- Results with praw 2.1.15 are identical. Any idea what's causing this? Thanks", "title": "PRAW Comment Errors", "url": "https://www.reddit.com/r/redditdev/comments/28hrir/praw_comment_errors/"}, {"author": "laptopdude90", "created_utc": 1403045966, "gilded": 0, "name": "t3_28etuk", "num_comments": 2, "score": 4, "selftext": "Is there a way to submit a post with praw? I found a r.submit() function, but it's not in the documentation so I don't know what arguments it wants.", "title": "Submit a post with Praw", "url": "https://www.reddit.com/r/redditdev/comments/28etuk/submit_a_post_with_praw/"}, {"author": "AnSq", "created_utc": 1402935946, "gilded": 0, "name": "t3_28aga6", "num_comments": 3, "score": 4, "selftext": "PRAW has [`get_my_moderation`](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.MySubredditsMixin.get_my_moderation) that uses [`/subreddits/mine/moderator`](http://www.reddit.com/dev/api#GET_subreddits_mine_moderator), and of course there's [`get_moderators`](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_moderators)/[`/r/[subreddit]/about/moderators`](http://www.reddit.com/dev/api#GET_about_moderators), bit I can't seem to find anything to go the other way for any user. Did I just miss it, or does it really not exist? The list of subs someone mods is now public in the sidebar of their profile, so it seems like there should be an easy way to get it programmatically. [That was only added a few months ago](http://www.reddit.com/r/modnews/comments/1wem4f/moderators_the_list_of_subreddits_you_moderate_is/) though, so maybe there isn't.", "title": "Is there any way to get a list of subreddits that someone else moderates?", "url": "https://www.reddit.com/r/redditdev/comments/28aga6/is_there_any_way_to_get_a_list_of_subreddits_that/"}, {"author": "F3AR3DLEGEND", "created_utc": 1402868523, "gilded": 0, "name": "t3_288anc", "num_comments": 1, "score": 2, "selftext": "Hey guys, I am working on a specific Reddit bot which will utilize PRAW. To minimize the amount of requests required (so that I do not have to wait one/two seconds between each request), am I able to send multiple replies through one request? If this is possible through PRAW, please tell me how! Thanks!", "title": "Reply to multiple messages in one request", "url": "https://www.reddit.com/r/redditdev/comments/288anc/reply_to_multiple_messages_in_one_request/"}, {"author": "Alexratman", "created_utc": 1402861360, "gilded": 0, "name": "t3_2880cz", "num_comments": 3, "score": 6, "selftext": "So this bot is used on /r/nerdcubed, what is does is when someone says, \"he is banned from Germany\", it will reply with he is banned from many places. I've got two issues, first its not finding the comments or its not responding to them, secondly I don't think the \"already_done = []\" works, this might be the reason why comments are not being found or responded to. import traceback import time import sys import praw import re nerd_exp = re.compile(ur'(he|dan|nerd).{2,15}?(banned from)', re.IGNORECASE) r = praw.Reddit(user_agent='Responds to banned from jokes in /r/nerdcubed. Created by /u/echocage for /u/tokyorockz, then edited by /u/alexratman') username = 'username' r.login('username', 'password') already_done = [] try: subreddit = r.get_subreddit('nerdcubed') posts = subreddit.get_new() for submission in posts: flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: if not hasattr(comment, 'body'): continue if not comment in already_done: res = re.search(nerd_exp, comment.body) if res is not None: if not 'many' in comment.body: if not comment.author == username: if not any([x for x in comment.replies if (str(x.author) == username) or ('many' in x.body)]): print comment comment.reply(\"He's banned from many places\") time.sleep(5) already_done.append(comment) except praw.errors.RateLimitExceeded as err: print \"Rate Limit Exceeded:\\n\" + str(err), sys.stderr time.sleep(err.sleep_time + .05) except: print traceback.format_exc() I'm new to python so this is the most advanced thing I have done. Any help is appreciated!", "title": "Comment bot not commenting, What have I done wrong?", "url": "https://www.reddit.com/r/redditdev/comments/2880cz/comment_bot_not_commenting_what_have_i_done_wrong/"}, {"author": "BananaPotion", "created_utc": 1402701223, "gilded": 0, "name": "t3_283bqr", "num_comments": 2, "score": 2, "selftext": "Can't find anything in the praw documentation.", "title": "[PRAW] How to get newest posts of last week from a given subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/283bqr/praw_how_to_get_newest_posts_of_last_week_from_a/"}, {"author": "nallen", "created_utc": 1402667325, "gilded": 0, "name": "t3_281u72", "num_comments": 6, "score": 2, "selftext": "In AskScience and Science we have a lot of flaired users, and we'd like to be able to add them to a private subreddit, right now it's done entirely by hand, which is a real chore if several people have added flair. Is there an existing script or bot that would automate the process, like the script that syncs flair between two subreddits? I'm not a programmer, but it seems like something that should be straight forward to do with PRAW.", "title": "Syncing a Flaired User List to Approved Submitter List", "url": "https://www.reddit.com/r/redditdev/comments/281u72/syncing_a_flaired_user_list_to_approved_submitter/"}, {"author": "polhemic", "created_utc": 1402611459, "gilded": 0, "name": "t3_28037u", "num_comments": 2, "score": 3, "selftext": "(Python 2.7 on ubuntu 12.04LTS) I've been looking at praw and it doesn't seem to be handle the after_field argument as I would expect. Have I made a dumbass mistake in here? import praw user_agent = 'Test dlr by /u/polhemic' subreddit = 'reactiongifs' r = praw.Reddit(user_agent=user_agent) sr = r.get_subreddit(subreddit) after_id=None while True: print after_id submissions = sr.get_new(limit=10,after_field=after_id) if submissions == None: break for sub in submissions: print sub.id + ' ' + sub.url after_id = subs.id The tty output comes back from this as: None 2801zr http://i.imgur.com/caqh0iY.gif 28014b http://i.imgur.com/eF8kh6w.gif 27zzm8 http://imgur.com/XUg3aqp 27zz6z http://i.imgur.com/oqh7Qt8.gif 27zya9 http://i.imgur.com/bZLtuUQ.gif 27zy24 http://i.imgur.com/meftV0q.gif 27zvkj http://i.imgur.com/OqM3JRT.gif 27zv2b http://i.imgur.com/TiBjpCN.gif 27zuq9 https://i.imgur.com/XujHL.gif 27ztpe http://i.imgur.com/NFGg4QP.gif 27ztpe 2801zr http://i.imgur.com/caqh0iY.gif 28014b http://i.imgur.com/eF8kh6w.gif 27zzm8 http://imgur.com/XUg3aqp 27zz6z http://i.imgur.com/oqh7Qt8.gif 27zya9 http://i.imgur.com/bZLtuUQ.gif 27zy24 http://i.imgur.com/meftV0q.gif 27zvkj http://i.imgur.com/OqM3JRT.gif 27zv2b http://i.imgur.com/TiBjpCN.gif 27zuq9 https://i.imgur.com/XujHL.gif 27ztpe http://i.imgur.com/NFGg4QP.gif 27ztpe 2801zr http://i.imgur.com/caqh0iY.gif 28014b http://i.imgur.com/eF8kh6w.gif 27zzm8 http://imgur.com/XUg3aqp 27zz6z http://i.imgur.com/oqh7Qt8.gif 27zya9 http://i.imgur.com/bZLtuUQ.gif 27zy24 http://i.imgur.com/meftV0q.gif 27zvkj http://i.imgur.com/OqM3JRT.gif 27zv2b http://i.imgur.com/TiBjpCN.gif 27zuq9 https://i.imgur.com/XujHL.gif 27ztpe http://i.imgur.com/NFGg4QP.gif Which then repeats these last blocks ad-infinitum, never progressing further back through history.", "title": "[praw] Can't get praw.Object.Subreddit.get_new to honor after_field argument", "url": "https://www.reddit.com/r/redditdev/comments/28037u/praw_cant_get_prawobjectsubredditget_new_to_honor/"}, {"author": "VoterBot", "created_utc": 1402490508, "gilded": 0, "name": "t3_27vebn", "num_comments": 2, "score": 1, "selftext": "I'd be grateful for suggestions on how to debug this issue with PRAW. I don't have direct access to praw/helpers.py or praw/decorators.py (nor would I want to dick around with them), and as a novice with Python I don't know how to reliably replicate the intermittent network flaw which throws the ValueError, which I assume is being caused by a trashed JSON request or something. Here's the log: [0m Traceback (most recent call last): [0m File \"/app/.heroku/python/lib/python3.4/site-packages/praw/__init__.py\" , line 471, in get_content [0m page_data = self.request_json(url, params=params) [0m File \"/app/.heroku/python/lib/python3.4/site-packages/praw/__init__.py\", line 516, in request_json [0m data = json.loads(response, object_hook=hook) [0m File \"myTest.py\", line 426, in [0m for submission in submissionStream: [0m File \"/app/.heroku/python/lib/python3.4/site-packages/praw/helpers.py\", line 118, in _stream_generator [0m File \"/app/.heroku/python/lib/python3.4/json/decoder.py\", line 343, in decode [0m for i, item in gen: [0m File \"/app/.heroku/python/lib/python3.4/site-packages/praw/decorators.py\", line 161, in wrapped [0m ValueError: Unterminated string starting at: line 1 column 2153 (char 2152) [0m return_value = function(reddit_session, *args, **kwargs) [0m obj, end = self.raw_decode(s, idx=_w(s, 0).end()) [0m File \"/app/.heroku/python/lib/python3.4/json/__init__.py\", line 331, in loads [0m obj, end = self.scan_once(s, idx) [0m return cls(**kw).decode(s) [0m sys:1: ResourceWarning: unclosed [0m /app/.heroku/python/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty [0m File \"/app/.heroku/python/lib/python3.4/json/decoder.py\", line 359, in raw_decode When this crashes praw.helpers.submission_stream() it also gives the 'unlcosed socket warning' at the bottom there, which I complained about a couple of days ago here. Socket timeouts also generate that warning. I've worked around the problem by simply re-calling the submission_stream() when it crashes, but this is inelegant and results in a heavier load on the reddit server than is necessary, especially since it can happen quite often at certain times of day on Heroku. Is this a PRAW bug? Or would you think it's something I'm doing wrong? The call to submission_stream works most of the time and is the standard way of doing it, as far as I know: while True: try: submissionStream = praw.helpers.submission_stream(r, '+'.join(subreddits.getInputList()), maxPosts,0) for submission in submissionStream: except ValueError as err: print(\"Submission stream ValueError. Sleeping for 5 seconds and trying again. Here's the error: \"+str(err), file=sys.stderr) time.sleep(5) continue Thanks.", "title": "[PRAW] ValueError in praw.helpers.submission_stream() on Heroku", "url": "https://www.reddit.com/r/redditdev/comments/27vebn/praw_valueerror_in_prawhelperssubmission_stream/"}, {"author": "VoterBot", "created_utc": 1402311831, "gilded": 0, "name": "t3_27os73", "num_comments": 1, "score": 2, "selftext": "Over the last three days comment_stream() and submission_stream() in Praw have been failing with socket.timeout errors. I can catch the exception easily enough and loop back to the submissions_stream() call without the script crashing, but I still get this error in the Heroku log: app[worker.1]:\u2190[0m bot.py:211: ResourceWarning: unclosed Line 211 in the script is a continue command in the socket.timeout exception handler which loops back to the submission_stream() call. It doesn't cause a performance problem so I'm not too bothered, but it bugs me and it would be nice if I could fix it so it doesn't dirty up my PRAW log. The timeouts don't happen on my local machine, only on Heroku. I'd be grateful for suggestions.", "title": "Recent socket timeouts under Heroku and PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/27os73/recent_socket_timeouts_under_heroku_and_praw/"}, {"author": "snaysler", "created_utc": 1402001909, "gilded": 0, "name": "t3_27ey83", "num_comments": 3, "score": 1, "selftext": "Hi there. I'm designing an online course in /r/ludobots, and am using RedditBots (PRAW) to mediate everything. I have a pretty large program with main functions I'm trying to run successively in one execution of the program. First I'm scouring the \"new\" posts of the sub to look for ones that haven't been looked at already according to data stored in a secret wiki page in my sub. Then it finds the first url reference in these posts, and if the url references another post within my sub, it connects the two as \"connected nodes\" in a sort of \"tree\" visualization. Then it updates the tree data in /r/ludobots/wiki/data, updates the page listings in /r/ludobots/wiki/tree, and creates an image of the tree visualization. Then it uploads this image to my sub. Now this all works just fine...if I only run one function at a time in each execution of the program. ***BUT***, if I run all the functions back to back in one execution of the program, then none of it seems to work, or only the first function has any effect. It seems like uploading to Reddit has no effect, because nothing has changed in my sub that should have changed during execution. I figured maybe these functions are stressing the Reddit servers too much, so I put a time.sleep(20) call between functions so it would wait 20 seconds before doing another task of the process. This makes it seem to run the first and third of three functions, omitting the middle one. Is this something that just happens with large praw-based programs?", "title": "I'm designing online class with Reddit; PRAW has started malfunctioning, what's going on?", "url": "https://www.reddit.com/r/redditdev/comments/27ey83/im_designing_online_class_with_reddit_praw_has/"}, {"author": "quadnix", "created_utc": 1401860605, "gilded": 0, "name": "t3_279qlu", "num_comments": 2, "score": 3, "selftext": "I'd like to add a user as an approved submitter with PRAW, but after combing the docs, I couldn't find a way to do it. Am I missing out on something?", "title": "Adding a user as an approved submitter with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/279qlu/adding_a_user_as_an_approved_submitter_with_praw/"}, {"author": "nuntipat", "created_utc": 1401461306, "gilded": 0, "name": "t3_26vmat", "num_comments": 0, "score": 3, "selftext": "I'm trying to get submission and their comment from Reddit based on the keywords given. The code below is what I have done so far. I have a several question regarding the code. 1. Is there a way to reduce the number of API call? Now it seems that there is 1 API call per each submission and their comment. I think that it might be possible to issue API for multiple submissions once (default limit is 25 right?) 2. Is there a way to know the number of submission that is relevant to the keyword. Currently I could limit the search space using period (day, month, year, ...) but how can I know how many submission match in the period specify. Thanks f = open(file=filename, mode='w', encoding='utf-8') r = praw.Reddit(...) subreddit = r.search(query=keyword, sort='new', limit=submission_limit, period=period) print ('\\nStart crawling ' + str(submission_limit) + ' submissions to ' + filename) countSubmission = 1 for submission in subreddit: f.write(\"\\n\\nSUBMISSION : \" + str(countSubmission) + \" DATE : \" + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(submission.created_utc)) + \"\\n\\n\") f.write(submission.selftext) #submission.replace_more_comments(limit=None, threshold=0) flat_comments = praw.helpers.flatten_tree(submission.comments) countComment = 1 for comment in flat_comments: if comment is praw.objects.Comment: f.write(\"\\n\\nCOMMENT : \" + str(countComment) + \"\\n\\n\") f.write(comment.body) countComment += 1 countSubmission += 1 f.close() print ('Done')", "title": "[PRAW] Reddit search API", "url": "https://www.reddit.com/r/redditdev/comments/26vmat/praw_reddit_search_api/"}, {"author": "tehusername", "created_utc": 1401350794, "gilded": 0, "name": "t3_26rr50", "num_comments": 1, "score": 3, "selftext": "I've been trying [this](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.Redditor.get_comments) to retrieve comments from a redditor but I can't seem to filter out comments only for today. I've tried setting *time* to *day* and *today* but still to no avail. Any suggestions?", "title": "[PRAW] Get redditor's comments for today only", "url": "https://www.reddit.com/r/redditdev/comments/26rr50/praw_get_redditors_comments_for_today_only/"}, {"author": "DrJosh", "created_utc": 1401206255, "gilded": 0, "name": "t3_26m45h", "num_comments": 22, "score": 10, "selftext": "[CommentTreeBot](http://www.reddit.com/user/CommentTreeBot)... * uses praw to re-construct the comment tree for a posting; * creates a tree visualization of it using GraphViz; * posts the resulting image to imgur; and * posts a comment in that posting pointing to the visualization.", "title": "I created CommentTreeBot.", "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "gavin19", "created_utc": 1400979401, "gilded": 0, "name": "t3_26ex2a", "num_comments": 5, "score": 2, "selftext": "I just noticed that the past few days that my AutoMods and scripts are all getting the same error. I have never had this before. I run two instances of AutoMod for two separate subs, under two different accounts (/u/RFootballBot and /u/COYBot). I also run four small one-off scripts that just update the sidebars in four different subreddits. 2 of those are run through this account, and 2 through /u/RFootballBot. Each is set to run once per hour, but they're run at 10/25/40 and 55 minutes past the hour respectively. They're all run from the same Digital Ocean server. Also, I just tried one script on the server and it got the error, but then tried it locally from my Windows machine and it ran fine. Is this just me being rate-limited because they're all coming from the same IP? If so, can I just increase the sleep time in PRAW between passes, or introduce some delay? Cheers.", "title": "Rate limit exceeded errors with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/26ex2a/rate_limit_exceeded_errors_with_praw/"}, {"author": "tehusername", "created_utc": 1400853291, "gilded": 0, "name": "t3_26aqel", "num_comments": 7, "score": 4, "selftext": "I've been sifting through the PRAW documentation and I can't seem to find anything about this. My last resort would be to somehow get sample submissions and if it fails, conclude that it must be a private one. Any thoughts on this?", "title": "[PRAW] How to check if a subreddit is private/restricted?", "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "AnSq", "created_utc": 1400757782, "gilded": 0, "name": "t3_2678bh", "num_comments": 2, "score": 0, "selftext": "I was searching for a way to get only unread mod mail messages like `get_unread()` (not `get_mod_mail()`) and I found [this post from 9 months ago](http://www.reddit.com/r/redditdev/comments/1jklbu/praw_is_there_a_function_for_getting_unread_mod/): >Does `get_unread()` also get unread mod mail (it doesn't seem like it should, because it's a part of `praw.__init__.PrivateMessagesMixin(*args, **kwargs)`)? Is there a way to only get unread mod mail through `get_mod_mail()`? > >Edit: Thinking about it further, I think there are some apps out there that provide notifications for new mod mail, so surely this must be possible somehow (?) with this reply from /u/_Daimon_: >Possible yes, implemented in PRAW no. I have some other coding matters to attend to first, but I should be able to add this functionality within the week. I'll be sure to add another comment to this post when I do. Does such a function actually exist now and I just missed it?", "title": "RE: [PRAW] Is there a function for getting unread mod mail?", "url": "https://www.reddit.com/r/redditdev/comments/2678bh/re_praw_is_there_a_function_for_getting_unread/"}, {"author": "jsmcgd", "created_utc": 1400755100, "gilded": 0, "name": "t3_26762p", "num_comments": 1, "score": 2, "selftext": "Hi, I've just started to use PRAW. The documentation seems thorough but I can't seem to find a list of what functions are made available to me. Can someone provide a link. Apologies if I'm being blind.", "title": "Is there a documented list of PRAW's API functions?", "url": "https://www.reddit.com/r/redditdev/comments/26762p/is_there_a_documented_list_of_praws_api_functions/"}, {"author": "SyedAJafri", "created_utc": 1400720634, "gilded": 0, "name": "t3_2664jz", "num_comments": 8, "score": 1, "selftext": "I working on a bot and apparently reddit doesn't let you pm multiple people in one message so I have to send a message for each person. Is there some way I could have all the pms sent at once? Would there be some kind of limit? The bot will send a pm to each user in a thread if the op requests it, so the list can get pretty huge. Edit: Just to clarify I meant using PRAW.", "title": "Trying to pm multiple people without waiting 2 seconds every time.", "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "spike77wbs", "created_utc": 1400394857, "gilded": 0, "name": "t3_25uii4", "num_comments": 7, "score": 1, "selftext": "I am getting a \"praw.errors.AlreadySubmitted: `that link has already been submitted` on field `url`\". So I tried this \"redd.submit('xyzsubreddit', title, url=url, resubmit='true')\" but that didn't recognize resubmit. Is it possible to resubmit? Sorry am a newbie trying to learn this stuff....", "title": "Is it possible to use PRAW to resubmit?", "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "God-of-the-gaps", "created_utc": 1400382119, "gilded": 0, "name": "t3_25u5gx", "num_comments": 1, "score": 1, "selftext": "I don't even know at this point. I've reinstalled and installed every version of Python, 3.4, 2.7 all of them, and I can't even get Pip to work yet, let alone praw. My computer is windows Xp. On the 3.4 version, I put \"python\" whatever into the command thing, and it gives back that python isnt even a command, so I went to path and added that in, but whenever I do \"python distribute-setup.py\" or something, it says that the file doesn't exist So then I tried found the two files for pip and distribute in the downloads area and actually clicked open with python for both But now when I try to say \"pip install praw\", it just says syntax error. :( EDIT:tried deleting python and re-installing to get a fresh start, but now it won't even uninstall. ;_;", "title": "Unable to install Praw. Please Help!", "url": "https://www.reddit.com/r/redditdev/comments/25u5gx/unable_to_install_praw_please_help/"}, {"author": "PaterCiller", "created_utc": 1400275293, "gilded": 0, "name": "t3_25qt11", "num_comments": 2, "score": 3, "selftext": "anyway to get the subject of a message in PRAW?", "title": "[PRAW] Get message subject?", "url": "https://www.reddit.com/r/redditdev/comments/25qt11/praw_get_message_subject/"}, {"author": "redditratings", "created_utc": 1400200437, "gilded": 0, "name": "t3_25o8xq", "num_comments": 4, "score": 1, "selftext": "Relevant docs: * [Step 3: Getting authorization form the user. (from praw docs)](http://praw.readthedocs.org/en/latest/pages/oauth.html#step-3-getting-authorization-from-the-user) * [Authorization (from reddit api wiki)](https://github.com/reddit/reddit/wiki/OAuth2#authorization) In the reddit api wiki, it says: > You should generate a unique, possibly random, string for each authorization request. This value will be returned to you when the user visits your REDIRECT_URI after allowing your app access - you should verify that it matches the one you sent. This ensures that only authorization requests you've started are ones you finish. Few questions: 1. Does this mean every time an OAuth request is started, a unique key should be generated for that request? 2. It says \"possibly random\". Should the key have a \"unique\" component and a \"random\" component? 3. What is a good way to generate this key?", "title": "[praw] get_authorize_url method requires a \"unique key\", but docs don't say much about it. What is a good way to generate this key?", "url": "https://www.reddit.com/r/redditdev/comments/25o8xq/praw_get_authorize_url_method_requires_a_unique/"}, {"author": "Seeing_Eye_Bot", "created_utc": 1399863570, "gilded": 0, "name": "t3_25bvld", "num_comments": 4, "score": 1, "selftext": "Hello World, I have been interested in Reddit bots for a little while now and I would like to make one for fun :) . I have zero coding knowledge, but I felt like learning a Reddit Bot would be a fun way to get some. I had a cool idea about a bot that would put ^this ^text into **bold** or something. Im not really sure how to do it and some guides I have seen have been a little confusing... this is what I have so far: import praw r = praw.Reddit(user_agent='My first bot...Put more info here when done') r.login('Seeing_Eye_Bot', '*password*') subreddit = r.get_subreddit('test') subreddit_comments = subreddit.get_comments() . I guess I am Wondering how I can do a search for ^this ^text and then replace it with **this text** Any and All Help is appreciated, I apologize if I wrote this out very poorly.", "title": "Please Help On First Bot", "url": "https://www.reddit.com/r/redditdev/comments/25bvld/please_help_on_first_bot/"}, {"author": "BrandieBrands", "created_utc": 1399532142, "gilded": 0, "name": "t3_250uqn", "num_comments": 3, "score": 1, "selftext": "As a complete beginner in Python I've been following the PRAW tutorials. Now when I run my python file that I wrote with help from [the third tutorial](https://praw.readthedocs.org/en/latest/pages/comment_parsing.html) I keep getting this error: http://imgur.com/d6yKbZw. The comment is, however, still being posted. Is there anyone who has gotten this error before and has resolved it, or knows what to do?", "title": "HTML DeprecationWarning when replying to a comment", "url": "https://www.reddit.com/r/redditdev/comments/250uqn/html_deprecationwarning_when_replying_to_a_comment/"}, {"author": "ubccompscistudent", "created_utc": 1399436378, "gilded": 0, "name": "t3_24xdaj", "num_comments": 10, "score": 2, "selftext": "Originally I tried to install setup via the command line with: curl https://bootstrap.pypa.io/ez_setup.py -o - | python But then I got the following message: The following error occurred while trying to add or remove files in the installation directory: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/test-easy-install-20455.pth' So now I'm just trying to run get-pip.py in the python interpreter and now I'm getting the exact same message. So clearly I need to provide administrator access to the python directory. But I have no idea how to do that. At no point am I being prompted for root and password. Installing pip/PRAW has been the most frustrating experience in my two years of programming for something that should be so easy.", "title": "Trying to install pip, but getting administrator", "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "JBHUTT09", "created_utc": 1399395392, "gilded": 0, "name": "t3_24vkho", "num_comments": 10, "score": 7, "selftext": "I was thinking of adding an account age check to the bot that helps me mod a subreddit since we're occasionally spammed by 0 day old accounts. It wasn't going to remove the posts, but rather send a modmail asking the mods to check to make sure it's a legit post. Anyway, I can't seem to find anything about account ages in either the PRAW code overview or the reddit API documentation. Am I just missing it, or does it not exist? Thank you.", "title": "Is there a way to get the age of an account?", "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "brianjherman", "created_utc": 1399352701, "gilded": 0, "name": "t3_24ua4d", "num_comments": 1, "score": 2, "selftext": "code: https://gist.github.com/brianherman/e92f55cf3561397236b9 I get the error: Traceback (most recent call last): File \"karmadecay.py\", line 25, in submission.delete() File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 477, in upvote return self.vote(direction=1) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 502, in vote return self.reddit_session.request_json(url, data=data) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 177, in wrapped raise error_list[0] praw.errors.NotLoggedIn: `please login to do that` on field `None`", "title": "Can't get praw to delete a post", "url": "https://www.reddit.com/r/redditdev/comments/24ua4d/cant_get_praw_to_delete_a_post/"}, {"author": "jimgur", "created_utc": 1399260949, "gilded": 0, "name": "t3_24qx0w", "num_comments": 2, "score": 3, "selftext": "Hello, I made a [reddit bot](https://github.com/Carpetfizz/Jimgur/blob/master/jimgur.py) using PRAW that ran for a couple hours and just stopped. It didn't error out or anything, nor did it give a `RateLimitExceeded` exception. I'm guessing this is just the website itself temporarily stopping it? Thanks for any clarification! EDIT: Thanks for the replies. It turns out that it stopped because there simply weren't any more posts during that time. I changed it to get new submissions, so it's been running consistently for about 4-6 hours.", "title": "Is it normal for PRAW bots to just stop for some time?", "url": "https://www.reddit.com/r/redditdev/comments/24qx0w/is_it_normal_for_praw_bots_to_just_stop_for_some/"}, {"author": "pietrod21", "created_utc": 1399170653, "gilded": 0, "name": "t3_24o064", "num_comments": 6, "score": 1, "selftext": "This is what I get when installing praw at the beginning: C:\\Users\\Admin\\Desktop\\pip>pip install praw Downloading/unpacking praw Running setup.py egg_info for package praw C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'entry_points' warnings.warn(msg) C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'install_requires' warnings.warn(msg) C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'test_suite' warnings.warn(msg) C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'tests_require' warnings.warn(msg) usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...] or: -c --help [cmd1 cmd2 ...] or: -c --help-commands or: -c cmd --help error: invalid command 'egg_info' Complete output from command python setup.py egg_info: C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'entry_points' warnings.warn(msg) and this is the log in C:\\Users\\Admin\\AppData\\Roaming\\pip: ------------------------------------------------------------ C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\Scripts\\pip-script.py run on 05/04/14 04:18:56 Downloading/unpacking https://github.com/praw-dev/praw Downloading from URL https://github.com/praw-dev/praw Cannot unpack file c:\\users\\admin\\appdata\\local\\temp\\pip-pcgqqj-unpack\\praw (downloaded from c:\\users\\admin\\appdata\\local\\temp\\pip-nekbib-build, content-type: text/html; charset=utf-8); cannot detect archive format Cannot determine archive format of c:\\users\\admin\\appdata\\local\\temp\\pip-nekbib-build Exception information: Traceback (most recent call last): File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\basecommand.py\", line 126, in main self.run(options, args) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\commands\\install.py\", line 223, in run requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\req.py\", line 961, in prepare_files self.unpack_url(url, location, self.is_download) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\req.py\", line 1079, in unpack_url return unpack_http_url(link, location, self.download_cache, only_download) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\download.py\", line 455, in unpack_http_url unpack_file(temp_location, location, content_type, link) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\util.py\", line 486, in unpack_file raise InstallationError('Cannot determine archive format of %s' % location) InstallationError: Cannot determine archive format of c:\\users\\admin\\appdata\\local\\temp\\pip-nekbib-build Then I solved the problem installing this: http://stackoverflow.com/questions/11425106/python-pip-install-fails-invalid-command-egg-info (pip install --upgrade setuptools) And rerunning \"pip install praw\" I got: C:\\Users\\Admin\\Desktop\\pip>pip install praw Requirement already satisfied (use --upgrade to upgrade): praw in c:\\users\\admin \\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages Requirement already satisfied (use --upgrade to upgrade): requests>=1.2.0 in c:\\ users\\admin\\appdata\\local\\enthought\\canopy\\app\\appdata\\canopy-1.3.0.1715.win-x86 _64\\lib\\site-packages (from praw) Requirement already satisfied (use --upgrade to upgrade): six>=1.4 in c:\\users\\a dmin\\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages (from praw) Requirement already satisfied (use --upgrade to upgrade): update-checker>=0.9 in c:\\users\\admin\\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages (from praw ) Cleaning up... Other maybe useful info: I have windows 8 I have python 3.2.2 and python 2.7 installed. Any help? ps I know I have to eradicate windows from my oc, the problem is that soon I have to return it...", "title": "I install praw via pip install, but doesn't work.", "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "Sc1F1", "created_utc": 1399159370, "gilded": 0, "name": "t3_24nm86", "num_comments": 1, "score": 4, "selftext": "Hey guys, I'm trying to implement a parent check function, but I'm having a few problems. First, when I try: `if string in parent.body` I get the message: `AttributeError: '' has no attribute 'body'` It works perfectly find though when I use: `print parent.body`. Also, when It performs more than one check: `if parent.id in already_done and string in parent.body` It returns the error: `TypeError: argument of type 'Comment' is not iterable` Any help on these would be greatly appreciated. Edit: I used `parent = r.get_info(thing_id=comment.parent_id)` to get the parent. Edit2: I just tried `print parent.body` again and it failed, my guess is I'm getting this error because parent is a submission not a comment(*facepalm*). So now I need to figure out how to test if parent is belongs to praw.objects.Submission class. Edit3: Used isinstance() to check. While I didn't receive any answers, posting here did help me think of a solution, so thanks guys.", "title": "[PRAW] Checking content of comment parent.", "url": "https://www.reddit.com/r/redditdev/comments/24nm86/praw_checking_content_of_comment_parent/"}, {"author": "actual_factual_bear", "created_utc": 1399069293, "gilded": 0, "name": "t3_24ku36", "num_comments": 7, "score": 3, "selftext": "I have it running on Python 2.7, but I tried moving a script to an OS X 10.6 machine running Python 2.6.1, and I get an exception as soon as I try to construct an instance: r = praw.Reddit(user_agent='my_cool_application') Traceback (most recent call last): File \"\", line 1, in File \"/Library/Python/2.6/site-packages/praw-2.1.15-py2.6.egg/praw/__init__.py\", line 1059, in __init__ super(AuthenticatedReddit, self).__init__(*args, **kwargs) ... File \"/Library/Python/2.6/site-packages/requests-2.2.1-py2.6.egg/requests/packages/urllib3/connection.py\", line 83, in _prepare_conn if self._tunnel_host: AttributeError: 'HTTPConnection' object has no attribute '_tunnel_host' I tried searching for this \"tunnel_host\" error without any luck. I have already tried using \"easy_install\" to update to the latest version of praw.", "title": "Is PRAW really compatible with Python 2.6?", "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": "GHETTO_CHiLD", "created_utc": 1398959680, "gilded": 0, "name": "t3_24gpae", "num_comments": 2, "score": 4, "selftext": "if possible i would love to request the feature of 'reason' be added to the get_banned() function in praw. right now some users are on a temporary ban and some users are not. if i could pull in the reason field i would be able to determine if my scripts should delete the user from our subreddits tools/web apps database.", "title": "feature request: add 'reason' to get_banned in praw.", "url": "https://www.reddit.com/r/redditdev/comments/24gpae/feature_request_add_reason_to_get_banned_in_praw/"}, {"author": "charredgrass", "created_utc": 1398904771, "gilded": 0, "name": "t3_24ezpi", "num_comments": 14, "score": 3, "selftext": "I don't have a computer that I can keep on 24/7, and I want to make a bot that can scan comments for certain words and reply to them. What is the best free way to host this? Also, sorry I'm such a noob at this, I just started learning PRAW a few days ago and Python a few months ago.", "title": "Where to host a simple PRAW bot that just scans comments of a sub?", "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "mgrieger", "created_utc": 1398882896, "gilded": 0, "name": "t3_24dzm3", "num_comments": 6, "score": 1, "selftext": "Hi everyone, I have a bot that I am experimenting with opening up to all of reddit, instead of just particular subreddits that have given me permission. I expect that inevitably the bot will be banned from subreddits that do not want the bot to post there anymore. So my question is the following: what is the best way to handle a bot trying to reply to a comment in a subreddit that it is banned in? Is this even something I have to worry about? I have never had an account banned before so I don't know if the comments even show up to the bot if it is banned. I did some googling on this question and I couldn't find anything, but I have my code setup like this for now: try: comment.reply(comment_text) except: pass I'm assuming that if there is an issue with bots attempting to reply to comments in a subreddit they are banned in that some sort of exception is thrown. I don't like having try/except blocks that catch every exception, so is there a certain exception that is thrown for this problem? Maybe I just overlooked something, but I couldn't find anything in PRAW's documentation or reddit's API documentation that specified an exception type. Thanks! **EDIT:** Thanks to /u/RemindMeBotWrangler, I got it working. For future reference, here is what worked for me: import requests try: comment.reply(comment_text) except requests.exceptions.HTTPError, err: if str(err) == '403 Client Error: Forbidden': pass", "title": "[PRAW] Best way to handle being banned from a subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/24dzm3/praw_best_way_to_handle_being_banned_from_a/"}, {"author": "elihusmails", "created_utc": 1398740153, "gilded": 0, "name": "t3_248v0w", "num_comments": 4, "score": 3, "selftext": "Is there anything in the praw API? I didn't see anything and since there's some specific formatting in the comments when returned in praw, I was hoping there would be something. Thanks.", "title": "Looking for a way to grab URL links out of comments", "url": "https://www.reddit.com/r/redditdev/comments/248v0w/looking_for_a_way_to_grab_url_links_out_of/"}, {"author": "HellFireKoder", "created_utc": 1398733909, "gilded": 0, "name": "t3_248ku8", "num_comments": 16, "score": 3, "selftext": "I am going through the tutorial at https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html I had to `print(r.user.send_message(username, message))` just to find out it returned anything, it doesn't print any errors to the screen on it's own. The code I used is at http://pastebin.com/gTFpbTvz For every time it finds one of the words, it returns \"{'errors': []}\" I give it the right username and password, I can't figure out why it won't work. P.S. Even if I don't have the `print` in `print(r.user.send_message('MyUserName', msg))` it still doesn't send me a message. Any help is appreciated. I am using python 3.4", "title": "user.send_message() Returning \"{'errors': []}\" And Not Sending Message", "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "Sc1F1", "created_utc": 1398610178, "gilded": 0, "name": "t3_243ueb", "num_comments": 2, "score": 1, "selftext": "Hey guys, I want to write a bot, but every time I try to install PRAW I get this output: Downloading/unpacking praw Could not fetch URL https://pypi.python.org/simple/praw/: There was a problem confirming the ssl certificate: Will skip URL https://pypi.python.org/simple/praw/ when looking for download links for praw Could not fetch URL https://pypi.python.org/simple/: There was a problem confirming the ssl certificate: Will skip URL https://pypi.python.org/simple/ when looking for download links for praw Cannot fetch index base URL https://pypi.python.org/simple/ Could not fetch URL https://pypi.python.org/simple/praw/: There was a problem confirming the ssl certificate: Will skip URL https://pypi.python.org/simple/praw/ when looking for download links for praw Could not find any downloads that satisfy the requirement praw Cleaning up... No distributions at all found for praw Any help with this would be greatly appreciated. EDIT: A word", "title": "Error when installing PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/243ueb/error_when_installing_praw/"}, {"author": "YouBetterGoFindIt", "created_utc": 1398575813, "gilded": 0, "name": "t3_2432ye", "num_comments": 4, "score": 1, "selftext": "praw.errors.RateLimitExceeded: `you are doing that too much. try again in 9 minutes.` on field `ratelimit`", "title": "What is the rate limit exceeded message and how can I avoid it?", "url": "https://www.reddit.com/r/redditdev/comments/2432ye/what_is_the_rate_limit_exceeded_message_and_how/"}, {"author": "5loon", "created_utc": 1398539116, "gilded": 0, "name": "t3_241rkt", "num_comments": 2, "score": 1, "selftext": "import praw r = praw.Reddit(user_agent='example') r.login(' ', ' ') if message in r.get_unread(): text = message.body author = message.author message.mark_as_read() returns \"NameError: name 'message' is not defined\" I'm reasonably new to PRAW. I'm probably making some little silly mistake on defining \"message\". Thanks in advance. **Edit**: Found out what was wrong. It's for message in r.get_unread(): Thanks /u/gavin19!", "title": "Having trouble parsing inbox messages. [Python 2.7] [PRAW]", "url": "https://www.reddit.com/r/redditdev/comments/241rkt/having_trouble_parsing_inbox_messages_python_27/"}, {"author": "CastleCorp", "created_utc": 1398522831, "gilded": 0, "name": "t3_2415uq", "num_comments": 12, "score": 1, "selftext": "Hey guys! I am new to PRAW, and pretty new to python (I am experienced in Java) so I am trying to learn as I go, and so far, with tutorials and all that good stuff. Here is what I am trying to do: * Get a list of subreddits the user is subscribed to (this is working currently) * Get the top (hot) post from each of the subscribed subreddits (not working) subscribed_reddits = r.get_my_subreddits(limit=500) # get all subscribed reddits, up to 500 subbed_reddits = [] # Array to hold all subscribed subreddits hot_posts= [] # Array to hold all the hot posts def getSubscribedReddits(): # store each subscribed reddit in array, print the array for subreddit in subscribed_reddits: subbed_reddits.append(subreddit) # add the subreddit's name to the array print subreddit.display_name return subbed_reddits # return for function call pass def getHotPosts(): # Get the top \"Hot\" post from each of the user's subscribed reddits for subreddit in subbed_reddits: i = iter(subbed_reddits) item = i.next() current_sub = r.get_subreddit(item.display_name) hot_posts.append(current_sub.get_hot(limit=1)) submissions = current_sub.get_hot(limit=1) return hot_posts pass Those are both working fine, but no matter what I try, I can't seem to get the list of posts from hot_posts[] in human-readable form. Does anyone have suggestions on how I can do this? Thanks!", "title": "Need some help iterating through submissions in an array", "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "Ambit", "created_utc": 1398463033, "gilded": 0, "name": "t3_23ziu3", "num_comments": 7, "score": 3, "selftext": "I'm attempting to use the Reddit API from Python using Google's OAuth2Client library. The issue I'm having is that when I get to `step2_exchange()`, Reddit responds with JSON that just says `{ \"error\": 401 }` The token URI I'm using is `https://ssl.reddit.com/api/v1/access_token`. OAuth2Client handles adding the parameters to the HTTP request. It adds some that the Reddit docs don't ask for, but I get the same response even after commenting them out. This was working when I was playing around with PRAW, so I'm guessing I'm just missing something small here.", "title": "API Returning 401 Response When Using OAuth2Client", "url": "https://www.reddit.com/r/redditdev/comments/23ziu3/api_returning_401_response_when_using_oauth2client/"}, {"author": "110011001100", "created_utc": 1398370004, "gilded": 0, "name": "t3_23w0jq", "num_comments": 5, "score": 5, "selftext": "Ideally a Python library, something I can use with PRAW I am trying to export reddit self text submissions to Kindle", "title": "Is there a library to translate markdown to HTML?", "url": "https://www.reddit.com/r/redditdev/comments/23w0jq/is_there_a_library_to_translate_markdown_to_html/"}, {"author": "Phteven_j", "created_utc": 1398324898, "gilded": 0, "name": "t3_23ucco", "num_comments": 1, "score": 7, "selftext": "In the spirit of not doing the same work twice, I'm going to show you how to do something that helps you avoid doing the same work twice. It is surprisingly easy to set up an SQL database and interface it with your bot. ###The problem: Not doing the same work twice. If you followed the PRAW documentation, you may have used [this technique](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html#not-doing-the-same-work-twice) to prevent your bot doing something twice like leaving a comment, sending a message, or really anything. We don't want our bot spamming up reddit just because it doesn't know what it already did. So, we make use of a temporary local variable like already_done and keep track of what we have processed in it via: already_done.append(submission.id) ###The other problem: Losing your records of work done In my apps, I use a two-factor sanity check to avoid doing the same work: the above list method and a thorough search. The problem with the list method (already_done.append(id)) is that if the bot crashes, you lose the record of what work you have done. What I will often do is comb through my bot's history to make sure I didn't already reply to a comment or process a command, often using something like this: def replied(comment): replies = comment.replies replied = False for reply in replies: if reply.author.name == username: return True return False This makes a TON of reddit calls, which results in a TON of delay. Just to see if we already did something! ###Why databases? The issue with my two-factor system is that reddit requests take time and are not very efficient compared to a local look-up. On the other hand, I can have a local database for my 2nd factor which isn't limited in calls by the reddit API and makes use of super efficient search algorithms. ###Installing prerequisites So we need a few things for our python app to use a database. I'm going to be using MySQL because fuck it, why not. So you need these packages installed: sudo apt-get install python-mysqldb sudo apt-get install mysql-server You will need to pick a username for mysql, I use \"root\". I don't like to write raw SQL queries, so I use a wrapper called \"peewee\" for friendly commands. [Peewee documentation](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CCkQFjAA&url=http%3A%2F%2Fpeewee.readthedocs.org%2Fen%2Flatest%2Fpeewee%2Fcookbook.html&ei=Lr9YU9pKhreSBbHjgIgP&usg=AFQjCNF8hSPe9PFvF-PeIt67QobCKT4Eow&sig2=IkJgo1KANGZjd2fZYz9SZw&bvm=bv.65397613,d.dGI) pip install peewee ###Basic set up You will need to create an SQL database, which is very easy. Enter MySQL with: mysql -u root -p mysql> create database redditbot In our example, we want to store submission IDs we have already processed in case our bot crashes and we lose our \"already_done\" list. In peewee, we need to connect to our database and create a \"model\" which represents our submission.id (or whatever we want to store): from peewee import * db = MySQLDatabase('redditbot', host='localhost', user='root', passwd='SQLPASS') class Submissions(Model): subid = TextField() class Meta: database = db db.connect() Submissions.create_table(True) #the True flag won't alarm if table exists Cool, so now we are connected to our database, we have established what a \"submission\" model is, and we created a table of submissions. Now, we can add submissions to our database, search for existing submissions, or even flush the database of all entries. I'll show you those functions here: def is_added(submission_id): #is ID already in database? try: submissions = Submissions.get(Submissions.subid == submission_id) return True except: return False def add_entry(submission_id): #add new entry if not is_added(submission_id): print \"Adding %s\" % submission_id Submissions(subid= submission_id).save() def flush_db(): #remove all entries subs = Submissions.select() for sub in subs: sub.delete_instance() Rather than typing out ANY queries when we want to add things, I have wrapped all of the peewee code in custom python methods. Now, we can simply use my function to add a submission to our database: submissions = reddit.get_new(limit =10) for submission in submissions: add_entry(submission.id) If the entry is already in the table, it won't be added again due to the is_added() method. This means at the \"reddit bot\" layer, we aren't doing any checks, which makes for much cleaner code. ###Integrating our 2-factor check Now we don't want to rely on the DB when we can use already_done *most* of the time, so let's put both together: already_done = [] submissions = reddit.get_new(limit =10) for submission in submissions: if submission.id not in already_done and not is_added(submission.id): #Do whatever to your submission on this line already_done.append(submission.id) add_entry(submission.id) Boom. So we will check already_done for our ID and if it isn't there (because it's new OR we crashed), it will check the database. If it isn't in the database, it will add it. This way, we don't have to do any reddit calls to check if we already did this work. This makes for a MASSIVE performance increase. If you want, you can create one function to handle already_done and your database, like so: def add_done(submission.id): already_done.append(submission.id) add_entry(submission.id) And a method to check if something is already done: def is_done(submission.id): if not submission.id in already_done and not is_added(submission.id): return True So our final code could be: already_done = [] submissions = reddit.get_new(limit =10) for submission in submissions: if not is_done(submission.id): #Do whatever to your submission on this line add_done(submission.id) --- I hope that all makes sense! Let me know if you have any questions. I'm not super experienced with databases, but this simple implementation works for most of my purposes.", "title": "[Guide] Using databases with your reddit bot in python", "url": "https://www.reddit.com/r/redditdev/comments/23ucco/guide_using_databases_with_your_reddit_bot_in/"}, {"author": "jahjaylee", "created_utc": 1398316528, "gilded": 0, "name": "t3_23u4eh", "num_comments": 1, "score": 3, "selftext": "Hey guys, I've been using PRAW for an app I'm developing and I'm trying to get the breakdown of upvotes and downvotes of certain posts. I can easily get the total karma amount but is there any way to get the breakdown?", "title": "Getting the number of upvotes and downvotes of a post", "url": "https://www.reddit.com/r/redditdev/comments/23u4eh/getting_the_number_of_upvotes_and_downvotes_of_a/"}, {"author": "fedo_tip_bot", "created_utc": 1398305987, "gilded": 0, "name": "t3_23tpns", "num_comments": 5, "score": 1, "selftext": "So, I have this stupid bot, and it posts replies to various subreddits but also including a private subreddit. Manual posts show up everywhere, but comments through PRAW seem to be filtered out (it's an approved submitter in that private subreddit, but replies still show up in the moderation queue and spam queue). Outside that private subreddit, none of its replies show up to other users, although they appear when I'm logged into the bot. What am I doing wrong?", "title": "How to get bot out of moderation/spam queue?", "url": "https://www.reddit.com/r/redditdev/comments/23tpns/how_to_get_bot_out_of_moderationspam_queue/"}, {"author": "JackOfCandles", "created_utc": 1398304274, "gilded": 0, "name": "t3_23tmto", "num_comments": 18, "score": 2, "selftext": "I wrote a bot and it seems to miss posts that should trigger it. I'm using the comment stream in the praw library to look at /r/all and it seems to be saying it's at about 25 comments per second. I assume that's what CPS means anyway. Is that normal? Edit: After changing it from /r/all to only 3 smaller subs, I noticed that the comments per second is being reported as 0 most of the time, so it seems that this isn't based on what my bot can handle, but rather that's simply all the comments that are currently currently available at that comment fetch iteration. Still, I'm not sure why it occasionally is missing valid comments. I've got limit set to none, like so: all_comments = praw.helpers.comment_stream(r, SUBREDDITS, limit=None)", "title": "How many comments per second should a bot be processing?", "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "Plague_Bot", "created_utc": 1398031807, "gilded": 0, "name": "t3_23jii6", "num_comments": 1, "score": 3, "selftext": "~~I'm trying to retrieve a list of posts that have recently been archived (6 months old). To do this I planned to use the search function and pass in something like \"6 months ago\" or \"November 2013\" to the [period parameter](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.search). But I have no idea what format it's expecting. Or even if that's its intended use.~~ ~~The [wiki](http://www.reddit.com/wiki/search) that the PRAW documentation links to doesn't even mention the period parameter, so I'm a little short on documentation.~~ Found it in the [Reddit API documentation](http://www.reddit.com/dev/api#GET_search). Should have thought to look there before. Whoops. It only accepts these, if anyone's wondering: one of (hour, day, week, month, year, all) ___ Also, since I am only concerned about the date and not the search term, is it possible to pass in an empty search term? I'm looking to do something like this: results = r.search(\"\", period=\"6 months ago\", limit=None) Thanks in advance.", "title": "[PRAW/Reddit API] Using the period parameter of the search function (plus other stuff).", "url": "https://www.reddit.com/r/redditdev/comments/23jii6/prawreddit_api_using_the_period_parameter_of_the/"}, {"author": "bobdammit", "created_utc": 1397939523, "gilded": 0, "name": "t3_23gl7i", "num_comments": 2, "score": 2, "selftext": "I have been using praw for an api that I have been building and have been using oauth2. I have been having issues with submitting comments with an oauth2 login. When I check to see if is_oauth_session equals true, it always checks out, but when I try to add a comment, using submission.add_comment, I keep getting an error that says I am not logged in with an oauth session. If I do a real login with r.login(username='blah', password='blah'), it works. How am I getting this error, if the boolean checks out from above?", "title": "submitting comments with oauth2", "url": "https://www.reddit.com/r/redditdev/comments/23gl7i/submitting_comments_with_oauth2/"}, {"author": "KnightHawk3", "created_utc": 1397876956, "gilded": 0, "name": "t3_23es5v", "num_comments": 6, "score": 2, "selftext": "I am really new to using PRAW, but I basically want a way of looking up the flair of a username on a specific subreddit. IE: What is the flair of /u/KnightHawk3 on /r/pokemontrades r = praw.Reddit(\"/u/KnightHawk3's IRC bot called Rhythm\") username = \"KnightHawk3\" user = r.get_redditor(username) for comment in user.get_comments(): print(comment.subreddit) if str(comment.subreddit) == \"pokemontrades\": print(\"I have found a comment in the subreddit!\") I can't find the documentation for variables in the comment object (ie: subreddit) Where is the documentation on this, and what is the way of finding the flair? EDIT: Without being a moderator.", "title": "Getting the flair of a comment", "url": "https://www.reddit.com/r/redditdev/comments/23es5v/getting_the_flair_of_a_comment/"}, {"author": "testingpraw", "created_utc": 1397667927, "gilded": 0, "name": "t3_2370hr", "num_comments": 5, "score": 0, "selftext": "I have noticed that reddit-stream.com can get comments in real time. However, when I try to hit the api to see if there are more comments, it seems the api is 30 seconds too slow and is sending me cached json objects. I am logged in when trying to retrieve the json data. I have tried multi-threading, using praw and changing the praw.ini chache time limit, but I haven't had any luck. Is there anyway that I can access a live stream of comments from a submission?", "title": "Comment Stream: Is there anyway to bypass reddit's caching?", "url": "https://www.reddit.com/r/redditdev/comments/2370hr/comment_stream_is_there_anyway_to_bypass_reddits/"}, {"author": "SOTB-human", "created_utc": 1397576944, "gilded": 0, "name": "t3_233n3n", "num_comments": 0, "score": 2, "selftext": "Since PRAW doesn't support setting moderator permissions, I've been posting a request directly via the Reddit API in order to diminish the permissions on moderator invites in /r/modeveryone (an operation known as \"hitlering invites\"). It used to work successfully every time; however, it recently stopped working, producing `[[u'USER_REQUIRED', u'please login to do that', None]]`. Was something changed in the authentication API recently? The change must have occurred between 2014-04-11 06:16:36 UTC (the last success) and 2014-04-12 05:32:15 UTC (the first failure), since I didn't alter or restart the script during that interval. The relevant code is as follows: import praw # etc. r = praw.Reddit(user_agent=USER_AGENT) r.login(USERNAME, PASSWORD) sss = r.http.post(\"http://www.reddit.com/api/login\", {'passwd': PASSWORD, 'rem': True, 'user': USERNAME, 'api_type': 'json'}) sssj = json.loads(sss.text) MODHASH = sssj[\"json\"][\"data\"][\"modhash\"] # etc. def hitlerInvite(username): params = { 'name': username, 'r': 'modeveryone', 'uh': MODHASH, 'api_type': 'json', 'type': 'moderator_invite', 'permissions': '-all,+access,+config,+flair,+mail,+posts,+wiki' } response = r.http.post(\"http://www.reddit.com/api/setpermissions\", params) responsej = json.loads(response.text) if responsej[\"json\"][\"errors\"]: printLog( \"Error in hitlering\", username, \":\", responsej[\"json\"][\"errors\"]) else: printLog( \"Successfully hitlered\", username)", "title": "Did something change in the authentication API recently? I can no longer set moderator permissions through the API.", "url": "https://www.reddit.com/r/redditdev/comments/233n3n/did_something_change_in_the_authentication_api/"}, {"author": "testingpraw", "created_utc": 1397482608, "gilded": 0, "name": "t3_2303hw", "num_comments": 5, "score": 3, "selftext": "I am making a live comment stream, and I have not really seen a direct answer to this on google or the reddit dev search. Is there a way that I can get the login session data, using praw, so the user does not have to re-login to reddit to comment? For example, instead of using a form to populate r.login(), is there anyway that I can use r.login(username=session.reddit_username, password = session.password), or something like that?", "title": "grabbing login session data with praw", "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "Alexratman", "created_utc": 1397175054, "gilded": 0, "name": "t3_22ql4t", "num_comments": 2, "score": 1, "selftext": "I'm trying to develop a Bot to help with reddit raffles; it would pull the comments from the post, extract the body & author, then it would get the authors comment karma. It would then output all of this information into a CSV file. I'm looking at using python and PRAW, however I'm a total newbie to python. I think that doing okay but I cannot get any sort of output from the script; import praw r = praw.Reddit(user_agent='RaffleCommentBot by /u/alexratman') r.login('RaffleCommentBot', '##Password##') id = \"thread_id\" submission = r.get_submission(submission_id = \"id\") submission.reddit_session.config.more_comments_max = 5000 print \"submission get!\" subm.replace_more_comments() comments = submission.flatten_tree(subm.comments) users = set () for idx, comment in enumerate(comments): if type(comment) != praw.objects.MoreComments: users.add(comment.author.name) with open(\"output.csv, \"w\") as output: output.write(\"\\n\".join(users) + \"\\n\") Im basing this off a script a friend used in 2012 so its very outdated. Any help is greatly appreciated!", "title": "Is there a way to pull comments (and additional information) into a CSV", "url": "https://www.reddit.com/r/redditdev/comments/22ql4t/is_there_a_way_to_pull_comments_and_additional/"}, {"author": "Carsonbizotica", "created_utc": 1397158680, "gilded": 0, "name": "t3_22pvic", "num_comments": 2, "score": 1, "selftext": "So all I really want to do is count the comments on a submission. Right now I'm relying on: submission.replace_more_comments(limit=None, threshold=0) flat_comments = praw.helpers.flatten_tree(submission.comments) len(flat_comments) The problem is that I don't care what's *in* the comments, only that they exist. I've been scouring the PRAW Docs and I can't seem to find, or invent, something that would give me access to this number without having to load all the comments. Anyone have any ideas? ***Related Question***: Does a deleted comment return `None` or something special?", "title": "[PRAW] I've got a question about counting comments", "url": "https://www.reddit.com/r/redditdev/comments/22pvic/praw_ive_got_a_question_about_counting_comments/"}, {"author": "d2lp_test2", "created_utc": 1397061578, "gilded": 0, "name": "t3_22m9wv", "num_comments": 2, "score": 1, "selftext": "Here's the loop. http://pastebin.com/vDP7aWsL See here: http://www.reddit.com/r/bottest/comments/22m7hz/h_legacy_keys/ It replied once and then errord out with this exception. I wonder why. I wait 20 secs between each comment and 30 mins before fetching the topics. Edit: Error trace: Traceback (most recent call last): File \"bot.py\", line 27, in submission.add_comment(comment_text) File \"D:\\python3.3\\lib\\site-packages\\praw\\objects.py\", line 902, in add_commen t response = self.reddit_session._add_comment(self.fullname, text) File \"D:\\python3.3\\lib\\site-packages\\praw\\decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"D:\\python3.3\\lib\\site-packages\\praw\\__init__.py\", line 1977, in _add_com ment retry_on_error=False) File \"D:\\python3.3\\lib\\site-packages\\praw\\decorators.py\", line 177, in wrapped raise error_list[0] praw.errors.RateLimitExceeded: `Du versuchst es zu oft. Probier es in 19 Sekunde n nochmal` on field `ratelimit`", "title": "[PRAW] I keep getting RateLimitExceeded Exception and i do not understand why.", "url": "https://www.reddit.com/r/redditdev/comments/22m9wv/praw_i_keep_getting_ratelimitexceeded_exception/"}, {"author": "zynix", "created_utc": 1396883656, "gilded": 0, "name": "t3_22fhy9", "num_comments": 4, "score": 1, "selftext": "I've been working on a project to harvest date time's of every single electronic action I've made for as far back as possible. With Reddit, this is not my only account ( I have 3 active and another 6-7 that go back to late 2006). PRAW seems more ideal for building bots and my second glance through it didn't seem like it had much support for harvesting likes, submissions, and comments. Keep in mind, I just want the datetime stamps and nothing else. Am I mistaken? Would praw be a good fit for this? Alternatively I have a system called ransack that I used to harvest my Google search history ( 80K data points ) via running a managed browser instance ( Python owns the browser and can inject user input/js at will while also intercepting/collecting SSL protected pages ).", "title": "PRAW or just do direct harvesting for collecting metadata about myself?", "url": "https://www.reddit.com/r/redditdev/comments/22fhy9/praw_or_just_do_direct_harvesting_for_collecting/"}, {"author": "AlmostARockstar", "created_utc": 1396811402, "gilded": 0, "name": "t3_22d3hj", "num_comments": 0, "score": 1, "selftext": "Writing a new bot and I have been notified by praw that a new version is available. I decided to upgrade it with easy_install as it says in the title but since then I've been getting the following error: C:\\Python27\\lib\\pkgutil.py:186: ImportWarning: Not importing directory 'C:\\Python27\\lib\\site-packages\\mpl_toolkits': missing __init__.py file, filename, etc = imp.find_module(subname, path) C:\\Python27\\lib\\pkgutil.py:186: ImportWarning: Not importing directory 'c:\\python27\\lib\\site-packages\\mpl_toolkits': missing __init__.py file, filename, etc = imp.find_module(subname, path) Any thoughts?", "title": "[PRAW] Getting ImportWarning after easy_install --upgrade praw ... Any body else had this issue?", "url": "https://www.reddit.com/r/redditdev/comments/22d3hj/praw_getting_importwarning_after_easy_install/"}, {"author": "Codyd51", "created_utc": 1396803407, "gilded": 0, "name": "t3_22crls", "num_comments": 2, "score": 3, "selftext": "I'm really new to the Reddit API and PRAW..how can I use PRAW to check for new mail? Or is there a better way to do it? Thanks", "title": "[PRAW] Check for new mail?", "url": "https://www.reddit.com/r/redditdev/comments/22crls/praw_check_for_new_mail/"}, {"author": "shrayas", "created_utc": 1396790710, "gilded": 0, "name": "t3_22ccgn", "num_comments": 7, "score": 2, "selftext": "I moderate over at the /r/bengalurufc subreddit and wanted a way to update the sidebar at regular intervals as the standings change. So today i learnt how to update the sidebar via a python script using PRAW. Thought i'd share the knowledge here. [Here](https://gist.github.com/shrayasr/10005943) is the GIST. Feedback is appreciated. Thanks!", "title": "Wrote a small Gist on how to update a subreddit's sidebar contents.", "url": "https://www.reddit.com/r/redditdev/comments/22ccgn/wrote_a_small_gist_on_how_to_update_a_subreddits/"}, {"author": "205", "created_utc": 1396731705, "gilded": 0, "name": "t3_22ap4k", "num_comments": 4, "score": 3, "selftext": "As the title says, how would I go about pulling the author of the parent comment with praw. Thanks.", "title": "Author of parent comment", "url": "https://www.reddit.com/r/redditdev/comments/22ap4k/author_of_parent_comment/"}, {"author": "redditTopics", "created_utc": 1396579129, "gilded": 0, "name": "t3_225rhj", "num_comments": 4, "score": 5, "selftext": "Is there any way of getting old Reddit content (submissions/comments) from the past year or more, or at least the last few months, via the API? I'm currently using praw, but I can't seem to find any functionality in praw or in the reddit API itself that allows you to access submissions or subreddits by date. Would Reddit provide a dump for academic research purposes? Are there other sites that would do the same? (I checked out and have contacted Gnip and DataSift.) I want to avoid scraping but if it comes down to that, how far back can you go just by following links? Would I have to resort to the wayback machine? Any help is appreciated. Thanks!", "title": "How can I get historical Reddit data?", "url": "https://www.reddit.com/r/redditdev/comments/225rhj/how_can_i_get_historical_reddit_data/"}, {"author": "argutus1", "created_utc": 1396454967, "gilded": 0, "name": "t3_220tbz", "num_comments": 2, "score": 1, "selftext": "I have written some code with Praw that uploads (overwrites) some already existing images in the stylesheet. This image is coded into the sidebar. The problem that I have is if I upload the the new image to the stylesheet, it won't be updated in the sidebar until I save the stylesheet. I had tried a method to fix this by getting the current stylesheet and saving it, and then updating the stylesheet, but I would get an error. Does anyone know a fix?", "title": "Update Image in Sidebar", "url": "https://www.reddit.com/r/redditdev/comments/220tbz/update_image_in_sidebar/"}, {"author": "JBHUTT09", "created_utc": 1396452311, "gilded": 0, "name": "t3_220p0f", "num_comments": 2, "score": 1, "selftext": "###Solved: Was a string issue. The escape character '\\\\' does not work on '%'. '%%' is required, instead. I wrote a bot a few months ago to help moderate /r/animesuggest, by checking post flair and taking certain actions if there is no flair on a post. I used a for loop and the get_new_by_date function to go through the most recent 20 posts in the subreddit: for submission in subreddit.get_new_by_date(limit=20): And it worked perfectly. But when I run the code it would give me a deprecation warning, telling me to use get_new instead. I brushed it off because it worked. But I'm now revamping the bot to address a few bugs, and I'd like to fix this, too. But I can't, for the life of me, figure out how to replace get_new_by_date with get_new. I thought it would be a simple matter of just changing the code to: for submission in subreddit.get_new(limit=20): But that doesn't seem to work. I've searched around and all I've been able to find are threads where people recommend using get_new_by_date instead of get_new. I've read the documentation, too, but I just can't seem to figure it out. Can anyone help me with this? **Edit:** I've dug around some more and I noticed that the [documentation entry for get_new](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Subreddit.get_new) is identical to the entry for get_new_by_date (directly below). Doesn't this mean I really should be able to simply replace get_new_by_date with get_new?", "title": "Question about PRAW's get_new_by_date vs get_new", "url": "https://www.reddit.com/r/redditdev/comments/220p0f/question_about_praws_get_new_by_date_vs_get_new/"}, {"author": "vafrederico", "created_utc": 1396049796, "gilded": 0, "name": "t3_21mtuq", "num_comments": 4, "score": 1, "selftext": "Hello there, I'm currently building a few bots with a friend of mine. Since we don't plan to pay reddit gold for each bot, so that it would be possible to work with mentions, we were thinking of having a comment scrapper which would redirect each comment to it's respective bot, so that we could reduce our number of requests to /all/comments. We are using PRAW and basically, we have our parser, `RedditParser`, that adds each matched comment to a `Bot` class `Queue`, each bot class extends a `MetaBot` class, which extends `threading.Thread`. I'm trying to use comment.reply but it says that there is no authenticated reddit session, even though at the `__init__` of every bot, there is a self.r.login. I already tried to edit `comment.reddit_session` to `self.r`, but still no success. From PRAW documentation i thought i would only need to extend `Thread`, and create a `self.r = praw.Reddit` for each Bot. But it does not work. Any suggestions? EDIT: Managed to do this by re-fetching the comment on each Bot as suggested by /u/SOTB-human using `self.r.get_submission(url=permalink).comments[0]`. From the PRAW documentation, if you use `r.get_submission` with a comment permalink, it will return the submission, but the first comment will be the one belonging to the permalink.", "title": "PRAW + Multiple Bots + Multiple Threads", "url": "https://www.reddit.com/r/redditdev/comments/21mtuq/praw_multiple_bots_multiple_threads/"}, {"author": "labtec901", "created_utc": 1395930836, "gilded": 0, "name": "t3_21i9iz", "num_comments": 2, "score": 0, "selftext": "I'm trying to create a version of _Daimon_'s tutorial reddit bot: http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html But instead of scraping the self post texts of a subreddit, I'd like to scrape all the new comments in a subreddit. I've tried writing my own script to do this, but I don't think I'm even close to on the right track. Can you help?", "title": "Trying to write something simple, and it's devolved into a spaghetti code nightmare and I need to start over. Can you help?", "url": "https://www.reddit.com/r/redditdev/comments/21i9iz/trying_to_write_something_simple_and_its_devolved/"}, {"author": "glider_integral", "created_utc": 1395720634, "gilded": 0, "name": "t3_21ann0", "num_comments": 2, "score": 2, "selftext": "Apparently I can't get my own flair in a subreddit (although this code in particular is to get anyone's flair in a subreddit). import praw r = praw.Reddit(user_agent='python_wrapper') flair = r.get_flair('subreddit', 'user') I hope I don't need to get mod privileges to know my own flair.", "title": "PRAW get my flair in a particular subreddit.", "url": "https://www.reddit.com/r/redditdev/comments/21ann0/praw_get_my_flair_in_a_particular_subreddit/"}, {"author": "nissoPT", "created_utc": 1395638090, "gilded": 0, "name": "t3_217ivx", "num_comments": 4, "score": 3, "selftext": "I installed praw (i guess) but when i try to import it in IDLE i get this message: import praw Traceback (most recent call last): File \"\", line 1, in import praw File \"/Library/Python/2.7/site-packages/praw-2.1.14-py2.7.egg/praw/__init__.py\", line 43, in from update_checker import update_check File \"build/bdist.macosx-10.9-intel/egg/update_checker.py\", line 15, in ImportError: No module named pkg_resources >>> can someone help me?", "title": "I'm a noob that can't seem to be able to install praw", "url": "https://www.reddit.com/r/redditdev/comments/217ivx/im_a_noob_that_cant_seem_to_be_able_to_install/"}, {"author": "makeupdev", "created_utc": 1395518370, "gilded": 0, "name": "t3_213ht6", "num_comments": 7, "score": 2, "selftext": "I'm doing a project for a data science class and I want to analyze a bunch of submissions to /r/makeupaddiction. I'm trying to write a scraper to grab the last 3000 (or more) posts with a certain tag. (It will only be run once, to collect the data!) I'm using PRAW. Just running a search gives me ~500 results. My query looks like: start = datetime.today() end = datetime.today() - timedelta(days=5) for x in range(0,50): postperiod = \"timestamp:%s..%s\" % (start.strftime(\"%s\"),end.strftime(\"%s\")) query = scraper.search('FOTD', 'MakeupAddiction','new', None, postperiod) self.do_query(query) start = start - timedelta(days=5) end = end - timedelta(days=5) time.sleep(2) The do_query() method does the processing of each query's results (filtering and saving to a csv file). The time.sleep(2) is to comply with reddit's API etiquette of not overloading their server although I'm not sure how necessary that is. Anyway, it just returns the same time period worth of results (the very newest) over and over. I'm pretty sure I'm following PRAW's [API for search](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Subreddit.search): search(query, subreddit=None, sort=None, syntax=None, period=None, *args, **kwargs) Not sure what I'm doing wrong. Going back in discrete time periods was my best idea to grab lots of records but let me know if there's a better way. Thanks for any insight! :(", "title": "Want to grab last ~3000 records", "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "Harakou", "created_utc": 1395366731, "gilded": 0, "name": "t3_20ykci", "num_comments": 2, "score": 3, "selftext": "Hey guys, I'm a bit inexperienced with Praw and Python in general, a little baffled with the trouble I'm having here. I'm writing a bot, and I want it to send my account a message when fails to fetch a page. However, when I pass my account's username and the message to the send_message function, the logged in account sends the message to itself, with the username as the message subject. Dummy script to show what I'm doing. (This replicates the problem in the Python shell as well.) bot = praw.Reddit(user_agent=\"user agent stuff\") bot.login(botUserName) bot.user.send_message('Harakou', \"Generic error report\") This seems to be in line with the usage of send_message in the documentation as well as the Getting Started tutorial, so I'm a bit confused. I've also tried specifying the arguments, but then Python complains that it's being supplied multiple values of 'recipient.' bot.user.send_message(recipient='Harakou', message=\"Generic error report\") I'm assuming this is a case of me doing something silly, but I'm not sure what. Any advice?", "title": "Having some trouble with PRAW send_message?", "url": "https://www.reddit.com/r/redditdev/comments/20ykci/having_some_trouble_with_praw_send_message/"}, {"author": "editer63", "created_utc": 1395180566, "gilded": 0, "name": "t3_20r8nb", "num_comments": 4, "score": 3, "selftext": "Hi folks. I'm a grad student working on a project that involves downloading posts and comments from a couple of Reddit subs, and I'm a complete noob at Python and so on. I'm trying to self-teach, but it's slow going, and I need a boost. So if anyone in Austin can meet me and show me how to do the stuff I need to do (or explain that it can't be done, though I'm confident it can) in PRAW and how to manage the output so it's in a form I can use, I'd be happy to pay for a lesson or two. (My project isn't \"about\" scraping Reddit data, my adviser doesn't care how I get the stuff off the site, so this isn't a \"do my homework for me\" request.) Thanks!", "title": "Any PRAW wizards in Austin?", "url": "https://www.reddit.com/r/redditdev/comments/20r8nb/any_praw_wizards_in_austin/"}, {"author": "nnnannn", "created_utc": 1394724037, "gilded": 0, "name": "t3_20bmac", "num_comments": 3, "score": 4, "selftext": "I was wondering if it was possible to coordinate RES and PRAW so that I could do things like: * Filter out a sub from /r/all if posts with a certain keyword began to dominate the sub, then erase the filter when those posts subsided. * Search a sub for regular submitters from the past month, then tag those users as such. How would I go about doing this? Can I post to #!settings/userTagger through PRAW or Requests? What kind of query would I have to make? Apologies if this is impractical/impossible.", "title": "Coordinating RES and PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/20bmac/coordinating_res_and_praw/"}, {"author": "Actualacc", "created_utc": 1394678917, "gilded": 0, "name": "t3_20aehd", "num_comments": 6, "score": 2, "selftext": "import time import praw # -*- coding: utf-8 -*- #i have no idea what this does r = praw.Reddit('') r.login('actualacc','notmypassword') already_done = [] prawWords = [u'\u00af\\_(\u30c4)_/\u00af',u'\u00af\\\\_(\u30c4)_/\u00af',] while True: subreddit = r.get_subreddit('all') all_comments = r.get_comments('all') for comment in all_comments: has_praw = any(face in comment.body for face in prawWords) if comment.id not in already_done and has_praw: msg = '[Misplaced Smiley](%s)' % comment.permalink print(msg) r.send_message('actualacc','smiley', msg) already_done.append(comment.id) print(comment.body) time.sleep(1) For some reason, this code does not detect anything and I receive no messages. It works fine if I set the prawWords to something that is just a string of letters. What should I change? And I am aware that I won't see the message as a \"new message\".", "title": "Trying to get notified when a certain string (\"\u00af\\_(\u30c4)_/\u00af\") is used in a comment.", "url": "https://www.reddit.com/r/redditdev/comments/20aehd/trying_to_get_notified_when_a_certain_string_\u30c4_is/"}, {"author": "brucemo", "created_utc": 1394675719, "gilded": 0, "name": "t3_20a9hl", "num_comments": 1, "score": 2, "selftext": "I am sorry that I am posting this question again, but my project is blocked until I get an answer, and while I appreciate the answer I got [last time,](http://www.reddit.com/r/redditdev/comments/1zhcrq/enumerating_the_ban_list/) it was not one that I could use. Can someone please tell me how to use PRAW to get get successive pages of the ban list?", "title": "How to iterate the ban list with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/20a9hl/how_to_iterate_the_ban_list_with_praw/"}, {"author": "DarkMio", "created_utc": 1394527304, "gilded": 0, "name": "t3_204i97", "num_comments": 7, "score": 2, "selftext": "Hi, I am learning python right now and as a training to my learning-process (and something to work on where I am really interested in), I am writing a bot with some NLP (natural language pattern) analyzing. The biggest problem I am facing over and over again are [HTTPErrors - like in this screenshot.](http://i.imgur.com/N4XbO5m.png) My bot is atm only listening to threads in /r/dota2 to get some valuable data. # -*- coding: utf-8 -*- ### THIS BOT RUNS ON /r/dota2 FOR NOW. ### import time import praw from unidecode import unidecode from datetime import datetime r = praw.Reddit('Getting valuable wordlists, from /u/DarkMio') r.login() already_done = [] print \"You're sucessfully logged in.\" wordlist = open('wordlist.txt').readlines() lolWords = map(lambda it: it.strip(), wordlist) while True: subreddit = r.get_subreddit('dota2') for submission in subreddit.get_new(limit=15): # make it lowercase for now op_text = submission.selftext.lower() has_lol = any(string in op_text for string in lolWords) # test if it contains LoL content if submission.id not in already_done and has_lol: sub_title = unidecode(submission.title) msg = 'Maybe LoL content: [%s](%s)' % (sub_title, submission.short_link) print \"[%s] Sending of following thread '%s' to DotoBot.\" % (datetime.now().time(), sub_title) r.user.send_message('DotoBot', msg) with open('reddit.txt', 'a') as textfile: content = '\\n\\nThread: %s\\n\\nContent:\\n%s' % ( sub_title, unidecode(submission.selftext)) textfile.write(content) already_done.append(submission.id) time.sleep(120) The solution of the problem is something I can't wrap around, since I don't know how to catch those exceptions specificly. This is due to my inexpirience with programming and python in general. My try would've been: try: except: except: But how do I catch those errors (since I want / need a bot that is as verbose as possible) and return to it? I have tried [this before] (http://www.reddit.com/r/redditdev/comments/1dmdcz/praw_and_httperror_handling/) which somehow ended my while-loop.", "title": "Learning python - can't deal with HTTPErrors.", "url": "https://www.reddit.com/r/redditdev/comments/204i97/learning_python_cant_deal_with_httperrors/"}, {"author": "bassguitarman", "created_utc": 1394489710, "gilded": 0, "name": "t3_2032ul", "num_comments": 5, "score": 2, "selftext": "I made this bot to annoy my friend and I can't get it to work. import time import praw from random import randint r=praw.Reddit(\"Annoybot\") r.login(\"annoysterninator\",\"secret\") message = (\"anything\") comment1 = \"lick\" comment2 = \"I found the vegan!\" comment3 = \"DdddddddDrop the bass\" comment4 = \"twerk eth twerk eth twerk\" comment5 = \"what nice leg muscles you have\" comment6 = \"The qua-a-adratic formula\" comment7 = \"it is ethjck twerk music swag money\" already_seen = set() while True: user = r.get_redditor(\"bassguitarman\") username = \"same as above\" comments = user.get_comments(limit=2) sub_comments = r.get_comments(\"all\") for comment in sub_comments: if comment.author.name == username and comment.id not in already_seen: comment.reply (\"Dudley!\") already_seen.add (comment.id) for comment in comments: reply = randint(1,7) if reply == 1: comment.reply (comment1) if reply == 2: comment.reply (comment2) if reply == 3: comment.reply (comment3) if reply == 4: comment.reply (comment4) if reply == 5: comment.reply (comment5) if reply == 6: comment.reply (comment6) if reply == 7: comment.reply (comment7) already_seen.add (comment.id)", "title": "What is wrong with this bot?", "url": "https://www.reddit.com/r/redditdev/comments/2032ul/what_is_wrong_with_this_bot/"}, {"author": "myessail", "created_utc": 1394464492, "gilded": 0, "name": "t3_201vo4", "num_comments": 2, "score": 2, "selftext": "# get onboard data, establish connection username = open(source_dir + \"username.txt\", \"r\").read().rstrip() password = open(source_dir + \"password.txt\", \"r\").read().rstrip() user_agent = (\"praw script which comments with information about pokemon. by Michael Yessaillian\") reddit = praw.Reddit(user_agent = user_agent) reddit.login(username = username, password = password) already_done = set() # keeps track of work already done #get data from comments, respond with information subreddit = reddit.get_subreddit('pokemon') subreddit_comments = subreddit.get_comments() for comment in subreddit_comments: # check for question if \"_pokedexbot\" in comment.body and comment.id not in already_done: comment.reply(\"working!\") already_done.add(comment.id) # adds to work done This is part of a bot I'm writing, modifying some code from [here](http://praw.readthedocs.org/en/latest/pages/comment_parsing.html). In theory, I think it should be replying \"working!\" to a comment that contains the bot's username, but when I run it, nothing happens. Do I have this set up correctly? The full project can be seen on [my github](https://github.com/myessail/pokedexbot).", "title": "[PRAW] Having trouble with comment parsing.", "url": "https://www.reddit.com/r/redditdev/comments/201vo4/praw_having_trouble_with_comment_parsing/"}, {"author": "ravik78c", "created_utc": 1394239020, "gilded": 0, "name": "t3_1zurpt", "num_comments": 2, "score": 3, "selftext": "getting the error as shown below while using praw to submit more than one links to a subreddit in my local reddit instance. (QUOTA FILLED) `You've submitted too many links recently. Please try again in an hour.` on field `None`. What to do?", "title": "[Praw] : Facing quota error while submit more than one links one after the other,", "url": "https://www.reddit.com/r/redditdev/comments/1zurpt/praw_facing_quota_error_while_submit_more_than/"}, {"author": "ravik78c", "created_utc": 1394159864, "gilded": 0, "name": "t3_1zrysj", "num_comments": 6, "score": 2, "selftext": "I used praw to submit a link on a subreddit, but when I tried to add one more it asked me to do it after an hour. But when I do the same thing using the reddit site, It allows me to add as many links as I want. Does praw inhibits us to add more than one link per hour per user or is it checked by reddit api?", "title": "[PRAW] How many submissions can be made using praw at a time?", "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "ravik78c", "created_utc": 1394153215, "gilded": 0, "name": "t3_1zroo7", "num_comments": 2, "score": 6, "selftext": "Does anybody know how to configure praw to be used on the local reddit instance? I have my instance installed on my webserver and I can get the data which do not require loggin in but when I try to log-in it fails and throws error. Can you help me to run the praw api on my local instance?", "title": "[PRAW] Using praw for local reddit instance", "url": "https://www.reddit.com/r/redditdev/comments/1zroo7/praw_using_praw_for_local_reddit_instance/"}, {"author": "Abe_Linkin", "created_utc": 1393961392, "gilded": 0, "name": "t3_1zk07r", "num_comments": 9, "score": 6, "selftext": "I'm messing around with PRAW and was looking for a way to print if a tag is NSFW if it has been tagged that way, or if a subreddit is 18+. Any way to accomplish this? All I can find is how to mark a post NSFW", "title": "[PRAW] Does Praw allow you to see if a post is tagged as NSFW?", "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "brucemo", "created_utc": 1393892037, "gilded": 0, "name": "t3_1zhcrq", "num_comments": 2, "score": 5, "selftext": "The following code prints \"kind\" and \"data\", those two text strings, that's all: import praw rh = praw.Reddit(\"Some descriptive name for this\") rh.login() sr = rh.get_subreddit(\"sub I'm a mod of\") for o in sr.get_banned(): print(o) If I examine \"o\" more closely, I can replace the \"for o in\" stuff with: it = sr.get_banned() for o in it['data']['children']: print(o) This gives me the first page of banned people, with no obvious clue about how to get more. Sticking the \"limit\" keyword in the obvious place doesn't work. I can't find examples of use of this call anywhere online. How do I more sensibly iterate this? This is obviously a category of object that I don't understand.", "title": "Enumerating the ban list", "url": "https://www.reddit.com/r/redditdev/comments/1zhcrq/enumerating_the_ban_list/"}, {"author": "TheTretheway", "created_utc": 1393881158, "gilded": 0, "name": "t3_1zgt0d", "num_comments": 3, "score": 2, "selftext": "I'm using PRAW to go through the comments on a particular submission, as per the example. I would like to make sure that, once it's been restarted and the *already_done* set cleared, it only responds to comments that were posted within the last x minutes. *comment.author.name* gets the name of the author, is there a way of doing time? Thanks!", "title": "Basic PRAW Question: Getting comment posted time", "url": "https://www.reddit.com/r/redditdev/comments/1zgt0d/basic_praw_question_getting_comment_posted_time/"}, {"author": "unigee", "created_utc": 1393274329, "gilded": 0, "name": "t3_1ytpmx", "num_comments": 4, "score": 2, "selftext": "Is there something wrong with the latest release of PRAW? I just upgraded and when I attempt to import praw I now get this error: http://i.imgur.com/fWNEDo5.png Any ideas?", "title": "Latest PRAW not working?", "url": "https://www.reddit.com/r/redditdev/comments/1ytpmx/latest_praw_not_working/"}, {"author": "chancrescolex", "created_utc": 1393095436, "gilded": 0, "name": "t3_1yn3ae", "num_comments": 5, "score": 4, "selftext": "So let's say I have a bot. This bot replies to comments that contain a trigger. If the bot tries to reply to a comment but is banned from that sub, it dies with the following error: Traceback (most recent call last): File \"Documents/Python3/bot.py\", line 46, in comment.reply(trigger_reply % (comment.author, parent_author)) File \"/usr/local/lib/python3.3/site-packages/praw/objects.py\", line 336, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"/usr/local/lib/python3.3/site-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python3.3/site-packages/praw/__init__.py\", line 1955, in _add_comment retry_on_error=False) File \"/usr/local/lib/python3.3/site-packages/praw/decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python3.3/site-packages/praw/__init__.py\", line 492, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.3/site-packages/praw/__init__.py\", line 362, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python3.3/site-packages/praw/internal.py\", line 173, in _raise_response_exceptions response.raise_for_status() File \"/usr/local/lib/python3.3/site-packages/requests/models.py\", line 773, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 403 Client Error: Forbidden So is there a way to either make it not crash and burn, or to check if it's banned from the sub the comment is in and not attempt to comment?", "title": "[PRAW] how to keep bot from crashing when it fails to post a comment reply due to being banned from a particular sub?", "url": "https://www.reddit.com/r/redditdev/comments/1yn3ae/praw_how_to_keep_bot_from_crashing_when_it_fails/"}, {"author": "argutus1", "created_utc": 1393035945, "gilded": 0, "name": "t3_1ylb52", "num_comments": 7, "score": 2, "selftext": "First of all, I understand that my code is incorrect, .lower() cannot be used on another function and 'or' cannot be used in this manner, I'm just confused as to how I can go about fixing it. import praw r = praw.Reddit( user_agent = \"VertcoinBot v1.0: A bot to automate several activities on Vertcoin Subreddits\" ) r.login('...','...') submission = r.get_submission(submission_id='1wulry') post_comments = submission.comments already_done = set() trade_count = 0 conf_count = 0 uneven = 0 for comment in post_comments: if \"bought\" in comment.body.lower() or \"sold\" in comment.body.lower() and comment.id not in already_done: trade_count = str(comment.body).lower().count(\"bought\") + str(comment.body).lower().count(\"sold\") conf_count = str(comment.replies).lower().count(\"confirmed\") if trade_count != conf_count: uneven = 1 if 1 = 25: r.get_subreddit('vertmarket').set_flair(comment.author, '', 'Gold') #Gold CSS Flair if uneven == 0: comment.reply('Your flair has been updated.') print \"Updated user flair.\" else: comment.reply('Your flair has been updated, however some of your trades have not been confirmed, in the meantime your flair will not reflect these trades.') print \"Updated user flair with unconfirmed trades.\" already_done.add(comment.id) The specific error I get is: > Line 12: > for (\"bought\", \"sold\") in comment.body.lower(): > SyntaxError: can't assign to literal", "title": "Searching comments for two words regardless of case", "url": "https://www.reddit.com/r/redditdev/comments/1ylb52/searching_comments_for_two_words_regardless_of/"}, {"author": "chancrescolex", "created_utc": 1392926949, "gilded": 0, "name": "t3_1ygwc7", "num_comments": 7, "score": 1, "selftext": "I'm a total noob. I'm trying to use the example bot [here](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) just to play around with making a reddit bot and when it tries to log in I get the following error: Warning (from warnings module): File \"C:\\Python33\\lib\\site-packages\\requests\\packages\\urllib3\\connection.py\", line 99 HTTPConnection.__init__(self, host, port, strict, timeout, source_address) DeprecationWarning: the 'strict' argument isn't supported anymore; http.client now always assumes HTTP/1.x compliant servers. After searching, the only advice I could find was to update requests, which I did, and I still get the same error. Any thoughts?", "title": "Requests DeprecationWarning when trying to log in with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "sf_spoileralert", "created_utc": 1392751308, "gilded": 0, "name": "t3_1y9lu7", "num_comments": 2, "score": 1, "selftext": "Hey everyone, I'm writing my first crawler and I've got a question about replace_more_comments(). I understand the rate limits and rules surrounding API calls to /morechildren, however I'm looking for a way to speed up my script. Currently it's taking me almost 25 minutes to pull down a popular submission comments alone (~4500 comments). Would this rate improve if I was authenticated? I'm a little confused from the PRAW docs how to go about that, so if anyone has any tips, I'd really appreciate it. Obviously I'm not looking to circumvent the rules or anything, I just need some advice on how to optimize my script. Thanks!", "title": "[PRAW] replace_more_comments() rate limiting", "url": "https://www.reddit.com/r/redditdev/comments/1y9lu7/praw_replace_more_comments_rate_limiting/"}, {"author": "bobbonew", "created_utc": 1392749291, "gilded": 0, "name": "t3_1y9hzq", "num_comments": 1, "score": 2, "selftext": "The information returned for certain API calls is *extremely* large. For example the **/subreddits/mine/** call is absolutely gigantic, when my use case is to only obtain the subreddit name and its ID. How can I limit the information returned? The only parameter I can see accomplishing this is 'show' - but the information listed on it is extremely limited: >**show** - optional parameter; if all is passed, filters such as \"hide links that I have voted on\" will be disabled. Can anybody point me in the right direction? (Note: I am not using PRAW for this.)", "title": "How can I limit the information returned in an API call?", "url": "https://www.reddit.com/r/redditdev/comments/1y9hzq/how_can_i_limit_the_information_returned_in_an/"}, {"author": "MetricPleaseBot", "created_utc": 1392724226, "gilded": 0, "name": "t3_1y8gt3", "num_comments": 3, "score": 0, "selftext": "I'm a bot being written by /u/Bur_Sangjun and I need to delete private messages if they don't match a certain criteria (don't match the format I will accept (/u/MetricPleaseBot ### U > U, where # is an integer and U is a unit). I can not for the life of my find a way to delete private messages through praw.", "title": "What is the proper way to delete a message?", "url": "https://www.reddit.com/r/redditdev/comments/1y8gt3/what_is_the_proper_way_to_delete_a_message/"}, {"author": "jonathan_morgan", "created_utc": 1392698798, "gilded": 0, "name": "t3_1y7pq7", "num_comments": 2, "score": 2, "selftext": "I am trying to use praw and python to retrieve all comments for a set of posts so I can build and compare comment cascades. I am testing on a post whose num_comments value is 22,014: * [http://www.reddit.com/r/AskReddit/comments/1bvkol/what_was_your_biggest_holy_shit_why_havent_i_done/](http://www.reddit.com/r/AskReddit/comments/1bvkol/what_was_your_biggest_holy_shit_why_havent_i_done/) I do the following: # make praw instance import praw r = praw.Reddit( user_agent = \"reddit comment collector with PRAW, v0.1 by /u/jonathan_morgan\" ) # log in - omitting actual values r.login( my_username, my_password ) # get post post_id = \"1bvkol\" post = r.get_submission( submission_id = post_id ) # use the replace_more_comments() method to pull in as many comments as possible. post.replace_more_comments( limit = None, threshold = 0 ) # flatten flat_comments = praw.helpers.flatten_tree( post.comments ) # count comments print( \"==> after flatten_tree(), comment count: \" + str ( len( flat_comments ) ) ) On repeated invocations, the count printed at the end has been either 12,970 or 13,364, neither is close to 22,014. Am I doing something wrong here? Is this possibly a limitation of the API, or because I am not a gold user? I've found that the \"num_comments\" count on the post doesn't always match the actual number of comments, but I've never seen it this far off. And, on posts with fewer comments, it seems to work just fine. For example, the following has 115 comments: * [http://www.reddit.com/r/programming/comments/1cp0i3/clang_c11_support_is_now_feature_complete/](http://www.reddit.com/r/programming/comments/1cp0i3/clang_c11_support_is_now_feature_complete/) Should I be expanding the \"More Comments\" by hand, as in this post: [http://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/](http://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/), instead of calling replace_more_comments()? A follow-on - what order is the comments list in? I did a quick loop over the 1st 15 things in \"flat_comments\" in a couple of different posts and outputted the created_utc value, converted to a readable date, and the comments are not in date order. Is there a parameter I can pass to comments() to tell it to sort oldest-first? If I can't get all comments, for whatever reason, I'd like to get as many comments as I can, earliest first, in order, so I am looking at the complete tree up to a given point in time, rather than having most recent comments.and not having the beginning of the cascade. Any help or advice will be greatly appreciated. Thanks, Jonathan Morgan", "title": "help with retrieving all comments for a given post with praw/python", "url": "https://www.reddit.com/r/redditdev/comments/1y7pq7/help_with_retrieving_all_comments_for_a_given/"}, {"author": "timotab", "created_utc": 1392659645, "gilded": 0, "name": "t3_1y5t6e", "num_comments": 5, "score": 0, "selftext": "While I'm not new to scripting (Unix sysadmin for 20+ years), I'm still relatively new to python. I have python 3.3.3 with praw installed. I'm looking to get the current ~~CSS code~~ sidebar, make a change, and submit that change. The full details of what I'm trying to do is in my [/r/requestabot post](http://www.reddit.com/r/RequestABot/comments/1xmj7q/need_a_bot_to_post_distinguish_then_update/), but I didn't get much feedback there. Edit: I'm stupid. I meant the sidebar, not the stylesheet", "title": "need help getting/updating CSS", "url": "https://www.reddit.com/r/redditdev/comments/1y5t6e/need_help_gettingupdating_css/"}, {"author": "acini", "created_utc": 1392656216, "gilded": 0, "name": "t3_1y5o0i", "num_comments": 2, "score": 3, "selftext": "I am seeing many users [abusing the bots](http://www.reddit.com/r/bestof/comments/1y5ga3/ufiguratively_hilter_gets_reddit_bots_stuck_in_an/) around different subs. So here are some [ready-made functions](https://github.com/acini/praw-antiabuse-functions) to prevent abuse of your bot/s. 1. `is_summon_chain(post)`: detects grandparent comment being bot's own. 1. `comment_limit_reached(post)`: stores comment counts per submission in memory (One should also maintain banned_users and banned_subreddits in a list object.) If you modify/add to code, please send a pull request on github. Happy coding!", "title": "Anti-abuse functions for new botmasters using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1y5o0i/antiabuse_functions_for_new_botmasters_using_praw/"}, {"author": "MrBonez", "created_utc": 1392627147, "gilded": 0, "name": "t3_1y4t0j", "num_comments": 2, "score": 1, "selftext": "I'm following [this tutorial](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) and seem to have run into a bit of an issue where it keeps replying to the same post. If someone could give me a bit of help I'd really appreciate it. import praw import time r = praw.Reddit('Auto replier' 'work in progress') r.login('username', 'password') already_done = [] HotWords = ['Word 1', 'Word 2', 'Word 3', 'Word 4'] while True: subreddit = r.get_subreddit('subreddit') for submission in subreddit.get_new(limit=10): op_title = submission.title.lower() has_hotword = any(string in op_title for string in HotWords) if submission.id not in already_done and has_hotword: print ('Found one...') msg = 'Lorem ipsum dolor sit amet...' submission.add_comment(msg) already_done.append(submission.id) time.sleep(60)", "title": "[PRAW] Spamming issue", "url": "https://www.reddit.com/r/redditdev/comments/1y4t0j/praw_spamming_issue/"}, {"author": "globalglasnost", "created_utc": 1392510185, "gilded": 0, "name": "t3_1y0t1v", "num_comments": 4, "score": 4, "selftext": "**UPDATE The answer I was looking for is MoreComments.comments(update=True), which returns an object of type list (more Comments or MoreComments) or an object of type NoneType, which I think is a deleted comment? Either way I'm able to recursively traverse and grab each Comment object now thanks for everyone's help!** Using PRAW in Python...the code below loads all the comments and counts what is loaded on the fly. The two /r/politics URLs I am loading each have over 1000 in reality, however the code is only loading 200-300? Do I need to collapse a MoreComment object down further? If so I did a dir(praw.objects.MoreComment) and I am kinda lost as how to proceed, I see a comment method but I can't call it? Thanks! **CODE** import praw r=praw.Reddit(user_agent='user_agent_example') iCount = 0 # submission = r.get_submission('http://www.reddit.com/r/politics/comments/1xzbcl/on_february_11the_day_we_fight_backour/') submission = r.get_submission('http://www.reddit.com/r/politics/comments/1ryfk0/americans_want_congress_members_to_pee_in_cups_to/') flat_comments = praw.helpers.flatten_tree(submission.comments(limit=none)) for comment in flat_comments: iCount = iCount + 1 if isinstance(comment, praw.objects.Comment): # code here print(iCount) **dir(praw.objects.MoreComment) returns:** > ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_get_json_dict', '_populate', '_update_submission', 'comments', 'from_api_response', 'fullname']", "title": "I want to load *all* the comments from a specific URL, however, I'm not sure how to load from MoreComment objects?", "url": "https://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/"}, {"author": "mattster42", "created_utc": 1392257819, "gilded": 0, "name": "t3_1xrkhi", "num_comments": 2, "score": 2, "selftext": "So I've been picking up PRAW and Python over the past week to build a bot to help moderate a subreddit. What I'm wanting is to have the script automatically post each link from a certain RSS feed. Easy enough, but with each new link I ALSO want the script to attach a certain link flair to the submission AND remove it from any previous submissions, so that it denotes the current entry for the feed. So far the script works perfectly, unless someone else posts the latest entry first. If that happens, the script correctly returns the ALREADY_SUB error, but then the whole flair thing goes out of whack. I could solve the problem if there was a way to get the Submission object for the submission that causes the ALREADY_SUB error, that is, the Submission that the other person posted. However, I don't know how to do that. I first tried the get_submission using the URL argument, but it seems like that argument is meant for the URL of the reddit thread, not the URL linked to. Any guidance?", "title": "Get a Submission object based on the URL of the link?", "url": "https://www.reddit.com/r/redditdev/comments/1xrkhi/get_a_submission_object_based_on_the_url_of_the/"}, {"author": "metalayer", "created_utc": 1392084352, "gilded": 0, "name": "t3_1xkp8n", "num_comments": 2, "score": 2, "selftext": "I'm trying to write a function to register new Reddit accounts. I've set the raise_captcha_exception flag so the API should respond with the CAPTCHA id but I'm not sure how to actually get it. Also, with raise_captcha_exception set to false I get no prompt in my terminal. Running python 3.3 Stack overflow question here - http://stackoverflow.com/questions/21617451/how-to-get-reddit-captcha-id-with-praw", "title": "[PRAW] Captcha handling problem", "url": "https://www.reddit.com/r/redditdev/comments/1xkp8n/praw_captcha_handling_problem/"}, {"author": "JJJollyjim", "created_utc": 1392021468, "gilded": 0, "name": "t3_1xi8ol", "num_comments": 1, "score": 2, "selftext": "Hi there, I'm new to Python and the reddit API in general, so apologies if I make any mistakes here. A bot I am working on is receiving messages, and then doing some non-reddit stuff which could take a day or two. I expect that the bot may restart in this time, and I don't want to be storing everything in miniscule amounts of RAM on my server. What I am currently doing is storing the sender's username in a database (Mongo) and sending a new message to that username later, which works fine: reddit.send_message(user_name, \"subject\", \"body\") However, I'd prefer for a couple of reasons to use a reply. Looking at the reddit API docs, I should store the id of the message and call `/api/comment` with the message id as the `thing_id`. Just using pure PRAW, I'm not sure how to do it. I can't create a `praw.objects.Message` with an id, as far as I can tell. Any suggestions?", "title": "[PRAW] How to store an inbox message (in a database) for replying to later", "url": "https://www.reddit.com/r/redditdev/comments/1xi8ol/praw_how_to_store_an_inbox_message_in_a_database/"}, {"author": "zd9", "created_utc": 1391899514, "gilded": 0, "name": "t3_1xe0fw", "num_comments": 3, "score": 1, "selftext": "Is there a way in PRAW to find out which sub a comment is from if I am getting comments from /r/all?", "title": "[PRAW] Check what subreddit a comment is from", "url": "https://www.reddit.com/r/redditdev/comments/1xe0fw/praw_check_what_subreddit_a_comment_is_from/"}, {"author": "notverysmart_bot", "created_utc": 1391785324, "gilded": 0, "name": "t3_1x9wzh", "num_comments": 6, "score": 2, "selftext": "First of all I am new to programming in general and I am sorry in advance for bothering all of you. I have however googled for solutions, but sadly I don't understand much. I was trying to make a reddit bot and log in to reddit. I am using the example code from the PRAW documentation: r = praw.Reddit(USER_AGENT) def login(): print (\"Logging in..\") r.login(username, password) However I am getting the following error: Warning (from warnings module): File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\requests\\packages\\urllib3\\connection.py\", line 99 HTTPConnection.__init__(self, host, port, strict, timeout, source_address) DeprecationWarning: the 'strict' argument isn't supported anymore; http.client now always assumes HTTP/1.x compliant servers. Any help and explanation of what's going on would be much appreciated.", "title": "Having trouble logging in with PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/1x9wzh/having_trouble_logging_in_with_praw/"}, {"author": "Mustermind", "created_utc": 1391773514, "gilded": 0, "name": "t3_1x9kzl", "num_comments": 7, "score": 3, "selftext": "In the praw \"tutorial\" for writing a bot, they store a list of comments ids for each time an action is performed. What if the bot needs to be restarted? Of course you can store the ids in a file, but is there any other way of protecting against replying to a comment twice? What about checking the replies to a comment to see if the bot has already replied? Is there anything that can go wrong there?", "title": "Is there more than one way to make sure a bot will not do the same thing twice?", "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "FramesPerSushi", "created_utc": 1391743431, "gilded": 0, "name": "t3_1x8q1b", "num_comments": 2, "score": 1, "selftext": "r.get_subreddit('example').get_moderators() works fine, but r.get_subreddit('example').get_modqueue() returns this error: AttributeError: '' has no attribute 'get_modqueue' Edit: Even though it's fairly obvious I should probably specify that I'm using PRAW (Python).", "title": "Am I not using \"get_modqueue()\" correctly?", "url": "https://www.reddit.com/r/redditdev/comments/1x8q1b/am_i_not_using_get_modqueue_correctly/"}, {"author": "mattster42", "created_utc": 1391383742, "gilded": 0, "name": "t3_1wug8a", "num_comments": 6, "score": 2, "selftext": "So the past couple days have introduced me to Python and PRAW simultaneously. I'm getting the hang of the very basics so far, but I'm having trouble figuring out how to set link flair the way I want. So I've adapted newsfrbot to submit a link for every new entry in an RSS feed (is there a better script to do this? Because it's implementation of cPickle crashes if the process is ever interrupted). What I want to do is assign a certain link flair to the post upon submission, while clearing that flair from any previous posts (essentially so that the latest post from the bot has a flair marked \"current\"). I'm sure there's a simple four line solution to this, so can someone help a newbie get in the right direction?", "title": "Set link flair upon submission?", "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "newpong", "created_utc": 1391244101, "gilded": 0, "name": "t3_1wptfz", "num_comments": 2, "score": 2, "selftext": "Im not sure if I just need to sleep, or what. This seems like this would be a straight forward task, but I can't for the life of me figure out what I'm doing wrong. Here's the traceback: >>> a.author Redditor(user_name='xxxxxxxxxx') >>> a.body u'xxxxxxxxxxx' >>> a.score Traceback (most recent call last): File \"\", line 1, in File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 83, in __getattr__ attr)) AttributeError: '' has no attribute 'score' But 'score' seems to be available: >>> pprint(dir(a)) [ [...] 'approve', 'author', 'body', 'body_html', 'clear_vote', 'context', 'created', 'created_utc', 'delete', 'dest', 'distinguish', 'downvote', 'edit', 'first_message', 'first_message_name', 'from_api_response', 'fullname', 'has_fetched', 'id', 'ignore_reports', 'is_root', 'json_dict', 'likes', 'link_title', 'mark_as_read', 'mark_as_unread', 'name', 'new', 'parent_id', 'permalink', 'reddit_session', 'remove', 'replies', 'reply', 'report', 'score', 'subject', 'submission', 'subreddit', 'undistinguish', 'unignore_reports', 'upvote', 'vote', 'was_comment'] Praw Version: $ pip freeze | grep praw praw==2.1.12 Any idea what I'm doing wrong? Thanks.", "title": "What is the correct way to return the comment score? I'm getting an attribute error.", "url": "https://www.reddit.com/r/redditdev/comments/1wptfz/what_is_the_correct_way_to_return_the_comment/"}, {"author": "TeroTheTerror", "created_utc": 1391158077, "gilded": 0, "name": "t3_1wmt4p", "num_comments": 2, "score": 1, "selftext": "Hi, newish to python and PRAW here. Looked at the PRAW docs, but couldn't find anything to help me. Basically I would like to check a post X minutes after being submitted and see if the user assigned link flair. If not I want to remove the post. Checking the link flair and removing posts seems straight forward, but the time variable is holding me up, any suggestions?", "title": "[PRAW] Is it possible to use Praw to check on posts after a certain amount of time?", "url": "https://www.reddit.com/r/redditdev/comments/1wmt4p/praw_is_it_possible_to_use_praw_to_check_on_posts/"}, {"author": "chpwssn", "created_utc": 1391105088, "gilded": 0, "name": "t3_1wkn3s", "num_comments": 1, "score": 2, "selftext": "Is it possible to point PRAW to a Reddit clone? I thought it may be easier/better to test a bot that scans all submissions on a clone than on here. Thanks.", "title": "PRAW on a Reddit clone?", "url": "https://www.reddit.com/r/redditdev/comments/1wkn3s/praw_on_a_reddit_clone/"}, {"author": "IAMA_YOU_AMA", "created_utc": 1390959488, "gilded": 0, "name": "t3_1wfb55", "num_comments": 5, "score": 3, "selftext": "First off, I just want to say that PRAW is great and I really appreciate the effort the developers put into it! I also find it very helpful for my text mining and other analysis projects. I even hope to make a useful bot one day. That being said, as someone still relatively new to python, I am really struggling with the documentation. For example, it took me way longer than it should have to figure out comments have a body attribute that I should be using or an id attribute too. I'm not even sure what else I still might be missing. I know that comments come from the content generator and I'm more confident with using it now, but only after much trial and error and inspecting other scripts. I feel like I'm missing out on a lot of potential, by not knowing everything I can do. :-/ If anyone has any pointers or helpful advice on how to get through the documentation, I would really appreciate it. Thanks!", "title": "PRAW documentation", "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "WikipediaCitationBot", "created_utc": 1390709056, "gilded": 0, "name": "t3_1w642h", "num_comments": 1, "score": 1, "selftext": "**Introducing /u/WikipediaCitationBot** ------ **What do you comment on?** > I comment on articles that: > * have *more than 7 problems*, and *exceed a 0.15 ratio* of the number of references to the number of [**problems**](http://en.wikipedia.org/wiki/Template:Citation_needed#Inline_templates), and though I scan for dead links, I don't count them to this sum of problems > or > * have *no references at all* and have at *least three problems*, although I will specify whether or not the page has external links > I'm fairly lenient; at the time of writing this, I have commented on 33 todayIlearned posts, but have scanned over 500 wikipedia submissions to the subreddit, so I tend to keep to myself unless it's a particularly egregious article. And since I was launched, I have only become more strict about what I comment on. **How do I get rid of you?** > If I have made a mistake or am bothering you, feel free to downvote my comments. Any comment I make with a *score of zero or below will be automatically deleted*. That means if it's just your word against mine, you win. **Where do you scan?** > I'm only concerned with subreddits focused on providing accurate information, so currently I only scan ~~/r/wikipedia~~ and /r/todayilearned (*please don't ban me*) **How do you operate?** >I'm less than a hundred lines of python thanks to praw and urllib; once this bot has more capabilities I'll make the code public on GitHub **Why is this necessary?** > I think it's good to keep citing and lack thereof in mind, but I also try to ensure the script isn't overzealous or annoying. Mostly, I hope if I point out that a wikipedia page is poorly cited, it will encourage someone knowledgeable about the subject to contribute to it or cite it themselves. **Who made you?** > A procrastinating second year EE student ------ **My questions:** Do you have any suggestions or desires for functionality? Do you think the numbers should be tweaked? More strict? Less strict? Should I look at wikipedia articles linked in comments? What other subreddits should I look in?", "title": "About WikipediaCitationBot", "url": "https://www.reddit.com/r/redditdev/comments/1w642h/about_wikipediacitationbot/"}, {"author": "FramesPerSushi", "created_utc": 1390589032, "gilded": 0, "name": "t3_1w1w1s", "num_comments": 12, "score": 2, "selftext": "both import praw def wut_the_fuck(): return 'idk' post = wut_the_fuck() print(post) and import requests def wut_the_fuck(): requests.get('http://example.com') return 'idk' post = wut_the_fuck() print(post) return \"idk\", but import requests import praw def wut_the_fuck(): requests.get('http://example.com') return 'idk' post = wut_the_fuck() print(post returns: testweb.py:5: ResourceWarning: unclosed requests.get('http://example.com') idk I have absolutely no idea what's going on. Does anyone know?", "title": "Can't use requests module if I've imported praw", "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "HAEC_EST_SPARTA", "created_utc": 1390363794, "gilded": 0, "name": "t3_1vtpmp", "num_comments": 4, "score": 1, "selftext": "I am using PRAW to program a Reddit bot. Assuming I have a Comment object, how do I determine which subreddit it is in?", "title": "Get which Subreddit a Comment Is In [PRAW]", "url": "https://www.reddit.com/r/redditdev/comments/1vtpmp/get_which_subreddit_a_comment_is_in_praw/"}, {"author": "Dwarflord", "created_utc": 1390331768, "gilded": 0, "name": "t3_1vs7v5", "num_comments": 4, "score": 2, "selftext": "Hi, I'm making a bot in python, 3.3.3 if it matters, and I'm coming to an issue where it can't reply to a comment. It reads everything fine and prepares the right output, but when it comes to the comment.reply('..') line I get, Traceback (most recent call last): File \"C:\\Python33\\Class\\RoseluckBot\\RoseBotv1.py\", line 51, in comment.reply(\"Looks like you love Roseluck too! \" + postrep) NameError: name 'comment' is not defined I can post code if needed. Does anyone else get this issue? Thank you for any help you can give! Edit: code #RoseBot import time import re import praw import random r = praw.Reddit('Roseluck and other pony Monitor V1.3 ' 'By Dwarflord, for the Plounge' 'Posts images of ponies based on character named') r.login(Login values) already_done = [] cache = [] def get_rose_img(): ###Returns a roseluck image based on random seeding roseImages = ['XyhgWeG','vAw1hwE','U9S6Uaq','gxsDDl4','MqVN9n2','0Sf9q6U','QkR5pz5','1F6p7ks','Ubii8iL','Gc5EjiU','KPdbXQ1'] ranInd = random.choice(roseImages) #Random list choice. thanks Python! fullRep = \"[You need this!](http://i.imgur.com/%s.jpg)\" % (ranInd) return fullRep def check_condition(comment): ###Chekcs whether or not the comment contains rose or roseluck, in lower. ###Returns True or False. r_pat = re.compile('rose') r_pat2 = re.compile('roseluck') text = comment.body result = re.search(r_pat, text.lower()) result2 = re.search(r_pat2, text.lower()) if result or result2: #comment.reply('Looks like you said Roseluck!') #No, savour it return True else: return False #Main while True: subreddit = r.get_subreddit('MLPLounge') subreddit_comments = subreddit.get_comments() for c in subreddit_comments: #For all comments in selected Subreddit Comments if c.id in cache: #If comment already commented on break cache.append(c.id) condition = check_condition(c) #condition can be True or False if condition: postrep = get_rose_img() print(c) print (\"Looks like you love Roseluck too! \" + postrep) comment.reply(\"Looks like you love Roseluck too! \" + postrep)", "title": "Replying to a comment with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1vs7v5/replying_to_a_comment_with_praw/"}, {"author": "Jyrroe", "created_utc": 1390313869, "gilded": 0, "name": "t3_1vrfu3", "num_comments": 3, "score": 6, "selftext": "I'm writing a bot that finds and removes posts/comments that violate my subreddit's rules. When a comment is deleted, the bot leaves a comment for the user explaining the removal. The problem is, I can't find a way to distinguish normal comments from ones that have already been removed, so every time the bot runs, it adds another explanation comment. Any ideas? *** **EDIT:** I have a pseudo-solution, I found out about praw.helpers.comment_stream() and I found get_spam(), so I'm basically checking that each comment is *not in* the spam list yet, and only then will the bot comment on it. The problem is, I'm not seeing what the limit is on returned spam comments/posts. Is there a chance I could end up checking a comment from comment_stream() that wasn't returned by get_spam()? Ideally I'd like something like *comment.is_removed* or *comment.is_spam* but I'm not finding anything like that... **Question part 2:** Using get_spam(), is there a way to return comments only (instead of comments and posts)?", "title": "[PRAW] How can I tell if a post/comment has been removed?", "url": "https://www.reddit.com/r/redditdev/comments/1vrfu3/praw_how_can_i_tell_if_a_postcomment_has_been/"}, {"author": "HAEC_EST_SPARTA", "created_utc": 1390292759, "gilded": 0, "name": "t3_1vqywg", "num_comments": 3, "score": 2, "selftext": "I am currently developing a Reddit bot using PRAW. It is working for the most part, but I am having one problem. When I fetch the newest comments from all of Reddit, it continually returns the same comments. Please help me to rectify this problem. Here is my source code: `while True: comments = r.get_comments('all', limit=100) for comment in comments: for word in comment.body.split(): (process the word)` Sorry for the large number of edits, but I have no idea how to properly format code for this site.", "title": "PRAW Comment Fetching Problem", "url": "https://www.reddit.com/r/redditdev/comments/1vqywg/praw_comment_fetching_problem/"}, {"author": "5loon", "created_utc": 1390205869, "gilded": 0, "name": "t3_1vnq5y", "num_comments": 6, "score": 2, "selftext": "I'm trying to make a bot that will read a message with a username, and then input that username along with a CSS class to add flair to that person's username. I can't find anything on praw reading incoming messages. Does anyone know how to do this, or know any bots that already do this? I remember /r/pokemon having something similar earlier on. Thanks in advance.", "title": "I'm new to using praw on python, I have one simple question.", "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "blpst", "created_utc": 1390187223, "gilded": 0, "name": "t3_1vn371", "num_comments": 0, "score": 1, "selftext": "Hi, I could not find a function to accomplish this in praw. Reddit does have an api for it, though. (http://www.reddit.com/api/multi/mine) I have taken a look at the source code, it shouldn't be that hard to implement except I do not have much time on my hands recently. Is there anything I'm missing? Is there a way using the current version of praw? Thanks.", "title": "Praw: Get list of multis?", "url": "https://www.reddit.com/r/redditdev/comments/1vn371/praw_get_list_of_multis/"}, {"author": "redguy13", "created_utc": 1390186084, "gilded": 0, "name": "t3_1vn25q", "num_comments": 2, "score": 1, "selftext": "I was wondering if anyone could assist me with checking to see if \"get_redditor\" returns an error or not. I have used the \"fetch=True\" argument and it still returns. However, if you go to the user \"Alaska88\" page then it does not exist. The error happens when the program reaches the \"for comment in comments\" line and I am assuming the try-except doesn't work due to it being a lazy object. Thank you in advance for any time or help. import praw import urllib2 r = praw.Reddit('testing scraper') r.login() account = r.get_redditor('Alaska88',fetch=True) comments = account.get_comments(sort ='new',time ='all') print 'before comment loop' try: for comment in comments: print 'in comment loop' print(comment.body.encode('utf-8')) print('/////////////////////////') except urllib2.HTTPError: print 'In Except' time.sleep(60) pass File \"reddit_bot.py\", line 9, in This is where the error starts => for comment in comments: This is where it finally ends => raise HTTPError(http_error_msg, response=self)requests.exceptions.HTTPError: 404 Client Error: Not Found", "title": "Praw: How to do a try-except for a lazy object", "url": "https://www.reddit.com/r/redditdev/comments/1vn25q/praw_how_to_do_a_tryexcept_for_a_lazy_object/"}, {"author": "Rosco_the_Dude", "created_utc": 1390108677, "gilded": 0, "name": "t3_1vkqib", "num_comments": 9, "score": 2, "selftext": "I've been playing with praw for the past week or two, and I began writing so many functions with it that I naturally began making a class to represent the bot. Here's a short summary of what I'm doing so far: The bot scans its friends' posts and the posts in its private subreddit for predetermined tags and keywords so that it can be controlled from Reddit's normal interface by my friends and me. Right now it only has two main functions: advanced keyword searches, and reposting things to the private subreddit by commenting on a post and including a special tag in the comment body. I have a few of other ideas for it, but I'm still learning my way around praw (and still learning my way around Python, to some extent). I'm still in experimentation phase, so I don't keep the program running indefinitely yet. However, I'd like some data to persist after the program shuts down, e.g. when I save posts' IDs so that my program doesn't perform the same functions on them twice, I want the IDs to be around even if I need to restart the program. I've looked up documentation on Pickle, cPickle, and Shelve. So far Shelve seems like the best idea, but I'm still new enough at this that I'd like to hear other people's opinions. If I read the docs correctly, it seems that I can't just save an instance of an entire class in one fell swoop; it seems like I'd need to save (and eventually load) each value/array/dictionary seperately. Am I correct in this? I guess my question isn't about whether things like Shelve, Pickle, etc. exist, but it's whether there is a preferred way to use them when writing Reddit bots. I'm still new enough that I'd love to hear how everyone keeps their data persistent in their own bots/applications.", "title": "I'm writing my first Reddit bot, and I was wondering what is the best technique for saving instances of objects so that my bot doesn't lose progress when it shuts down.", "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "stats94", "created_utc": 1390046566, "gilded": 0, "name": "t3_1viomp", "num_comments": 5, "score": 0, "selftext": "When I run 'pip install praw' in cmd it comes up with: Downloading/unpacking praw Cannot fetch index base URL https://pypi.python.org/simple/ Could not find any downloads that satisfy the requirement praw Cleaning up... No distributions at all found for praw Storing debug log for failure in C:\\ ... My python scripts seem to always fail to download any content, and I have absolutely no idea why this is but I guess this may be the problem here too. I've not found anybody that's had this problem yet - anybody know how to fix? **Edit: PROBLEM SOLVED, THANKS EVERYONE!**", "title": "pip unable to download praw", "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": "idProQuo", "created_utc": 1389753108, "gilded": 0, "name": "t3_1v8sbx", "num_comments": 4, "score": 3, "selftext": "I'm currently working on a reddit bot that's gotten quite complex. I'd like to implement a testing framework just to restore some sanity. However, I'm finding it difficult to write useful tests when PRAW is involved. Does anyone know of a good testing framework for working with PRAW, or a good way of mocking its functionality? If you know of a project on GitHub that has good tests, a link would be awesome! Ideally, I'd like to not need a local reddit instance running when I test.", "title": "Using PRAW with a unit testing framework and mocks", "url": "https://www.reddit.com/r/redditdev/comments/1v8sbx/using_praw_with_a_unit_testing_framework_and_mocks/"}, {"author": "PointsOutTrains", "created_utc": 1389573165, "gilded": 0, "name": "t3_1v2ho0", "num_comments": 9, "score": 1, "selftext": "I have a list of users for a network project I'm working on. I want to connect the users to the subreddits they have commented in or posted in, and get the number of times they have commented or posted in that subreddit. I thought the best way to do this would be by scraping the comments and getting the subreddit from there, but I don't know how using PRAW. Is there a way using get_comments, or is this not possible?", "title": "Is it possible to get the subreddit of a comment with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "SwedishBoatlover", "created_utc": 1389103411, "gilded": 0, "name": "t3_1umg87", "num_comments": 4, "score": 0, "selftext": "I've tried >for post in self.redditor.get_overview(): But it will only give me the 25 latest posts. How do I go ahead to get every single post the user has made to iterate through them? Ninjaedit: All posts that are not archived yet. Maybe it's obvious, but I'm using PRAW, Python 3.3, on W7.", "title": "How to get all posts from a user?", "url": "https://www.reddit.com/r/redditdev/comments/1umg87/how_to_get_all_posts_from_a_user/"}, {"author": "Justinsaccount", "created_utc": 1388628104, "gilded": 0, "name": "t3_1u7553", "num_comments": 6, "score": 13, "selftext": "Hi everyone. I was just tracking down why links to phoronix.com in /r/linux never work right when using reddit sync. Turns out it is related to [a two year old issue](https://github.com/reddit/reddit/issues/283) that is not going to be fixed. As far as I can tell, Reddit uses [This function](https://github.com/reddit/reddit/blob/master/r2/r2/lib/filters.py?source=cc#L56) for encoding json, which escapes &,. https://github.com/reddit/reddit/wiki/JSON Mentions that a few fields are escaped, but it would appear that all data is escaped. From what I tested, clients don't url decode the url field that reddit returns. for example: In [1]: import praw In [2]: r=praw.Reddit(user_agent='test') In [3]: thing=r.get_subreddit('linux').get_hot(limit=5).next() In [4]: thing.url Out[4]: u'http://www.phoronix.com/scan.php?page=news_item&px=MTU1NzA' Not sure what to do about this... minimally the documentation should be updated..", "title": "Client developers: I think most (all?) of your clients are subtly broken.", "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "pachufir", "created_utc": 1388441298, "gilded": 0, "name": "t3_1u1pwy", "num_comments": 10, "score": 7, "selftext": "So, I've been following the PRAW tutorials (wonderfully written btw) to write my own reddit bot, and I saw many times in the published code the login line is just r.login() or r.login(USERNAME, PASSWORD). I was wondering if this was just so that the code can be published without revealing the credentials, or is there some way that the fields are being dynamically filled in?", "title": "open sourcing code with login credentials", "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "imareddituserhooray", "created_utc": 1388382800, "gilded": 0, "name": "t3_1tzwqd", "num_comments": 1, "score": 2, "selftext": "Hey everyone, I have a best practice question related to PRAW's get_mentions function. How do you normally keep track of what has been seen; for example, can you control the read/unread status of mentions with PRAW? Do you normally write the id to a permanent storage (file/db, not a variable) for what you have already processed? If my bot crashes, I just don't want to double reply to mentions. Thanks!", "title": "Python (PRAW) get_mentions and old mentions after crash", "url": "https://www.reddit.com/r/redditdev/comments/1tzwqd/python_praw_get_mentions_and_old_mentions_after/"}, {"author": "pachufir", "created_utc": 1388347220, "gilded": 0, "name": "t3_1tyk0o", "num_comments": 9, "score": 1, "selftext": "I was wondering if there was a quick way through praw to get all the urls in a comment? Specifically I want to get all the gifs linked in a comment, but I feel if I can just easily get all the links, finding the gifs shouldn't be too hard. Thanks in advance!", "title": "Anyway to extract links from a comment?", "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "UWillAlwaysBALoser", "created_utc": 1388045160, "gilded": 0, "name": "t3_1tq6kg", "num_comments": 6, "score": 3, "selftext": "I apologize if this has been asked before, but it's very difficult to search for questions about R on reddit. I want to pull the subreddits in which a specified user has commented, just as /u/chicken_bridges has [using Praw](https://github.com/chickenbridges/reddit_cluster_scripts/blob/master/collect_data.py). Ideally, I'd like to do this in R, without having to use Python (or the rPython package). I'm curious if anyone has used R to scrape reddit through the API.", "title": "Any documentation or tutorials for using the API in R?", "url": "https://www.reddit.com/r/redditdev/comments/1tq6kg/any_documentation_or_tutorials_for_using_the_api/"}, {"author": "Ph0X", "created_utc": 1387882951, "gilded": 0, "name": "t3_1tlmrf", "num_comments": 8, "score": 1, "selftext": "Looking around, I couldn't really find a good answer for this. Closest I found was [this thread from almost 2 years ago](http://www.reddit.com/r/redditdev/comments/pz4xj/how_can_i_get_a_list_of_all_posts_in_a_subreddit/). I was wondering if there has been any update on this. I would like a dump of all the submissions on a subreddit I help run for statistical purposes, and I was going to write a script using PRAW to go through and save all the posts, but from what I understand, the normal methods are capped at 1000 submissions. Is there no way to get an enumeration of all posts. How long it takes doesn't really matter since it would be a one time thing.", "title": "List of all submissions in a subreddit", "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "spinnelein", "created_utc": 1387740839, "gilded": 0, "name": "t3_1th5ar", "num_comments": 1, "score": 2, "selftext": "On /r/BackYardChickens we have a set of images that go on the sidebar. Currently I'm using PRAW and set_stylesheet to change the stylesheet every minute or so to cycle through the images. Is there a better way to do this? My method works fine, but should I be concerned about making that many stylesheet edits?", "title": "Cycling through a set of images on the sidebar - is there a better way to do it?", "url": "https://www.reddit.com/r/redditdev/comments/1th5ar/cycling_through_a_set_of_images_on_the_sidebar_is/"}, {"author": "rhiever", "created_utc": 1387514492, "gilded": 0, "name": "t3_1tay79", "num_comments": 2, "score": 7, "selftext": "When was this API call added, and how does it work behind the scenes? Example: >import praw >r = praw.Reddit(user_agent=\"bot by /u/{0}\".format(\"rhiever\")) >r.get_subreddit_recommendations([\"redditdev\"]) >>[Subreddit(display_name='badkarma'), Subreddit(display_name='ModerationLog'), Subreddit(display_name='help'), Subreddit(display_name='ideasfortheadmins'), Subreddit(display_name='goldbenefits'), Subreddit(display_name='modhelp'), Subreddit(display_name='redditrequest'), Subreddit(display_name='webgl')] From the subreddits I frequent, the recommendations seem pretty nice. Is reddit finally implementing a recommendation system?! :-D", "title": "Noticed new API call: get_subreddit_recommendations()", "url": "https://www.reddit.com/r/redditdev/comments/1tay79/noticed_new_api_call_get_subreddit_recommendations/"}, {"author": "AngusDWilliams", "created_utc": 1387480621, "gilded": 0, "name": "t3_1t9l74", "num_comments": 2, "score": 3, "selftext": "I found a PRAW example for tracking karma by subreddit, but it only looks at the last N submissions. I want a way to continuously track multiple user's cumulative karma per subreddit. I plan to hit the reddit API once per day to update this total. How would I go about implementing this? Is it possible?", "title": "How should I implement tracking karma by subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/1t9l74/how_should_i_implement_tracking_karma_by_subreddit/"}, {"author": "pranavrc", "created_utc": 1386868480, "gilded": 0, "name": "t3_1sq7yl", "num_comments": 8, "score": 2, "selftext": "Hi! I have a bot over at /r/cricket called /u/howstat, which seems to reply twice to comments often. Is this an issue with PRAW? [Here](http://github.com/pranavrc/howstat)'s the code for the bot. If not, I suppose I could use comment timestamps to avoid that, so is there a way to get a comment's timestamp with PRAW? I couldn't find a suitable method for that, in the documentation. Thanks!", "title": "[Praw] Bot double-posts often.", "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "Innokin_Paul", "created_utc": 1386577257, "gilded": 0, "name": "t3_1sg934", "num_comments": 7, "score": 1, "selftext": "Hi, I hope you can help me with this. We are running an [ECig Christmas giveaway at /r/Innokin.](http://www.reddit.com/r/Innokin/comments/1rrnpp/innokinreddit_contest_2_reboot_new_rules_and_a/) I have installed and tested Praw and I would like to use it to collect the usernames from the contest thread. If possible I would like to also have them listed in a way which is easy to export into excel. Is this possible and can you help us? Thank you!", "title": "Help with a Praw Script to collect usernames from a contest thread.", "url": "https://www.reddit.com/r/redditdev/comments/1sg934/help_with_a_praw_script_to_collect_usernames_from/"}, {"author": "im14", "created_utc": 1386392194, "gilded": 0, "name": "t3_1saqs9", "num_comments": 4, "score": 1, "selftext": "These Python PRAW calls work, but they process messages from newest to oldest. I want to process messages in the order they arrived to inbox. Looked up info on \"get_content generator\" but didn't find any obvious ways to do this. Anyone know of a solution? for m in reddit.get_unread(limit=1000): //process message.. m.mark_as_read() In my case order of messages is actually important. Will tip good answers!", "title": "I want to process messages from the inbox from oldest to newest. How?", "url": "https://www.reddit.com/r/redditdev/comments/1saqs9/i_want_to_process_messages_from_the_inbox_from/"}, {"author": "Plague_Bot", "created_utc": 1385909626, "gilded": 0, "name": "t3_1ru8jl", "num_comments": 6, "score": 1, "selftext": "My bot just threw an internal server error and stopped running. What are some of the possible causes for this, and how can I avoid it? Or is there a way to catch it and continue the program? The full error that python threw: Traceback (most recent call last): File \"C:\\Python27\\programs\\redditPlague.py\", line 95, in for comment in comments: File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 440, in get_content page_data = self.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 158, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 476, in request_json response = self._request(url, params, data) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 349, in _request _raise_response_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 173, in _raise_response_exceptions response.raise_for_status() File \"C:\\Python27\\lib\\site-packages\\requests\\models.py\", line 725, in raise_for_status raise HTTPError(http_error_msg, response=self) HTTPError: 500 Server Error: Internal Server Error", "title": "HTTPError: 500 Server Error: Internal Server Error \u2013 Possible causes and solutions? [PRAW]", "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "echblog", "created_utc": 1385827045, "gilded": 0, "name": "t3_1rs00u", "num_comments": 2, "score": 3, "selftext": "I'm trying to parse the 'all' comments feed, however I seem to be running up against either the cache or the 2 second limit, although from what I can tell I shouldn't be. Relevant code: r = praw.Reddit(user_agent='testing_praw/0.1 by echblog') r.login() submissions_list = [] while True: comments = r.get_comments('all', limit=None) newSubmission = stuff #I've stripped out the code relevant to this for comment in comments: if newSubmission not in submissions_list: submissions_list.append(newSubmission) time.sleep(60) What's happening is the first time I call r.get_comments, it will work as expected and retrieve as many as it can. After waiting for the sleep period, it tries again, but this time it retrieves the same comments one at a time, with a two second interval between each one. If the cache has a 30 second timeout, having a sleep value of 60 should be more than enough time for it to clear, shouldn't it? I've tried for different values of both *limit* and *time.sleep*, but to no avail. What I don't get if it *is* the cache, why is it only returning one comment at a time?", "title": "[PRAW] Retrieving 'all' comments works once, then returns 1 every 2 seconds.", "url": "https://www.reddit.com/r/redditdev/comments/1rs00u/praw_retrieving_all_comments_works_once_then/"}, {"author": "SeaCowVengeance", "created_utc": 1385670181, "gilded": 0, "name": "t3_1rnznl", "num_comments": 5, "score": 9, "selftext": "Hi, I'm making a conversion bot that'll be specific to a few select subreddits. It'll search for comments that match criteria and reply to them. Obviously you won't want to a bot to reply to the same thing more than once in this situation. My first thought to solving this was to go though the replies for the match to see if my bot has already reached it but that's slow and takes to many API calls. My second thought was to make a hash table containing the IDs of the comments that have been replied to, but I would have to stores them in an external file when the bot goes down and reload them. Anyone have any efficient tricks for doing this? I'm using python/praw if that matters.", "title": "Best way for a bot to keep track of the replies it's already made.", "url": "https://www.reddit.com/r/redditdev/comments/1rnznl/best_way_for_a_bot_to_keep_track_of_the_replies/"}, {"author": "deten", "created_utc": 1385584629, "gilded": 0, "name": "t3_1rlhzs", "num_comments": 3, "score": 1, "selftext": "I am really new to PRAW and I just cant find simple documentation on how to do the things I am trying... so maybe I can learn a bit from here. I wanted to get the newest X (10 for now) submissions and store their URL and the name of the submitter to an object or a file. I am not very good with python, but I figure it is easiest to make an array of objects that includes this information. Any advice on where to start?", "title": "[PRAW] Retrieving the newest 10 submissions in a sub and storing their link and the username of the poster", "url": "https://www.reddit.com/r/redditdev/comments/1rlhzs/praw_retrieving_the_newest_10_submissions_in_a/"}, {"author": "WaitForItTheMongols", "created_utc": 1385569601, "gilded": 0, "name": "t3_1rkv8u", "num_comments": 6, "score": 2, "selftext": "I'm trying to install PRAW, but PRAW requires PIP, which itself requires SetupTools, and there's just too much happening here, and I have honestly no idea what I'm doing. I've never installed external additions, so I'm unfamiliar with all this. I'm running Windows 7, if that's relevant. The main issue is that all the tutorials say \"Do this\" followed by some form of code. I don't know where to run any of that. Command prompt? In python? If someone could walk me through the steps, that'd be awesome.", "title": "Yet another PRAW installation issue...", "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": "swollen_pickle", "created_utc": 1385562287, "gilded": 0, "name": "t3_1rkmao", "num_comments": 2, "score": 5, "selftext": "I have a long string which is close to, but less than 40,000 characters, which I believe is the limit for a self post in a subreddit which only allows self posts. However when I try and submit this post to a self post only subreddit using PRAW I am getting `praw.errors.APIException: (TOO_LONG) 'this is too long (max: 15000)' on field 'text'`. When I output the string `text` to a text file and then copy and paste it and post it manually it works. Does anyone know why this might be happening?", "title": "[PRAW] I'm getting APIException TOO_LONG when trying to submit a long self post using PRAW, however when I try and post the same string manually by copy and pasting it, it works.", "url": "https://www.reddit.com/r/redditdev/comments/1rkmao/praw_im_getting_apiexception_too_long_when_trying/"}, {"author": "unixfan", "created_utc": 1385303498, "gilded": 0, "name": "t3_1rcl4u", "num_comments": 1, "score": 1, "selftext": "Hey guys! I'm working on a small experiment and I'm using Praw. When I try to get the subscribed subreddits for the logged user it throws me an ArgumentError, even though the method exists and it's on the [docs](https://praw.readthedocs.org/en/PRAW-1.0.9/praw.html#praw.objects.LoggedInRedditor.my_reddits). This is the code: >>> import praw >>> r = praw.Reddit('ffdf') >>> r.user >>> r.login('unixfan', 'mypassword') >>> r.user.name 'unixfan' >>> r.user.my_reddits() Traceback (most recent call last): File \"\", line 1, in File \"/usr/local/lib/python2.7/site-packages/praw/objects.py\", line 81, in __getattr__ attr)) AttributeError: '' has no attribute 'my_reddits' Also, all other `LoggedInRedditor` methods throw the same exception, so IDK what's going on.", "title": "[PRAW] Praw throwing an AttributeError when trying to call 'my_reddits()'", "url": "https://www.reddit.com/r/redditdev/comments/1rcl4u/praw_praw_throwing_an_attributeerror_when_trying/"}, {"author": "atomicUpdate", "created_utc": 1385170265, "gilded": 0, "name": "t3_1r9ci4", "num_comments": 2, "score": 2, "selftext": "Here is the relevant snippet of code I'm using: info = rGetUserData.refresh_access_information(refresh_token=dbUserRow[\"refresh_token\"], update_session=True) user = rGetUserData.get_me() name = '%s' % (user.name) linkKarma = '%u' % (user.link_karma) commentKarma = '%u' % (user.comment_karma) orangered = '%s' % (False) numMessages = 0 numComments = 0 numModMail = 0 for message in rGetUserData.get_mod_mail(): orangered = '%s' % (True) numModMail += 1 for message in rGetUserData.get_unread(): orangered = '%s' % (True) if message.was_comment == True: numComments += 1 else: numMessages += 1 It's able to perform the get_mod_mail() call successfully, however, it dies at get_unread() and returns the 403 error. If I comment out the get_unread() block, everything completes successfully. I've tried adding in a delay between get_mod_mail() and get_unread() of 3 seconds for the rate limiting (which I believe Praw handles automatically anyway), but that didn't help either. Doing the get_unread() before the get_mod_mail() doesn't help either. Here is the full traceback for the error: 192.168.1.1 - - [22/Nov/2013 18:21:20] \"GET /get_user_data/atomicUpdate HTTP/1.1\" 500 - Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1836, in __call__ return self.wsgi_app(environ, start_response) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1820, in wsgi_app response = self.make_response(self.handle_exception(e)) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1403, in handle_exception reraise(exc_type, exc_value, tb) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1817, in wsgi_app response = self.full_dispatch_request() File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1477, in full_dispatch_request rv = self.handle_user_exception(e) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1381, in handle_user_exception reraise(exc_type, exc_value, tb) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1475, in full_dispatch_request rv = self.dispatch_request() File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1461, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File \"/home/tjvanpat/Programs/redditIWidget/redditIWidget_Server.py\", line 92, in getUserData for message in rGetUserData.get_unread(): File \"/usr/lib/python2.7/site-packages/praw/__init__.py\", line 440, in get_content page_data = self.request_json(url, params=params) File \"/usr/lib/python2.7/site-packages/praw/decorators.py\", line 158, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/lib/python2.7/site-packages/praw/__init__.py\", line 476, in request_json response = self._request(url, params, data) File \"/usr/lib/python2.7/site-packages/praw/__init__.py\", line 349, in _request _raise_response_exceptions(response) File \"/usr/lib/python2.7/site-packages/praw/internal.py\", line 173, in _raise_response_exceptions response.raise_for_status() File \"/usr/lib/python2.7/site-packages/requests/models.py\", line 725, in raise_for_status raise HTTPError(http_error_msg, response=self) HTTPError: 403 Client Error: Forbidden So...any ideas why get_mod_mail() completes successfully, while get_unread() returns the 403 when they both require the same privatemessages OAuth authentication (assuming that's what the 403 is trying to tell me)?", "title": "get_unread() Giving HTTPError: 403 Client Error: Forbidden", "url": "https://www.reddit.com/r/redditdev/comments/1r9ci4/get_unread_giving_httperror_403_client_error/"}, {"author": "NotSinceYesterday", "created_utc": 1384775614, "gilded": 0, "name": "t3_1qw2u3", "num_comments": 8, "score": 2, "selftext": "Hi all Trying to get a bot to post a thread and sticky it. However I keep receiving the error: >AttributeError: '' has no attribute 'sticky' But it is [listed in the documentation](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.sticky). Relevant part of code: >submission = r.submit('pkmntrades4mods', title, text=body) >submission.distinguish() >submission.add_comment(\"Holy shit, did this work?\") >submission.sticky()", "title": "Sticky posts", "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "NEBRASSKICKER", "created_utc": 1384328378, "gilded": 0, "name": "t3_1qiu59", "num_comments": 13, "score": 3, "selftext": "I'm trying to make a bot for automatically posting game threads for my sports subreddit. I have praw installed and i'm trying to go off of [this guide] (https://praw.readthedocs.org/en/latest/) (but it's kinda confusing). Can anyone guide me through what code I need to write to make this happen? I've already figured out how to login with to the bot account, but I can't figure how to post, and I need to figure out how to have this code execute so it can post threads at certain times.. Any help?? Thank You for you time.", "title": "Help with making auto posting bot.", "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "davidystephenson", "created_utc": 1383752562, "gilded": 0, "name": "t3_1q13hd", "num_comments": 6, "score": 2, "selftext": "I am attempting to use the `place_holder` argument to `get_new` to return only items after the latest post previously found. However, it seems to still return items created before the specified post. The full source of my script: import os import praw r = praw.Reddit('test 1.0 by /u/davidystephenson') terms = ['ukraine'] if os.path.exists('last.txt'): with open('last.txt', 'r') as file: last = file.read() last_created = r.get_submission(submission_id=last).created print('last test', last, last_created) else: print('Warning: no last.txt file') last = None subreddits = ['geopolitics'] submissions = [] for subreddit in subreddits: submissions += list( r.get_subreddit(subreddit).get_new(limit=None, place_holder=last) ) print('length test', len(submissions)) for submission in submissions: text = submission.selftext.lower() title = submission.title.lower() matches = [term for term in terms if term in text or term in title] print( 'after test', submission.created, last_created, submission.created > last_created ) if matches: print('matches test', submission.short_link, matches) else: pass # no matches if submission.created > last_created: with open('last.txt', 'w') as file: file.write(submission.id) However, even after running this script several times, items older than the \"last_created\" value continue to be returned, confirmed by individual testing as shown in line 34. For example, running the above code, even several times at intervals, repeatedly returns: last test 1q0zkw 1383778127.0 length test 20 after test 1383743049.0 1383778127.0 False after test 1383738593.0 1383778127.0 False after test 1383720420.0 1383778127.0 False matches test http://redd.it/1pzesl ['ukraine'] after test 1383672394.0 1383778127.0 False after test 1383657110.0 1383778127.0 False after test 1383596146.0 1383778127.0 False after test 1383572155.0 1383778127.0 False after test 1383561922.0 1383778127.0 False after test 1383546313.0 1383778127.0 False after test 1383540940.0 1383778127.0 False after test 1383522016.0 1383778127.0 False after test 1383487187.0 1383778127.0 False after test 1383309728.0 1383778127.0 False after test 1383294259.0 1383778127.0 False after test 1383291938.0 1383778127.0 False after test 1383266995.0 1383778127.0 False after test 1383240858.0 1383778127.0 False after test 1383230463.0 1383778127.0 False after test 1383155985.0 1383778127.0 False after test 1383099494.0 1383778127.0 False Thoughts?", "title": "PRAW: Using place_holder still returns posts older than the place_holder", "url": "https://www.reddit.com/r/redditdev/comments/1q13hd/praw_using_place_holder_still_returns_posts_older/"}, {"author": "rog1121", "created_utc": 1383665671, "gilded": 0, "name": "t3_1pyczr", "num_comments": 2, "score": 2, "selftext": "Ive got a script that iterates trhough comments and flairs people. There are several conditions it includes and right now some of the actions in the script are making more api calls than it should be. On line 25 it makes an API requst to get the comments which it uses in line 27: flat_comments = praw.helpers.flatten_tree(submission.comments) And then on line 33, 70 and 77 it references to line 25 again so for each comment it's making 3 api requests. With posts that have 300+ comments this gets very inefficient and slow. Is there any way to store the api results as a string and use that as a reference? Also if you have any ideas on how to cleanup the code that would be great as well Here is the code: import sys, os from ConfigParser import SafeConfigParser import logging import praw # load config file containing_dir = os.path.abspath(os.path.dirname(sys.argv[0])) cfg_file = SafeConfigParser() path_to_cfg = os.path.join(containing_dir, 'config.cfg') cfg_file.read(path_to_cfg) #configure logging logging.basicConfig(level=getattr(logging, cfg_file.get('logging', 'level'))) reddit_username = cfg_file.get('reddit', 'username') logging.info('Logging in as /u/'+reddit_username) r = praw.Reddit(user_agent=cfg_file.get('reddit', 'user_agent')) r.login(cfg_file.get('reddit', 'username'), cfg_file.get('reddit', 'password')) subreddit = cfg_file.get('reddit', 'subreddit') submission = r.get_submission(submission_id=cfg_file.get('reddit', 'link_id')) with open (\"id.txt\", \"r\") as myfile: completed=myfile.read() flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: content = comment.body if 'confirm' in content.lower() and comment.is_root == False and comment.id not in completed: parent = [com for com in flat_comments if com.fullname == comment.parent_id][0] if comment.author_flair_css_class: child_css = str(int(comment.author_flair_css_class) + 1) if comment.author_flair_text: child_text = comment.author_flair_text if parent.author_flair_css_class: parent_css = str(int(parent.author_flair_css_class) + 1) if parent.author_flair_text: parent_text = parent.author_flair_text if not comment.author_flair_css_class: child_css = '1' if not comment.author_flair_text: child_text = '' if not parent.author_flair_css_class: parent_css = '1' if not parent.author_flair_text: parent_text = '' # Prevent users from confirming under their own comments if comment.author == parent.author: comment.reply('You have confirmed a trade under your own post, this action has been reported to the Moderators') comment.report() parent.report() # Karma check verification elif comment.author.link_karma + comment.author.comment_karma", "title": "Store API call and code cleanup help", "url": "https://www.reddit.com/r/redditdev/comments/1pyczr/store_api_call_and_code_cleanup_help/"}, {"author": "The_Gingey", "created_utc": 1383597799, "gilded": 0, "name": "t3_1pwda4", "num_comments": 6, "score": 2, "selftext": "I am trying to learn how to use the API, but am running into a problem. When I run the tutorial code through the terminal using python it works. But when I run it in a python script, it fails, saying: AttributeError: 'module' object has no attribute 'Reddit' Any advice? This is the code I am running. import praw r = praw.Reddit(user_agent='my_cool_application') submissions = r.get_subreddit('opensource').get_hot(limit=5) [str(x) for x in submissions] Edit: I am sorry if this is a trivial question. I have used multiple python libraries before but never had a problem like this. I am also running on Mac 10.8, with python 2.7", "title": "Problem using reddit API", "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "MURICAN_BOT", "created_utc": 1382838447, "gilded": 0, "name": "t3_1pag1x", "num_comments": 1, "score": 2, "selftext": "I am aware that you can do this with PRAW import praw r = praw.Reddit('placeholder example') subreddit = r.get_subreddit('redditdev') for submission in subreddit.get_new(limit=100, place_holder='1bu7ak'): print submission.title However, I would like to get submissions *older* than the placeholder. This is used for when I'm scanning a LARGE amount of comments, and I get an error for too many requests. I would like to stop and wait, and then start again from the last comment (getting older and older comments) How can I do this?", "title": "Get comments older than placeholder", "url": "https://www.reddit.com/r/redditdev/comments/1pag1x/get_comments_older_than_placeholder/"}, {"author": "kirbz1692", "created_utc": 1382837335, "gilded": 0, "name": "t3_1paezw", "num_comments": 4, "score": 1, "selftext": "I'm trying to learn how to use praw, and when I run [this program](http://pastebin.com/VBq5yAej) it throws an keyerror because when it looks for `self.sleep_time = self.response['ratelimit']` it can't find it. Anyone know whats up? I'm a beginner with this so its possible I'm doing something extremely wrong. **EDIT:** here's the traceback: Traceback (most recent call last): File \"C:/Users/ME/PycharmProjects/RedditBot/tutorial\", line 9, in r.login(\"MYUSERNAME\",\"MYPASSWORD\"); File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\__init__.py\", line 1157, in login self.request_json(self.config['login'], data=data) File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\decorators.py\", line 172, in wrapped return_value)) File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\errors.py\", line 325, in __init__ self.sleep_time = self.response['ratelimit'] KeyError: 'ratelimit' KeyError: 'ratelimit'", "title": "Help with a keyerror using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1paezw/help_with_a_keyerror_using_praw/"}, {"author": "Zarqu0n", "created_utc": 1382737710, "gilded": 0, "name": "t3_1p7zhy", "num_comments": 6, "score": 1, "selftext": "I am using praw and i need the subscriber count of some subreddits,", "title": "How do you find out the subscriber count of a specific subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "ascetica", "created_utc": 1382665632, "gilded": 0, "name": "t3_1p5z3c", "num_comments": 2, "score": 1, "selftext": "I'm creating a bot that responds upon request with a random quote from Jurassic Park. I'm trying to have my bot reply with a multi-line comment. I am able to do it for a single comment [using this method](http://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/car4p6j). However I'd like to have my bot select a reply from a large number of possible replies, and I want to store these in a python list or an XML file. I tried the following list: jurassic_park_quote_list = ['''Dr. Alan Grant: T-Rex doesn't want to be fed. He wants to hunt. Can't just suppress 65 million years of gut instinct.''', '''Dr. Alan Grant: [finding egg shells] Oh my God. Do you know what this is? This is a dinosaur egg. The dinosaurs are breeding. Tim: But Grandpa said all the dinosaurs were girls. Dr. Alan Grant: Amphibian DNA. Lex: What's that? Dr. Alan Grant: Well, on the tour, the film said they used frog DNA to fill in the gene sequence gaps. They mutated the dinosaur genetic code and blended it with that of a frog's. Now, some West African frogs have been known to spontaneously change sex from male to female in a single sex environment. Malcolm was right. Look... [we see a trail of baby dinosaur footprints Dr. Alan Grant: Life found a way.''', '''Dr. Ian Malcolm: [while being chased by the T-Rex] Must go faster.'''] There are three entries in the list. However, when the bot replies with the second multi-line comment it ignores the formatting. I also haven't had any luck importing from an XML file and using \\n\\n for a line break. Any ideas?", "title": "[PRAW] Creating multi-line comment from list of strings or XML file", "url": "https://www.reddit.com/r/redditdev/comments/1p5z3c/praw_creating_multiline_comment_from_list_of/"}, {"author": "hinayu", "created_utc": 1382588495, "gilded": 0, "name": "t3_1p3qau", "num_comments": 13, "score": 3, "selftext": "I'm doing some comment scraping for a certain redditor. For each comment that I get, I'd like to construct the URL that points back to the comment. Here's an example of what I'm doing currently. r = praw.Reddit(user_agent) username = r.get_redditor('username') comments = username.get_comments(limit=1) for comment in comments: pprint(vars(comment)) So I'm looking at what I can do with a comment object, and I see id, link_id, name, parent_id, and subreddit_id. I'm not sure if I can use one of those to potentially construct a link back to the comment or not, but I wasn't able to figure anything out. Thanks for the help in advance!", "title": "[PRAW] Constructing url from a comment", "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "rog1121", "created_utc": 1382497673, "gilded": 0, "name": "t3_1p0xc9", "num_comments": 4, "score": 4, "selftext": "I have this script setup to track trades on a subreddit and incremental add to a flair every time. The problem is that if there are multiple child comments from the same user it counts all 3 comments as one. http://i.imgur.com/wqKNFWk.png Here are the flairs after I run the script http://i.imgur.com/1XLHeqK.png As you can see the Parent post is flaired correctly but the 3 child posts from the same user are all counted as one. import sys, os import praw from ConfigParser import SafeConfigParser # load config file containing_dir = os.path.abspath(os.path.dirname(sys.argv[0])) cfg_file = SafeConfigParser() path_to_cfg = os.path.join(containing_dir, 'config.cfg') cfg_file.read(path_to_cfg) r = praw.Reddit(user_agent=cfg_file.get('reddit', 'user_agent')) r.login(cfg_file.get('reddit', 'username'), cfg_file.get('reddit', 'password')) subreddit = cfg_file.get('reddit', 'subreddit') submission = r.get_submission(submission_id=cfg_file.get('reddit', 'link_id')) flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: if 'confirm' in comment.body and comment.is_root == False: parent = [com for com in flat_comments if com.fullname == comment.parent_id][0] if comment.author_flair_css_class or comment.author_flair_text: child_css = str(int(comment.author_flair_css_class) + 1) child_text = comment.author_flair_text if not comment.author_flair_css_class: child_css = '1' if not comment.author_flair_text: child_text = '' if parent.author_flair_css_class or parent.author_flair_text: parent_css = str(int(parent.author_flair_css_class) + 1) parent_text = parent.author_flair_text if not parent.author_flair_css_class: parent_css = '1' if not parent.author_flair_text: parent_text = '' comment.subreddit.set_flair(comment.author, child_text, child_css) comment.author_flair_css_class = child_css print 'Changed Child CSS' parent.subreddit.set_flair(parent.author, parent_text, parent_css) parent.author_flair_css_class = parent_css print 'Changed Parent CSS'", "title": "Script counting multiple comments as one", "url": "https://www.reddit.com/r/redditdev/comments/1p0xc9/script_counting_multiple_comments_as_one/"}, {"author": "nareik15", "created_utc": 1382321738, "gilded": 0, "name": "t3_1ovi6a", "num_comments": 4, "score": 5, "selftext": "Further details [here](http://www.reddit.com/r/redditdev/comments/1oi5fb/a_number_of_questions_about_the_replace_more/) I am trying to download, using PRAW, the comments from submissions which have many comments (2000+) and this is proving to be very time consuming when fetching the default 200 comments or even 500 (when changing this in preferences) and then using the \"replace_more_comments\" method. So, seeing that you can fetch 1500 comments with reddit gold, I went ahead and bought this but now can't figure out how to make my default number of comments fetched equal 1500. If anyone has any idea how to do this, that would be great. P.S I have already tried to do the following but get a JSON error (if anyone knows how to fix this that would also be incredibly useful): r.get_submission(\"url_of_a_submission?limit=1500\")", "title": "Just bought Reddit Gold. How do I change default No of comments fetched to 1500.", "url": "https://www.reddit.com/r/redditdev/comments/1ovi6a/just_bought_reddit_gold_how_do_i_change_default/"}, {"author": "rog1121", "created_utc": 1382205019, "gilded": 0, "name": "t3_1osc2c", "num_comments": 4, "score": 1, "selftext": "I'm making a bot to handle the reputation system over at /r/hardwareswap We basically have one top level comment and then a user replies under it to verify the trade. This is what I have so far, obviously the bottom part of the code doesn't work import praw r = praw.Reddit('rog1121') r.login('rog1121', '') subreddit = 'hardwareswap' submission = r.get_submission(submission_id='1od22u') flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: if comment.body == 'second' and comment.is_root == False: new_flair = str(int(comment.author_flair_css_class) + 1) # Update the child flair if not comment.author_flair_text: comment.subreddit.set_flair(comment.author, '', new_flair) elif not comment.author_flair_css_class: comment.subreddit.set_flair(comment.author, comment.author_flair_text, '1') elif not comment.author_flair_text and not comment.author_flair_css_class: comment.subreddit.set_flair(comment.author, '', '1') else: comment.subreddit.set_flair(comment.author, comment.author_flair_text, new_flair) # Update the parent flair if not comment.parent_id.author_flair_text: comment.parent_id.subreddit.set_flair(comment.author, '', new_flair) elif not comment.parent_id.author_flair_css_class: comment.parent_id.subreddit.set_flair(comment.author, comment.author_flair_text, '1') elif not comment.parent_id.author_flair_text and not comment.author_flair_css_class: comment.parent_id.subreddit.set_flair(comment.author, '', '1') else: comment.parent_id.subreddit.set_flair(comment.author, comment.author_flair_text, new_flair)", "title": "Parent_id of comment actions?", "url": "https://www.reddit.com/r/redditdev/comments/1osc2c/parent_id_of_comment_actions/"}, {"author": "Admsugar", "created_utc": 1381978256, "gilded": 0, "name": "t3_1omaox", "num_comments": 1, "score": 1, "selftext": "I meant importing not installing. woops. Im trying to get praw installed so I can start messing around with reddits API and learn some new things, I have succesfully installed pip I think and in terminal (mac) I entered 'pip install praw' I got this result in terminal: ------ Downloading/unpacking praw Downloading praw-2.1.10.tar.gz (83kB): 83kB downloaded Running setup.py egg_info for package praw Downloading/unpacking requests>=1.2.0 (from praw) Downloading requests-2.0.0.tar.gz (362kB): 362kB downloaded Running setup.py egg_info for package requests Downloading/unpacking six (from praw) Downloading six-1.4.1.tar.gz Running setup.py egg_info for package six Downloading/unpacking update-checker>=0.6 (from praw) Downloading update_checker-0.6.tar.gz Running setup.py egg_info for package update-checker Requirement already satisfied (use --upgrade to upgrade): setuptools in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/setuptools-1.1.6-py2.7.egg (from update-checker>=0.6->praw) Installing collected packages: praw, requests, six, update-checker Running setup.py install for praw Installing praw-multiprocess script to /Library/Frameworks/Python.framework/Versions/2.7/bin Running setup.py install for requests Running setup.py install for six Running setup.py install for update-checker Successfully installed praw requests six update-checker Cleaning up... ---- I figured it had worked but then when I try 'import praw' in python I'm getting an ImportError: No module named praw. I know this is probably a simple fix, just wanted to get this cleared up. Thanks", "title": "Installing praw into python", "url": "https://www.reddit.com/r/redditdev/comments/1omaox/installing_praw_into_python/"}, {"author": "nareik15", "created_utc": 1381848852, "gilded": 0, "name": "t3_1oi5fb", "num_comments": 2, "score": 3, "selftext": "So I've been extracting comments from a number of submissions recently for a project i'm doing which focusses on conversations and engagement. For those comment trees that have more than 200 and therefore are littered with \"more_comments\", I have been using the *replace_more_comments* function in praw, which as I understand it, makes a new API call for each of these it comes across . So i had a few questions: 1. Is there anyway to group together fetched comments to reduce the number of api calls required? i.e. at the moment if there are 30 \"more_comments\" markers, that means there are 30 api calls and thus a minute to wait for all comments. but if under each of these \"more_comments\", there is only 5 comments then that is a somewhat wasteful api call (given that each api call can retreive 100 comments) 2. Is there any way to mark the progress of the *replace_more_comments* functions? i.e. if there are 200 \"more_comments\" and the functions has retreived 100, being able to say that half have been retreived the purpose of this is really just to allow me to get some sense of how long I have left to wait for some of these things to load. :) Thanks in advance for any advice. :)", "title": "A number of questions about the \"replace_more_comments\" function in PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1oi5fb/a_number_of_questions_about_the_replace_more/"}, {"author": "JBHUTT09", "created_utc": 1381632011, "gilded": 0, "name": "t3_1obxel", "num_comments": 17, "score": 7, "selftext": "This is a bot that I built today in order to help moderate /r/AnimeSuggest. It checks new posts and pm the author if they do not tag their post with flair within 3 minutes. Here is my code: import praw import time import re import sys def main(): #log in username=open(\"Username.txt\",\"r\").read().rstrip() password=open(\"Password.txt\",\"r\").read().rstrip() user_agent=(\"Auto flair moderator for /r/AnimeSuggest\") r=praw.Reddit(user_agent=user_agent) r.login(username=username,password=password) JBHUTT09=\"JBHUTT09\" #initialize array to hold checked posts already_done=[] #specify subreddit subreddit=r.get_subreddit('animesuggesttesting') #start endless loop while True: for submission in subreddit.get_new_by_date(limit=5): if submission.id not in already_done: time.sleep(180) if (submission.link_flair_text is None): author=submission.author subLine='You have not tagged your post.' msg=\"[Your recent post](%s) in /r/AnimeSuggest does not have any flair. Please add flair to your post. \\n\\n If you are unsure of how to add flair, refer to [this post](http://redd.it/1ml8km).\\n\\n*This is a brand new bot I wrote to help mod. It's also the first bot I've written. If shit is broken and everything is going to hell, please pm me: /u/JBHUTT09.\" % submission.short_link r.send_message(author,subLine,msg) JBSubLine=\"Message sent to /u/%s\"%author JBmsg=\"Message sent concerning [this post.](%s)\"%submission.short_link r.send_message(JBHUTT09,JBSubLine,JBmsg) print \"Sent message to: %s\" % author already_done.append(submission.id) print \"Posts checked: %d\" % len(already_done) main() Now, this code was running absolutely fine this afternoon. No problems at all. I had to shut down my computer so I had to stop the bot. That's when the trouble started. Ever since I reran the bot, it's been crashing when sending PMs. But it only crashes sometimes and I think it's based on the time delay. I had set it to a 20 second delay for testing purposes when trying to figure out the problem, because I wasn't going to wait 3 minutes. It suddenly worked. So I thought it was a weird one time thing. But when I put the delay back to 3 minutes, it started crashing again. Does anyone have any idea why this happens? **Edit:** Crash message: Traceback (most recent call last): File \"C:\\Python27\\botTest.py\", line 34, in main() File \"C:\\Python27\\botTest.py\", line 27, in main r.send_message(author,subLine,msg) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 303, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 205, in wrapped return function(obj, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 1909, in send_message retry_on_error=False) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 141, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 479, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 348, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 321, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\adapters.py\", line 354, in send raise ConnectionError(e) ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /api/compose/.json (Caused by : [Errno 10054] An existing connection was forcibly closed by the remote host)", "title": "Having issues with a time delay in a bot", "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "sixteenmiles", "created_utc": 1381581077, "gilded": 0, "name": "t3_1oabiy", "num_comments": 6, "score": 1, "selftext": "I don't really have any idea what I'm doing but I'm trying to get my head around python/praw with a simple (I hope) little project. The idea is to parse the entire history of a specific sub and return percentage statistics based on the prefix of each post, but I can't figure out the method for doing this correctly. get_new(limit=None) This only goes back so far (about 29 pages by my count), but ideally what I would like to do is start at the newest post and iterate backwards until I hit the beginning or as far back as I can go. Is this even possible? Any help appreciated, thanks.", "title": "[PRAW] Is it possible to get the content of an entire sub?", "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": "Lauren_of_Lore", "created_utc": 1381266898, "gilded": 0, "name": "t3_1o0hcg", "num_comments": 6, "score": 3, "selftext": "I am trying to write a bot for my subreddit that will automatically assign flair to new posts, and allow mobile users to change the flair by posting a comment in their thread. I was wondering two things: * How do I set flair on a submission? * How do I get a submission's current flair? I read a list of things you can do with PRAW on the PRAW website but didn't see anything about thread flair listed, only user flair. I also tried searching this subreddit and didn't find anything :/ Thank you :)", "title": "PRAW thread flair?", "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "Gambit89", "created_utc": 1381220386, "gilded": 0, "name": "t3_1nyznn", "num_comments": 2, "score": 1, "selftext": "Hello, I'm having a caching issue and was hoping you guys can shed some light as to why PRAW is behaving like this. I have an PRAW bot that polls /r/all/comments/ every 30 seconds, as recommended per the docs, but it takes every 4th request to get a new batch of comments (approx. 2 minutes) and thus I'm missing ~1.5 minutes of comments. Yet, if I navigate to it in my browser, I get the newest comment for that second. I estimate there are about 100 new comments every 15 secs, so that's good if /r/all/comments/ updates every fetch - which it doesn't with PRAW. To operate with the 2 minute cache, I'll have to fetch 2 mins/15 secs = ~8 times, or almost to the 1000 limit. I'd rather not do this because I don't quite know where the cache cutoff is exactly, and I might miss some comments in between requests. E.g. comments revealed by cache |------ ~2 mins ------| margin of error |------ ~2 mins -----| |++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++| actual comments i.e, there could be some comments in the margin of error which aren't being revealed. But I suppose a majority of comments are better than the 25% I'm getting with my current method. But maybe my method can be fixed... is there a way to get PRAW to fetch new comments for /r/all/comments/ every 30 seconds?", "title": "PRAW is caching comments for 2m when configured for 30s?", "url": "https://www.reddit.com/r/redditdev/comments/1nyznn/praw_is_caching_comments_for_2m_when_configured/"}, {"author": "rreyv", "created_utc": 1380776725, "gilded": 0, "name": "t3_1nmwta", "num_comments": 6, "score": 3, "selftext": "I've written a bot that is supposed to update just the sidebar description at regular intervals. However, the code is also updating the spam settings to go from [this](http://i.imgur.com/yFHEXlU.png) to [this](http://i.imgur.com/6HfvFW2.png). Initially, I felt that this was just cosmetic and not really changing anything in the backend of reddit, but then I viewed the moderation log and I noticed that the changes were being logged so something was probably changed. [Screenshot](http://i.imgur.com/4gWgbcU.png). This is code I ran to test this. All I want to do is add a line at the end of the description. Could you guys tell me if something is wrong with my code here? If not, is this some sort of a bug in PRAW? What are the spam settings being changed to? Also note that none of the other settings are changed that I am aware of. It's just the spam settings. import praw import HTMLParser if __name__==\"__main__\": r = praw.Reddit('/r/cricket sidebar updating and match thread creating bot by /u/rreyv. Version 1.0') #reddit stuff r.login() #sign in! subredditName='rreyv' settings=r.get_settings(subredditName) description_html=settings['description'] html_parser = HTMLParser.HTMLParser() description = html_parser.unescape(description_html) description=description + \"\\n\\nAdded Something\" settings=r.update_settings(r.get_subreddit(subredditName),description=description)", "title": "Potential bug with updating sidebar in PRAW, or am I doing something wrong?", "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "bVector", "created_utc": 1380738765, "gilded": 0, "name": "t3_1nll3d", "num_comments": 4, "score": 3, "selftext": "I'm looking to setup OAuth on my app using praw, but I'm a little confused as to what I need to persist the OAuth tokens across app restarts. After using the code I'm given an access key and a refresh token. praw documentation states that the access code is only valid for 60 minutes, but doesn't explicitly state that the refresh token is permanent. Is this what I should save? Do I need anything else to refresh access after a restart of the app?", "title": "Saving OAuth codes in db", "url": "https://www.reddit.com/r/redditdev/comments/1nll3d/saving_oauth_codes_in_db/"}, {"author": "im14", "created_utc": 1380222181, "gilded": 0, "name": "t3_1n7164", "num_comments": 6, "score": 8, "selftext": "I need to scrape certain bot's comments to create a statistics page (I don't control the bot). I need to be able to parse every single comment of the bot. I know there's a 'limit' variable that can't go higher than 1,000. Through Reddit UI, I can continuously press \"Next\" to get all user's comments. Is there an equivalent in PRAW? Thanks.", "title": "Can I get all comments of a user by calling something like user.get_comments().next() repeatedly?", "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "titusjan", "created_utc": 1379754094, "gilded": 0, "name": "t3_1mtvu2", "num_comments": 1, "score": 3, "selftext": "A have two related questions about PRAW: 1. Is it possible to access the raw JSON response that is send by the reddit server so that I can archive it? 2. Is it possible to create praw objects, for instance a *praw.objects.Comment* object, without a reddit_session? When I look at the [praw Comment documentation](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Comment) it seems that a reddit_session is mandatory. However I'd like to instantiate a comment off-line, preferably using the JSON that i've archived earlier.", "title": "PRAW - saving JSON response and using that to instantiate praw objects", "url": "https://www.reddit.com/r/redditdev/comments/1mtvu2/praw_saving_json_response_and_using_that_to/"}, {"author": "Delocaz", "created_utc": 1379751735, "gilded": 0, "name": "t3_1mtup9", "num_comments": 1, "score": 2, "selftext": "So, I made a bot a while ago that'd stat an account, and upload the result to a self post in its own subreddit. Now, that bot doesn't work, and I have no idea why. Here's the code: http://pastebin.com/ngK2AQgt The error is at this line: r = praw.Reddit(user_agent=\"RedStatsBot by u/Delocaz\") And the error is: Traceback (most recent call last): File \"E:\\Filer\\Kode\\Python\\RedStatsWeb\\RedStats_Bot.py\", line 21, in user = r.get_redditor(USER_TO_TEST) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 839, in get_redditor return objects.Redditor(self, user_name, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 617, in __init__ fetch, info_url) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 71, in __init__ self._populated = self._populate(json_dict, fetch) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 125, in _populate json_dict = self._get_json_dict() File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 117, in _get_json_dict as_objects=False) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 342, in _request response = handle_redirect() File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 315, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python33\\lib\\site-packages\\requests\\sessions.py\", line 438, in send r = adapter.send(request, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests\\adapters.py\", line 292, in send timeout=timeout File \"C:\\Python33\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 423, in urlopen conn = self._get_conn(timeout=pool_timeout) File \"C:\\Python33\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 238, in _get_conn return conn or self._new_conn() File \"C:\\Python33\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 205, in _new_conn strict=self.strict) File \"C:\\Python33\\lib\\http\\client.py\", line 737, in __init__ DeprecationWarning, 2) File \"C:\\Python33\\lib\\idlelib\\PyShell.py\", line 60, in idle_showwarning file.write(warnings.formatwarning(message, category, filename, AttributeError: 'NoneType' object has no attribute 'write' If I copy the code up to line 20, it apparently works, for some reason.", "title": "PRAW - Code errors at line 20 (r = praw.Reddit(blahblah))", "url": "https://www.reddit.com/r/redditdev/comments/1mtup9/praw_code_errors_at_line_20_r_prawredditblahblah/"}, {"author": "mgrieger", "created_utc": 1378958367, "gilded": 0, "name": "t3_1m85d0", "num_comments": 34, "score": 14, "selftext": "Hey everyone, I am the dev of /u/VerseBot (which runs on /r/Christianity and /r/TrueChristian), and I have come across a very strange issue lately. The bot was working perfectly fine for over two weeks, and then all of a sudden it started spamming, replying to the same comment multiple times. Here's what seems to happen when the problem arises: - Bot finds a comment that it needs to reply to - Bot replies to comment with appropriate response - PRAW throws a HTTPError exception (usually a 502 Bad Gateway error) from the PRAW reply calls within commenter.py - Comment id of comment bot replies to does not get stored in the PostgreSQL database, OR a Python set - Bot starts spamming on the comment I am really clueless on why this is happening as it still sometimes works fine, but most of the time just spams like crazy. Has something changed in the reddit API that requires a PRAW update? Does anybody else running a bot with PRAW and/or Heroku have this problem? I have tried several variations of try/except blocks on the PRAW reply calls, but nothing seems to work. [Here is my code on GitHub.](https://github.com/matthieugrieger/versebot) Hopefully somebody knows a solution to this annoying problem that has cropped up recently. Thanks!", "title": "Bot randomly started spamming with no changes to the code", "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "qviri", "created_utc": 1377570958, "gilded": 0, "name": "t3_1l5tsa", "num_comments": 6, "score": 2, "selftext": "While working on an idea I had, I tried getting the contents of http://www.reddit.com/r/polandball/about/contributors/ programmatically. That particular subreddit's contributors page is public. For a contrary example, http://www.reddit.com/r/vancouver/about/contributors gives me a \"page not found\" error (I'm not a mod there). Using stock PRAW, I can't get get\\_subreddit('polandball').get\\_contributors() without being logged in as a moderator for that subreddit. If I modify PRAW source to have mod=False decorator on get\\_contributors() in \\_\\_init\\_\\_.py (ugly I know), it comes through just fine, even without r.login(): >>> import praw >>> r = praw.Reddit(user_agent='polandballscraper/0.01') >>> r.get_subreddit('polandball').get_contributors() If I try the changed PRAW with get\\_subreddit('vancouver').get\\_contributors(), I get a 404 from requests (\"requests.exceptions.HTTPError: 404 Client Error: Not Found\"). If I had mod permissions on /r/vancouver, would I be able to see http://www.reddit.com/r/vancouver/about/contributors and do get\\_subreddit('vancouver').get\\_contributors() after an r.login()? Is changing get\\_contributors() so that mod permissions are only necessary for subreddits that don't make the contributors list public feasible and desirable? It would cause a runtime error if you don't have permissions just as it does now if you don't have mod, I'm just not sure about the elegance about doing it in the function somehow rather than in the decorator.", "title": "PRAW: why is Subreddit.get_contributors() mod-only?", "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "Eliminioa", "created_utc": 1377488954, "gilded": 0, "name": "t3_1l3hkp", "num_comments": 2, "score": 1, "selftext": "So I'm just delving into PRAW, and I'm having trouble getting, well, everything to work. It seems that many of the basic functions refuse to take input. The most pressing one is the r.login() function. No matter how i pass it data, it returns a huge error report that ends with: File \"C:\\Python33\\lib\\idlelib\\PyShell.py\", line 60, in idle_showwarning file.write(warnings.formatwarning(message, category, filename, AttributeError: 'NoneType' object has no attribute 'write' Any idea what's going wrong?", "title": "PRAW help: NoneType has no attribute write", "url": "https://www.reddit.com/r/redditdev/comments/1l3hkp/praw_help_nonetype_has_no_attribute_write/"}, {"author": "BJ2094", "created_utc": 1377393857, "gilded": 0, "name": "t3_1l185p", "num_comments": 1, "score": 1, "selftext": "I use this code to get the comments from a page: comments = praw.helpers.flatten_tree(post.comments) c = str(comments[0]) but when i try to print c, it only displays the first few words and an ellipsis. How can I fix this?", "title": "Printing submission comments doesn't print entire comment.", "url": "https://www.reddit.com/r/redditdev/comments/1l185p/printing_submission_comments_doesnt_print_entire/"}, {"author": "SauceProviderBot", "created_utc": 1377305702, "gilded": 0, "name": "t3_1kz820", "num_comments": 4, "score": 2, "selftext": "When I try and run: comments = r.get_all_comments() I get: /Library/Python/2.7/site-packages/praw/__init__.py:639: DeprecationWarning: Please use `get_comments('all', ...)` instead DeprecationWarning) So instead i'm using : comments = r.get_comments('all',limit = 5000) But I feel this is inefficient. Ideas? This is my first reddit bot.", "title": "Having an issue using get_all_comments in PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/1kz820/having_an_issue_using_get_all_comments_in_praw/"}, {"author": "Amablue", "created_utc": 1377235845, "gilded": 0, "name": "t3_1kxd1n", "num_comments": 1, "score": 1, "selftext": "This seems like it ought to be really basic. Lets say I have a comment, like [this one](http://www.reddit.com/r/funny/comments/1kx27i/i_started_chatting_with_a_local_police_officer/cbthe8d). In my script I run this: import praw reddit = praw.Reddit(\"Misbhaving robot :(\") comment = reddit.get_info(thing_id = \"t1_cbthe8d\") print type(comment), comment print comment.replies When I run it, this is my output: Have you gotten a reply yet? Or the bail money? [] That comment clearly has a bunch of replies, why am I getting nothing?", "title": "How can I get the replies to a comment with praw?", "url": "https://www.reddit.com/r/redditdev/comments/1kxd1n/how_can_i_get_the_replies_to_a_comment_with_praw/"}, {"author": "Squidifier", "created_utc": 1377102385, "gilded": 0, "name": "t3_1kt95a", "num_comments": 3, "score": 2, "selftext": "Hey! Just wondering, is there a way to use PRAW to set the subreddit sticky, as is outlined in [THIS post.](http://redd.it/1jr429) Thanks!", "title": "Using PRAW to set Subreddit Sticky?", "url": "https://www.reddit.com/r/redditdev/comments/1kt95a/using_praw_to_set_subreddit_sticky/"}, {"author": "MrFanzyPanz", "created_utc": 1376978060, "gilded": 0, "name": "t3_1kpuzo", "num_comments": 6, "score": 5, "selftext": "Hi, redditdev! I'm trying to use PRAW to build an H index score for a user based on their combined link/comment history. The code treats every comment/link submitted by the user as a publication, and every reply/root comment on each comment/link as a citation. The problem is that the code makes too many requests. I'm using get_ comments() and get_ submitted() to pull a user's history (I wrote the code before seeing get_ overview() -___- ). My code is pasted below. while True: h_index_list = list() try: for comment in r.get_redditor(account).get_comments(limit = None): h_index_list.append(len(comment.replies)) print(len(comment.replies)) except requests.exceptions.HTTPError: print('Server Error') continue except IndexError: h_index_list.append(0) try: for link in r.get_redditor(account).get_submitted(limit = None): h_index_list.append(len(link.comments)) print(len(link.comments)) except requests.exceptions.HTTPError: print('Server Error') continue except IndexError: h_index_list.append(0) break In both the 'for comment' and 'for link' loops, the 'comment.replies' and 'link.comments' calls are treated as requests. I have a different script which makes this call: for post in r.get_subreddit(subreddit).get_hot(limit=500): unique_id = post.name subred = post.subreddit.display_name if not post.author: user = '*' else: user = post.author.name createdtime = post.created_utc This code pulls items in groups of 100, despite the numerous 'post.___' calls it makes. I've updated praw to the newest version. Are .replies and .comments simply slower?", "title": "My code is making too many requests.", "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "CMThF", "created_utc": 1376923121, "gilded": 0, "name": "t3_1ko256", "num_comments": 6, "score": 1, "selftext": "Hello everyone! I hope you can help me. I'm trying to get a dataset of all submissions to reddit for research purposes at my University. I use python and praw for this task. My approach is that I get a generator object from genObj = r.get_subreddit('all').get_new(limit=None) and want to iterate it via for s in genObj: Works perfectly fine, for about 500k requests. Then I get a HTTPError: 504 Server Error: Gateway Time-out, which is raised when accessing the generator object. When catching the exception and further accessing the generator, two different things happen: Either the generator object died due to a killed session, or I can continue for another ~500k requests until again the generator is killed. I tried to time.sleep every few requests or when catching an exception, thinking too many requests might have been the problem - but this doesn't help. Since recreating the generator would result in restarting from the most recent post, and skipping to a certain submission id and continuing from there is not supported by praw, the closed session pretty much ends all efforts. The most basic version (without the exception catching) is [here](https://gist.github.com/CMThF/f5745c1971743c5f755c) to be found. It runs on an Ubuntu 12.04.2 server. Running it on a different machine and different internet connection did not change this behavior, so possible parallel working Reddit API or praw scripts should not be the problem. My advisor used the same script for crawling some time ago, where it worked perfectly fine. Thank you for your time, regards, C.", "title": "Using PRAW, I get a Gateway Timeout when crawling Reddit submissions", "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "MrFanzyPanz", "created_utc": 1376609016, "gilded": 0, "name": "t3_1kgauh", "num_comments": 3, "score": 5, "selftext": "I wanted to rebuild a comment tree to make some pretty graphs. I've been using comment.replies to pull child posts, and most of these can requests can be funneled through one call. However, while the code works, after 5 levels down the comment tree it stops pulling comments unless a \"more_comments\" call is made. This call has greatly increased the number of queries my code makes, and following PRAW's 2 second delay, larger comment trees take forever to pull. Is there a more efficient way to go through the whole comment tree in PRAW? Thanks, /r/redditdev!", "title": "In PRAW, is there a way to request an entire comment tree without using more_comments?", "url": "https://www.reddit.com/r/redditdev/comments/1kgauh/in_praw_is_there_a_way_to_request_an_entire/"}, {"author": "cosileone", "created_utc": 1376599278, "gilded": 0, "name": "t3_1kfy6s", "num_comments": 5, "score": 1, "selftext": "I've had a look through the praw docs but trying to post things (comments/image links) requires the user to manually enter a captcha. Is there an automated way to post images or is there some requirement I'm missing?", "title": "How do you post images using praw?", "url": "https://www.reddit.com/r/redditdev/comments/1kfy6s/how_do_you_post_images_using_praw/"}, {"author": "pipsqueaker117", "created_utc": 1376349543, "gilded": 0, "name": "t3_1k8ocm", "num_comments": 8, "score": 9, "selftext": "I wrote a simple bot for reddit which needs to run every ten minutes or so to adequately perform its job. As I didnt want to leave my computer turned on forever in order to let the script I decided to try hosting it on google app engine. Now I'm a newb to both app engine and praw, so bear with me here. When I uploaded my script to the app engine it threw me an error saying that it could not find/import any packages named \"praw.\" That's fine. To fix this, and all the later dependency errors that came up, I just ran the following commands in a python shell import dependencypackage print(dependencypackage) I then copied all the files/folders that the shell spit out into my app's src directory. By doing this a few times I managed to trade one error for another, but now I'm running into the following error: Traceback (most recent call last): File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/SpelunkyDailyBot.py\", line 2, in import praw File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/praw/__init__.py\", line 34, in from praw import decorators, errors ImportError: cannot import name decorators I'm stumped by this one, because when I check the praw folder in my app's src dir there exist both a decorators.py and a decorators.pyc. So the question here is twofold- 1) What am I doing wrong here? Is the decorators module not supported by App Engine or something? Can I even use praw in a GAE script? I'd really love to get this bot on the cloud, so this question is the main one 2) Is there any easier way to do this? I feel like the process I've followed to get all the dependencies in the src is one of the least efficient possible. Is there some sort of tool I can use, some sort of magic button I can use in my IDE (pycharm or pydev for eclipse) which will take care of uploading all these dependencies for me? Answers and explanations are welcome (duh) Looking forward to your answers! EDIT: I made some changes to how the packages were imported, and now I'm getting the following error Traceback (most recent call last): File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/SpelunkyDailyBot.py\", line 2, in import praw File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/praw/__init__.py\", line 43, in from update_checker import update_check File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\", line 87, in class UpdateChecker(object): File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\", line 93, in UpdateChecker @cache_results File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\", line 43, in cache_results filename = os.path.join(gettempdir(), 'update_checker_cache.pkl') File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/tempfile.py\", line 45, in PlaceHolder raise NotImplementedError(\"Only tempfile.TemporaryFile is available for use\") NotImplementedError: Only tempfile.TemporaryFile is available for use", "title": "Is there any way to easily host a reddit bot using praw on Google App Engine?", "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "TheNoodlyOne", "created_utc": 1375779086, "gilded": 0, "name": "t3_1jsuht", "num_comments": 3, "score": 0, "selftext": "I'm working on a basic bot which I call the \"Six Degrees of Karmanut.\" Basically, it loads however many comments of the first user, gets the authors of any replies, and repeats the process for each of them. Even if I'm only loading five comments, it takes at least twelve seconds for each redditor, because you have to make a separate request for the children. Is there a way to include these children in the request for comments? Code: import praw USERNAME_1 = \"TheNoodlyOne\" USERNAME_2 = \"Karmanut\" MAX_DEPTH = 5 MAX_COMMENTS = 5 r = praw.Reddit(user_agent=\"six degrees of Karmanut\") queue = [(USERNAME_1,0)] processed = [] while True: if len(queue) == 0: break u = queue.pop(0) if u[1] > MAX_DEPTH: break done = False for p in processed: if u[0] == p[0]: done = True print \" \"+u[0]+\" (\"+str(u[1])+\")\" if done == True: continue processed.append(u) red = r.get_redditor(u[0]) for c in red.get_comments(limit=MAX_COMMENTS): rep = c.replies for c2 in rep: queue.append((str(c2.author), u[1]+1))", "title": "Get comment and replies in one request?", "url": "https://www.reddit.com/r/redditdev/comments/1jsuht/get_comment_and_replies_in_one_request/"}, {"author": "IAmAnAnonymousCoward", "created_utc": 1375555569, "gilded": 0, "name": "t3_1jmzm7", "num_comments": 15, "score": 7, "selftext": "Didn't find anything about it in the reddit API or PRAW documentation. Is this something the admins object to?", "title": "[PRAW] Can a bot give gold?", "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "isolani", "created_utc": 1375460604, "gilded": 0, "name": "t3_1jklbu", "num_comments": 1, "score": 3, "selftext": "Does `get_unread()` also get unread mod mail (it doesn't seem like it should, because it's a part of `praw.__init__.PrivateMessagesMixin(*args, **kwargs)`)? Is there a way to only get unread mod mail through `get_mod_mail()`? Edit: Thinking about it further, I think there are some apps out there that provide notifications for new mod mail, so surely this must be possible somehow (?)", "title": "[PRAW] Is there a function for getting unread mod mail?", "url": "https://www.reddit.com/r/redditdev/comments/1jklbu/praw_is_there_a_function_for_getting_unread_mod/"}, {"author": "droidBehavior", "created_utc": 1375226850, "gilded": 0, "name": "t3_1jduep", "num_comments": 0, "score": 2, "selftext": "I want to go to a subreddit, get all the comments, and view the 'more comments' section. If the id is 't3_1jce6p', what do i put for children? it says put in the children id's delimited by a comma, but I am not supposed to actually put in children 1 by 1? there are hundreds of them. this post helped a bit but I am still lost, http://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/ I want to return json of hidden comments, so what is the correct way to do this? and what if there are more comments after clicking 'more comments' here is the non working code I have come up with data = { 'link_id' : 't3_1jce6p', 'children' : 'cbdcft5', #not sure what to put here, but i think its a list of the children id's 'api_type': 'json' } client = requests.session() r = client.post('http://www.reddit.com/api/morechildren', data=data) j = json.loads(r.content) print j['data']['children'] right now I have a working program to type in a user name and return all their posts, but I want to go a step further and give the post as well as the question it answers(not even sure how to do this yet)", "title": "confused about morechildren", "url": "https://www.reddit.com/r/redditdev/comments/1jduep/confused_about_morechildren/"}, {"author": "naffarama", "created_utc": 1375189943, "gilded": 0, "name": "t3_1jcgsw", "num_comments": 2, "score": 1, "selftext": "I'm pretty new so apologies if this is a stupid. When I try to get a list of friends from the logged in user I end up getting a redirect exception. Inputting: import praw r = praw.Reddit('blah blah blah') r.login('User','****') r.user.get_friends() gives: RedirectException: Unexpected redirect from http://www.reddit.com/prefs/friends/.json to https://ssl.reddit.com/prefs/friends/.json Any ideas what's going on? Any help greatly appreciated.", "title": "Unexpected redirect getting friends list", "url": "https://www.reddit.com/r/redditdev/comments/1jcgsw/unexpected_redirect_getting_friends_list/"}, {"author": "MonkeyNin", "created_utc": 1375059806, "gilded": 0, "name": "t3_1j8wfe", "num_comments": 9, "score": 2, "selftext": "There are a list of subs, (ex: /r/earthporn , /r/spaceporn/ ) that if I upvote an image I want to download it using praw. Do I have to 1. Grab the list like normal submissions = r.get_subreddit('EarthPorn').get_hot(limit=10) 2. Another query [LoggedInRedditor.get_liked()](http://python-reddit-api-wrapper.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.LoggedInRedditor.get_liked) and check for collisions? Or am I missing a function somewhere on submissions?", "title": "[PRAW] download images in subs, if you upvote them?", "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "IAmAnAnonymousCoward", "created_utc": 1374775886, "gilded": 0, "name": "t3_1j1j6q", "num_comments": 2, "score": 3, "selftext": "Example: Searching for http://www.livememe.com/vg972qp in /r/AdviceAnimals. Is it possible to get the submission object in such a case? RedirectException: Unexpected redirect from http://www.reddit.com/r/all/search/.json?q=http%3A%2F%2Fwww.livememe.com%2Fvg972qp&sort=new to http://www.reddit.com/submit.json?url=http%3A%2F%2Fwww.livememe.com%2Fvg972qp P.S.: I'm still looking for an answer to [this question](http://www.reddit.com/r/redditdev/comments/1hvgjf/praw_actually_getting_the_top_200_submissions/) as well. It was caught by the spam filter and unfortunately to this day, the mods didn't find the time to approve it.", "title": "[PRAW] RedirectException when searching for URL if there's only one result.", "url": "https://www.reddit.com/r/redditdev/comments/1j1j6q/praw_redirectexception_when_searching_for_url_if/"}, {"author": "Dominoed", "created_utc": 1374709743, "gilded": 0, "name": "t3_1izqfy", "num_comments": 21, "score": 5, "selftext": "Okay, so, I would like help with installing PRAW. At this point, I need step-by-step instructions. I went on the PRAW website, and it said that the reccomended way to install PRAW was to use \"pip\". I went and tried installing pip manually, but it didn't work well. So I decided to go on the website to find installation instructions. It said to use \"virtualenv\" to install pip. On the virtualenv website, it says to use pip to install virtualenv. So, what is the easiest, quickest way to install PRAW? At this point all I have is Python 3.3.", "title": "Help with installing PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "isolani", "created_utc": 1374641360, "gilded": 0, "name": "t3_1ixqu0", "num_comments": 24, "score": 12, "selftext": "Do you guys run your PRAW bots on a VPS or heroku or...?", "title": "[PRAW] Where do you all host your python-based bots?", "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "xiggy", "created_utc": 1374430339, "gilded": 0, "name": "t3_1iredp", "num_comments": 9, "score": 18, "selftext": "This bot grabs 1000 submissions from /r/gifs. It then filters out the submissions that are from 'i.imgur.com' and adds them to a list. After that it cycles through the list and embeds one image into a HTML file as a background-image. The webpage then refreshes every X amount of seconds to display the new image. The bot also grabs the submission title and overlays it at the bottom. I had an old monitor I wasn't using and was looking for a project to work on with my RPi. I figured I would just use the monitor as a sort of digital picture frame. Then I thought, lets make it more interesting by using gifs instead of images. If you have any suggestions on making it more efficient, I'd be glad to hear them. This is my first time working with Python and PRAW, so I'm sure there are better ways of doing this. import praw import time r=praw.Reddit(user_agent='Dynamic gif Slideshow by /u/xiggy') subreddit = r.get_subreddit('gifs') submissions = subreddit.get_hot(limit=1000) imglinks = [] used = [] c = 0 while True: for submission in submissions: if submission.id not in used: if submission.domain == 'i.imgur.com': tit = submission.title #tit... lol imglinks.append(submission.url) used.append(submission.id) html_file = open(\"style.css\", \"w\") html_file.write(\"body { margin:0px; padding: 0px;width:100%; height:100%; background:url(\\'\" + imglinks[c] + \"\\') center center no-repeat; background-size:contain; overflow:hidden; background-color:#121211; position:relative; font-family: sans-serif; } #title { position:absolute; width:100%; min-height:60px; text-align:center; color:#FFF; bottom:0px;v left:0px; background:rgba(0,0,0,0.6); line-height:60px; align:middle; font-weight: bold; font-size: 125%; }\") html_file.close() html_file = open(\"bg-test.html\", \"w\") html_file.write(\"\" + tit + \"\") html_file.close() print imglinks[c] c += 1 time.sleep(60) time.sleep(1300)", "title": "It's not much, but it's my first bot. I made it to run on my Raspberry Pi that's connected to a display.", "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "Amablue", "created_utc": 1374118831, "gilded": 0, "name": "t3_1ijb3m", "num_comments": 2, "score": 2, "selftext": "So I've been trying to run a script and I keep getting this error: Traceback (most recent call last): File \"__main__.py\", line 14, in main() File \"__main__.py\", line 11, in main bot.go() File \"c:\\path\\to\\my\\script.py\", line 381, in go before_id = self.scan(before_id) File \"c:\\path\\to\\my\\script.py\", line 161, in scan if self.do_stuff(comment): File \"c:\\path\\to\\my\\script.py\", line 254, in do_stuff comments = self.get_thread_comments(orig_comment) File \"c:\\path\\to\\my\\script.py\", line 315, in get_thread_comments new_comments = reply.comments() File \"C:\\Program Files\\Python27\\lib\\site-packages\\praw\\objects.py\", line 583, in comments not in self.submission._comments_by_id] AttributeError: 'NoneType' object has no attribute '_comments_by_id' The lines in question are as follows: for reply in item.replies: if type(reply) is praw.objects.MoreComments: new_comments = reply.comments() #Crashes here for comment in new_comments: stack.append(comment) stack.append(reply) The call to comments() is dying for some reason that's a complete mystery to me. What's going on?", "title": "Error when running a PRAW script", "url": "https://www.reddit.com/r/redditdev/comments/1ijb3m/error_when_running_a_praw_script/"}, {"author": "_Skrillex_", "created_utc": 1374103351, "gilded": 0, "name": "t3_1iirrw", "num_comments": 2, "score": 2, "selftext": "I'm new to Python and PRAW. I just installed PRAW today and was following some example code I found online. I keep getting an error when it reaches the line r.login(). Traceback (most recent call last): File \"C:/Users/Sam/Documents/learning.py\", line 7, in r.login() File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 1120, in login self.request_json(self.config['login'], data=data) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 342, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 313, in handle_redirect response = self.handler.request(request=request.prepare(), AttributeError: 'Request' object has no attribute 'prepare' I took out the r.login() part and it ran until it got to: for submission in subreddit.get_hot(limit=10): Then it gave me the same AttributeError: 'Request' object has no attribute 'prepare'. I'm sure this is something simple, but I'm a complete beginner and clueless as to what it is.", "title": "Just installed PRAW. Keep getting error when trying to running a script.", "url": "https://www.reddit.com/r/redditdev/comments/1iirrw/just_installed_praw_keep_getting_error_when/"}, {"author": "NH4ClO4", "created_utc": 1373921311, "gilded": 0, "name": "t3_1id53y", "num_comments": 5, "score": 3, "selftext": "I can't seem to find anything in the PRAW docs detailing a variable that holds the text of a message retrieved with get_unread(). I tried selftext in hopes that would work, as it was the closest I could find, but it didn't work. What is the magic variable?", "title": "[PRAW] How to get the full text of a message?", "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "MrFanzyPanz", "created_utc": 1373612799, "gilded": 0, "name": "t3_1i51by", "num_comments": 9, "score": 4, "selftext": "A picture of the traceback can be found [here](http://imgur.com/lY8vSLd). My code simply pulls data regarding submissions from a large number of subreddits and places them in a database. I'm not entirely sure what's happened here, but it seems like an internal error from praw. Maybe one of you guys can help me figure it out? Thanks!", "title": "My PRAW code ran for 3 days then died.", "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "MrFanzyPanz", "created_utc": 1373063895, "gilded": 0, "name": "t3_1hptt3", "num_comments": 9, "score": 10, "selftext": "Hello, Redditdev! I am a UCLA student, and my roommates and I are conducting a large research project on reddit communities. We are planning on taking data hourly from reddit using praw and about 10 VMs for 1 month. The code will not upvote or downvote anything, or interact with reddit in any way other than simply pulling data. The user-agent names will follow the format user_agent = 'UCLA Reddit Research Project: __' with the __ being filled by the VM number. I just wanted to let you guys know that these bots are harmless, and also to say thank you so much for your help over the last 6 months as I worked on getting this off the ground! You guys are the best!", "title": "Large Reddit Research Project", "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "zzpza", "created_utc": 1372679822, "gilded": 0, "name": "t3_1hf6fi", "num_comments": 4, "score": 2, "selftext": "I am a noob at both PRAW, python and CSS, so please bear with me... I am writing a script to post several weekly community threads. I am also working on a CSS mod that generates a menu at the top of the page, so I can link to the the latest version of each of the three weekly community threads, effectively making a 'sticky' post at the top of the page. The CSS menu takes the menu content (names and links to each post) from the text in the sidebar. Is there anyway to get PRAW to replace the link in the sidebar text for last weeks thread with the new one? I have tested the basic posting script in a test subreddit and it works OK. I created a new account to use to post the new community threads from, but each time I run the script I get asked to type in a captcha. How would this work when I run it from a cron job? Many thanks :)", "title": "[PRAW] Can it change the sidebar (description) text? Also, PRAW keeps asking for a captcha when I test the basic script.", "url": "https://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/"}, {"author": "TEST_BRAVERY", "created_utc": 1372487213, "gilded": 0, "name": "t3_1has5q", "num_comments": 1, "score": 5, "selftext": "So, if you say e.g. `subreddit.add_moderator(\"someusername\")`, then it adds that user as a moderator to the subreddit (assuming you're logged in as someone with sufficient permissions). How do you restrict permissions upon adding a moderator? E.g., suppose we want to add only \"mail\" permissions. I couldn't figure out how to do this from [reading the docs](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Subreddit.add_moderator). Thanks", "title": "(PRAW) How do you add moderators with non-full permissions?", "url": "https://www.reddit.com/r/redditdev/comments/1has5q/praw_how_do_you_add_moderators_with_nonfull/"}, {"author": "tgdm", "created_utc": 1372451336, "gilded": 0, "name": "t3_1h9rda", "num_comments": 6, "score": 1, "selftext": "I'm really new to all of this but I finally got my bot to work. It's over 1100 lines of code (a good chunk of it is from the 25+ varied responses it chooses from), all VERY organized but I'm sure there are ways I could condense it... but that's not the problem here. I have no idea how to get it to keep running. After it runs once, it just stops. When I run in via command prompt (by loading it with python), it runs to completion but then no new command line comes up. It just sort of hangs I guess. I'm using: import time import praw r = praw.Reddit(user_agent = '') subreddit = r.get_subreddit() subreddit_comments = subreddit.get_comments(limit = 10) subreddit_submissions = subreddit.get_new(limit = 10) r.login() already_doneComment = [] already_doneSubmission = [] ############# #skipping a bunch of strings and definitions that go here ############# while True: for comment in subreddit_comments: print comment.body if comment.id not in already_doneComment and definedFunction1(comment.body.lower()): comment.reply(STRING1) already_doneComment.append(comment.id) elif comment.id not in already_doneComment and definedFunction2(comment.body.lower()): comment.reply(STRING2) already_doneComment.append(comment.id) #submissions for comment in subreddit_submissions: print submission.title print submission.selftext if comment.id not in already_doneSubmission and (definedFunction1(submission.selftext.lower()) or definedFunction1(submission.title.lower())): submission.add_comment(STRING1) already_doneSubmission.append(submission.id) elif comment.id not in already_doneSubmission and (definedFunction2(submission.selftext.lower()) or definedFunction2(submission.title.lower())): submission.add_comment(STRING2) already_doneSubmission.append(submission.id) time.sleep(300) If that isn't enough context, I could try post more on pastebin I guess. I need help figuring out why the `while True:` and `time.sleep(300)` aren't working like I thought they would.", "title": "Need help getting bot to keep running", "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": "osmotischen", "created_utc": 1372384633, "gilded": 0, "name": "t3_1h7yno", "num_comments": 3, "score": 0, "selftext": "Hi, i'm very interested in using PRAW and i think its great, but i can't figure out how to get the actual text out of a submission. For example, if i do >>>t = list(r.get_content('http://www.reddit.com/r/redditdev', limit = 10)) >>>str(t[0]) '1:: [Praw] [Request] Use timestamps in submission query' however, i don't want the title of the post, i want the text which is > I recently had good luck using timestamps in an undocumented manner. It would be awesome to be able to use them in Praw and it might even get around the 1k assuming 'new' sorts chronologically (?). I've searched through the documentation and have read through a good portion of it, but i still haven't figured out how to do this, if it's possible. Can anyone help? Thank You! example post from here: http://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/ If it makes any difference i'm trying using this with a subreddit where all posts are text posts", "title": "how to get text of a submission", "url": "https://www.reddit.com/r/redditdev/comments/1h7yno/how_to_get_text_of_a_submission/"}, {"author": "DrewRWx", "created_utc": 1372370217, "gilded": 0, "name": "t3_1h7hbr", "num_comments": 1, "score": 0, "selftext": "I recently had good luck using [timestamps in an undocumented manner](http://www.reddit.com/r/bugs/comments/1h6mse/timestamps_not_recognized_in_searchbox/). It would be awesome to be able to use them in Praw and it might even get around the 1k assuming 'new' sorts chronologically (?).", "title": "[Praw] [Request] Use timestamps in submission query", "url": "https://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/"}, {"author": "SOTB-human", "created_utc": 1372362688, "gilded": 0, "name": "t3_1h7722", "num_comments": 4, "score": 1, "selftext": "I noticed that special characters are sometimes handled inconsistently in PRAW; they appear as HTML entities in some places and as normal Python characters in other places. To demonstrate this, I'll execute some code to use PRAW to repost the selftext of this submission as a comment on the submission. > quoted text \u0ca0_\u0ca0", "title": "(PRAW) HTML entities and special characters in selftext/body fields?", "url": "https://www.reddit.com/r/redditdev/comments/1h7722/praw_html_entities_and_special_characters_in/"}, {"author": "cgillett", "created_utc": 1372311200, "gilded": 0, "name": "t3_1h5u3a", "num_comments": 9, "score": 3, "selftext": "I'm trying to write a comment with PRAW, but I've realized that I don't know how to make comments multi-line. I want to use a variable in it too. Something like this: username = \"cgillett\" submission.add_comment(\"Username: \" + username + \"\\n hey\") Thanks for your help!", "title": "[PRAW] Creating multi-line comments", "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "jonathan_morgan", "created_utc": 1372205865, "gilded": 0, "name": "t3_1h2p60", "num_comments": 1, "score": 1, "selftext": "I am writing a collector for a research project, and I am trying to find a good way to pull in more than 500 comments related to a given post. PRAW does this (replace_more_comments()), but I inspected the attributes of a Comment object, and it doesn't look like it keeps the upvotes, downvotes, etc. around (output below). I did look at the output of _get_json_dict(), but at least for the comments I tested with, it just had this: {u'after': None, u'before': None, u'children': [], u'modhash': u''} Thanks for helping me understand! **attributes of Comment object** {'__class__': , '__delattr__': , '__dict__': , '__doc__': 'A class that represents a reddit comments.', '__eq__': , '__format__': , '__getattr__': , '__getattribute__': , '__hash__': , '__init__': , '__module__': 'praw.objects', '__ne__': , '__new__': , '__reduce__': , '__reduce_ex__': , '__repr__': , '__setattr__': , '__sizeof__': , '__str__': , '__subclasshook__': , '__unicode__': , '__weakref__': , '_get_json_dict': , '_populate': , '_update_submission': , 'approve': , 'clear_vote': , 'delete': , 'distinguish': , 'downvote': , 'edit': , 'from_api_response': , 'fullname': , 'is_root': , 'mark_as_nsfw': , 'mark_as_read': , 'mark_as_unread': , 'permalink': , 'remove': , 'replies': , 'reply': , 'report': , 'score': , 'submission': , 'undistinguish': , 'unmark_as_nsfw': , 'upvote': , 'vote': }", "title": "Does PRAW provide access to traits of comments like upvotes, downvotes and created date?", "url": "https://www.reddit.com/r/redditdev/comments/1h2p60/does_praw_provide_access_to_traits_of_comments/"}, {"author": "Acebulf", "created_utc": 1372190620, "gilded": 0, "name": "t3_1h2586", "num_comments": 0, "score": 0, "selftext": "File \"C:\\Documents and Settings\\Owner\\Desktop\\cmvbot.py\", line 316, in get_thread_comments new_comments = reply.comments() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 583, in comments not in self.submission._comments_by_id] AttributeError: 'NoneType' object has no attribute '_comments_by_id' Caused by this bit of code: if type(reply) is praw.objects.MoreComments: new_comments = reply.comments() Is there a reason why a MoreComments object would throw this error?", "title": "Praw AttributeError in MoreComments.comments()?", "url": "https://www.reddit.com/r/redditdev/comments/1h2586/praw_attributeerror_in_morecommentscomments/"}, {"author": "_deffer_", "created_utc": 1372171398, "gilded": 0, "name": "t3_1h1fi9", "num_comments": 9, "score": 0, "selftext": "I am very new to this, so bear with me. My goal: I want to create a bot that finds the following information from a specific subreddit: * Username * Title * Date and once it has that information, posts that information to a different subreddit, preferably into a new post every day. ______________________________ Where I am now: I've installed python, praw and pip and can run basic things like: >>> import praw >>> r = praw.Reddit(user_agent='_deffer_') >>> submissions = r.get_subreddit('gameswap').get_hot(limit=10) >>> [str(x) for x in submissions] and it outputs the title names. A gigantic success for me considering I have a hard time making a calculator add. A few questions: * What am I writing the code in? Just notepad? When I ran the above, I typed it in manually, but I'm assuming there's a much better/easier way. * How do I run the code once I have it where it should be stored? I'm sure I'll have a lot more questions, but I should probably stick with the very basics for now.", "title": "Okay. I've installed python, pip and praw (if it's even called 'installing.' What now?", "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "peteyMIT", "created_utc": 1371872739, "gilded": 0, "name": "t3_1gu6y5", "num_comments": 5, "score": 6, "selftext": "I'm running a script which sucks down new and top posts to /r/pics. It works fine for new but has recently stopped working for pics. Compare, for example, these results via the interpreter, after connecting via PRAW new_submissions_generator = r.get_subreddit('pics').get_new(limit=100) for submission in new_submissions_generator: name = submission.author.name name u'anoteduser' Works as expected (assigns and returns username). Similarly: hot_submissions_generator = r.get_subreddit('pics').get_hot(limit=25) for submission in hot_submissions_generator: nameh = submission.author.name nameh u'preggit' Works as expected (assigns and returns username). But: top_submissions_generator = r.get_subreddit('pics').get_top(limit=25) for submission in top_submissions_generator: namet = submission.author.name Traceback (most recent call last): File \"\", line 2, in AttributeError: 'NoneType' object has no attribute 'name' Breaks with an error that there is no attribute name for the object. What is going on? I am 99% sure this wasn't happening a few days ago...", "title": "Did something just break with PRAW's get_top method?", "url": "https://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/"}, {"author": "brucemo", "created_utc": 1371109938, "gilded": 0, "name": "t3_1g98a4", "num_comments": 5, "score": 9, "selftext": "[This ancient thread is related.](http://www.reddit.com/r/redditdev/comments/yd2c3/praw_question_resolving_a_comment_object_from/) That guy knew he had a comment, so he just did `submission = rh.get_submission(url = )`, and then looked at `submission._comments[0]`, and that was his comment. Assume I don't know whether my thing is a submission or a comment. If I don't want to parse the URL, presumably I can do this: submission = rh.get_submission(url = my_url) if submission.permalink == my_url: object = submission elif submission._comments[0].permalink == my_url: object = submission._comments[0] else So, is this the proper way to do this? I'm concerned because I don't have total control over the URL, and the URL might not be normalized. edit: I'm concerned this would crash if the URL is not normalized and there are no comments, but that would presumably be an easy fix. It may also make sense to test the comment first and see if its ID is present in the URL, then test the submission ID against the URL: submission = rh.get_submission(url = my_url) if len(submission._comments) > 0 and my_url.find(submission._comments[0].id) >= 0: object = submission._comments[0] elif my_url.find(submission.id) >= 0: object = submission else But this doesn't sound like the right way to do something that I would think should be easy to do: \"Here's a URL -- get the object.\"", "title": "PRAW: Looking for the proper way to get a submission or comment object from a URL", "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "redditpad", "created_utc": 1370927934, "gilded": 0, "name": "t3_1g3p8y", "num_comments": 6, "score": 2, "selftext": "Does this exist in the API? using praw I would like to have something like this. submissions = r.get_subreddit('askreddit').get_new(since=date/last request thing) Thanks", "title": "Getting only new submissions since last request.", "url": "https://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/"}, {"author": "brucemo", "created_utc": 1370909392, "gilded": 0, "name": "t3_1g308t", "num_comments": 3, "score": 5, "selftext": "It is difficult to find an example of this code anywhere, but what I came up with is: def ParentObj(self, obj): assert type(obj) == praw.objects.Comment submission = self.rh.get_submission(url = obj.permalink) if obj.is_root: return submission return self.rh.get_submission(url = submission.permalink + obj.parent_id[3:])._comments[0] This scares me for three reasons: 1. I am using _comments, which has an underscore in front of it, which implies to me that it's dubious or marginal or internal. 2. I am converting from an ID of the form \"t1_cag5h2j\" to one of the form \"cag5h2j\" by flushing the first three characters down the toilet, which seems a scary way to convert from type A to type B. 3. I would think there would be something explicit somewhere that allows you to move up and down the tree, but I can't find it. Did I implement this in a sane fashion? Should I scrape the comment tree or something instead? I promise that I'm not an idiot, really.", "title": "In PRAW, getting the object that is the parent of this comment", "url": "https://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/"}, {"author": "BittyTang", "created_utc": 1370761086, "gilded": 0, "name": "t3_1fyz52", "num_comments": 2, "score": 1, "selftext": "I am getting this error: File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 82, in __getattr__ attr)) AttributeError: '' has no attribute 'body' From this script: import praw r = praw.Reddit(user_agent = \"comment_sift_bot\") sub = r.get_subreddit(\"games\") siftWords = [\"reddit\"] for post in sub.get_hot(limit = 10): flatComments = praw.helpers.flatten_tree(post.comments) for comment in flatComments: if any(string in comment.body for string in siftWords): print(comment.body) I'm following the PRAW tutorials, and they used comment.body, so I'm not sure how I'm using it wrong.", "title": "Problem viewing comments with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1fyz52/problem_viewing_comments_with_praw/"}, {"author": "Memeifier", "created_utc": 1370758967, "gilded": 0, "name": "t3_1fyxv7", "num_comments": 4, "score": 13, "selftext": "I am currently running [here](http://www.reddit.com/r/memeifier/), with an extremely long sleep time. Was built with PRAW and am using the PRAW defaults for timeouts.", "title": "Am I an evil bot? (Generates memes from comments)", "url": "https://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/"}, {"author": "202halffound", "created_utc": 1370695032, "gilded": 0, "name": "t3_1fx8ws", "num_comments": 5, "score": 2, "selftext": "I have a simple bot written in Python (PRAW) that automatically flairs posts on a subreddit. However, the bot crashes when my internet drops out, reddit.com crashes, or I have a blackout for a few hours. How can I make the bot account for these and survive the problem?", "title": "How can I keep a bot running continuously when reddit.com crashes?", "url": "https://www.reddit.com/r/redditdev/comments/1fx8ws/how_can_i_keep_a_bot_running_continuously_when/"}, {"author": "JimmyRecard", "created_utc": 1370679017, "gilded": 0, "name": "t3_1fx1qr", "num_comments": 4, "score": 4, "selftext": "I'm not sure if this is the best subreddit, but I've seen similar posts here so I guess I'll try it. If there is a better subreddit, please point me to it. I wanted to run /u/AndrewNeo's [groompbot](https://github.com/AndrewNeo/groompbot). However, I don't have a 24/7 server to run it on. But then it hit me. I actually do. My router. I have ASUS RT-N66U running [Toastman](http://toastmanfirmware.yolasite.com/)'s [TomatoUSB](http://tomatousb.org/) and it is super rock solid and quite beefy in terms of specs. It also includes [busybox](http://www.busybox.net/). I added [Entware](https://code.google.com/p/wl500g-repo/) and added Python 2.7.3. So, now I'm stuck. I can SSH into the router and run Python, but past that I can't figure out how to install the dependencies ([praw](https://github.com/praw-dev/praw/) and [gdata](https://code.google.com/p/gdata-python-client/)). They call for [pip](https://pypi.python.org/pypi/pip), but I can't figure out how to install that either. I've read all the documentation and most of it goes over my head. Can anyone point me in the right direction? If I'm able to successfully figure this out, I plan to put together a little tutorial/guide about running reddit bots on routers, just to kind of give back to the community and consolidate the knowledge for next person who stumbles on this while Googling.", "title": "Running a reddit bot on my router.", "url": "https://www.reddit.com/r/redditdev/comments/1fx1qr/running_a_reddit_bot_on_my_router/"}, {"author": "stickytruth", "created_utc": 1370541310, "gilded": 0, "name": "t3_1fsz2m", "num_comments": 5, "score": 3, "selftext": "Hi, Is there a way to store an instance of a Reddit object? I've tried pickle, but when I load the pickled data I get a 'maximum recursion depth exceeded' exception: File \"praw/objects.py\", line 78, in __getattr__ if not self._populated: RuntimeError: maximum recursion depth exceeded while calling a Python object Is there another way to store and reuse a logged in session? Thanks Edit: Writing out the question got me to think about it, and solve it. Sorry for jumping the gun. Does this look right? def init(): global r handler = MultiprocessHandler('127.0.0.1', 6000) r = praw.Reddit(user_agent=conf.Get('useragent'), site_name='reddit_nossl', handler=handler) def login_store(): init() r.login(conf.Get('username'), conf.Get('password')) store = {'http':r.http, 'modhash':r.modhash} pickleFile = open(pickleFileName, 'wb') pickle.dump(store, pickleFile, pickle.HIGHEST_PROTOCOL) pickleFile.close() def read_load(): pickleFile = open(pickleFileName, 'rb') data = pickle.load(pickleFile) pickleFile.close() init() r.http = data['http'] r.modhash = data['modhash'] r._authentication = True msgs = r.get_inbox() for msg in msgs: print msg.body", "title": "Storing PRAW objects?", "url": "https://www.reddit.com/r/redditdev/comments/1fsz2m/storing_praw_objects/"}, {"author": "brucemo", "created_utc": 1370421887, "gilded": 0, "name": "t3_1fplrx", "num_comments": 10, "score": 2, "selftext": "Assume please that I have called \"get_stylesheet\" on a sub that I moderate, and I have that object. It's a dict, and an element of the dict is the CSS for the sub, and it's correct, so I know the \"get_stylesheet\" call worked properly. What can I pass to \"set_stylesheet\" that will not result in this error: > praw.errors.APIException: (BAD_CSS) `invalid css` on field `stylesheet_contents` I have tried passing the dictionary, and dictionary[\"stylesheet\"], and both produce this error. rh = praw.Reddit(\"whatever\") sh = rh.get_subreddit(sub) ss = sh.get_stylesheet() sh.set_stylesheet() ss, stylesheet = ss, ss[\"stylesheet\"], stylesheet=ss[\"stylesheet\"], all of these result in the above error. I am logged on and a moderator of the sub. I have also tried: ss = rh.get_stylesheet(sub) rh.set_stylesheet(sub, ) ... with the same results. I would like to eventually be able to make changes to the stylesheet, but for now I'd settle for a no-op.", "title": "How do you use PRAW's \"set_stylesheet\"?", "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "stickytruth", "created_utc": 1370212144, "gilded": 0, "name": "t3_1fjl11", "num_comments": 8, "score": 5, "selftext": "Solved -- It was indeed the proxy. The following praw.ini configuration is working for me on PythonAnywhere: [reddit_nossl] domain: www.reddit.com oauth_domain: oauth.reddit.com short_domain: redd.it check_for_updates: false http_proxy: http://proxy.server:3128 Hi, I'm trying to get PRAW working over at PythonAnywhere.com and have run up against a wall. **log**: retrieving: http://www.reddit.com/r/opensource/.json params: {'limit': 5} data: None Traceback (most recent call last): File \"test.py\", line 4, in print [str(x) for x in submissions] File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py\", line 438, in get_content page_data = self.request_json(url, params=params) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py\", line 473, in request_json response = self._request(url, params, data) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py\", line 346, in _request response = handle_redirect() File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py\", line 317, in handle_redirect timeout=timeout, **kwargs) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/handlers.py\", line 90, in request return self.http.send(request, proxies=proxies, timeout=timeout, allow_redirects=False) File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/site-packages/requests/adapters.py\", line 246, in send raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /r/opensource/.json?limit=5 (Caused by : [Errno 111] Connection refused) **test.py**: import praw r = praw.Reddit(user_agent='Trying out PRAW on PythonAnywhere',site_name='reddit_nossl') submissions = r.get_subreddit('opensource').get_hot(limit=5) print [str(x) for x in submissions] The reddit_nossl settings are as the name implies, and log_requests: 2, check_for_updates: false Using wget to access the same url works, [pastie link](http://pastie.org/pastes/7998257/text?key=9gpwt88jid8zww3ykt0yg) I also tried using just requests.get, which also works. [script](http://pastie.org/pastes/7998286/text?key=49wrr8rknnn4gkignsqflq), [output](http://pastie.org/pastes/7998283/text?key=t1qfmdty0jkkxtq5uj2svg) An admin at PythonAnywhere has said it's not a proxy issue, which is showing other users accessing reddit. I'm stumped, but that's not saying a whole lot. Is something staring me in the face? **Edit**: I added some debugging, and modified the request pool as suggested. The new script looks like: #!/usr/bin/python import praw import requests import logging import httplib httplib.HTTPConnection.debuglevel = 1 logging.basicConfig() logging.getLogger().setLevel(logging.DEBUG) requests_log = logging.getLogger(\"requests.packages.urllib3\") requests_log.setLevel(logging.DEBUG) requests_log.propagate = True # Works print 'Making basic Requests get' requests.get('http://www.reddit.com/r/opensource/.json?limit=5') # Doesn't work print '\\n\\nImporting PRAW and requesting submissions' r = praw.Reddit(user_agent='Trying out PRAW on PythonAnywhere',site_name='reddit_nossl') submissions = r.get_subreddit('opensource').get_hot(limit=5) print [str(x) for x in submissions] I manually edited PRAW's handlers.py to modify the pool, the first 2 lines of the PRAW output shows the settings. The output is rather long, so I'll [link to it here](http://pastie.org/pastes/7998858/text?key=pzb01ahx79yzmbsp9k7na) Interestingly, with just requests.get(url), the debugging shows it connecting to proxy.server, but with PRAW it says it is connecting to www.reddit.com. How would I go about setting PRAW to use the proxy?", "title": "PRAW Exception: Connection refused", "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "jcannon98188", "created_utc": 1370125281, "gilded": 0, "name": "t3_1fhjbt", "num_comments": 2, "score": 2, "selftext": "I am getting the following error when attempting to follow the parsing comment example on the praw wiki: Traceback (most recent call last): File \"C:\\Users\\Jason\\Desktop\\utahbot.py\", line 6, in flat_comments = praw.helpers.flatten_tree(submission.comments_flat) File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 82, in __getattr__ attr)) AttributeError: '' has no attribute 'comments_flat' The following is my code: import time import praw r = praw.Reddit('Test Bot by /u/jcannon98188') r.login('BOTUSERNAME','BOTPASSWORD') submission = r.get_submission(submission_id='1fhhxs') flat_comments = praw.helpers.flatten_tree(submission.comments_flat) already_done = [] for comment in flat_comments: if comment.body == \"Utah\" and comment.id not in already_done: comment.reply(' world!') already_done.append(comment.id) Any ideas on what is happening?", "title": "PRAW error when using flat_comments", "url": "https://www.reddit.com/r/redditdev/comments/1fhjbt/praw_error_when_using_flat_comments/"}, {"author": "brucemo", "created_utc": 1369990651, "gilded": 0, "name": "t3_1fe7pj", "num_comments": 4, "score": 2, "selftext": "In particular I want to maintain state via a Reddit Wiki page. I'm sorry if this something obvious but when I google \"praw wiki\" the signal to noise ratio is zero. If doing this via a Wiki is insane, is there another obvious way to maintain state? I could do it via a thread in a subreddit, but there are length limits on self-posts and comments, and stuff gets archived, etc. Thank you,", "title": "Does PRAW support Wiki stuff?", "url": "https://www.reddit.com/r/redditdev/comments/1fe7pj/does_praw_support_wiki_stuff/"}, {"author": "im14", "created_utc": 1369938834, "gilded": 0, "name": "t3_1fcolg", "num_comments": 23, "score": 2, "selftext": "I run /r/ALTcointip bot, and beginning this morning, bot began processing the messages in its inbox multiple times. It seems PRAW m.mark_as_read() call doesn't have proper effect. I'm also seeing this issue through browser, where messages in my inbox stay unread even though I've already clicked on them.", "title": "m.mark_as_read() is broken since this morning - known/old issue?", "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "jdp407", "created_utc": 1369840675, "gilded": 0, "name": "t3_1f9t5s", "num_comments": 4, "score": 6, "selftext": "When is praw.errors.RateLimitExceeded actually thrown? The API docs say it's 'An exception for when something has happened too frequently', but doesn't PRAW just delay API calls, rather than throwing an exception if the 2 second rule isn't obeyed? Or do I have to write a 2 second delay into my bot?", "title": "When is praw.errors.RateLimitExceeded thrown?", "url": "https://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/"}, {"author": "whodunit28", "created_utc": 1369738181, "gilded": 0, "name": "t3_1f6xw4", "num_comments": 1, "score": 1, "selftext": "My praw.ini is located at: /usr/local/lib/python2.7/dist-packages/praw As given here (https://praw.readthedocs.org/en/latest/pages/configuration_files.html), I added following to my praw.ini: [reddit.com] http_proxy:username:pwd@server:port https_proxy:username:pwd@server:port Now when I try to run the example given on praw home page, I get the following error: Traceback (most recent call last): File \"reddit.py\", line 4, in [str(x) for x in submissions] File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 434, in get_content page_data = self.request_json(url, params=params) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 342, in _request response = handle_redirect() File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 315, in handle_redirect timeout=timeout, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 90, in request allow_redirects=False) File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 438, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 272, in send conn = self.get_connection(request.url, proxies) File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 197, in get_connection conn = ProxyManager(self.poolmanager.connection_from_url(proxy)) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/poolmanager.py\", line 123, in connection_from_url return self.connection_from_host(u.host, port=u.port, scheme=u.scheme) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/poolmanager.py\", line 109, in connection_from_host pool = self._new_pool(scheme, host, port) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/poolmanager.py\", line 72, in _new_pool pool_cls = pool_classes_by_scheme[scheme] KeyError: 'username' What am I doing wrong?", "title": "Not able to configure proxy settings in praw.ini", "url": "https://www.reddit.com/r/redditdev/comments/1f6xw4/not_able_to_configure_proxy_settings_in_prawini/"}, {"author": "I_SLEEP_NORMALLY", "created_utc": 1369699430, "gilded": 0, "name": "t3_1f61mp", "num_comments": 3, "score": 5, "selftext": "For some research I'm working on, I was contemplating looking for random threads posted within the previous 7 days. After some tooling around with PRAW, I've hit a bit of a wall. Has anyone managed to pull off grabbing random threads within a series of given constraints? (e.g. someone else may be interested in looking at a random thread within the previous year, or above a given vote threshold.)", "title": "Is it possible to roll random threads with given constraints?", "url": "https://www.reddit.com/r/redditdev/comments/1f61mp/is_it_possible_to_roll_random_threads_with_given/"}, {"author": "whodunit28", "created_utc": 1369643025, "gilded": 0, "name": "t3_1f4mtc", "num_comments": 4, "score": 2, "selftext": "I want to configure proxy settings in PRAW. Documentation says praw.ini should be in \"/home/foobar/.config/praw.ini\" but there is no foobar folder in my home directory. There is a .config folder but it doesn't contain praw.ini.", "title": "Where do I find my praw.ini in Ubuntu 12.04?", "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "Meeshu", "created_utc": 1369429389, "gilded": 0, "name": "t3_1ezssi", "num_comments": 6, "score": 4, "selftext": "When I use r.login('username','password') in the python shell, it works fine and logs me in. However, in a script it gives an error. This is the only code in my script. Help? import praw r = praw.Reddit(user_agent = 'An automated reddit project - Meeshu') r.login('Meeshu','password')", "title": "Error with r.login()", "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": "im14", "created_utc": 1369356357, "gilded": 0, "name": "t3_1exvqb", "num_comments": 8, "score": 2, "selftext": "So the exception usually looks something like this: ERROR 2013-05-24 00:08:14,296 _check_inbox(): couldn't mark message as read: 502 Server Error: Bad Gateway ERROR 2013-05-24 00:08:14,297 Caught exception in main() loop: 502 Server Error: Bad Gateway Traceback (most recent call last): File \"/home/dv/git/cointipbot/src/cointipbot.py\", line 457, in main self._check_inbox() File \"/home/dv/git/cointipbot/src/cointipbot.py\", line 346, in _check_inbox m.mark_as_read() File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 313, in mark_as_read return self.reddit_session.user.mark_as_read(self) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 663, in mark_as_read retval = self.reddit_session._mark_as_read(ids, unread=unread) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 262, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 1721, in _mark_as_read response = self.request_json(self.config[key], data=data) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 343, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python2.7/dist-packages/praw/internal.py\", line 182, in _raise_response_exceptions response.raise_for_status() File \"/usr/local/lib/python2.7/dist-packages/requests/models.py\", line 689, in raise_for_status raise HTTPError(http_error_msg, response=self) HTTPError: 502 Server Error: Bad Gateway The code I have in place to catch it is: # Mark message as read while True: try: m.mark_as_read() break except urllib2.HTTPError, e: if e.code in [429, 500, 502, 503, 504]: lg.warning(\"_check_inbox(): Reddit is down (error %s), sleeping...\", e.code) time.sleep(60) pass else: raise except Exception, e: lg.error(\"_check_inbox(): couldn't mark message as read: %s\", str(e)) raise Am I not catching the right HTTPError? From stack trace, I've found exception definition to be at __/usr/local/lib/python2.7/dist-packages/requests/exceptions.py__: class HTTPError(RequestException): \"\"\"An HTTP error occurred.\"\"\" def __init__(self, *args, **kwargs): \"\"\" Initializes HTTPError with optional `response` object. \"\"\" self.response = kwargs.pop('response', None) super(HTTPError, self).__init__(*args, **kwargs) No mention of urllib2 in that file. So, am I catching the wrong type of HTTPError?", "title": "After weeks of trial and error I still can't catch Reddit downtime exceptions (HTTP 429, 502, etc). Help?", "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "RampagingKoala", "created_utc": 1369291932, "gilded": 0, "name": "t3_1ew2gk", "num_comments": 3, "score": 2, "selftext": "I'm making a bot, and whenever r.login runs, I get this error: raise SSLError(e) requests.exceptions.SSLError: Can't connect to HTTPS URL because the SSL module is not available My user_agent string doesn't contain 'bot', and it was working before the praw update. Are there any packages I'm missing or something I should be doing?", "title": "r.login failing", "url": "https://www.reddit.com/r/redditdev/comments/1ew2gk/rlogin_failing/"}, {"author": "feblehober123", "created_utc": 1369259633, "gilded": 0, "name": "t3_1ev2ex", "num_comments": 2, "score": 2, "selftext": "I have been using praw to make a bot that makes comments. I got everything to work, but 2 or 3 days ago there was a new update for praw. I installed this, and since then I have had many errors. I login without getting errors, but at the point when it comments it tells me that I am not logged in. I ran this many times in the shell using different accounts, user agents, and messages. It always does the same thing. Is this just a mistake, or is there something I need to do to fix this?", "title": "unexplained login errors using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1ev2ex/unexplained_login_errors_using_praw/"}, {"author": "alphanovember", "created_utc": 1369169525, "gilded": 0, "name": "t3_1ese01", "num_comments": 3, "score": 1, "selftext": "I have a number value on line 3 of my sidebar that needs to be decremented once a day. That's all it should do. I'll schedule it with a cron job, I just need someone to write the script for me. This sounds like something that could be whipped up in a few lines of PRAW, but I don't know enough to do it myself. There actually [exists](https://github.com/matchu/reddit-countdown) a reddit bot that does does just this, the only problem is that it was written to edit HTML (`54`) in the sidebar rather than markdown for some reason, so I can't use it. It also does way more than I need it to. **Edit:** The author of that script has updated it for MD. I'm now using that.", "title": "Seeking a very simple countdown/value decrement Python script for my sidebar", "url": "https://www.reddit.com/r/redditdev/comments/1ese01/seeking_a_very_simple_countdownvalue_decrement/"}, {"author": "77739", "created_utc": 1369152234, "gilded": 0, "name": "t3_1erqio", "num_comments": 1, "score": 2, "selftext": "This seems like it would be pretty basic so there's probably an easy answer, but I couldn't find it on the PRAW website or in the help() docs. Basically, I want to get only links submitted to a particular subreddit within a time frame (and their scores, comments, etc.). I found a way to do it constructing the API calls without PRAW, but I'd like to know if this is possible (and easier) using PRAW.", "title": "Get submissions by date and subreddit with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1erqio/get_submissions_by_date_and_subreddit_with_praw/"}, {"author": "Biggerontheinside", "created_utc": 1369072114, "gilded": 0, "name": "t3_1epfrj", "num_comments": 5, "score": 2, "selftext": "I'm just beginning to learn how to use PRAW and python, so this may be something very obvious that I'm missing, but I keep getting this when I try to log in using PRAW: C:\\Python33\\lib\\http\\client.py:1172: DeprecationWarning: the 'strict' argument isn't supported anymore; http.client now always assumes HTTP/1.x compliant servers. source_address) Traceback (most recent call last): File \"C:\\Python33\\Files\\Test5.py\", line 11, in r.login(USERNAME,PASSWORD) # necessary if your bot will talk to people File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 1120, in login self.request_json(self.config['login'], data=data) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 342, in _request response = handle_redirect() File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 315, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python33\\lib\\site-packages\\requests\\sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests\\adapters.py\", line 256, in send r = self.build_response(request, resp) File \"C:\\Python33\\lib\\site-packages\\requests\\adapters.py\", line 125, in build_response extract_cookies_to_jar(response.cookies, req, resp) File \"C:\\Python33\\lib\\site-packages\\requests\\cookies.py\", line 105, in extract_cookies_to_jar jar.extract_cookies(res, req) File \"C:\\Python33\\lib\\http\\cookiejar.py\", line 1647, in extract_cookies if self._policy.set_ok(cookie, request): File \"C:\\Python33\\lib\\http\\cookiejar.py\", line 931, in set_ok if not fn(cookie, request): File \"C:\\Python33\\lib\\http\\cookiejar.py\", line 952, in set_ok_verifiability if request.unverifiable and is_third_party(request): File \"C:\\Python33\\lib\\http\\cookiejar.py\", line 707, in is_third_party if not domain_match(req_host, reach(request.origin_req_host)): AttributeError: 'MockRequest' object has no attribute 'origin_req_host' I did some searching, and the best I could come up with is a problem with requests, but I've installed it and all its requirements and I'm just at a loss.", "title": "Can't sort this one out: trying to log in using PRAW, keep getting \"AttributeError: 'MockRequest' object has no attribute 'origin_req_host'\"", "url": "https://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/"}, {"author": "Buffer_Underflow", "created_utc": 1368995351, "gilded": 0, "name": "t3_1enepj", "num_comments": 3, "score": 3, "selftext": "I'm trying to comment on a submission, then comment on a comment inside the submission. I made the bot sleep 9 minutes in between both comments, but I still can't seem to shake the RateLimitExceeded exception. So what exactly causes the RateLimitExceeded exception? And how can I avoid it? I was trying to do this on /r/test if it matters at all. btw I'm using praw.", "title": "RateLimitExceeded problem", "url": "https://www.reddit.com/r/redditdev/comments/1enepj/ratelimitexceeded_problem/"}, {"author": "undergroundmonorail", "created_utc": 1368738515, "gilded": 0, "name": "t3_1eh4ih", "num_comments": 5, "score": 1, "selftext": "I'm too new to Python to pin down the problem exactly (it might not even be praw), but when I try to send a message I get the error in the title. This is the offending line of code: r.send_message(\"undergroundmonorail\", \"Comment reply from {}.\".format(message.author.name), \"{}\\n\\n[[x]](http://www.reddit.com{})\".format(message.body, message.context)) Here's all of the error text I get from the interpreter: Traceback (most recent call last): File \"C:\\Users\\undergroundmonorail\\Desktop\\python\\redditbot\\youtube.py\", line 74, in \"{}\\n\\n[[x]](http://www.reddit.com{})\".format(message.body, message.context)) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.2-py2.7.egg\\praw\\decorators.py\", line 261, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.2-py2.7.egg\\praw\\decorators.py\", line 160, in wrapped if sys.stdin.closed or raise_captcha_exception: File \"\", line 523, in __getattr__ File \"C:\\Program Files\\PyScripter\\Lib\\rpyc.zip\\rpyc\\core\\netref.py\", line 150, in __getattr__ return syncreq(self, consts.HANDLE_GETATTR, name) File \"C:\\Program Files\\PyScripter\\Lib\\rpyc.zip\\rpyc\\core\\netref.py\", line 71, in syncreq return conn.sync_request(handler, oid, *args) File \"C:\\Program Files\\PyScripter\\Lib\\rpyc.zip\\rpyc\\core\\protocol.py\", line 434, in sync_request raise obj AttributeError: DebugOutput instance has no attribute 'closed' I have no idea where to even start *looking* for a solution. Any help would be greatly appreciated.", "title": "I'm about 60% sure that this is a [PRAW] problem, but maybe it's just Python: I get \"AttributeError: DebugOutput instance has no attribute 'closed'\" when I try to send a message.", "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "noun_exchanger", "created_utc": 1368514314, "gilded": 0, "name": "t3_1eauwc", "num_comments": 6, "score": 1, "selftext": "Another nooby question from me.. but I can't seem to find the answer. I'm using Windows and have installed PIP, Distribute, and PRAW. I had reddit bot code working fine.. but then a couple days after the most recent PRAW update, there are a bunch of errors and the codes won't run. So the question is, how do I update PRAW with the command prompt using PIP? Thanks", "title": "How to update PRAW with PIP?", "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "shaggorama", "created_utc": 1368476670, "gilded": 0, "name": "t3_1e9lwp", "num_comments": 24, "score": 17, "selftext": "There ought to be a subreddit-level flag (and perhaps a user-level flag) to instruct bots not to engage on that subreddit (or with that user). Obviously this would only be effective for those bots that respected such a flag, but I believe something like this could reduce the amount the moderators need to fight with bots. There seem to be a lot of subreddits that have a \"we see em, we ban em\" attitude towards reddit bots: why not just modify the ecosystem so the bots already get the message without evening needing the mods to ban them? Here's how I imagine this being implemented: 1. The option to set this flag needs to be made available in subreddit settings. I think this should default to \"bots are ok here.\" 2. The main reddit API wrappers (e.g. praw) should be set to respect this flag by default. Overriding this flag should be easy, but this way people just messing around with bots as hobby projects (which I assume is most of the population of praw users) will respect the flag by default. EDIT: Some great ideas in here, but I'm thinking such a feature would be more trouble than it's worth and probably actually cause more work for everyone (mods, bot creators, and app developers). There's nothing really wrong with the current system of just banning bots when mods find them. Maybe if we work this idea through more we could come up with a better implementation, but I'm concerned that the options we've considered would make people's lives harder instead of easier. Maybe we should bring more mods into the discussion?", "title": "Proposal: robots.txt equivalent for reddit bots.", "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "testuser12345678", "created_utc": 1368119517, "gilded": 0, "name": "t3_1e0ffk", "num_comments": 2, "score": 6, "selftext": "I've been running a script on PythonAnywhere which logs in using PRAW. I've run it every day, and it hasn't failed until just now, which is strange because I didn't change anything in my code or my PRAW installation. I therefore can only conclude that the Reddit API must have changed, or PythonAnywhere must have changed something. Does anyone know how I might fix this? Traceback (most recent call last): File \"bravery20.py\", line 1314, in r.login(username=username, password=password) File \"/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\", line 906, in login self.request_json(self.config['login'], data=data) File \"/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\", line 223, in error_checked_function return_value = function(cls, *args, **kwargs) File \"/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\", line 407, in request_json response = self._request(url, params, data) File \"/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\", line 294, in _request timeout=timeout) File \"/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\", line 64, in __call__ result = self.function(reddit_session, url, *args, **kwargs) File \"/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\", line 167, in __call__ return self.function(*args, **kwargs) File \"/home/person/.local/lib/python2.7/site-packages/praw/helpers.py\", line 137, in _request allow_redirects=False, auth=auth) File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 399, in post return self.request('POST', url, data=data, **kwargs) File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 354, in request resp = self.send(prep, **send_kwargs) File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/site-packages/requests/adapters.py\", line 246, in send raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPConnectionPool(host='proxy.server', port=3128): Max retries exceeded with url: http://www.reddit.com/api/login/.json (Caused by : [Errno 111] Connection refused) I'm using PRAW 2.0.15 because the latest version doesn't work for some reason; also, I've removed `\"login\"` from the `SSL_PATHS` list in `__init__.py` in order to prevent PRAW from attempting to connect securely (which PythonAnywhere doesn't support). Thanks in advance.", "title": "Did something in the Reddit API change last night? I can no longer log in insecurely with PRAW 2.0.15 hosted on PythonAnywhere", "url": "https://www.reddit.com/r/redditdev/comments/1e0ffk/did_something_in_the_reddit_api_change_last_night/"}, {"author": "rapture_survivor", "created_utc": 1367807310, "gilded": 0, "name": "t3_1dro2b", "num_comments": 2, "score": 3, "selftext": "I'm pretty sure this is a newbie question; I've just started using the API today. I want to be able to retrieve a comment given a url for the comment without searching through the submission for a matching ID. I tried using the get_content method with the full URL of the target comment, but when I try to use the generator it creates, the API throws an error: >>> gen = r.get_content('http://www.reddit.com/r/pics/comments/1dplco/took_a_ride_and_suddenly_found_myself_in_a/c9sqtmx') >>> data = gen.next() Traceback (most recent call last): File \"\", line 1, in data = gen.next() File \"C:\\Python27\\lib\\site-packages\\praw-2.1.1-py2.7.egg\\praw\\__init__.py\", line 438, in get_content root = page_data[root_field] TypeError: list indices must be integers, not str As I have my program set up now, I take the submission ID from the url, and then get the submission using praw. I then proceed to search through all if the comments if the submission, checking each to see if their ID matches the one in the url. This usually doesn't take too long, but if the comment I'm looking for is in one of the 'more comments' areas, then it takes forever to open all of the sections to look for it. I know there must be some way to access a comment in that way, but I must not be doing it right.", "title": "Problems with the PRAW get_content() method", "url": "https://www.reddit.com/r/redditdev/comments/1dro2b/problems_with_the_praw_get_content_method/"}, {"author": "johnflim", "created_utc": 1367691633, "gilded": 0, "name": "t3_1doto3", "num_comments": 8, "score": 6, "selftext": "What would be the best method for allowing a bot/program to keep track of read and unread private messages? I was planning on having my program mark unread messages in the inbox as 'read' after it finishes processing the message, but it seems like there are flaws in this plan. For example, if I accidentally log on to the reddit account associated with the bot, I might accidentally open up a message and it wouldn't be processed by the bot. The PRAW tutorials have a method in which processed submissions are kept track of using the submission ID. Would this be the best method for handling messages? Thanks!", "title": "Reddit bot reading inbox", "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "killver", "created_utc": 1367237115, "gilded": 0, "name": "t3_1dbzvd", "num_comments": 6, "score": 6, "selftext": "Hey! One thing I have not yet completely figured out is how tags work on Reddit. Are they assigned to submissions by users? Can they \"invent\" new tags or do they have to use existing ones? Is there a way to crawl the tags? E.g., via PRAW? Thanks!", "title": "Tags on Reddit", "url": "https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/"}, {"author": "acidzest", "created_utc": 1367151766, "gilded": 0, "name": "t3_1d9ops", "num_comments": 6, "score": 1, "selftext": "Hi, I've made an App.net (like Twitter) bot that's meant to post new submissions there every ~hour, which it does but not as efficiently as I want it to. I have an array called already_done, if a submission's id is in that array, it's meant to go to the next submission after than, using place_holder, which works but then it just loops through again and again. I'm not really sure how to get it working, I'm sure it's simple enough, just a logic problem. Here's the code; import praw, time r = praw.Reddit(user_agent='ADNPost') already_done = [] while True: for submission in r.get_top(limit=1): id = submission.id title = submission.title url = submission.short_link save_state = (id) if id not in already_done: already_done.append(submission.id) post = title + \" | \" + url print post print save_state if id in already_done: for submission in r.get_front_page(limit=1, place_holder=submission.id): id = submission.id title = submission.title url = submission.short_link print title, url save_state = (id) already_done.append(submission.id) time.sleep(2) Current Output: Post 1 Post 2 Post 2 Post 2 When it should be: Post 1 Post 2 Post 3 It successfully uses place_holder=Post 1 but doesn't do it for Post 2, Post 3, etc. Any help would be greatly appreciated.", "title": "Trouble with an if loop/while loop combination", "url": "https://www.reddit.com/r/redditdev/comments/1d9ops/trouble_with_an_if_loopwhile_loop_combination/"}, {"author": "bboe", "created_utc": 1366882240, "gilded": 0, "name": "t3_1d2nr4", "num_comments": 1, "score": 11, "selftext": "Hey everyone, I spent a little time over the last few days refactoring how PRAW actually handles requests including how PRAW performs both rate limiting and caching. The new approach is modular allowing you to more easily change PRAW's request handling behavior by providing your own handler class, or utilizing a non-default handler provided by PRAW. One such handler I wrote is a multiprocess handler that interfaces with a request-handler server that is now included in PRAW. Before I push this code out with version 2.1.0 it would be awesome if some of the regular PRAW users could test the multiprocess handler with your programs. The primary benefit of the multiprocess handler is you no longer have to worry about rate limiting when running multiple versions of your PRAW programs. To get started, fetch PRAW from github. While using git is recommended, here's a [zip](https://github.com/praw-dev/praw/archive/master.zip) of the latest source. Installing PRAW this way requires you to run `python setup.py install` from the root of the source tree (where setup.py lives). Once up and running give this script a try: https://gist.github.com/bboe/5458377 At this point you'll probably notice warnings like: Cannot connect to multiprocess server. Is it running? Retrying in 2 seconds. From a separate terminal, start PRAW's multiprocess request handling server by running `praw-multiprocess`. From here you should be able to run as many PRAW instances you want so long as you pass in an instance of the `MultiprocessHandler` when creating the Reddit instance. What I'm looking for is any issues you encounter. I've handled most of the obvious connection issues on my ubuntu OS, but perhaps the socket error messages are different on other OSes, or I completely neglected to test something. That's where you can help! At the moment the only unresolved issue that I am aware of is when the request times out, the server fails to pickle the resulting TimeOut object. This in turn will cause an EOFError on the client that it already handles. However, after 3 consecutive EOFErrors PRAW will raise a ClientException. Any feedback you have would be greatly appreciated. Thanks!", "title": "Multiprocess PRAW -- testing needed", "url": "https://www.reddit.com/r/redditdev/comments/1d2nr4/multiprocess_praw_testing_needed/"}, {"author": "SOTB-human", "created_utc": 1366873369, "gilded": 0, "name": "t3_1d2ino", "num_comments": 4, "score": 3, "selftext": "I'm using PRAW, but if the only way to do this is by bypassing PRAW, then that'll work too. For example, suppose we're given a comment ID \"c9m894d\" and we need to figure out some more information about it: its content, author, score, etc. However, we don't know the ID of its submission, so we can't do the `praw.objects.Submission.get_info(r, \"http://www.reddit.com/r/all/comments/\"+submission_id+\"/_/\"+comment_id).comments[0]` thing. Comment IDs are unique, right? So is there a way to retrieve a comment using only its ID?", "title": "Is it possible to fetch a Comment using only its ID, not knowing its submission ID?", "url": "https://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/"}, {"author": "killver", "created_utc": 1366812202, "gilded": 0, "name": "t3_1d0cln", "num_comments": 4, "score": 4, "selftext": "Hey there! I am currently trying to permanently store submission objects to have later access to them. I also want to leave the option open to later easily have possibilities to retrieve further data like comments or user infos. My code looks the following way: import praw import datetime r = praw.Reddit('void') submissions_per_day = list() curr_date = None i = 0 for submission in r.get_subreddit('all').get_new(limit=None): date = datetime.datetime.fromtimestamp(int(submission.created_utc)).strftime('%Y-%m-%d') submissions_per_day.append(submission) #or vars(submission) if i == 0: curr_date = date if date != curr_date: SOMEHOW STORE THE LIST OF SUBMISSION OBJECTS So my goal is to get all submissions of one day store them, then for the next day and store them again. I have tried various things like the most obvious cPickle solution. But this is very slow, even when just crawling a few submissions and using the newest protocol. I have also tried to only take the dictionary of the object (i.e. vars(submission)) without any improvements. I know that I could just go ahead and manually parse the object and e.g. store the values to a csv file. But this seems so odd to me and I want a more fluent solution. I could also think about getting the json objects and storing them. Another solution would be to use databases but this is also not my prefered method. I hope anyone of you can give me some hints of how to cope with my problems. Thanks, Philipp", "title": "PRAW store submission data", "url": "https://www.reddit.com/r/redditdev/comments/1d0cln/praw_store_submission_data/"}, {"author": "alexleavitt", "created_utc": 1366758403, "gilded": 0, "name": "t3_1cyxjr", "num_comments": 11, "score": 1, "selftext": "Been trying to get user data from PRAW, but keep getting this error: Traceback (most recent call last): File \"redditusers.py\", line 126, in for each in user_comments: File \"/usr/local/lib/python2.6/dist-packages/praw/__init__.py\", line 290, in get_content page_data = self.request_json(page_url, url_data=url_data) File \"/usr/local/lib/python2.6/dist-packages/praw/decorators.py\", line 164, in error_checked_function return_value = function(self, *args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw/__init__.py\", line 325, in request_json response = self._request(page_url, params, url_data) File \"/usr/local/lib/python2.6/dist-packages/praw/__init__.py\", line 217, in _request url_data, timeout) File \"/usr/local/lib/python2.6/dist-packages/praw/decorators.py\", line 59, in __call__ result = self.function(reddit_session, page_url, *args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw/decorators.py\", line 144, in __call__ return self.function(*args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw/helpers.py\", line 101, in _request response = reddit_session._opener.open(request, timeout=timeout) File \"/usr/lib/python2.6/urllib2.py\", line 391, in open response = self._open(req, data) File \"/usr/lib/python2.6/urllib2.py\", line 409, in _open '_open', req) File \"/usr/lib/python2.6/urllib2.py\", line 369, in _call_chain result = func(*args) File \"/usr/lib/python2.6/urllib2.py\", line 1172, in http_open return self.do_open(httplib.HTTPConnection, req) File \"/usr/lib/python2.6/urllib2.py\", line 1147, in do_open raise URLError(err) urllib2.URLError: from this code (which I adapted from the PRAW examples): #get recent karma & subreddit activity got_user = r.get_redditor(user) #THIS IS LINE 111 user_submissions = got_user.get_submitted(limit=None) user_comments = got_user.get_comments(limit=None) submission_karma_by_subreddit = {} submission_count_by_subreddit = {} for each in user_submissions: subreddit = each.subreddit.display_name submission_karma_by_subreddit[subreddit] = (submission_karma_by_subreddit.get(subreddit, 0) + each.score) submission_count_by_subreddit[subreddit] = (submission_count_by_subreddit.get(subreddit, 0) + 1) comment_karma_by_subreddit = {} comment_count_by_subreddit = {} for each in user_comments: #THIS IS LINE 126 subreddit = each.subreddit.display_name comment_karma_by_subreddit[subreddit] = (comment_karma_by_subreddit.get(subreddit, 0) + each.score) comment_count_by_subreddit[subreddit] = (comment_count_by_subreddit.get(subreddit, 0) + 1) I've run this code in the past, and it worked fine then. Not sure why the timeouts are happening now... Any thoughts?", "title": "PRAW Timeouts?", "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "lamerx", "created_utc": 1366613200, "gilded": 0, "name": "t3_1cumng", "num_comments": 12, "score": 5, "selftext": "Every time i try to upvote or downvote i get this user='name' passwd='mypassword' reddit.login(user,passwd) m=reddit.get_submission(submission_id='1crjy2') m.upvote() ##OUTPUT Traceback (most recent call last): File \"C:\\Python27\\Scripts\\testB0T2.py\", line 61, in reddit.get_submission(submission_id='19zce4').upvote() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 447, in upvote return self.vote(direction=1) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 341, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 464, in vote return self.reddit_session.request_json(url, data=data) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 239, in error_checked_function raise error_list[0] NotLoggedIn: `please login to do that` on field `None` And when I try to remove something that I am the author of with this code user='name' passwd='mypassword' reddit.login(user,passwd) m=reddit.get_submission(submission_id='1crjy2') m.remove() ##OUTPUT Traceback (most recent call last): File \"C:\\Python27\\Scripts\\sheboon\\testB0T2.py\", line 73, in m.remove() File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 331, in wrapped if mod_req and not is_mod_of_all(obj.user, subreddit): File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 288, in is_mod_of_all mod_subs = user.get_cached_moderated_reddits() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 688, in get_cached_moderated_reddits for sub in self.reddit_session.get_my_moderation(limit=None): File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 372, in get_content page_data = self.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 223, in error_checked_function return_value = function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 407, in request_json response = self._request(url, params, data) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 294, in _request timeout=timeout) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 64, in __call__ result = self.function(reddit_session, url, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 167, in __call__ return self.function(*args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\helpers.py\", line 154, in _request .format(prev_url, url)) ClientException: Unexpected redirect from http://www.reddit.com/reddits /mine/moderator/.json to http://www.reddit.com/subreddits/login.json?dest=%2Freddits%2Fmine%2Fmoderator%2F.json%3Flimit%3D1024", "title": "PRAW + Voting & Removing", "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "frumious", "created_utc": 1366556582, "gilded": 0, "name": "t3_1csvar", "num_comments": 9, "score": 5, "selftext": "I know praw already cache's results from reddit for 30 seconds but I'm looking for a way to add a more persistent data store so that I only hit reddit once for any \"thing\" in its API, ever. For now, I don't care about seeing any changes due to edits or votes but if one did, I guess that would complicate things. I can come up with something explicit that is layered on top of praw but having this store insinuated into praw so it's transparent to any existing praw-using code seems like a good design. Any thoughts on this? Any work in this direction yet? Edit: I'm looking for something that persists between executions.", "title": "Persistent store for praw?", "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "ben444422", "created_utc": 1366436841, "gilded": 0, "name": "t3_1cq7g1", "num_comments": 7, "score": 8, "selftext": "I've been using PRAW to crawl through all of the subreddits, but I've found that some subreddits have been banned (thereby not existing), abruptly stopping the crawling process via a 404 error. Is there a way to test whether a subreddit exists?", "title": "How to know if subreddit exists?", "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": "wtf_are_my_initials", "created_utc": 1365825097, "gilded": 0, "name": "t3_1c931l", "num_comments": 7, "score": 2, "selftext": "I'm currently using PRAW to make a bot, but I don't want the bot to ever respond to itself, so before I the bot comments, I want to check if the comment it is replying to was posted by the bot's username. I have searched through the docs and have not found it in the source so im asking here. How do I get the username of a commenter from a comment object? Thanks", "title": "PRAW and comments.", "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "nannal", "created_utc": 1365695338, "gilded": 0, "name": "t3_1c52gt", "num_comments": 15, "score": 4, "selftext": "hey guys, I'm playing about with praw and am a total python nubbin. I've got it to login, make posts and read page titles and self post text but for what I'm planing I'm going to need to find a way of replying to a users last comment and reading the bot's inbox. any help you guys would could offer would be muchly appreciated.", "title": "Submit reply to users most recent comment.", "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "yowmamasita", "created_utc": 1365355195, "gilded": 0, "name": "t3_1bv06t", "num_comments": 1, "score": 7, "selftext": "The script is watching and fetching submissions on a subreddit's \"new\" feed every 30 minutes and I'm stuck thinking of a way to only fetch new posts. I'm using PRAW and thinking of storing the reddit id of the first post (ex. dtg4j), everytime I fetch, in a flat file. (since the first post is always the newest) But my program fetches 50 posts everytime, what if only 1 of that is new, then the other 49 posts I fetched are useless, right? So I'm not sure yet with that. And even if there's only a small chance, when I fetch 50 posts, what if there were 60 new submissions. That means I will miss the other 10. Please help me. Thanks!", "title": "Stuck at processing each submission on a subreddit without repeating", "url": "https://www.reddit.com/r/redditdev/comments/1bv06t/stuck_at_processing_each_submission_on_a/"}, {"author": "naive_babes", "created_utc": 1365311334, "gilded": 0, "name": "t3_1bu7ak", "num_comments": 3, "score": 5, "selftext": "PRAW being a third-party module makes it hard to use it on Google App engine. There seem to be two options to be able to use it on app engine - install it in the local directory or use buildout. I'm really really confused about how to use buildout for this on app engine and there dont seem to be informative tutorials for this online yet. I extracted the egg files and copied them onto my app's directory, but there are persistent problems with dependencies. I've done this method for a lot of other third-party libraries, but i can't seem to get this going with PRAW. Has anyone done either of these things? Any tips on what to try next? Update: I did manage to get this going. I unzipped all the egg files of the praw installation and put them in the local directory of my app. There is some trouble with the update_checker dependency, and i got rid of it completely. There's an additional problem with obtaining the platform of google app engine while constructing the user agent string. I got rid of that bit as well. Now i have it working fine.", "title": "How do i run PRAW on google appengine?", "url": "https://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/"}, {"author": "ritonlajoie", "created_utc": 1365178172, "gilded": 0, "name": "t3_1bqnri", "num_comments": 0, "score": 0, "selftext": "Hey ! I wondered if someone had the idea to create a proxy for reddit but which would cache the queries results for some time ? This would allow getting at least data from reddit without hitting it. Does this exist ? Would you need it ? I could code something around PRAW that would first hit a public cache then if the result is not there, would hit reddit and fill the cache.. Would you use that ?", "title": "any api proxy for reddit ?", "url": "https://www.reddit.com/r/redditdev/comments/1bqnri/any_api_proxy_for_reddit/"}, {"author": "utopiah", "created_utc": 1365176680, "gilded": 0, "name": "t3_1bqlqv", "num_comments": 5, "score": 7, "selftext": "Else what about using https://github.com/praw-dev/praw as a Jabber bot like https://github.com/fritzy/SleekXMPP/wiki ?", "title": "Is there a solution (e.g. irssi script or bitblee plugin) to have Reddit inbox and messages in an IRC client?", "url": "https://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/"}, {"author": "ipitythefoobar", "created_utc": 1364109411, "gilded": 0, "name": "t3_1awkvu", "num_comments": 2, "score": 3, "selftext": "I'm trying to run some tests on a shared web server and but I get a permissions error when I try to get praw on the server. \\# easy_install praw >[Errno 13] Permission denied: '/usr/lib/python2.6/site-packages/test-easy-install-20367.write-test' >The installation directory you specified (via --install-dir, --prefix, or the distutils default setting) was: > /usr/lib/python2.6/site-packages/ >Perhaps your account does not have write access to this directory? If the installation directory is a system-owned directory, you may need to sign in as the administrator or \"root\" account. If you do not have administrative access to this machine, you may wish to choose a different installation directory, preferably one that is listed in your PYTHONPATH environment variable. I assume this is because I'm on a shared server and don't have permissions into the communal Python directory. How would I go about getting around this issue?", "title": "Permissions issue when trying to easy-install praw on a hosted web server", "url": "https://www.reddit.com/r/redditdev/comments/1awkvu/permissions_issue_when_trying_to_easyinstall_praw/"}, {"author": "StealsTopComments", "created_utc": 1364092860, "gilded": 0, "name": "t3_1aw7jc", "num_comments": 3, "score": 3, "selftext": "I'm trying to use PRAW's get_info() method (a wrapper for the /api/info API method) to get posts for some URLs. For some reason the behavior seems to be inconsistent. I've been testing some URLs while learning just to see the format of responses and whatnot, but some URLs (that I got from actual posts) give responses indicating that there are no posts for that URL. For example, the URL: http://i.imgur.com/jOG9a.jpg is used in [this post](http://www.reddit.com/r/gaming/comments/10no3v/the_pokemon_holy_grail/). If I fire up Python / PRAW in my terminal, this is the result: >>> import praw >>> r = praw.Reddit(user_agent='testing /u/StealsTopComments') >>> r.get_info(url='http://i.imgur.com/jOG9a.jpg') [] Can anyone shed some light on this? Am I misunderstanding what /info is for?", "title": "PRAW get_info(): not working?", "url": "https://www.reddit.com/r/redditdev/comments/1aw7jc/praw_get_info_not_working/"}, {"author": "AlexanderSalamander", "created_utc": 1363978474, "gilded": 0, "name": "t3_1atckm", "num_comments": 1, "score": 0, "selftext": "A form exists somewhere (I could host it on a webserver). The form has a single field: \"username\". You enter your username. Through PRAW, a reddit account bot makes you a moderator of /r/godmode. Simple as that. That way people can goof around and anyone can be re-added as a moderator at any time. What is entailed here? Is there a tutorial for little things like this? Is it actually complicated?", "title": "A seemingly simple task, but I have no clue. Any help?", "url": "https://www.reddit.com/r/redditdev/comments/1atckm/a_seemingly_simple_task_but_i_have_no_clue_any/"}, {"author": "shrayas", "created_utc": 1363520211, "gilded": 0, "name": "t3_1agkmm", "num_comments": 2, "score": 5, "selftext": "Hi, Im trying to make an app just for learning purposes. I want to get the list of subreddits that a user is subscribed to. Of course i can use the `.login` method within PRAW but i felt that using oauth would give the user better confidence. I managed to get this going and you can find it over [here](https://github.com/shrayas/slashsub) on my Github. But my question is this: Why does one have to authorize the app every time he tries to login with reddit? isn't that a one time thing? If he's already logged in to Reddit it should automatically redirect back to the callback URL is what i feel. Maybe i'm doing something wrong.. In a gist here's what im doing #get object r = praw.Reddit(...) # set oauth info r.set_oauth_app_info(...) # get URL r.get_authorize_url(...) # Open a web browser and go to URL # Here is where the authorization page is shown everytime #on callback accessInfo = r.get_access_details(...) r.set_access_credentials(...) #do whatever i want r.get_me() #for instance Any help would be appreciate. Please excuse the n00bness in the question (if any)", "title": "PRAW and Oauth", "url": "https://www.reddit.com/r/redditdev/comments/1agkmm/praw_and_oauth/"}, {"author": "bboe", "created_utc": 1363368466, "gilded": 0, "name": "t3_1aczcr", "num_comments": 4, "score": 12, "selftext": "`pip install -U praw` will update you to the latest version that supports listing wiki pages via `r.get_wiki_pages('subreddit')`, creating / editing wiki pages via `r.edit_wiki_page('subreddit', 'page_title', 'content', 'reason')`. Read the [2.0.13 changelog](http://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-0-13) for additional changes. If there is additional functionality that you would like to see added, please [create an issue](https://github.com/praw-dev/praw/issues?state=open).", "title": "PRAW 2.0.13 adds basic wiki editing support", "url": "https://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/"}, {"author": "mrg3_2013", "created_utc": 1362624200, "gilded": 0, "name": "t3_19tiyz", "num_comments": 4, "score": 7, "selftext": "I am getting started w/ praw-dev and from other posts it looks like there is a limit of 1000 results that cannot be violated. What I am trying to do is to crawl for posts with title string containing key word (say \"happy dog\") along with an image. I don't need the comments. Can anyone please recommend (or point me to sample code) for this ? I am wondering if there is a way to get just the title + image associated without burdening the backend (and hopefully somehow get past the 1000 limit). Thanks! I don't need to do this real-time - so pacing requests are perfectly fine.", "title": "Using PRAW to search posts with a title keyword", "url": "https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/"}, {"author": "SN4T14", "created_utc": 1362575533, "gilded": 0, "name": "t3_19rubs", "num_comments": 14, "score": 7, "selftext": "A few days ago, I added a YouTube bot, [Groompbot](https://github.com/AndrewNeo/groompbot), to the /r/nerdcubed subreddit, I made it run every minute via cron job, so it would always be the first to post, and it worked great. Then one day it stops posting, I take a look to find that it throws an error 429 (Too many requests) every time, I contacted the maker of it a few days ago, but he hasn't replied yet. Here's the error: ERROR:root:Error logging into Reddit. Traceback (most recent call last): File \"groompbot.py\", line 27, in getReddit r.login(settings[\"reddit_username\"], settings[\"reddit_password\"]) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\", line 857, in login self.request_json(self.config['login'], data=data) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\", line 223, in error_checked_function return_value = function(cls, *args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\", line 396, in request_json response = self._request(url, params, data) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\", line 283, in _request timeout=timeout) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\", line 64, in __call__ result = self.function(reddit_session, url, *args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\", line 167, in __call__ return self.function(*args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/helpers.py\", line 161, in _request response.raise_for_status() File \"/usr/local/lib/python2.6/dist-packages/requests/models.py\", line 638, in raise_for_status raise http_error HTTPError: 429 Client Error: Too Many Requests Any ideas, Reddit?", "title": "Issues with YouTube bot", "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "MrFanzyPanz", "created_utc": 1362121650, "gilded": 0, "name": "t3_19g943", "num_comments": 2, "score": 9, "selftext": "Hello, Redditdev! I want to pull the score and author of the comments attached to a link. Preferably on a timely basis. Can get_comments be used on a submission? Is there a way to do this using PRAW that I'm missing? Thanks! :D **P.S.** Shout out to bboe and Deimorz for all your help!", "title": "Any way to glean comments from a link?", "url": "https://www.reddit.com/r/redditdev/comments/19g943/any_way_to_glean_comments_from_a_link/"}, {"author": "johnflim", "created_utc": 1361943598, "gilded": 0, "name": "t3_19b8mp", "num_comments": 2, "score": 3, "selftext": "Hi! I'm new at using PRAW and When using the below code, I can print my messages, but if a message is long enough, some of the message text is cut off. Am I using the generator wrong or is there a way to read the whole message? Thanks! import praw r = praw.Reddit(\"new bot by /u/johnflim\") r.login() inbox = r.get_inbox(11) for message in inbox: print message", "title": "PRAW get_inbox(), printing whole message from inbox", "url": "https://www.reddit.com/r/redditdev/comments/19b8mp/praw_get_inbox_printing_whole_message_from_inbox/"}, {"author": "MrFanzyPanz", "created_utc": 1361830594, "gilded": 0, "name": "t3_197uz8", "num_comments": 3, "score": 5, "selftext": "Hello, redditdev! I'm using PRAW to pull some basic information from Reddit, namely the top 500 posts from /r/pics. My code is very straightforward: import praw r = praw.Reddit(user_agent='MrFanzyPanz Datascraper :D') for post in r.get_subreddit('pics').get_hot(limit=500, url_data={'limit': 100}): *post.name *post.score *post.author *post.created_utc I'm writing it out to a csv to play around with. My problem is that, while this worked a couple weeks ago, I've revisited the scraper, and I'm now getting this error when I run it: TypeError: get_content() got an unexpected keyword argument 'url_data' Was url_data removed in an update? I've update PRAW to the newest version. Thanks!", "title": "PRAW: url_data removed from get_content()", "url": "https://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/"}, {"author": "lamerx", "created_utc": 1361818048, "gilded": 0, "name": "t3_197erw", "num_comments": 5, "score": 6, "selftext": "I cannot seem to get a bot to remove or delete a submission even when both the submission and the subreddit being posted to belongs to the account im using. Consider reddit = praw.Reddit(user_agent='BotScript 1.0, by u/LamerX') print \"Logging in...\" user='LamerX' passwd='password' reddit.login(user,passwd) print \"Logged In\" reddit.get_submission(submission_id=id of submission i own) submission.remove() This come ALWAYS draws the following error NotLoggedIn: `please login to do that` on field `None`", "title": "Praw Moderator Bot & Remove Method", "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "atomicUpdate", "created_utc": 1361508544, "gilded": 0, "name": "t3_1905ed", "num_comments": 8, "score": 3, "selftext": "I followed the example for how to use OAuth with PRAW to gain access to a user's account that's [here](https://github.com/praw-dev/praw/wiki/OAuth). However, I'm wondering what is the right way to reuse that access once I've gotten the tokens from Reddit. I created a 'get_user_data()' function to reuse the token, and within that I perform an r.refresh_access_information(refresh_token). Unfortunately, this doesn't seem to immediately update the user information for the user that the refresh token is for. Instead, it seems to take 2 calls to get_user_data() in order to get the current information for the user I'm interested in. If r.refresh_access_information() is the correct function to use, how to I force PRAW to block long enough to get the information for the user that the refresh token is for, rather than returning the information for the last user that get_user_data() was called for? [My code is available here.](http://pastebin.com/tib9Hm5z) As you can probably tell from looking at it, the idea is to be able to have multiple widgets use the same server to request data from Reddit. However, I'm pretty new to all of this, so I may be going about it all wrong...", "title": "How to reuse OAuth Tokens with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "JRDubstepcom", "created_utc": 1361176344, "gilded": 0, "name": "t3_18qq0d", "num_comments": 8, "score": 7, "selftext": "it seems everyone knows how to add certain functions to there programs yet i cant find a standard place to view examples. for instance \"submission.selftext.lower()\" how or where can i find information on this function? from what i can tell is that this code snipbit returns a links text. but what could i change? aka submission.titletext()? submission.linkurl()? or something of the sort. where would i be able to find information on different functions? i guess im having a hard time understanding praw because i dont know what i can do with it. thanks guys, sorry for newb question.", "title": "is there a praw library of some sort?", "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "ipitythefoobar", "created_utc": 1361107451, "gilded": 0, "name": "t3_18ov4m", "num_comments": 16, "score": 10, "selftext": "I want to use PRAW and do some automated data collection on a regular basis. I could run this from home but I'm pretty OCD and don't want an Internet hiccup, computer crash, or power outage to miss one of the polling times so ideally I could run this on a server. I have a cheap hosting account but I don't know if I can run a bot script from there. I'm fine writing PHP and JavaScript and stuff but running command line type stuff gives me pause especially when it involves importing stuff. I don't want the hosting company to get mad at me if I'm not releasing resources correctly. So what are some options and best practices for writing a Reddit bot? Thanks!", "title": "Advice on running a Reddit bot written in Python on a shared server?", "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "Bornhuetter", "created_utc": 1360703448, "gilded": 0, "name": "t3_18eeep", "num_comments": 2, "score": 9, "selftext": "I recently ported moderatorbot to PRAW 2, and had a couple of issues. bboe was helpful as always, and I thought I'd reproduce here what I did to get my code working, in case others can learn from this. **Issue 1 - my_moderation** In PRAW 1.x the way to get a list of subreddits that the logged in user moderates, you use: For subreddit in r.user.my_moderation(): In PRAW 2.x the correct way is now: For subreddit in r.get_my_moderator(): (and don't forget the () after r.get_my_moderator as I initially did) **Issue 2 - _request** In PRAW 1.x, I used the following to make HTTP requests response = r._request(url, params) In PRAW 2.x params has been changed to data, and the following needs to be used. response = r._request(url, data=data) In this particular case I was trying to make an HTTP request to remove a comment. bboe recommended that I use r.request_json(url, data=data) > *you should use r.request_json as you can't count on r._request to always be around.* In this case that will work (even though I am not actually trying to get json data). In other cases I believe that I still need to use _request. I believe that some pages do not return a json properly (or the json does not have all the data I need), but the result of _request can be parsed using beatifulsoup. I don't have any code currently in production that uses beautifulsoup on _request, so I can't remember the exact situations I used it for. So, I'd like to ask bboe to consider putting some sort of _request functionality into supported \"user space\"", "title": "Resolved issues porting to PRAW 2", "url": "https://www.reddit.com/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/"}, {"author": "shaggorama", "created_utc": 1359923103, "gilded": 0, "name": "t3_17tn7r", "num_comments": 5, "score": 8, "selftext": "Hey, I have a bot that uses all_comments_flat to parse a submission's comments. I just set up a new development station and noticed that the newest version of praw doesn't have that method on the Submission object anymore, and the comment attribute is a tree. What's the easiest way to just get all the comments on a submission? I'm suspecting that I'll need to use some kind of \"load more comments\" function to get all the available comments but I'm not sure where it is. Thanks,", "title": "Replacement for all_comments_flat", "url": "https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/"}, {"author": "MrFanzyPanz", "created_utc": 1359765151, "gilded": 0, "name": "t3_17q7b3", "num_comments": 13, "score": 5, "selftext": "Hello! I'm using PRAW to get some of the links/comments from a list of users' history and their data, but after only two are three accounts I'll be returned a 404 error. I check if a user account still exists before pulling the data, so I know it's not that the account was deleted. The 404 error doesn't appear to be connected to a problem in my code: it fails at random times while running the script. Is it possible that I'm getting kicked from my connection to the server? I'm using get_comments() and get_submitted() to pull the information.", "title": "404 Error during account scraping", "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "IcyRespawn", "created_utc": 1359163692, "gilded": 0, "name": "t3_17aiml", "num_comments": 6, "score": 7, "selftext": "Hi guys, I've just started using PRAW today and I'm absolutely loving it. I did run into some issues while writing a bot. It's supposed to check a specific subreddit for the newest posts, read their contents and alert me whenever it finds something that matches a certain regular expression within them. I'm checking for new submissions every 10 seconds, by calling s.get_new_by_date(limit=5) on my subreddit's Subreddit object. The problem is that after the initial call, new posts are usually only fetched after anywhere from 30 seconds to a few minutes AFTER they were created, where I expect them to show up within a maximal amount of 10 seconds from the moment they were created. They do appear when I refresh the page in my browser. Things I tried doing to resolve this: - My user agent follows the API guidelines and isn't generic. - Setting my local praw.ini file so that cache_timeout=5 to ensure I'm not reading from the cache. - Calling s.refresh() on my Subreddit object after every sleep(10). Do you guys have any idea what might be causing this to happen? Is it because pages are cached server-side, and so I cannot force a refresh on the user's side? If so, why do pages usually take more than a minute (which is far more than the forced 30-second limitation if there is in fact one) to be detected by my algorithm? Thanks for reading, I appreciate your time. Icy", "title": "[PRAW] Getting newest posts to show up as quickly as  possible", "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "jokoon", "created_utc": 1359130013, "gilded": 0, "name": "t3_179di4", "num_comments": 6, "score": 5, "selftext": "import praw r = praw.Reddit(user_agent='just to dl my comments and likes') r.login('jokoon', 'xxxxxx') f = open(\"comments.txt\",'w') r.set_oauth_app_info(client_id='xxxxxxxx-xx', client_secret='xxxxxxxxxxxxxxxxxx', redirect_url='http://127.0.0.1:65010/authorize_callback') url = r.get_authorize_url('xxxxxxxx', 'xxxxxx-xx', True) import webbrowser webbrowser.open(url) # user = r.get_redditor('jokoon') # f.write(r.get_me().get_liked()) I get the error set_oauth_app_info() got an expected keyword argument 'redirect_url'", "title": "Trying to get my comments and likes", "url": "https://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/"}, {"author": "takitesi", "created_utc": 1358894052, "gilded": 0, "name": "t3_1730dr", "num_comments": 3, "score": 0, "selftext": "I tried posting this at an earlier date but received no responses so I'm trying again. I'm not sure if anyone here takes requests, but here it goes. I'd like a web app or chrome extension similar to [Group Reddit Saved Links] (https://chrome.google.com/webstore/detail/group-reddit-saved-links/gkchebpjpehcnlbjamhfgoconkmoalaf?hl=en) (which doesn't work for me for some reason). In a recent thread, /u/ButtCrackFTW provided a simple program to [download all saved links] (http://www.reddit.com/r/redditdev/comments/161odw/wrote_a_simple_script_in_python_using_praw_to/). In addition to the saved link's title, URL, and comments, I'd like to get the subreddit and, if possible, the date on which it was saved. I'd also like to be able to add my own \"tag\" to a post and change its title. I now need a way to organize the information for each link. I'm thinking if it was exported to a table structure with (sortable) headers something like this: Link Title (editable) | URL | Subreddit | Comments Link | Tags | Date It may be a longshot, but if anyone could make this a reality it would be much appreciated! Bonus functionality: Any saved comments that are in a saved link get displayed in the Comments Link column field", "title": "A request for saved links", "url": "https://www.reddit.com/r/redditdev/comments/1730dr/a_request_for_saved_links/"}, {"author": "red_foot", "created_utc": 1358712705, "gilded": 0, "name": "t3_16y13b", "num_comments": 9, "score": 11, "selftext": "I keep getting a JSONDecodeError, anyone gotten PRAW to work on Pythonanywhere? **EDIT:** Got past the first hurdle, but then experienced this error when using PRAW login. Traceback (most recent call last): File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1687, in wsgi_app response = self.full_dispatch_request() File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1360, in full_dispatch_request rv = self.handle_user_exception(e) File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1358, in full_dispatch_request rv = self.dispatch_request() File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1344, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File \"/home/myapp.py\", line 21, in login return saved_links_page(fetch_Links(str(form.username.data), str(form.password.data))) File \"/home/myapp.py\", line 33, in fetch_Links r.login(str(username), str(password)) File \"/home/.local/lib/python2.7/site-packages/praw/__init__.py\", line 804, in login self.request_json(self.config['login'], data=data) File \"/home/.local/lib/python2.7/site-packages/praw/decorators.py\", line 211, in error_checked_function return_value = function(cls, *args, **kwargs) File \"/home/.local/lib/python2.7/site-packages/praw/__init__.py\", line 375, in request_json response = self._request(url, params, data) File \"/home/.local/lib/python2.7/site-packages/praw/__init__.py\", line 266, in _request timeout=timeout) File \"/home/.local/lib/python2.7/site-packages/praw/decorators.py\", line 64, in __call__ result = self.function(reddit_session, url, *args, **kwargs) File \"/home/.local/lib/python2.7/site-packages/praw/decorators.py\", line 155, in __call__ return self.function(*args, **kwargs) File \"/home/.local/lib/python2.7/site-packages/praw/helpers.py\", line 149, in _request response.raise_for_status() File \"/home/.local/lib/python2.7/site-packages/requests/models.py\", line 638, in raise_for_status raise http_error HTTPError: 501 Server Error: Not Implemented", "title": "Pythonanywhere.com and PRAW", "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "takitesi", "created_utc": 1358552912, "gilded": 0, "name": "t3_16uio8", "num_comments": 0, "score": 3, "selftext": "I'm not sure if anyone here takes requests, but here it goes. I'd like a web app or chrome extension similar to [Group Reddit Saved Links] (https://chrome.google.com/webstore/detail/group-reddit-saved-links/gkchebpjpehcnlbjamhfgoconkmoalaf?hl=en) (which doesn't work for me for some reason). In a recent thread, /u/ButtCrackFTW provided a simple program to [download all saved links] (http://www.reddit.com/r/redditdev/comments/161odw/wrote_a_simple_script_in_python_using_praw_to/). In addition to the saved link's title, URL, and comments, I'd like to get the subreddit and, if possible, the date on which it was saved. I'd also like to be able to add my own \"tag\" to a post and change its title. I now need a way to organize the information for each link. I'm thinking if it was exported to a table structure with (sortable) headers something like this: Link Title (editable) | URL | Subreddit | Comments Link | Tags | Date It may be a longshot, but if anyone could make this a reality it would be much appreciated! Bonus functionality: Any saved comments that are in a saved link get displayed in the Comments Link column field", "title": "Does /r/redditdev take small requests?", "url": "https://www.reddit.com/r/redditdev/comments/16uio8/does_rredditdev_take_small_requests/"}, {"author": "takitesi", "created_utc": 1358429203, "gilded": 0, "name": "t3_16r0m0", "num_comments": 3, "score": 4, "selftext": "Basically, I'd like to be able to go to a website that has an online Python compiler/IDE that will run small python scripts that use PRAW, rather than having to set up a local environment. Does something like this exist? Or is there a way to do it fairly easily? Any help is much appreciated.", "title": "Has anyone built a web app that works with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/16r0m0/has_anyone_built_a_web_app_that_works_with_praw/"}, {"author": "lamerx", "created_utc": 1358385471, "gilded": 0, "name": "t3_16q02d", "num_comments": 2, "score": 4, "selftext": "Im sorry to have to come back here and ask yet another question. The praw documentation is too complicated and I dont really understand it. I cant find any information on the methods, functions, etc. I cant seem to figure out how to get a comments karma bu its id, consider the following for submission in reddit.get_subreddit(sub).get_top(limit=40): flat_comments = submission.all_comments_flat for comment in flat_comments: ident = str(comment.id) karma=comment.?? I cant seem to find the karma score of the comment", "title": "Praw + karma", "url": "https://www.reddit.com/r/redditdev/comments/16q02d/praw_karma/"}, {"author": "bboe", "created_utc": 1358245022, "gilded": 0, "name": "t3_16m0uu", "num_comments": 4, "score": 24, "selftext": "PRAW users, I've been working on-and-off for the last few weeks (with significant help from /u/_Daimon_) on extending the initial OAuth2 support added to PRAW by /u/intortus. My original plan was for addition-only changes thus resulting in a backward-compatible 1.1 release. However, properly handling reddit's OAuth2 scopes necessitated a number of backwards incompatible namespace changes (which in turn prompted numerous other namespace changes) thus I am bumping the major version number. To minimize the amount of code from breaking that has a PRAW dependency, I wanted to give everyone a little time to update their projects before I add PRAW 2.0 to the cheeseshop (pypi). If you have code that depends on PRAW and you don't want it to break for other users of your projects you have two primary options: The first option is to simply update your `setup.py` or `requirements.txt` file to depend on any PRAW version less than 2.0 (1.0.16 will be the last 1.0 version). If your package installation does not automatically handle dependencies then you can inform your users to run `pip install praw==1.0.16` to get an appropriate version, or point them to [this source tarball](https://github.com/praw-dev/praw/archive/praw-1.0.16.tar.gz) or [this source zipfile](https://github.com/praw-dev/praw/archive/praw-1.0.16.zip). The second option, of course, is to update your package to work with PRAW versions >= 2.0. To help you get started you'll want to checkout the PRAW 2.0 [Change Log](https://github.com/praw-dev/praw/wiki/Changelog). Until I actually release the package on pypi, you'll need to clone the github repository (`git clone git://github.com/praw-dev/praw.git`), or manually download the source ([zipfile](https://github.com/praw-dev/praw/archive/master.zip)). I'm happy to answer any questions you have as replies to this submission. Assuming no major issues are discovered, I will release PRAW 2.0 Wednesday evening (PST). Edit: I should add that if you are interested in using PRAW via OAuth2, check out the [PRAW OAuth wiki page](https://github.com/praw-dev/praw/wiki/OAuth).", "title": "PRAW 2.0 is Coming (release in ~2 days)", "url": "https://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/"}, {"author": "mpheus", "created_utc": 1357833724, "gilded": 0, "name": "t3_16bh8j", "num_comments": 5, "score": 20, "selftext": "I have a little obsession of checking /r/gamedeals frequently for new game deals. Just wrote my first useful python script to make my life a little easier. The script checks for new posts made to that sub-reddit and pushes a Growl notification. The Growl notification looks like - http://i.imgur.com/ET9vL.png #!/usr/bin/env python # # This script looks up /r/gamedeals/new every 120 seconds and pushes the notification # for new posts to Growl app on Mac OS # # Uses PRAW - https://github.com/praw-dev/praw (for easy access to Reddit API) # and GNTP - https://github.com/kfdm/gntp (for pushing Growl notification) # import gntp.notifier import praw import time # icon for use with growl notification ICON_URL = \"http://cdn2.iconfinder.com/data/icons/crystalproject/128x128/apps/package_games.png\" USER_AGENT = 'new /r/gamedeals notifier by /u/mpheus' # reddit doesn't shows new posts made to a subreddit without logging in # REDDIT_ID = '' # not required # REDDIT_PASS = '' # not required growl = gntp.notifier.GrowlNotifier( applicationName = \"/r/gamedeals notifier\", notifications = [\"New Deal\"], defaultNotifications = [\"New Deal\"], # hostname = \"computer.example.com\", # Defaults to localhost # password = \"abc123\" # Defaults to a blank password ) growl.register() r = praw.Reddit(user_agent=USER_AGENT) # r.login(REDDIT_ID, REDDIT_PASS) already_done = [] # for storing the uids of posts already notified while True: data = r.get_subreddit('gamedeals').get_new_by_date(limit=10) for x in data: if x.id not in already_done: already_done.append(x.id) growl.notify( noteType = \"New Deal\", title = x.domain, description = x.title, icon = ICON_URL, sticky = True, # so that notification remains on the screen until closed priority = 1, callback = x.permalink ) print \"Notified about\", x.title, \"at\", time.strftime(\"%d %b - %I:%M:%S %p\") print \"Last checked for game deals at\", time.strftime(\"%d %b - %I:%M:%S %p\") time.sleep(120) I just launch this script in a terminal window and leave it running. I'm learning python right now and plan on improving the script by storing the uids of posts that already have been notified about in a database and leave the script running on Raspberry Pi that runs 24/7. Growl can be set to receive notification pushed from remote computer (in this case, RPi) as well. Thanks to reddit dev and PRAW dev for all their amazing work! EDIT: Removed login call and replaced `get_new` function with `get_new_by_date`.", "title": "Wrote my first python script using PRAW to check /r/gamedeals for new posts and notify me", "url": "https://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/"}, {"author": "MrFanzyPanz", "created_utc": 1357512160, "gilded": 0, "name": "t3_1630jj", "num_comments": 6, "score": 5, "selftext": "Hi, redditdev! I'm having some trouble with my code. I'm collecting a large amount of data for a survey experiment and I'm new to Python as well as Stata, so I'm just writing all the data out to .txt files in CSV. Usually my code works fine, but I've tried running it recently and keep encountering an error related to post author's names. The error is: \"'None-type' object has no attribute 'name'\". I've looked at the posts which it corresponds to and they seem like normal posts. I'm at a loss for what is wrong, can anybody help? I've attached my code for posterity, although I don't think it's needed for this fix. from datetime import datetime import praw import time r = praw.Reddit(user_agent='your unique user agent') def datascraper(counter, subreddit): #Open up the files to which the data will be written filename = 'dataset_' + subreddit + '.txt' filename_titles = 'dataset_' + subreddit + 'titles.txt' namelist = [] txt = open(str(filename), 'a') title = open(str(filename_titles), 'a') # If this is the first time that this document is being # opened, we need to insert the headings first (for CSV format) if counter == 0: txt.write(\"rank , subreddit , num_comments , net_score , is_self, over_18 , downs , url , is_imgur , author , time_created , time_current , ups \\n\") title.write('rank, title \\n') counter = 1 for post in r.get_subreddit(subreddit).get_hot(limit=500, url_data={'limit': 100}): # write to CSV/titles_subreddit files # data listed as needed is in order: txt.write(str(counter) + ' , ') txt.write(post.subreddit.display_name + ' , ') txt.write(str(post.num_comments) + ' , ') txt.write(str(post.score) + ' , ') txt.write(str(post.is_self) + ' , ') txt.write(str(post.over_18) + ' , ') txt.write(str(post.downs) + ' , ') txt.write(post.url + ' , ') txt.write(str(post.domain.endswith('imgur.com')) + ' , ') name = post.author.name **This is where the problem is** namelist.append(name) txt.write(name + ' , ') txt.write(str(post.created_utc) + ' , ') txt.write(str(int(time.time())) + ' , ') txt.write(str(post.ups) + '\\n') post_title = post.title.encode('ascii', 'ignore') title.write(post_title + '\\n') counter += 1 txt.close() title.close() return counter, namelist Thanks for your input!", "title": "PRAW is returning post.author.name errors", "url": "https://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/"}, {"author": "bheklilr", "created_utc": 1357225935, "gilded": 0, "name": "t3_15w2cs", "num_comments": 3, "score": 5, "selftext": "I'm completely new to `praw`, so please excuse my ignorance. I want to write a simple bot that reads from an RSS feed and submits posts based on that feed. I already have permission from the RSS owner to post to their subreddit this way. Is there a quick example for this somewhere? I just need to submit a link, no self posts. Thanks!", "title": "Help with submitting with Praw", "url": "https://www.reddit.com/r/redditdev/comments/15w2cs/help_with_submitting_with_praw/"}, {"author": "burntsushi", "created_utc": 1357183583, "gilded": 0, "name": "t3_15v7l4", "num_comments": 3, "score": 8, "selftext": "The title says it all. I have some hope that a workaround is possible, particularly since I can still *see* older comments (from old saved links or old submissions), they just don't seem to be tied to the user page. My instinct says that the comments still exist in the database some where, but is there any way to get to them? N.B. I'm using PRAW.", "title": "Retrieving a user's comments seems to be limited to 1000... is there a way around this?", "url": "https://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/"}, {"author": "lamerx", "created_utc": 1356846633, "gilded": 0, "name": "t3_15nuau", "num_comments": 4, "score": 10, "selftext": "Could someone be so kind as to direct me to any documentation that may exist for using praw to create a new or even write to an existing CSS of a subreddit?", "title": "Praw + CSS", "url": "https://www.reddit.com/r/redditdev/comments/15nuau/praw_css/"}, {"author": "downbound", "created_utc": 1356681257, "gilded": 0, "name": "t3_15kc2j", "num_comments": 3, "score": 14, "selftext": "Honestly this new fangled API structure is a bit foreign to me but I am learning some. However, I have hit a roadblock that is certain to ruin all my programmer cred. I program in python and have gotten OAuth working. I am getting a access_token. Now I feel stupid. I have not the foggiest how to make an API call through PRAW or anything else that uses that access_token. I am simply trying to find the username (eventually some other things but not now) of the person who got the access_token. It really must be something so easy that I am missing that there isn't even a guide. import requests url=\"https://ssl.reddit.com/api/me.json?\" data= { 'access_token': 'access_token', 'scope' : 'http://www.my.url' } print requests.get(url, data=data).text This gets me an empty JSON response {} but, HTTP return of 200. . .I dunno", "title": "Embarrased I need the \"Hello World\" of API", "url": "https://www.reddit.com/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/"}, {"author": "expiredtofu", "created_utc": 1356583886, "gilded": 0, "name": "t3_15i5c8", "num_comments": 6, "score": 0, "selftext": "~~I was playing with PRAW earlier on my computer, and I think I ran my script (which just displays new posts from r/all) a bit too fast. Now, I cannot get any information with PRAW, and when I go to ssl.reddit.com, I am greeted with the message \"you appear to be a bad robot\". Is it possible for this ban to be lifted?~~ EDIT: Tried again with a new script, the errors were just my fault. Thanks for the help!", "title": "Is it possible for my bot to be unbanned?", "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "MrFanzyPanz", "created_utc": 1355858609, "gilded": 0, "name": "t3_152d2w", "num_comments": 2, "score": 5, "selftext": "Hello! Silly question here, sorry for the noobacity. I'm trying to use praw's get_content() generator in order to pull data from subreddits, however I don't know how to iterate over this object. I keep receiving an error that pics.json is an unknown url type: import praw import pprint r = praw.Reddit('Test Code - MrFanzyPanz') content_scrape = r.get_content(\"pics\", limit = 10) for submission in content_scrape: print submission **or** print submission.title If there is an alternative method which will allow me to simply parse www.reddit.com/.json as text, that would be also fine, although I need to be able to do it through python requests (preferably praw), rather than copy-pasting the text from the page manually.", "title": "Praw URL parameters", "url": "https://www.reddit.com/r/redditdev/comments/152d2w/praw_url_parameters/"}, {"author": "LudoA", "created_utc": 1355857718, "gilded": 0, "name": "t3_152c5b", "num_comments": 9, "score": 5, "selftext": "I'm using PRAW and trying to access the saved links of a logged in user. Basically, I'm following the steps mentioned under \"[A Few Short Examples](https://github.com/praw-dev/praw/wiki)\", which include an example on how to get the saved links. Unfortunately, I'm getting an error (\"has no attribute 'get_saved'\"). C:\\>python Python 2.7.2 (default, Jun 12 2011, 15:08:59) [MSC v.1500 32 bit (Intel)] on win32 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import praw >>> r = praw.Reddit(user_agent='example') >>> r.login('LudoA', 'foobar') >>> r.user.link_karma 2592 >>> r.user.get_saved() Traceback (most recent call last): File \"\", line 1, in File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 70, in __getattr__ attr)) AttributeError: '' has no attribute 'get_saved' >>> r.user Redditor(user_name='LudoA') >>> user = r.get_redditor('ludoa') >>> user Redditor(user_name='ludoa') >>> user.link_karma 2592 >>> user.get_saved() Traceback (most recent call last): File \"\", line 1, in File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 70, in __getattr__ attr)) AttributeError: '' has no attribute 'get_saved' >>> So it does see *r.user* as being of the right type (i.e. *LoggedInRedditor*), but it can't find the method *get_saved()*, just like with the non-logged in user object. Any idea as to what I'm doing wrong? Thanks!", "title": "PRAW: can't access saved links - no attribute get_saved", "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "esacteksab", "created_utc": 1355086020, "gilded": 0, "name": "t3_14kakv", "num_comments": 6, "score": 6, "selftext": "Doing a quick r.user.has_mail, it continues to return false, even after sending myself a message. End the session, poll Reddit again, I have a message. Check messages, mark as read via web...poll via ipython/cli and it still continues to return true. Anyone have any experience with this? I normally just use requests + json to interact with Reddit, but thought I'd use PRAW this time. Any thoughts, advice, etc would be greatly appreciated! Thanks!", "title": "Does PRAW cache results with iPython? ", "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "rhiever", "created_utc": 1348954947, "gilded": 0, "name": "t3_10omtd", "num_comments": 3, "score": 10, "selftext": "Hi all, I'm trying to implement global ratelimiting for my bot so I can run multiple PRAW bots in parallel, but apparently I'm not catching every time PRAW makes a request to reddit. Can someone please clarify how many requests are being made in the following code snippets? I count this as 1 request: r.get_subreddit(\"all\").get_top(limit=100) I count r.get_front_page() as 1 request, then each submission.comments as another request. posts = [] for submission in r.get_front_page(): for comment in submission.comments: posts.append(comment) I count this as 1 request: r.get_redditor(user).get_overview(limit=2000)", "title": "PRAW: when are requests being made? (code included)", "url": "https://www.reddit.com/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/"}, {"author": "binaryechoes", "created_utc": 1348457271, "gilded": 0, "name": "t3_10dizi", "num_comments": 4, "score": 3, "selftext": "Just looking over some data for a project. I've processed over 2.5 million comments and banned_by and num_reports is always blank. Although the project isn't using these fields I could see how they could be of use. Surely, of all those comments one of these would have something (maybe). Is this mod only territory? Anything I'm missing or think my code is flawed? Does someone have a comment/submission handy for me to verify? http://www.reddit.com/r/redditdev/comments/o4op9/accessing_moderatorrestricted_data_via_api/ https://github.com/reddit/reddit/issues/429 PS I'm using praw. Thanks", "title": "banned_by and num_reports fields", "url": "https://www.reddit.com/r/redditdev/comments/10dizi/banned_by_and_num_reports_fields/"}, {"author": "TankorSmash", "created_utc": 1341637471, "gilded": 0, "name": "t3_w60cs", "num_comments": 11, "score": 3, "selftext": "There have been several answers over the last two years, (all by /u/bboe) in regards to this question, but I'm just not able to figure it out for myself. http://www.reddit.com/r/redditdev/comments/icbh1/how_do_i_use_the_more_entries_in_the_json_reply/ http://www.reddit.com/r/redditdev/comments/o4jq0/is_it_possible_to_get_a_complete_comment_when/ http://www.reddit.com/r/redditdev/comments/aql1m/some_questions_on_morechildren_api_or_load_more/ Basically I should know what I'm doing but the official docs are a bit iffy, and looking at the PRAW code isn't helping. Apparently I need to send the following parameters: params = {\"link_id\": 't3_w5iwp', \"children\": 'c5afy83', \"depth\": '', \"id\": 'c5ag775', \"pv_hex\":\"\", \"r\":'gaming', \"renderstyle\":\"\", \"api_type\":'json'} Then I send it like this: url = r'http://reddit.com/api/morechildren' requests.get(url,params=params) #it's actually a requests.session object, logged in. But I'm getting a `not found` error page. Even copying the url from the Apigee page doesn't seem to be working", "title": "Using morechildren without PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "bboe", "created_utc": 1341113171, "gilded": 0, "name": "t3_vv4tg", "num_comments": 2, "score": 10, "selftext": "Three things: With /u/mellort's support, I have moved what was previously [mellort/reddit_api](https://github.com/mellort/reddit_api) to [praw-dev/praw](https://github.com/praw-dev/praw) on github. /u/mellort has made a clone from the new location, so that existing links to the repository are not broken. Additionally I've official updated the name to __PRAW__ which is an acronym for \"Python Reddit API Wrapper\". The primary impetus for the name change was to resolve the confusion I see from many people regarding what to call the package. I admit it was very confusing that the repository was called `reddit_api` and the package was called `reddit`. Hopefully it should be clear what name to use now: PRAW. Finally, to clearly distinguish the python package from reddit's own source, I have renamed the package from `reddit` to `praw` and restarted PRAWs version number at 1.0. Aside from the readthedocs documentation, all the documentation (on github) should be updated to reflect the change. Making the switch is pretty simple, here are the steps: 0. Install the `praw` package (instructions on github) 0. Replace `import reddit` with `import praw` in your code and of course update any `reddit.NAME` references with `praw.NAME` such as `reddit.Reddit` with `praw.Reddit`. 0. If you had a user-level, or script level `reddit_api.cfg` file, please replace that with `praw.ini`. There is more information [here](https://github.com/praw-dev/praw/wiki/The-Configuration-Files). If you have any questions / comments please don't hesitate to ask / share.", "title": "Python Reddit API Wrapper package rename and repository move", "url": "https://www.reddit.com/r/redditdev/comments/vv4tg/python_reddit_api_wrapper_package_rename_and/"}, {"author": "RedditSrc4Research", "created_utc": 1337070114, "gilded": 0, "name": "t3_tnxsj", "num_comments": 5, "score": 5, "selftext": "Two things I noticed earlier tonight while using the python reddit api wrapper [PRAW](https://github.com/mellort/reddit_api): 1) The domain in .config/reddit_api/reddit_api.cfg has to match the .ini config for the server, even if they both point to the same IP. I had a couple DNS names configured for my site, and PRAW started complaining that I hadn't logged in even though I had. E.G., if the .ini file has reddit1.example.com and your reddit_api.cfg file has reddit2.example.com (both of which point to your server), PRAW will not recognize your login (and not throw an error either until you try to do something that requires the login). It took a while to debug :/ 2) Here is an example for make_moderator, which is not shown on the wiki: import reddit r = reddit.Reddit('some user agent','myreddit') sub = r.get_subreddit('mysubreddit') r.login('mymoderator','password') sub.make_moderator('thenewmoderator')", "title": "A couple PRAW notes (domain issue and make_moderator)", "url": "https://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/"}, {"author": "bboe", "created_utc": 1332267227, "gilded": 0, "name": "t3_r5e5l", "num_comments": 5, "score": 7, "selftext": "Edit: 1.3.0 is official now. `pip install reddit` or `pip install --upgrade reddit` will install six if it is not already installed. I recently finished adding hybrid support for both Python 2.6+ and 3.2+ and I would really appreciate if some PRAW users would upgrade to the 1.3.0dev version of PRAW to ensure everything works as expected. I have extended PRAW's tests to include unicode testing and thus given that all the tests pass on both 2.6, and 3.2 I don't expect there to be any issues. Nevertheless it doesn't hurt to be sure. 1.3.0dev is not available via pip, thus you'll need to manually fetch it from [github](https://github.com/bboe/reddit_api/tree/python3). Going forward PRAW will depend on the `six` module which can be obtained via `pip install six`. To make sure you are running the correct version you can run: `python -c 'import reddit; print(reddit.VERSION)'` The output should be `1.3.0dev`. If you run into any issues, please follow up either here, or on #reddit-dev in irc.freenode.net. Thanks!", "title": "Python Reddit API Developers: Python 2/3 hybrid beta testing needed", "url": "https://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/"}, {"author": "13steinj", "created_utc": 1453907367, "gilded": 0, "name": "t1_czdvngk", "num_comments": null, "score": 3, "selftext": "If you want something even simpler (IMO), /u/SmBe19 made [`praw-OAuth2Util`](https://github.com/SmBe19/praw-OAuth2Util)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "GoldenSights", "created_utc": 1453604982, "gilded": 0, "name": "t1_cz9mvz6", "num_comments": null, "score": 2, "selftext": "Check out the _raw attribute. This should be the actual exception raised by the `requests` module. `e._raw.status_code`, etc. The thing is, PRAW has custom exceptions for 404 and 403 -- `praw.errors.NotFound`, `praw.errors.Forbidden`. That might be something you missed while testing, because you don't need to use the response code to distinguish those.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "zurtex", "created_utc": 1453592144, "gilded": 0, "name": "t1_cz9fwk4", "num_comments": null, "score": 2, "selftext": "Then the praw exception hasn't been given a response code. Make sure it's up to date and otherwise think about filling a bug with them.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "13steinj", "created_utc": 1453570484, "gilded": 0, "name": "t1_cz935mw", "num_comments": null, "score": 1, "selftext": "1. It would look like a normal comment permalink, and would be the permalink of the parent comment. The reason why you disregard the first 3 chars is because permalinks don't work with the tX\\_ prefix. Further more, you should make sure that the parent id isn't the id of the submission itself, because it theoretically can be and then you'd be grabbing a non existent parent. 2. You'd have to parse the replies backwards, recursively, so something like: def flat_branch(comment, reverse_root_id): if comment.replies[0].id == reverse_root_id: return [comment, comment.replies[0]] listed= [comment] listed.extend(flat_branch(comment.replies[0], reverse_root_id) return listed And then use `reversed(flat_branch(yourupmostcomment, therootyouarelpokingforsid))`. Keep in mind I haven't tested this so my recursive function may be wrong. This all said and done I have a longstanding pr because I unfortunately had to do this often and wanted to make this easier for people ([#512](https://github.com/praw-dev/praw/pull/512)), however combined with me being busy, it taking forever cause there's also a bug caused by an inconsistency on reddit's side, and my vm breaking a few days ago (but that last bit is just an excuse at this point), I haven't been able to continue.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "Joshjayk", "created_utc": 1453533278, "gilded": 0, "name": "t1_cz8qrg6", "num_comments": null, "score": 1, "selftext": "So... comments = praw.helpers.comment_stream(randombot, 'all') for comment in comments: parent = r.get_info(thing_id=comment.parent_id) parent2 = r.get_info(thing_id=parent.parent_id) print('%s' % parent.body) if parent2 is not None: print('%s' % parent2.body) Would that snippet of code get me the parent of the current comment, and the parent to that parent, if it exists?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "13steinj", "created_utc": 1453350878, "gilded": 0, "name": "t1_cz65i4m", "num_comments": null, "score": 2, "selftext": "Can you view that link in a browser under the account you are logged in as? If you can, and you are logged in via oauth on praw, do you have the `read` scope?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "SmBe19", "created_utc": 1453580085, "gilded": 0, "name": "t1_cz98x9m", "num_comments": null, "score": 2, "selftext": "If you don't want to handle OAuth yourself you can use something like [this](https://github.com/SmBe19/praw-OAuth2Util).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "kemitche", "created_utc": 1453240131, "gilded": 0, "name": "t1_cz4fbvv", "num_comments": null, "score": 1, "selftext": "If you're familiar with HTTP, you can spin up your own PRAW-like library for personal use (minus some bells and whistles) fairly easily. The trickiest part is figuring out the response format of API calls (since only inputs are documented on [/dev/api](/dev/api)) but a few example calls will get you most of the way (and peeking at PRAW source could help you too). OAuth 2 can be a tricky beast, but if it's just for a bot (i.e., you don't need access to more than a single account, and you control the credentials to that account), then you can use the password grant type to get your access token, which is a single call and should be straightforward.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "rubynew", "created_utc": 1452999536, "gilded": 0, "name": "t1_cz13wt1", "num_comments": null, "score": 1, "selftext": "It'll be something like this for overall comment score: import praw r = praw.Reddit(user_agent='my_cool_application') submissions = r.get_subreddit('learnprogramming').get_top_from_day(limit=15) for submission in submissions: print submission.title print submission.url print \"=\" * 30 submission.comments = sorted(submission.comments, key=lambda x: x.score) for comment in submission.comments[0:6]: print comment.body print \"score: \" + str(comment.score) print \"#\" * 10 print \"\\n\" * 3 output will look like this http://hastebin.com/duduyiquve.txt You can also use flat_comments = praw.helpers.flatten_tree(submission.comments) if you want more than just top level comments.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/416r43/praw_how_to_get_the_most_downvoted_comments_from/"}, {"author": "GoldenSights", "created_utc": 1452649286, "gilded": 0, "name": "t1_cyw3akw", "num_comments": null, "score": 1, "selftext": "Basically, what's happening is this: - _use_oauth is False - User makes a request - If the request needs OAuth, set _use_oauth to True - Request is made with OAuth headers - Set _use_oauth back to False - _use_oauth is False The problem is that certain exceptions during the request can break this system such that _use_oauth is never set back to False, and the assertion fails every time after that. The bandaid solution is to place `r._use_oauth = False` above the line that is raising the assertionerror. The better solution is to figure out which exceptions are breaking through, and making sure PRAW resets this variable automatically. This is something I've been meaning to do but haven't dedicated the time yet.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "GoldenSights", "created_utc": 1452650107, "gilded": 0, "name": "t1_cyw3tlw", "num_comments": null, "score": 1, "selftext": "I haven't fully read through your code, and I don't have a lot of experience with making a threadsafe PRAW script, but there's also the possibility that one request is starting while the other one is still working, meaning the variable hasn't been toggled back off yet. If that sounds like something your bot might be doing, you should look into preventing that with custom locks or [praw-multiprocess](http://praw.readthedocs.org/en/stable/pages/multiprocess.html).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "tusing", "created_utc": 1452652780, "gilded": 0, "name": "t1_cyw5jp0", "num_comments": null, "score": 1, "selftext": "Thanks. It seems to happen even when using praw-multiprocess. I'm kinda new to programming on Python on this level, and I'm not sure how to set _use_oauth to False in this file, since ```r``` is not passed along. Do you have any ideas on how I might implement it?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "GoldenSights", "created_utc": 1452655180, "gilded": 0, "name": "t1_cyw73yk", "num_comments": null, "score": 1, "selftext": "No, it shouldn't be set to True after that. Like my bullet points above, the variable is supposed to be false at all times until PRAW temporarily sets it while making a request. When it's True, you get these AssertionErrors. As far as where you should put it... anywhere that makes the crashes stop. This is something you shouldn't have to be dealing with in the first place, just use it to seal up the cracks. I can see you've updated the GitHub copy, so as long as that works you don't need to be doing it anywhere else yet. Again, the only time I get these AssertionErrors is when I'm having connectivity issues and the requests bail out unexpectedly. If you're getting them in otherwise good circumstances, it might be a result of a non-threadsafe program.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "GoldenSights", "created_utc": 1452553577, "gilded": 0, "name": "t1_cyum3w0", "num_comments": null, "score": 1, "selftext": ">If I change the 'all' to a specific subreddit it'll work In that case, it sounds like the bot simply can't keep up with the /r/all/comments stream, which moves super fast. If you're using OAuth, you're allowed to make 1 request per second, though PRAW still uses the default of 2. You can override it like this: wordbot.config.api_request_delay = 1 Also, since you're using the same compiled regex each time, why not store it in a variable? Calling `findWholeWord` so many times kind of defeats the purpose of compiling. If it still can't find comments, what subreddits have you been testing on? edit: Oh! I just realized you're doing `str(comment)`. Well, take a look at this: >>> str(c) 'This makes me curious, how do electronic tickets work for will call? I feel l...' >>> c.body 'This makes me curious, how do electronic tickets work for will call? I feel like an email system for will call could be quite the hassle.' >>> You're not getting the full text when you do that.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40jfcf/praw_matching_a_keyword_in_comment_stream/"}, {"author": "GoldenSights", "created_utc": 1452563781, "gilded": 0, "name": "t1_cyusewn", "num_comments": null, "score": 1, "selftext": "You had it! Put it above the for loop, like this: word = \"random\" wordFound = re.compile(r'\\b({0})\\b'.format(word), flags=re.IGNORECASE).search comments = praw.helpers.comment_stream(wordbot, 'all') for comment in comments: if wordFound(comment.body) is not None:", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40jfcf/praw_matching_a_keyword_in_comment_stream/"}, {"author": "Almenon", "created_utc": 1452116450, "gilded": 0, "name": "t1_cyojh8t", "num_comments": null, "score": 1, "selftext": "No, i tested it outside of pycharm too. I don't think the subreddit matters since json.load loads them in random order (no idea why) and the error happens around the same time. I'll check to make sure. EDIT: not subreddit dependent. seems to happen at random times early on in the iteration EDIT 2: I found it. It happens when I pass in a subreddit with a question mark at the end of the name. PRAW should have caught the error however. I think i'll submit it as a bug to the github page.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "GoldenSights", "created_utc": 1452117370, "gilded": 0, "name": "t1_cyok6mm", "num_comments": null, "score": 1, "selftext": "Hmm, I don't know. Using Python 3.5.0, PRAW 3.3.0, I'm not having any trouble fetching subreddit objects like this. Maybe someone else can toss in some ideas. As for the json order, Dictionaries are called unordered types, where the elements are ordered based on a hash, which can change from session to session. Check out these SO threads: http://stackoverflow.com/questions/526125/why-is-python-ordering-my-dictionary-like-so http://stackoverflow.com/questions/15479928/why-is-the-order-in-python-dictionaries-and-sets-arbitrary", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "GoldenSights", "created_utc": 1452115295, "gilded": 0, "name": "t1_cyoikih", "num_comments": null, "score": 3, "selftext": "PRAW actually includes a config variable for watching request activity: `r.config.log_requests`. By default it is 0, but setting it to 1 or 2 will print a message every time a request is being made (1 tells you the URL being used, 2 also tells you the status code when it's done). Here's an example that looks kind of like your scenario: >>> r.config.log_requests=1 >>> subreddit = r.get_subreddit('redditdev') >>> >>> hot = subreddit.get_hot() >>> >>> submissions = list(hot) substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/redditdev/.json >>> >>> submissions[0] >>> submissions[0].id '3zrucp' >>> submissions[0].comments substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/redditdev/comments/3zrucp/praw_having_trouble_figuring_out_when_requests/.json [] >>> Notice that 1. You're correct that `get_hot` simply prepares the generator, but does not create requests right away. 2. You're incorrect about the comments. The json for /hot, /new, etc do NOT include any comment data. You can view it in your browser here: https://www.reddit.com/r/redditdev/hot.json. PRAW must fetch the submission page directly. Hope that helps!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrucp/praw_having_trouble_figuring_out_when_requests/"}, {"author": "GoldenSights", "created_utc": 1451928834, "gilded": 0, "name": "t1_cylp357", "num_comments": null, "score": 1, "selftext": "Does `r.get_subreddit('my_private_subreddit', fetch=True)` work? It's unusual to use `r.request` with actual URLs in PRAW. I also notice that the request which 403'd did not print the \"substituting oauth...\" message like the other ones did, so this may give us a hint as to what the problem is. But first, let's see if the other methods work normally.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "created_utc": 1451929974, "gilded": 0, "name": "t1_cylpuot", "num_comments": null, "score": 2, "selftext": "Thanks for helping. `r.get_subreddit('my_private_subreddit', fetch=True)` does work: >>> import mybot >>> r=mybot.login() substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.get_subreddit(\"\", fetch=True) substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r//about/.json status: 200 Subreddit(subreddit_name='') ---- >It's unusual to use r.request with actual URLs in PRAW. Oh, I didn't know that. I thought that was the only way to use it: https://praw.readthedocs.org/en/stable/pages/code_overview.html#praw.__init__.BaseReddit.request https://praw.readthedocs.org/en/stable/pages/code_overview.html#praw.__init__.BaseReddit.request_json I need to use `request_json()` with an actual URL because one of the JSON feeds I want is `https://www.reddit.com/r//about/traffic/.json`, which does not have a method like `get_contributors()` which `https://www.reddit.com/r//about/contributors/.json`has. >I also notice that the request which 403'd did not print the \"substituting oauth...\" message like the other ones did I noticed that too. Hopefully that will help.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "GoldenSights", "created_utc": 1451931126, "gilded": 0, "name": "t1_cylqmzg", "num_comments": null, "score": 1, "selftext": "Great, I'm glad that worked! I've actually never used the json version of /traffic before. I'm assuming those numbers are [timestamp, uniques, pageviews, subscribers], right? When I get home I'll see if it's something we can add to PRAW, I feel like it won't be too much trouble to implement the basics. `request` and `request_json` are used internally by other methods, and if you have to use it manually it probably means we're missing a feature. If you scroll around some more on that docs page you'll see all kinds of other functions. Just FYI, you don't need `fetch=True` for most cases. It's useful here because `r.get_subreddit` does not actually make any web requests until you do something on the subreddit object. The Fetch parameter forces the request right away. Let me know if you have any other questions!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "created_utc": 1451940828, "gilded": 0, "name": "t1_cylxjwn", "num_comments": null, "score": 2, "selftext": "That would be amazing! Thank you! I assumed there was a reason a `get_traffic()` method wasn't provided, and I didn't want to bother anyone with feature requests. `r.request_json(\"https://www.reddit.com/r/AskReddit/about/traffic/.json\")` works without errors for public subreddits which make traffic public. For some reason traffic URLs aren't documented on https://www.reddit.com/dev/api, but you're right about the format for the first key, `day`. There is also `hour` and `month`. You can compare these links in your browser to work out the values: - https://www.reddit.com/r/askreddit/about/traffic - https://www.reddit.com/r/askreddit/about/traffic.json (with something like [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc)) I did it below. The JSON response is one object with three keys, `day`, `hour`, and `month`. Each key has an array of arrays. Each inner array holds 3 or 4 integers. { day: [ [ 1451865600, // Timestamp, midnight of day (Mon, 04 Jan 2016 00:00:00 GMT) 750711, // Uniques for day 2733228, // Pageviews for day 9554 // Subscriptions for day ], [ 1451779200, // Timestamp, midnight of day (Sun, 03 Jan 2016 00:00:00 GMT) 1316557, // Uniques for day 5072635, // Pageviews for day 13566 // Subscriptions for day ], ... ], hour: [ [ 1451934000, // Timestamp, start of hour (Mon, 04 Jan 2016 19:00:00 GMT) 0, // Uniques for hour (0 until hour ends) 0 // Pageviews (0 until hour ends) ], [ 1451930400, // Timestamp, start of hour (Mon, 04 Jan 2016 18:00:00 GMT) 122922, // Uniques for hour 282349 // Pageviews for hour ], ... ], month: [ [ 1451606400, // Timestamp, midnight first day of month (Fri, 01 Jan 2016 00:00:00 GMT) 2829756, // Uniques for month 15342830 // Pageviews for month ], [ 1448928000, // Timestamp, midnight first day of month (Tue, 01 Dec 2015 00:00:00 GMT) 19867073, // Uniques for month 157554074 // Pageviews for month ], ... ] } ---- All I want is the raw JSON response. But other PRAW users might want the object functionality the other get_() methods have. For what it's worth, I've added a third Interpreter example to my original post. In case you or anyone else wants to investigate the original problem. Thank you once again.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "GoldenSights", "created_utc": 1452113952, "gilded": 0, "name": "t1_cyohjiu", "num_comments": null, "score": 2, "selftext": "There we go: https://github.com/praw-dev/praw/pull/575 Your explanation of the endpoint helped, thank you!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "created_utc": 1452172450, "gilded": 0, "name": "t1_cyp9s2o", "num_comments": null, "score": 2, "selftext": "Thanks for taking the time to do this! Sadly, the new `get_traffic()` method is affected by the problem this thread mentions. It works perfectly for public subreddits (not requiring OAuth) but the first call that requires OAuth (e.g. private subreddits) still returns the 403 error. Subsequent calls work fine. >>> import mybot >>> r=mybot.login() substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> mybot.get_traffic(\"AskReddit\") # Public subreddit, works fine GET: https://api.reddit.com/r/askreddit/about/traffic/.json status: 200 {'hour': [[ >>> mybot.get_traffic(\"\") # Private subreddit moderated by account, does not work, returns 403 on first call GET: https://api.reddit.com/r//about/traffic/.json status: 403 >>> mybot.get_traffic(\"\") # Exact same call requiring OAuth, second time, works fine substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r//about/traffic/.json status: 200 {'hour': [[ Like you mentioned earlier, the first call requiring OAuth (which returned 403) does not substitute the subdomain. It made the call to `https://api.reddit.com`. The second call substituted properly and made the call to `https://oauth.reddit.com` ---- In your pull request you mentioned you couldn't determine if it's meant to be used with a particular OAuth scope. I investigated this, and there is an undocumented scope called `modtraffic`. On the authorization page the description is \"Access traffic stats in subreddits I moderate.\", but it doesn't work. Requesting the traffic for a private subreddit the account moderates using this scope returns `praw.errors.OAuthInsufficientScope`. There is another scope called `modconfig`. This is the OAuth scope that authorizes access to the traffic, even though the description is \"Manage the configuration, sidebar, and CSS of subreddits I moderate.\".", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "GoldenSights", "created_utc": 1452283090, "gilded": 0, "name": "t1_cyqzd10", "num_comments": null, "score": 2, "selftext": "Hi, sorry for the delay, Thanks for showing me that `modconfig` was the correct scope after all. I tried a whole bunch of different scopes, but I must have forgotten that or had some other problem when I tried it. I've opened [a new PR](https://github.com/praw-dev/praw/pull/576) to add that in. This also makes it so the request passes on the first try because PRAW is sending the right oauth headers now. You should start dissecting the PRAW source code, I think you would make some great pull requests! You're a good troubleshooter and you like to explain things, we can always use more contributors.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "created_utc": 1452302545, "gilded": 0, "name": "t1_cyrbwiz", "num_comments": null, "score": 1, "selftext": "No need to apologize for any delay, your help is very much appreciated. `get_traffic()` works perfectly for the first OAuth request now. It took me a few minutes to figure out that the scope is `modconfig`. I don't know if that's intentional. Thank you for the compliment! I believe if someone is helping you with open source, the least you can do is give them as much detail as possible. To be honest, I have taken on far too many commitments recently and I have no extra time. Even this PRAW script is too much right now. I'm very impressed by the responsiveness and helpfulness of PRAW contributors, and I've added it to my list of open source projects to dive into when I have more time. You're a great representative for the project. Thanks again for your help. Have a great weekend!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "kemitche", "created_utc": 1451933414, "gilded": 0, "name": "t1_cyls7v2", "num_comments": null, "score": 1, "selftext": "~~You're setting the **wrong domain**. Use oauth.reddit.com, not www.reddit.com.~~ Edit: Didn't see the second example on my first read. It looks then like praw or oauth2util aren't setting headers properly until the first request has failed.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "kemitche", "created_utc": 1451946259, "gilded": 0, "name": "t1_cym1fyj", "num_comments": null, "score": 1, "selftext": "Ah I see now. Sorry for mis-reading! There's definitely something else going on in PRAW or OAuth2Util there.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "created_utc": 1451948903, "gilded": 0, "name": "t1_cym38l3", "num_comments": null, "score": 1, "selftext": "No problem at all! Thanks for looking. I have tested without OAuth2Util (using the tutorial linked in the original post) and it's exactly the same, so it's something with PRAW. /u/GoldenSights mentioned above that `request()` is usually used internally, so I guess PRAW does some other internal set up before calling it. It's just unfortunate because it works perfectly after that first 403 error. Maybe there is an easy fix. Out of curiosity, [regarding my comment above](https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/cylxjwn), is there a reason the subreddit traffic endpoint isn't documented at https://www.reddit.com/dev/api?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "askLubich", "created_utc": 1451762060, "gilded": 0, "name": "t1_cyjj9xj", "num_comments": null, "score": 1, "selftext": "I am using Python 2.7.11 (PC) and 2.7.9 (Raspberry Pi) together with Praw 3.3.0 (both). There error occurs on both devices.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "pcjonathan", "created_utc": 1451762364, "gilded": 0, "name": "t1_cyjjgdp", "num_comments": null, "score": 1, "selftext": "This is very similar to the [issue](https://github.com/praw-dev/praw/issues/473) I reported a while ago when dealing with unicode characters.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "RemindMeBotWrangler", "created_utc": 1451720732, "gilded": 0, "name": "t1_cyj4akv", "num_comments": null, "score": 1, "selftext": "If you're going to use praw and worry about doing too many requests, you can do this http://praw.readthedocs.org/en/stable/pages/multiprocess.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z3ff3/two_bots_one_ip/"}, {"author": "GoldenSights", "created_utc": 1451598114, "gilded": 0, "name": "t1_cyhp8d5", "num_comments": null, "score": 2, "selftext": "It pretty much always happens, and it's okay. See here: https://github.com/praw-dev/praw/issues/329", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yy8ai/error_in_praw_resourcewarning_unclosed/"}, {"author": "GoldenSights", "created_utc": 1451690084, "gilded": 0, "name": "t1_cyipvy9", "num_comments": null, "score": 1, "selftext": "Wait, what problem are we trying to solve? Like I said, the unclosed socket stuff is completely normal with PRAW, and to suppress it all you need to do is put `warnings.simplefilter('ignore')` at the top of your script. Does this stop the warnings properly? `>> Cron.txt 2>&1` is not valid Python, so when you say you put it at the end of your file, I'm not sure what you mean.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yy8ai/error_in_praw_resourcewarning_unclosed/"}, {"author": "not_an_aardvark", "created_utc": 1451555986, "gilded": 0, "name": "t1_cyh68e6", "num_comments": null, "score": 2, "selftext": "Actually, PRAW has a [lazy object](https://praw.readthedocs.org/en/stable/pages/lazy-loading.html) model, so it won't make any more requests than it needs to. With this line of code: `users = [i.author.id for i in cms]` The issue is that you're getting each author's *id*, not their username. This is why it needs to make a separate request for each author. Assuming that's actually what you want, using a different wrapper won't really help with that. However, if you're just looking for the *usernames* of the authors, you can do this: `users = [i.author.name for i in cms]` ...and it should run much faster, since it doesn't need to make a new request for each username.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "not_an_aardvark", "created_utc": 1451600756, "gilded": 0, "name": "t1_cyhqmt2", "num_comments": null, "score": 1, "selftext": "Basically, it depends on what information has already been sent by reddit. For example, if I load this thread, I can immediately see that your username is `BearlyBreathing`. However, I don't know some other information (other posts you've submitted, your trophies, your comment karma, etc.) just from viewing this thread -- I have to go to your user profile page to figure that out. It's sort of the same thing with the API. PRAW fetches the thread from reddit, and gets a JSON response that looks like [this](/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/.json). One of the keys in the JSON response is: `\"author\": \"BearlyBreathing\"` So it creates a `Redditor` object with `name = \"BearlyBreathing\"`. All of the other properties (e.g. `get_submitted`, `id`) will have `has_fetched = False`, which means that another api request will be required to obtain them.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "13steinj", "created_utc": 1451536900, "gilded": 0, "name": "t1_cygzgim", "num_comments": null, "score": 1, "selftext": "No matter what you do on a code level, you won't see many speed gains. That's because the code is still executed one by one. You'll want to use python's multiprocessing module, starting multiple processes in what *appears* to be succession (e.g. same as normal code), but the multiprocessing module will execute the different processes in consecutive order, instead of after the previous completes. Praw also has some tools for multiprocessing, but I don't know how they work nor have I ever attempted to do something like this with praw.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "bboe", "created_utc": 1451327005, "gilded": 0, "name": "t1_cye0dg6", "num_comments": null, "score": 1, "selftext": "PRAW doesn't use python's built-in logging mechanism (it should in a future major version). Its stdout-based logging is configured via the config variable `log_requests` as described in: http://praw.readthedocs.org/en/stable/pages/configuration_files.html One way to disable logging is by setting `r.config.log_requests = 0`. That value is `0` by default, so you probably have it explicitly enabled somewhere in your code, or in a praw.ini file.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ycb4r/praw_stop_praw_from_writing_requests_to_log/"}, {"author": "13steinj", "created_utc": 1450974364, "gilded": 0, "name": "t1_cya4f2s", "num_comments": null, "score": 2, "selftext": "1. Do not use that while loop. Off the top of my head it would fuck things up, and it's not really needed, because if there are no more comments it will do it, and it has its own \"built in\" while loop similar to that. 2. Comments are a list tree. Submission.comments is a list of comments, and each comment has a .replies attribute that is a list of comments in reply to that comment. You'll want to use `comments = praw.helpers.flatten_tree(submission.comments)` and then iterate over that new variable.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "Squid__", "created_utc": 1450981565, "gilded": 0, "name": "t1_cya8c4c", "num_comments": null, "score": 2, "selftext": "I'm not sure what you mean by timing issue but both of the users posted within a few hours of the original post, and I ran the script ~20 hours after the post. They 100% should have been picked up. I ran the script again and they still aren't showing up, so I'm fairly certain it isn't an issue with my code, and I'm pretty sure it isn't PRAW's fault.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "washerdreier", "created_utc": 1450702247, "gilded": 0, "name": "t1_cy6iv60", "num_comments": null, "score": 1, "selftext": "Yeah, I'm specifically asking for slices under the 1k limit to avoid refetching posts. For example, if I wanted to fetch my saves since the last time I fetched/archived them. So a query that is bound by either a time slice or a placeholder/reference post like clockstalker did. Is this still possible with PRAW and the API and I just can't figure out the syntax, or was it removed so you can only fetch N", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "13steinj", "created_utc": 1450703322, "gilded": 0, "name": "t1_cy6j7ld", "num_comments": null, "score": 1, "selftext": "You were never able to get a time based slice. This was never in praw. However you could have filled in the url parameters `after` and `before` as mentioned [here](/dev/api) via params={dictionary of url params}.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "13steinj", "created_utc": 1450070244, "gilded": 0, "name": "t1_cxydgor", "num_comments": null, "score": 2, "selftext": "That's praw's docs. Not reddit's. Also, every object has the attributes with the same names of the json keys as provided by reddit, which is listed in reddit's documentation on github. I'd link that but I'm on mobile.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3wq68t/documentation_doesnt_list_attributes_of_redditor/"}, {"author": "spookyyz", "created_utc": 1450217326, "gilded": 0, "name": "t1_cy09q1x", "num_comments": null, "score": 3, "selftext": "Very nicely documented, this would have been pretty useful for me about 3 weeks ago :) But, the PRAW documentation is pretty solid too, so I made do.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3wpn6e/quick_buildabot_tutorial/"}, {"author": "gavin19", "created_utc": 1449745029, "gilded": 0, "name": "t1_cxtyv4k", "num_comments": null, "score": 1, "selftext": "When you installed pip (assuming you did), it only installs PRAW (or any module) for the version you specified (or whichever was the current version at the time). Easiest way that I know of is to sudo apt-get install python3-pip then you can pip3 install praw to have it install PRAW for 3. You should also be able to use python3 -m pip install praw without installing anything.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3w7h97/praw_cant_get_it_to_work_with_python_3/"}, {"author": "xiongchiamiov", "created_utc": 1449784406, "gilded": 0, "name": "t1_cxulk8f", "num_comments": null, "score": 1, "selftext": "You probably shouldn't have `/usr/local/bin/python3` at all; if you remove it, `which python3` should point to `/usr/bin/python3`. It probably should *anyways*, since generally I think `/usr/bin` should appear before `/usr/local/bin` in your `PATH`. From what I can gather from your post, you somehow ended up with two different versions of Python installed, and were using the shell of one while praw was installed into the other; that's why I was asking about installing multiple versions of python. If you've still got multiple `pip3`s hanging around, it'd be useful to investigate and see where they go. You can also see what package (if any) a file belongs to by using `dpkg -S /path/to/file`.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3w7h97/praw_cant_get_it_to_work_with_python_3/"}, {"author": "joe-murray", "created_utc": 1449452349, "gilded": 0, "name": "t1_cxpry4q", "num_comments": null, "score": 2, "selftext": "If you are authenticated via PRAW you can get saved links via the following command (in Python): saved_links = r.user.get_saved() I'm not sure what object is returned but you should be able to tinker with it and figure out exactly how to strip out links from the correct subreddit. For example, pseudocode might look something like this: desired_links = [] saved_links = r.user.get_saved() for link in saved_links: if link.subreddit == 'news': desired_links.append(link)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "joe-murray", "created_utc": 1449461225, "gilded": 0, "name": "t1_cxpwx9c", "num_comments": null, "score": 1, "selftext": "Oh, I understand now. I don't know if that is directly possible through PRAW. If it is, you might be able to just pass the name of the subreddit as an argument to the get saved function like this: saved_links = r.user.get_saved(subreddit='news') If that doesn't work, you will have to do it via data scraping. The fact that data requires authentication might make the scraping more difficult but I'm not 100% sure because I haven't tried that before.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "SmBe19", "created_utc": 1449498926, "gilded": 0, "name": "t1_cxq9igh", "num_comments": null, "score": 1, "selftext": "If the server is configured accordingly you can try [server mode](https://github.com/SmBe19/praw-OAuth2Util/blob/master/OAuth2Util/README.md#server-mode). Otherwise you have to start the script locally the first time and then upload the `oauth.ini` with the tokens in it to the server.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "wanderingbilby", "created_utc": 1449077001, "gilded": 0, "name": "t1_cxkk9q6", "num_comments": null, "score": 2, "selftext": "I haven't used PRAW, but it looks like the subreddit.get_comments() call does a [GET /r/subreddit/top](https://www.reddit.com/dev/api#GET_top) call, which returns a [listing](https://www.reddit.com/dev/api#listings) which contains (among other things) the thing name. You can then use an [info call](https://www.reddit.com/dev/api#GET_api_info) to get more data... possibly. Unfortunately the API documentation falls a bit short here so you'll have to do some experimenting. Sorry it's not a complete solution, hopefully that gets you on the right path at least.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lecherous_hump", "created_utc": 1449087377, "gilded": 0, "name": "t1_cxkrurg", "num_comments": null, "score": 1, "selftext": "I would imagine it's part of a Comment object, but I don't use PRAW. It's part of the data sent back by the API for a comment, so it should be.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "Triplanetary", "created_utc": 1449010216, "gilded": 0, "name": "t1_cxjoqar", "num_comments": null, "score": 2, "selftext": "Have you considered fetching the comment's JSON from the reddit server? Just do a GET request for the comment's URL + .json. It won't be a serialized PRAW object, of course, but you can get the comment's PRAW object at any later time by using get_info on the comment's name, as provided by the JSON data.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "helpmewebbit", "created_utc": 1449012430, "gilded": 0, "name": "t1_cxjq6xr", "num_comments": null, "score": 1, "selftext": "Fetching the comment's json using comment permalink URL + .json actually gives me a lot of superfluous data such as the submission's json and the children of the comment. I haven't actually tried doing this programatically yet, but so far this solution isn't looking as neat as I'd like it to be. Another reason I'm so keen on using the PRAW dict object is that it'll avoid me from having to make too many requests to the server. I mean, I already have all the data I want/need. The only problem is that I want to trim the \"replies\" field in some simple and consistent way. EDIT: For instance, here's a [permalink.json](https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/cxjoqar.json) to your comment while below is a praw.objects.Comment object's json_dict attribute: { 'parent_id' : 't3_3v1yuv', 'distinguished' : None, 'subreddit_id' : 't5_2qizd', 'removal_reason' : None, 'likes' : None, 'id' : 'cxjoqar', 'name' : 't1_cxjoqar', 'edited' : False, 'body' : \"Have you considered fetching the comment's JSON from the reddit server? Just do a GET request for the comment's URL + .json. It won't be a serialized PRAW object, of course, but you can get the comment's PRAW object at any later time by using get_info on the comment's name, as provided by the JSON data.\", 'archived' : False, 'mod_reports' : [], 'ups' : 1, 'downs' : 0, 'link_id' : 't3_3v1yuv', 'created' : 1449039016.0, 'subreddit' : 'redditdev', 'score_hidden' : False, 'user_reports' : [], 'approved_by' : None, 'report_reasons' : None, 'created_utc' : 1449010216.0, 'banned_by' : None, 'gilded' : 0, 'replies' : { 'kind' : 'Listing', 'data' : { 'before' : None, 'children' : [ ], 'modhash' : '', 'after' : None } }, 'author_flair_text' : None, 'saved' : False, 'author_flair_css_class' : None, 'body_html' : 'Have you considered fetching the comment's JSON from the reddit server? Just do a GET request for the comment's URL + .json. It won't be a serialized PRAW object, of course, but you can get the comment's PRAW object at any later time by using get_info on the comment's name, as provided by the JSON data.\\n', 'score' : 1, 'controversiality' : 0, 'num_reports' : None, 'author' : 'Triplanetary' }", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "theonefoster", "created_utc": 1448569539, "gilded": 0, "name": "t1_cxe0skc", "num_comments": null, "score": 1, "selftext": "It does. If you hover over the submission title in the comments section, the link appears in the bottom right (on Chrome) with .jpg at the end. Right-clicking and copying the link address also copies a link with .jpg. Following the link also navigates to an image only with .jpg in the url, rather than an imgur page without. Except calling the api link from praw finds an imgur page url with no .jpg at the end - see the highlighted attribute in the image.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "theonefoster", "created_utc": 1448570399, "gilded": 0, "name": "t1_cxe19nc", "num_comments": null, "score": 2, "selftext": "As per [this comment](https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/cxe1010) and subsequent investigation, I've found that the culprit is RES. It seems to be converting indirect links into direct links at the page-source level. I'd imagine there's a setting to disable this behaviour but I've not checked yet.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "Triplanetary", "created_utc": 1448559865, "gilded": 0, "name": "t1_cxdvda6", "num_comments": null, "score": 2, "selftext": "Like this: for x in y: try: do stuff except praw.errors.NotFound: continue What this does is, if the user in the current loop iteration doesn't exist, it will skip to the next loop iteration (which is what `continue` does) instead of crashing.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ubstm/deleted_users/"}, {"author": "13steinj", "created_utc": 1448514152, "gilded": 0, "name": "t1_cxdfj4w", "num_comments": null, "score": 3, "selftext": "Wasn't the latest python 2 version 2.7.10? There could just be a bug in 2.7.9's http lib. Regardless I recommend uninstalling PRAW and all dependencies, then reinstalling via `pip install praw`", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3uapp8/praw_error_data_must_be_a_byte_string/"}, {"author": "thetoethumb", "created_utc": 1448597123, "gilded": 0, "name": "t1_cxeedw0", "num_comments": null, "score": 1, "selftext": "Thanks for the advice! I upgraded to 2.7.10 and reinstalled ``praw`` and ``requests`` using ``pip``: >>> print sys.version 2.7.10 (default, May 23 2015, 09:40:32) [MSC v.1500 32 bit (Intel)] >>> print praw.__version__ 3.3.0 >>> print requests.__version__ 2.5.0 ~~Still no dice with the MWE in the original example. Any further ideas? Any other packages I should be reinstalling/upgrading?~~ Edit: Now appears to be working. I think I just needed to re-import the requests module after upgrading (still had a console open)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3uapp8/praw_error_data_must_be_a_byte_string/"}, {"author": "thetoethumb", "created_utc": 1448505301, "gilded": 0, "name": "t1_cxdb8n4", "num_comments": null, "score": 2, "selftext": "Traceback (most recent call last): File \"\", line 1, in runfile('[redacted path]', wdir='[redacted path]') File \"C:\\Python27\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py\", line 601, in runfile execfile(filename, namespace) File \"C:\\Python27\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py\", line 66, in execfile exec(compile(scripttext, filename, 'exec'), glob, loc) File \"[redacted path]\", line 13, in print [str(x) for x in submissions] File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\__init__.py\", line 557, in get_content page_data = self.request_json(url, params=params) File \"\", line 2, in request_json File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\__init__.py\", line 444, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"C:\\Python27\\lib\\site-packages\\requests\\sessions.py\", line 573, in send r = adapter.send(request, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests\\adapters.py\", line 370, in send timeout=timeout File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 518, in urlopen body=body, headers=headers) File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 330, in _make_request conn.request(method, url, **httplib_request_kw) File \"C:\\Python27\\lib\\httplib.py\", line 1001, in request self._send_request(method, url, body, headers) File \"C:\\Python27\\lib\\httplib.py\", line 1035, in _send_request self.endheaders(body) File \"C:\\Python27\\lib\\httplib.py\", line 997, in endheaders self._send_output(message_body) File \"C:\\Python27\\lib\\httplib.py\", line 850, in _send_output self.send(msg) File \"C:\\Python27\\lib\\httplib.py\", line 826, in send self.sock.sendall(data) File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\contrib\\pyopenssl.py\", line 220, in sendall sent = self._send_until_done(data) File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\contrib\\pyopenssl.py\", line 210, in _send_until_done return self.connection.send(data) File \"C:\\Python27\\lib\\site-packages\\OpenSSL\\SSL.py\", line 947, in send raise TypeError(\"data must be a byte string\") TypeError: data must be a byte string", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3uapp8/praw_error_data_must_be_a_byte_string/"}, {"author": "AllHailTheCATS", "created_utc": 1448059780, "gilded": 0, "name": "t1_cx7ezgj", "num_comments": null, "score": 1, "selftext": "I had the markup correct it just formatted weird on reddit, oh yea thanks for pointing that out with fetchone(). I'm getting the traceback still on the second time the bot trys comment, that is if im running the bot, he comments under me then i comment a second time I get the traceback instead of the bot commenting on my second comment. I assume this is because a bot can reply to the one account only so often? heres the traceback anyway: Traceback (most recent call last): File \"C:/Users/Conor/PycharmProjects/HypeManProject/HypeManBot.py\", line 42, in HypeManBot() File \"C:/Users/Conor/PycharmProjects/HypeManProject/HypeManBot.py\", line 33, in HypeManBot comment.reply(\"Yeaaaah!!\") File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\objects.py\", line 408, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 2569, in _add_comment retval = decorator(add_comment_helper)(self, thing_id, text) File \"\", line 2, in add_comment_helper File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\decorators.py\", line 268, in wrap return function(*args, **kwargs) File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 2562, in add_comment_helper retry_on_error=False) File \"\", line 2, in request_json File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\decorators.py\", line 139, in raise_api_exceptions raise error_list[0] praw.errors.RateLimitExceeded: `you are doing that too much. try again in 8 minutes.` on field `ratelimit` Process finished with exit code 1", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "cutety", "created_utc": 1447947339, "gilded": 0, "name": "t1_cx5rnz6", "num_comments": null, "score": 2, "selftext": "/u/GoldenSights has a good tutorial on his subreddit for setting up your bot to work with OAuth [here](https://www.reddit.com/r/GoldTesting/comments/3cm1p8/how_to_make_your_bot_use_oauth2/). Or you can just watch his youtube video [here](https://www.youtube.com/watch?v=Uvxu2efXuiY&feature=youtu.be). There is also /u/SmBe19's [praw-OAuth2Util](https://github.com/SmBe19/praw-OAuth2Util) that makes OAuth a simple as copying and pasting a .ini file into your bots directory and adding a line to your bots .py file. [Instuctions for setting that up](https://github.com/SmBe19/praw-OAuth2Util/blob/master/OAuth2Util/README.md). [This](https://github.com/voussoir/reddit/tree/master/ReplyBot) is /r/RequestABot's replybot template. I've written several bots that respond to comments when a certain phrase is in a comment: [!d10](https://gitlab.com/EDGYALLCAPSUSERNAME/D10-Dice-Roll-Bot/tree/master). [+decodebot](https://gitlab.com/EDGYALLCAPSUSERNAME/Decode-Bot/tree/master). [Replies to various keywords](https://gitlab.com/EDGYALLCAPSUSERNAME/ReputationManager/tree/master). All those could be modified to watch for their username instead. Another way to do it is monitor the messages that come into the account and reply to any where \"Username Mention\" is the subject.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "lumbdi", "created_utc": 1447997467, "gilded": 0, "name": "t1_cx6m9jf", "num_comments": null, "score": 1, "selftext": "thanks :) I already got my bot up and running [see here](https://www.reddit.com/r/DotA2/comments/3tikrm/account_buying_piece_of_shit_trash_who_thinks_he/) but I still have to handle a lot of errors. The majority of them not due to PRAW. My bot is talking to Steam API and I haven't handled the potential connection errors yet.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "13steinj", "created_utc": 1447933673, "gilded": 0, "name": "t1_cx5lwl6", "num_comments": null, "score": 1, "selftext": "Oh, forgot you're using PRAW. No idea if it stores information about requests / responses somewhere/how, but /u/bboe might be able to help.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3te2i6/im_getting_a_500_internal_server_error_when_i_try/"}, {"author": "tobiasvl", "created_utc": 1447782791, "gilded": 0, "name": "t1_cx3i1hh", "num_comments": null, "score": 2, "selftext": "So I found the issue. Praw depends on `requests`, which has this issue currently: [2818](https://github.com/kennethreitz/requests/issues/2818) Same as [issue 719](https://github.com/shazow/urllib3/pull/719) in `urllib3`. It works again after downgrading `requests` to 2.7.0.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3t1mum/error_when_logging_in_with_praw/"}, {"author": "13steinj", "created_utc": 1447703975, "gilded": 0, "name": "t1_cx2eujz", "num_comments": null, "score": 1, "selftext": "What python version? Also, try making a virtualenv with virtualenv and just get pip and praw.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3t1mum/error_when_logging_in_with_praw/"}, {"author": "Frenchiie", "created_utc": 1447721370, "gilded": 0, "name": "t1_cx2q5ql", "num_comments": null, "score": 1, "selftext": "I'm just not understanding what information does the PRAW method get_authorize_url() send to the Reddit API which then sends to the Reddit server that basically says \"hey this an authorization permission on behalf of /r/Frenchiie\". The method that we call, get_authorize_url() returns a url that we open to get a code specific to a user. [It passes 3 arguments](http://praw.readthedocs.org/en/stable/pages/code_overview.html#praw.__init__.OAuth2Reddit.get_authorize_url), none of which gives any sort of information that validates a user. What else is going on **behind** the scene? How does the reddit API know who the user is during this step?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "_Daimon_", "created_utc": 1447722795, "gilded": 0, "name": "t1_cx2r0ht", "num_comments": null, "score": 1, "selftext": "`get_authorize_url` says \"Hey Reddit, I would like to get authorized with x scope, y state and possibly permanently. Which user has not been specified. Based on these parameters PRAW builds a url. No direct interaction with Reddit has been neccesary. Basically PRAW just takes the arguments you've given and package them into a url, with GET parameters. There is nothing user-specific inherently about this url. You can give it to 1 user, 0 users or many many users. It doesn't matter. Let's say our FooBar website want state \"123\", scope \"identity\" and no refreshable token. We build the url and send it to /u/Frenchile and ask the user to go to this page. The user goes to this page. It is a reddit page. It says, hello \"/u/Frenchile\" app \"FooBar\" would like to temporarily get access to your \"identity\" permission. It lists what \"FooBar\" can do with the permissions you are about to give. You click ok. Now, lets say we also give this url to /u/_Daimon I too go on this page and agree to temporarily give \"identity\" permissions to \"FooBar\" app. The webserver receives a callback on the callback url with the code both times a url authorized the app. The callback is just a GET request with the code and state argument. The two code arguments are different. The state argument are identical to what we choose when we called `get_authorize_url`. Now, we take one of the codes and auhenticate on Reddit. As per step 5 in the documentation. We get a token that we can use to make requests based on this user. If we try to authenticate with the code again, it will fail. We are now authenticated as either /u/Frenchile or /u/_Daimon_. Since we have the `identity` permission, we can use `get_me` to find out who we are authenticated as. Final note. State in the above example are identical for all users. This is fine for testing and demonstration purposes. In production it should be unique for security reasons.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "noobit", "created_utc": 1447355078, "gilded": 0, "name": "t1_cwxzbmz", "num_comments": null, "score": 1, "selftext": "no, there isn't; yes, it's annoying; yes, it'd be useful. Between praw's [ATDYDN](#rant \"All the detail you don't need' - I have a rant on this.\") documentation and the need to install a scheduler program (rather than just some existing bot manager script) for scheduling anything more complex than a simple cronjob, I haven't touched bot stuff in a while", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "13steinj", "created_utc": 1447362099, "gilded": 0, "name": "t1_cwy46bh", "num_comments": null, "score": 2, "selftext": "You wouldn't need the url. r = praw.Reddit('analyser for your thing's name's) s = r.get_subreddit('redditdev') for post in s.get_top_from_month(limit=None): print(post.created_utc)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "hullcrush", "created_utc": 1446581226, "gilded": 0, "name": "t1_cwn9cak", "num_comments": null, "score": 1, "selftext": "> import praw > r = praw.Reddit('user agent for hellcrush!') > sr = r.get_subreddit('python') > for _ in sr.get_comments(limit=100): > print(_) YES! Baller. Now back to boring life.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "hullcrush", "created_utc": 1446414179, "gilded": 0, "name": "t1_cwkub72", "num_comments": null, "score": 1, "selftext": "You are. Thanks for answering! http://imgur.com/kUMTb3x I'd prefer the latter code but I can't get any PRAW code to work. I appended your comments code and it did not print. I'm using IDLE 2.7", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "13steinj", "created_utc": 1446073476, "gilded": 0, "name": "t1_cwghq29", "num_comments": null, "score": 1, "selftext": "I don't know when or if it actually changed, but praw returns generator objects for listings. To cycle through the posts, use for post in submissions: # do something", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qmign/prawinstead_of_displaying_posts_my_script_is/"}, {"author": "GoldenSights", "created_utc": 1445802246, "gilded": 0, "name": "t1_cwciehj", "num_comments": null, "score": 5, "selftext": "What's happening is that `fetch=True` tries to fetch all of the information about the subreddit, such as ID and creation timestamp. /r/all doesn't have these because it isn't a real subreddit, and https://reddit.com/r/all/about.json returns 404 Not Found, exactly as you're seeing in that traceback. To override PRAW's 30 second cache, you don't need to re-request the subreddit, because that object doesn't have any effect on the /comments listing that you retrieve. All it does is toss its name into `r.get_comments` so you don't have to do it manually. You can use `r.evict('https://oauth.reddit.com/r/all/comments')` or the more general `r.handler.clear_cache()` to get a fresh listing next time.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3q6mhj/im_getting_a_http_error_on_rget_subredditall/"}, {"author": "bboe", "created_utc": 1445813288, "gilded": 0, "name": "t1_cwcplbn", "num_comments": null, "score": 3, "selftext": "You might consider using the `comment_stream` feature of PRAW as it seems like that may be what you're actually trying to create yourself.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3q6mhj/im_getting_a_http_error_on_rget_subredditall/"}, {"author": "GoldenSights", "created_utc": 1445549150, "gilded": 0, "name": "t1_cw9btud", "num_comments": null, "score": 2, "selftext": "What's happening here is that the decorator toggles the `_use_oauth` property [on](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L250) while making the request and then [back off again](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L270) when it's done. When you made multiple threads, one of them tried to make a request while the other was already working on one, causing the error. How many streams do you plan on using? Would you consider setting up a single stream for something like /r/redditdev+botwatch+learnpython and then just doing different things based on where the submission comes from? I've literally never used the praw-multiprocess so unfortunately I can't give any help with that. You could also give each stream it's own reddit session, making sure that the total `r.config.api_request_delay` (number of seconds between requests) of all your sessions works out to being only 1 request per second since you're using oauth.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "GoldenSights", "created_utc": 1445574896, "gilded": 0, "name": "t1_cw9qlt2", "num_comments": null, "score": 2, "selftext": "Ah, okay, so something like this: handler = praw.handlers.DefaultHandler() r1 = praw.Reddit('...', handler=handler) r2 = praw.Reddit('...', handler=handler) At first I didn't think this was going to obey the ratelimiting correctly if two requests are made at the exact same time, but I guess that's what the locks are for! Seems to work just fine, that's really interesting.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "bboe", "created_utc": 1445576727, "gilded": 0, "name": "t1_cw9rcm2", "num_comments": null, "score": 1, "selftext": "Yeah that's one way to do it with the same `DefaultHandler` object. It should also work across any instance of the `DefaultHandler` class as the lock is on the class, not the instance. So the following should also work: r1 = praw.Reddit('...') r2 = praw.Reddit('...')", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "13steinj", "created_utc": 1444521438, "gilded": 0, "name": "t1_cvva9sy", "num_comments": null, "score": 2, "selftext": "Can you try making a virtual environment via virtualenv and then install praw to that virtual environment and see if the error persists?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "GoldenSights", "created_utc": 1444458185, "gilded": 0, "name": "t1_cvukxuo", "num_comments": null, "score": 1, "selftext": "PRAW actually has an internal cache that lasts 30 seconds, which means that your first and third request will get data, but the second will not (because it happens at 20s). In terms of server-side caching, it can get pretty slow if you're using a logged out / unauthenticated session, but if you're logged in it should be pretty snappy. Can you paste the relevant part of your bot to give us a better idea?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "schmodd", "created_utc": 1444454017, "gilded": 0, "name": "t1_cvujnjy", "num_comments": null, "score": 1, "selftext": "speed up things hopefully :) -> https://praw.readthedocs.org/en/latest/pages/multiprocess.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "cogsbox", "created_utc": 1444352648, "gilded": 0, "name": "t1_cvt6mr4", "num_comments": null, "score": 1, "selftext": "I am a noob to praw. Would it be possible for me to see what that would look like? I tried comment = submission.comments[1] to see what other items their were and got the error: IndexError: list index out of range", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o1fu2/return_author_name_on_rborrow/"}, {"author": "GoldenSights", "created_utc": 1444278958, "gilded": 0, "name": "t1_cvs611d", "num_comments": null, "score": 1, "selftext": "Uh oh. What version of PRAW? I'm using 3.3.0 and I get [this](http://i.imgur.com/ndl06ey.png) If you `print(submission)`, what does that dict look like?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "Fireislander", "created_utc": 1444279434, "gilded": 0, "name": "t1_cvs68bo", "num_comments": null, "score": 1, "selftext": "I can't remember how to check my PRAW version at the moment. I did just update it today to resolve another issue. When I printed my submission object I got this: {u'errors': []} Im assuming that means there is something strange going on with the creation of my submission object. The post creates itself correctly so the submission is working, but, if I am understanding this correctly, the submission is not properly passing itself back to the submission variable", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "GoldenSights", "created_utc": 1444280767, "gilded": 0, "name": "t1_cvs6rzz", "num_comments": null, "score": 1, "selftext": "You can check the praw version with `praw.__version__`, although if you did `pip install praw --upgrade` then you should be on 3.3.0. I'm not familiar with this issue at all. Can you try setting `r.log_requests=1` before submitting? You should see a POST and a subsequent GET. I would also try modifying your local PRAW install, adding some additional print statements to see where things are going. For example, printing the `result` variable in between lines 2634 and 2635 [here](https://github.com/praw-dev/praw/blob/351d408ce763cc63b93acecbf0a1bfaa7c0ed530/praw/__init__.py#L2634), and the `url` variable between 2640 and 2641 [here](https://github.com/praw-dev/praw/blob/351d408ce763cc63b93acecbf0a1bfaa7c0ed530/praw/__init__.py#L2640).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "GoldenSights", "created_utc": 1444254886, "gilded": 0, "name": "t1_cvrsdt4", "num_comments": null, "score": 1, "selftext": "I'll have to look around and see if the bytes thing is PRAW's fault. Thanks for your help!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "GoldenSights", "created_utc": 1443129131, "gilded": 0, "name": "t1_cvd0vwi", "num_comments": null, "score": 1, "selftext": "I'm pretty sure I know what the issue is, but I won't be able to make a pull request until I get home this evening. I'll let you know what happens -- thanks for pointing this out! edit: Here we go! https://github.com/praw-dev/praw/pull/534", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3m8ya2/praw_v321_crash_on_unhide_using_oauth2/"}, {"author": "13steinj", "created_utc": 1442854044, "gilded": 0, "name": "t1_cv95vce", "num_comments": null, "score": 1, "selftext": "Username mentions have have a `[\"new\"]` json dictionary key (or in PRAW, a `new` attribute) which is Boolean (True/False) If it is unread, it is true. If it is false, then it was already read. However; there are several dillemas. Do you want it to retain it's generator functionalities? To do that you'd have to make a function def strip_not_new(generator_object): for thing in generator_object: if thing.new is True: yeild thing And runt it on the `get_mentions()` generator. Or, if you want them as a list: new_messages = [thing for thing in mentions if thing.new is True] Or, if you want to turn that into a list iterator, have the previous statement and add new_messages = iter(new_messages) However be wary. I don't recall which ones if any, but I know at least one of the above will raise an index error if there is no new messages and you call the [0] index on it. You'll need to try and catch that exception.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "avinassh", "created_utc": 1442815955, "gilded": 0, "name": "t1_cv8s13c", "num_comments": null, "score": 2, "selftext": "password based authentication is going to be removed and also PRAW4 won't have this at all. Only you have to migrate to OAuth2 anyways. If you are writing an app which is used by other users, then OAuth is better since you don't need handle their username, password at all. Reddit will do all that for you. Your app/script will have fine grained permissions. A bot using HTTP Login will have full access to account. But with OAuth2 based ones, we can restrict it. Lastly, Reddit is nice for Oauth2 users. The rate limits are less for OAuth. Admins are encouraging everyone to move to OAuth", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "13steinj", "created_utc": 1442770421, "gilded": 0, "name": "t1_cv81t55", "num_comments": null, "score": 1, "selftext": "While this is definitely nice; You might want to remove the underlying sense of bias. Some people such as myself would much rather use [OAuth2Util](http://www.github.com/smbe19/praw-Oauth2util), as it's much easier to go through IMO, just add your scopes, client id and app secret in a config; then in your script call o = OAuth2Util.OAuth2Util(your_reddit_client) o.refresh(force=True) and then PRAW will automatically refresh your tokens when necessary without user intervention, and at least IMO, it's much simpler.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "dClauzel", "created_utc": 1442951524, "gilded": 0, "name": "t1_cvajrob", "num_comments": null, "score": 1, "selftext": "I agree, I personally find \u201cpraw-oauth2util\u201d far more easier to use. But eh, whatever float your goat \ud83d\udc10 See https://github.com/dClauzel/Reddit for some examples. Just this and a config file with the tokens is enough. r = praw.Reddit(user_agent=\"posix:MonScript:v0 (by /u/MonIdentifiantReddit)\") o = OAuth2Util.OAuth2Util(r, print_log=False) o.refresh()", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "13steinj", "created_utc": 1442952587, "gilded": 0, "name": "t1_cvaki4a", "num_comments": null, "score": 0, "selftext": "On top of that, if you pass the `force=True` kwarg to `o.refresh()`, on PRAW >= 3.2.0, you never have to call the method again, as PRAW will handle refreshing the token from then on.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "created_utc": 1442773536, "gilded": 0, "name": "t1_cv83yv3", "num_comments": null, "score": 1, "selftext": "> The way it was written was as if it's only possible to use oauth with your wrapper. I don't see where it is. please highlight that part, I shall edit it from OP. > and then force refresh once, and then everything else is handled for you until the end of time if thats the case, I stand corrected. I think I missed about refresh, so I will look praw at API and try to release that feature. Without having to worry about refresh is a good thing.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "13steinj", "created_utc": 1442774298, "gilded": 0, "name": "t1_cv84h9g", "num_comments": null, "score": 1, "selftext": "The title >How to migrate your existing bots from HTTP to OAuth2 combined with >Hey folks, I am the author of `prawoauth2`, a library which makes writing Reddit bots/apps using OAuth2 super easy and simple. Lately I have been receiving private messages, seeking help in migration, so I thought I would write a tutorial and redirect them here next time. Most of the text below is copied from the documentation. Plus the given TLDR makes it seem as if this is the *only* way, but in reality, you could use this, OAuth2Util, some separate wrapper, some custom wrapper, or no wrapper at all. >>and then force refresh once, and then everything else is handled for you until the end of time >if thats the case, I stand corrected. >I think I missed about refresh, so I will look praw API and try to release that feature. Without having to worry about refresh is a good thing. Nice, I look forward to using it in some scripts in the future (I do use your wrapper in several use cases where it's more practical for me; but otherwise OAuth2Util is my way to go). Be careful though, this auto refresh functionality is only available in praw 3.2 and higher, but at this time people use versions lower, such as 3.0 and 3.1. Also, some people like to manually refresh the token at a given point in time regardless if the hour is up, so I'd suggest adding the ability as well.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "created_utc": 1442816398, "gilded": 0, "name": "t1_cv8s5rr", "num_comments": null, "score": 1, "selftext": "just checked, since praw will handle the auto refresh, I don't need to make any changes in `prawoauth2` so I removed the refresh part from above tutorial.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "created_utc": 1442815298, "gilded": 0, "name": "t1_cv8rtw0", "num_comments": null, "score": 1, "selftext": "hey I just checked, but praw docs does not say anything about praw doing auto refresh: https://praw.readthedocs.org/en/stable/pages/oauth.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "GoldenSights", "created_utc": 1442815686, "gilded": 0, "name": "t1_cv8ry5c", "num_comments": null, "score": 2, "selftext": "It's true, auto-refresh was added in 3.2.0: https://github.com/praw-dev/praw/blob/master/CHANGES.rst#praw-320 I'll update the docs, sorry about that.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "jpflathead", "created_utc": 1442671892, "gilded": 0, "name": "t1_cv6wq0y", "num_comments": null, "score": 1, "selftext": "Thanks. As an example, your comment, retrieved from an unlogged in browser. Holy shit, 8K of crapola sent for 66 bytes of your comment, and it includes the original post, ... TWICE! [ { \"kind\": \"Listing\", \"data\": { \"modhash\": \"diyerhf89v9b48746bad03491cb1e3a2bcded3644e87f7fa62\", \"children\": [ { \"kind\": \"t3\", \"data\": { \"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": { }, \"subreddit\": \"redditdev\", \"selftext_html\": \"<!-- SC_OFF --><div class=\\\"md\\\"><p>I want to build the world&#39;s simplest application. When someone on my website pastes in a permalink to a comment, I want my site to retrieve that comment and return it. <\\/p>\\n\\n<p>The person doing the pasting does not even need to log in. <strong>I just want to retrieve a comment.<\\/strong><\\/p>\\n\\n<p>Yet, it seems like I&#39;m being forced to jump through 1000 hoops of fire to get everything to work. Why is Reddit&#39;s API so complicated to use compared to Twitter, AWS, or any other API?<\\/p>\\n\\n<p>Based off what I read, it seemed like I needed an API wrapper, so I went and downloaded <a href=\\\"https:\\/\\/github.com\\/jcleblanc\\/reddit-php-sdk\\\">https:\\/\\/github.com\\/jcleblanc\\/reddit-php-sdk<\\/a>. <\\/p>\\n\\n<p>And so, the problems began. <\\/p>\\n\\n<ol>\\n<li><p>Oh, my project uses composer. This API wrapper doesn&#39;t. I tried for hours to try and get class autoloading set up. It didn&#39;t work. I then had to fork it, reorganize the project, add PSR-0 support, and then add it to packagist. That took up all my time last night.<\\/p><\\/li>\\n<li><p>Cool, now I&#39;m using an semi-abandoned API wrapper to try and interact with a poorly documented API. <\\/p><\\/li>\\n<li><p>Now I can&#39;t get OAuth2 to work correctly. Why do I need to even use OAuth2 for this?! <\\/p><\\/li>\\n<li><p>Spend some more hours wasting my time looking at other API wrappers, PRAW, Guzzle, OAuth-2 repositories.<\\/p><\\/li>\\n<\\/ol>\\n\\n<p>I&#39;ve spend the last 2 days of my time doing this, with no results to show for it. The simplest task ever, and there&#39;s roadblocks everywhere.<\\/p>\\n\\n<p>Sorry for the rant, but I feel like I&#39;m doing something fundamentally wrong here. Is it really this difficult to retrieve a comment from Reddit? I feel like I&#39;m not even using the correct architecture here, should I just be using a bot? How does that work and how does it differ from an &quot;app&quot;?<\\/p>\\n<\\/div><!-- SC_ON -->\", \"selftext\": \"I want to build the world's simplest application. When someone on my website pastes in a permalink to a comment, I want my site to retrieve that comment and return it. \\n\\nThe person doing the pasting does not even need to log in. **I just want to retrieve a comment.**\\n\\nYet, it seems like I'm being forced to jump through 1000 hoops of fire to get everything to work. Why is Reddit's API so complicated to use compared to Twitter, AWS, or any other API?\\n\\nBased off what I read, it seemed like I needed an API wrapper, so I went and downloaded https:\\/\\/github.com\\/jcleblanc\\/reddit-php-sdk. \\n\\nAnd so, the problems began. \\n\\n1. Oh, my project uses composer. This API wrapper doesn't. I tried for hours to try and get class autoloading set up. It didn't work. I then had to fork it, reorganize the project, add PSR-0 support, and then add it to packagist. That took up all my time last night.\\n\\n2. Cool, now I'm using an semi-abandoned API wrapper to try and interact with a poorly documented API. \\n\\n3. Now I can't get OAuth2 to work correctly. Why do I need to even use OAuth2 for this?! \\n\\n4. Spend some more hours wasting my time looking at other API wrappers, PRAW, Guzzle, OAuth-2 repositories.\\n\\nI've spend the last 2 days of my time doing this, with no results to show for it. The simplest task ever, and there's roadblocks everywhere.\\n\\nSorry for the rant, but I feel like I'm doing something fundamentally wrong here. Is it really this difficult to retrieve a comment from Reddit? I feel like I'm not even using the correct architecture here, should I just be using a bot? How does that work and how does it differ from an \\\"app\\\"?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [ ], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ligwq\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"EchoLogic\", \"media\": null, \"name\": \"t3_3ligwq\", \"score\": 3, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"mod_reports\": [ ], \"secure_media_embed\": { }, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"\\/r\\/redditdev\\/comments\\/3ligwq\\/how_does_a_reddit_bot_differ_from_an_application\\/\", \"hide_score\": false, \"created\": 1442659476, \"url\": \"http:\\/\\/www.reddit.com\\/r\\/redditdev\\/comments\\/3ligwq\\/how_does_a_reddit_bot_differ_from_an_application\\/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does a reddit bot differ from an application? Are they the same thing? When do I need OAuth2?\", \"created_utc\": 1442630676, \"ups\": 3, \"upvote_ratio\": 1, \"num_comments\": 17, \"visited\": false, \"num_reports\": null, \"distinguished\": null } } ], \"after\": null, \"before\": null } }, { \"kind\": \"Listing\", \"data\": { \"modhash\": \"diyerhf89v9b48746bad03491cb1e3a2bcded3644e87f7fa62\", \"children\": [ { \"kind\": \"t1\", \"data\": { \"subreddit_id\": \"t5_2qizd\", \"banned_by\": null, \"removal_reason\": null, \"link_id\": \"t3_3ligwq\", \"likes\": true, \"replies\": \"\", \"user_reports\": [ ], \"saved\": false, \"id\": \"cv6sskz\", \"gilded\": 0, \"archived\": false, \"report_reasons\": null, \"author\": \"47Toast\", \"parent_id\": \"t3_3ligwq\", \"score\": 2, \"approved_by\": null, \"controversiality\": 0, \"body\": \"Just append .json at the end of the link and you'll get a json object of the comment. Use a json function from PHP and you have the comment. Just make sure you're using a good user-agent string and not querying more often than 30 times per minute.\\n\\nNo need to login in, except if you want the 60 queries per minute.\", \"edited\": false, \"author_flair_css_class\": null, \"downs\": 0, \"body_html\": \"<div class=\\\"md\\\"><p>Just append .json at the end of the link and you&#39;ll get a json object of the comment. Use a json function from PHP and you have the comment. Just make sure you&#39;re using a good user-agent string and not querying more often than 30 times per minute.<\\/p>\\n\\n<p>No need to login in, except if you want the 60 queries per minute.<\\/p>\\n<\\/div>\", \"subreddit\": \"redditdev\", \"score_hidden\": false, \"name\": \"t1_cv6sskz\", \"created\": 1442685860, \"author_flair_text\": null, \"created_utc\": 1442657060, \"distinguished\": null, \"mod_reports\": [ ], \"num_reports\": null, \"ups\": 2 } } ], \"after\": null, \"before\": null } } ]", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "earth-tone", "created_utc": 1442633577, "gilded": 0, "name": "t1_cv6m5om", "num_comments": null, "score": 3, "selftext": "You should use Oauth all the time. Aside from getting twice the number of requests and not having to deal with cookie/session/modhash management anymore, non-Oauth access is unpredictable at best. It is for me, anyway. Most of my scripts are Oauth, and the one that wasn't started getting 403s constantly (unpredictably, but constantly). So much so that it became basically useless and I went full Oauth. Other people say this hasn't happened to them, so your mileage may vary. I don't know about that API wrapper, but I know it's very simple in the [Perl wrapper](http://search.cpan.org/~earthtone/Reddit-Client/lib/Reddit/Client.pm). my $reddit = new Reddit::Client(username, password, client id, secret, arbitrary user agent string); my $info = $reddit->info(the comment's fullname, aka t1_foobar); Most people around here will probably be using the Python wrapper, though (PRAW). But basically, in any functioning wrapper, you should be able to plug in your 4 pieces of information (username, password, client id, token) and make a request.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "13steinj", "created_utc": 1442429226, "gilded": 0, "name": "t1_cv3sbey", "num_comments": null, "score": 1, "selftext": "~~Define \"download\".~~ ~~Python stores objects (including praw comment objects ) in RAM, not on the hard disk. With the pickle or cpickle libraries you can natively store and retrieve the object from a file on the hard disk (if that's what you would mean by download).~~ ~~If you don't need the full data, get what you need and putting it in a file of your choice, whether it be json, sql, etc for later retrieval.~~ What happens when you only read the title? You answer the wrong question. Speedwise you can't do much. If you literally need to get all those comments there's not much you can do. The number discrepancy is due to mod removed and deleted comments still being in the counter.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "mjgcfb", "created_utc": 1442439454, "gilded": 0, "name": "t1_cv3zf9c", "num_comments": null, "score": 1, "selftext": "Have you tried the comment stream helper that PRAW offers? https://praw.readthedocs.org/en/stable/pages/code_overview.html#praw.helpers.comment_stream", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "zurtex", "created_utc": 1442294108, "gilded": 0, "name": "t1_cv21yx7", "num_comments": null, "score": 2, "selftext": "Try this: def foo(): return range(10) print(foo) print(foo()) The parenthesis will instantiate a method / function etc... Otherwise you just are just assigning the method / function to some other variable name. So it seems that get_upvoted is a method to return a get_content generator and not one itself. This could of been done differently by PRAW, just gotta understand the objects you're working with.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kxsbb/praw_get_upvoted_returning_a_bound_method_rather/"}, {"author": "13steinj", "created_utc": 1441857519, "gilded": 0, "name": "t1_cuwfavl", "num_comments": null, "score": 1, "selftext": "Firstly; may I say I like the \"hacked open\" handler, and if you would like to add it to praw modify it slightly to work via some form of keyword argument; then submit a pull request. Secondly; it **does** limit the rate; at least from what I know (try checking without a steam helper function). The stream helper functions are bugged in the latest stable release (3.2.1) and I believe lower. In the dev version, they are similarly bugged(I don't think /u/bboe had the chance to fix it since the issue i still open). [Disucssion here](https://github.com/praw-dev/praw/issues/501)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kci9s/how_does_tze_defaulthandler_work_in_praw/"}, {"author": "DarkMio", "created_utc": 1441859525, "gilded": 0, "name": "t1_cuwg7qb", "num_comments": null, "score": 1, "selftext": "Actually, it didn't. After a day working on several tasks it struck me: https://github.com/praw-dev/praw/blob/master/praw/handlers.py#L104 The regular handler gets monkey patched to call class.rate_limit(request()) instead of class.request(). This is a use-case of a decorator (which is even written as one) and then patched up. Changing my PRAW Handler to this works then fine: @RateLimitHandler.rate_limit def request(self, request, proxies, timeout, verify, **_): self.logger.debug('{:4} {}'.format(request.method, request.url)) return self.http.send(request, proxies=proxies, timeout=timeout, allow_redirects=False, verify=verify) Looking deeper into the source, I believe [issue #319](https://github.com/praw-dev/praw/issues/319) is easily fixable by rewriting a chunk of the basic RateLimitHandler (which I probably will do today)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kci9s/how_does_tze_defaulthandler_work_in_praw/"}, {"author": "DarkMio", "created_utc": 1441868373, "gilded": 0, "name": "t1_cuwj8je", "num_comments": null, "score": 1, "selftext": "Yeah, I worked a ton on it and threw together a completely new Handler: https://github.com/DarkMio/RedditRover/blob/master/core/PRAWHandler.py#L8 This one is right now like RateLimitHandler and supports OAuth, Cookie-Auth and No-Auth independently: OAuths can send a request every second, Cookie/No-Auths can send every two seconds. This should also fix this issue: https://github.com/praw-dev/praw/issues/319 Editedit: I ment issue #319 all the time, not 501. If you want, I can refactor this handler, add an optional logger for it and build the other handlers on top of it. A log of the current requests can be found here: http://hastebin.com/jumekicija.avrasm I am not sure how API Limitations are ment: Do I have to limit API-Calls to 30 calls per minute per IP when I am using no-auth and this applies to OAuth users aswell or are OAuth users completely independent?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kci9s/how_does_tze_defaulthandler_work_in_praw/"}, {"author": "13steinj", "created_utc": 1441469166, "gilded": 0, "name": "t1_curl5ds", "num_comments": null, "score": 2, "selftext": "Well there's more than one way to do that, but I would do something like this for the sake of ease (though, this would create a shit ton more requests than needed since /r/Iama is slow) DESIRED_AMOUNT = 10 file = open('output.txt', 'a') r = praw.Reddit('USERAGENT') while count", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "created_utc": 1441553400, "gilded": 0, "name": "t1_cusiol0", "num_comments": null, "score": 2, "selftext": "1\\. Can you post a [gist](http://gist.github.com) or your code? 2\\. Yeah, but there are other ways to stop the submission_stream, such as forcing an error and catching it after a certain amount of time, like here's an unconventional use: class SubmissionStreamBreaker(Exception): pass counter = 0 try: while True: # By Default the session is always True for post in praw.helpers.submission_stream(r, 'IAMA', limit=None): count +=1 # do something to posts if count >= 37: # we did something for 37 new posts # it's time to do something else now # raise the exception that was made so that it is # caught and then continue raise SubmissionStreamBreaker('Break It') except SubmissionStreamBreaker: pass #do something else now.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "metaranha", "created_utc": 1441555117, "gilded": 0, "name": "t1_cusji1a", "num_comments": null, "score": 1, "selftext": "Functionally, it's basically the same code from yesterday, but I made a few changes (removed the stream call and added the try envelope) print(\"creating output file\") file = open('output.txt', 'a') r = praw.Reddit('IAMA') print(\"grabbing subreddit\") subreddit = r.get_subreddit('IAMA') while count", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "created_utc": 1441329375, "gilded": 0, "name": "t1_cupxx23", "num_comments": null, "score": 2, "selftext": "I recommend using [OAuth2Util](https://www.reddit.com/r/botwatch/comments/38k30h/oauth2util_a_wrapper_around_praws_oauth2/) as it makes the process much easier (the post is outdated; but [here's the latest update](https://www.reddit.com/r/botwatch/comments/3hwzkh/oauth2util_update/)).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/"}, {"author": "GoldenSights", "created_utc": 1440887999, "gilded": 0, "name": "t1_cuk5miy", "num_comments": null, "score": 5, "selftext": "While method names are typically lowercased and underscore_separated, class names are typically upper and camel-cased in Python style guides. It's `praw.Reddit`.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3iw7ij/module_object_has_no_attribute_reddit/"}, {"author": "TomSparkLabs", "created_utc": 1440427928, "gilded": 0, "name": "t1_cudxbwi", "num_comments": null, "score": 2, "selftext": "Never mind. I found https://github.com/avinassh/prawoauth2 while browsing the subreddit. Made it a whole lot easier.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3i7dcc/how_do_i_upgrade_to_oauth2_authentication_with/"}, {"author": "13steinj", "created_utc": 1440438313, "gilded": 0, "name": "t1_cue3v9k", "num_comments": null, "score": 2, "selftext": "It's not an error, it's a warning. Sooner or later CookieAuth will be heavily throttled, and when that time comes PRAW will be OAuth/Unauthenticated only while removing the login method. That said, I use /u/smbe19's praw-OAuth2Util.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3i7dcc/how_do_i_upgrade_to_oauth2_authentication_with/"}, {"author": "TomSparkLabs", "created_utc": 1440700004, "gilded": 0, "name": "t1_cuhqhfj", "num_comments": null, "score": 2, "selftext": "I'm currently using /u/avinassh's prawoath2. https://github.com/avinassh/prawoauth2 Pretty easy fix. Plus, doesn't require me to start a Django/Flask server or anything.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3i7dcc/how_do_i_upgrade_to_oauth2_authentication_with/"}, {"author": "GoldenSights", "created_utc": 1440275328, "gilded": 0, "name": "t1_cuc46n6", "num_comments": null, "score": 2, "selftext": "The praw search should return Submission objects. You can then get the id with `submission.id`. The votes and title I think is the str() representation of the object. I'm not quite sure what you're doing to get that though.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hzg8r/get_submission_id_from_the_results_of_a_praw/"}, {"author": "13steinj", "created_utc": 1440187233, "gilded": 0, "name": "t1_cub1nej", "num_comments": null, "score": 1, "selftext": "That's extremely weird. Are you running on a server or a local machine? In anycase try to run as an admin, if that still doesn't work, install Python 3.4.3 side by side it, and go to that scripts folder and run \"pip34 install praw-oauth2util\".", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "GoldenSights", "created_utc": 1440010754, "gilded": 0, "name": "t1_cu8ksam", "num_comments": null, "score": 2, "selftext": "/u/13steinj is working on a pull request this very moment. It will be part of the praw development build by the end of the day! *https://github.com/praw-dev/praw/commit/f6b073c6716c55561f5bd78199a609259619d117", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "13steinj", "created_utc": 1440013590, "gilded": 0, "name": "t1_cu8mue8", "num_comments": null, "score": 2, "selftext": "It's been merged. Use pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip To upgrade to the dev build", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "GoldenSights", "created_utc": 1439982384, "gilded": 0, "name": "t1_cu84gqa", "num_comments": null, "score": 3, "selftext": "My understanding is that you can't just change one setting at a time. The endpoint literally requires you to provide ALL of the subreddit settings when submitting the change. It's a pretty colossal inconvenience, but that's beside the point. If you go to your subreddit's settings and try to enter it with a blank title, you'll get a \"we need something here\" error. Given that the title is a mandatory field, and you're required to explicitly provide all the settings, I suppose that's why it's a required parameter in praw. You need to fetch your subreddit's current settings, and pass the ones you want to keep back into set_settings while making the desired changes. Or keep a template of all the settings so you don't lose them with a single click. Maybe there should be a method in praw that does a \"smart settings update\" where it will check the previous data for you and only apply the new changes. That'd be nice. https://www.reddit.com/dev/api/oauth#POST_api_site_admin", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hkgou/praw_why_is_title_a_required_argument_for_set/"}, {"author": "GoldenSights", "created_utc": 1439943323, "gilded": 0, "name": "t1_cu7oq53", "num_comments": null, "score": 3, "selftext": "Each of the new PRAW custom exceptions have an attribute called `_raw` which contains the original exception from the requests module. You can do: try: stuff() except praw.errors.HTTPException as e: if e._raw.status_code == 503: print('reddit is busy!') else: raise You might also want to look out for other 500 errors, such as 502 bad gateway, 504 gateway timeout, and 520 origin error. There might be even more variants but that's a subject I need to learn more about.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hil7j/how_to_add_reddit_overload_support_to_my_script/"}, {"author": "solaceinsleep", "created_utc": 1439944460, "gilded": 0, "name": "t1_cu7pdtj", "num_comments": null, "score": 1, "selftext": "Thank you. Exactly what I'm looking for. I have several except statements targeting specific things, and one general except to catch any other ones I missed. So far I haven't encountered any of those except the 503 (one time). So I will try to add them if they become an issue. By the way the else statement reraises the exception right? If so how would that be handled? Can it be caught by a next except? Kind of like here: try: stuff() except praw.errors.HTTPException as e: if e._raw.status_code == 503: print('reddit is busy!') else: raise except Exception as e: print(traceback.format_exc())", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hil7j/how_to_add_reddit_overload_support_to_my_script/"}, {"author": "GoldenSights", "created_utc": 1439945007, "gilded": 0, "name": "t1_cu7pp8k", "num_comments": null, "score": 1, "selftext": "No, the re-raised exception from the else statement will not be caught by the next except block. You'll probably have to do something like this: try: stuff() except Exception as e: if isinstance(e, praw.errors.HTTPException): if e._raw.status_code == 503: print('reddit is busy') else: # It's some other httpexc raise else: traceback.print_exc() At least, that's the first thing that comes to mind. I wouldn't be surprised if there's a smarter, more pythonic way of doing this, but I'm blanking at the moment. Maybe someone else can make a suggestion.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hil7j/how_to_add_reddit_overload_support_to_my_script/"}, {"author": "GoldenSights", "created_utc": 1439943042, "gilded": 0, "name": "t1_cu7okbm", "num_comments": null, "score": 3, "selftext": "Hi OP, when I do this: s = r.get_subreddit('StevenUniverse') flairs = s.get_flair_choices() print(len(flairs['choices'])) I get 234. I'm using PRAW 3.2.0 on Python 3.4.3, with \"flair\" and \"identity\" as my only active OAuth scopes. It also works when using `r.get_flair_choices('stevenuniverse')`. Are you still having problems with this method? Any other info you can give maybe?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "bboe", "created_utc": 1439873534, "gilded": 0, "name": "t1_cu6qnld", "num_comments": null, "score": 1, "selftext": "Could you try downgrading PRAW to an older version to confirm that it in fact due to the PRAW version?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "powderblock", "created_utc": 1439911124, "gilded": 0, "name": "t1_cu73p34", "num_comments": null, "score": 1, "selftext": "Odd about the wiki thing. But I think this would be a problem with reddit. I read in the [praw Git repo in objects.py at line 1168](https://github.com/praw-dev/praw/blob/242190fe7e8fd029c4c79612a9c1fa10bddfb764/praw/objects.py#L1168): \"def get_flair_choices(self, *args, **kwargs): \"\"\"Return available link flair choices and current flair. ... :returns: The json response from the server.\" So I think reddit probably limited the number of responses in the json to 20? Does this make sense to anyone else? I am a novice developer. Edit: I tried to find a changelog or something that would support the fact that reddit doesn't give more than 20 flair objects but I can't find anything to back this up. Hm. All the code I've read though seems like reddit really does return all the flairs.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "13steinj", "created_utc": 1439703229, "gilded": 0, "name": "t1_cu4lyco", "num_comments": null, "score": 1, "selftext": "No idea. You could probably do it with html scraping if you also use CookieLib to \"log in\", but I'm not sure if that kind of cookieauth will also be disabled soon. Sidenote: YAY Some features that used to have to be done manually are now directly in a stable version of PRAW :P", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h40nx/praw_getting_the_expiration_time_of_a_ban_and_the/"}, {"author": "13steinj", "created_utc": 1439703808, "gilded": 0, "name": "t1_cu4m5oz", "num_comments": null, "score": 1, "selftext": "Looking at the json from a test sub: {\"kind\": \"Listing\", \"data\": {\"modhash\": \"yilm2j681w62622384365fc27be3fd3eb79dbcc1191579a678\", \"children\": [{\"date\": 1439703553.0, \"note\": \"test\", \"name\": \"Username\", \"id\": \"t2_lx7th\"}], \"after\": null, \"before\": null}} So...it's not in the API as of yet, and as such not in PRAW. You could use html scraping like I mentioned in the previous comment (which would be done with BeautifulSoup4, requests, and as it's a log in only page CookieLib, but I'm not sure if that form of cookie auth will be removed soon or not).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h40nx/praw_getting_the_expiration_time_of_a_ban_and_the/"}, {"author": "bboe", "created_utc": 1439962178, "gilded": 0, "name": "t1_cu7ysw6", "num_comments": null, "score": 1, "selftext": "It's possible that a private message reply currently doesn't work in PRAW. We've yet to update all the tests to their OAuth equivalent. I unfortunately don't currently have the time to verify.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "GoldenSights", "created_utc": 1439650150, "gilded": 0, "name": "t1_cu3wu8f", "num_comments": null, "score": 3, "selftext": "A little while ago, PRAW started wrapping all of its exceptions under custom names. In this case, PRAW caught the 503 requests.exceptions.HTTPError, then reraised a new praw.errors.HTTPException, which you can see at the bottom of your traceback. This is the exception you should be catching. I think the system could use some improvement, but that's where we're at currently. As for your second question, it seems that the praw exceptions return a blank string for str(). Personally I would raise an issue on http://github.com/praw-dev/praw/issues, that seems like a pretty big deal to me.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3hmm/how_do_you_catch_a_specific_exception/"}, {"author": "soymilkbot", "created_utc": 1439651251, "gilded": 0, "name": "t1_cu3xbuy", "num_comments": null, "score": 1, "selftext": "I see, that makes sense. However that raises another problem. Is there an equivalent of requests' response.status_code for praw.errors.HTTPException? I looked through the documentation and I can't find praw.errors.HTTPException's attributes. I need the status_code to identify the kind of HTTPError. About the blank string, I'll report it right now. Thank you so much for the help! Edit: I was wondering, is this kind of question more suited to this subreddit or /r/botwatch? I don't want to post to the wrong place. Edit2: After some more searching I figured out that the function that wraps the original `requests.exceptions.HTTPError` raises the `HTTPException` with `requests.exceptions.HTTPError.response` as its `_raw` argument, so I should be able to get the `status_code` from `_raw`. Here is the relevant part of the function: try: response.raise_for_status() # These should all be directly mapped except exceptions.HTTPError as exc: raise HTTPException(_raw=exc.response) I changed my code to look like this: try: # some code except KeyboardInterrupt: print(\"Shutting down.\") break except praw.errors.HTTPException as e: exc = e._raw print(\"Some thing bad happened! HTTPError\", exc.status_code) if exc.status_code == 503: print(\"Let's wait til reddit comes back! Sleeping 60 seconds.\") time.sleep(60) except Exception as e: print(\"Some thing bad happened!\", e) traceback.print_exc() I hope this works...", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3hmm/how_do_you_catch_a_specific_exception/"}, {"author": "bboe", "created_utc": 1439564415, "gilded": 0, "name": "t1_cu2tml7", "num_comments": null, "score": 1, "selftext": "This tool looks very awesome. Can you describe your ingest methods and your comparison to PRAW? It'd be awesome to know in what cases and how your tool can receive more comments and submissions. Also were you able to benchmark your SSE server for the number of concurrent connections it can support? If so what tools did you use for that? Thanks for all the hard work.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "Stuck_In_the_Matrix", "created_utc": 1439593947, "gilded": 0, "name": "t1_cu3ckl2", "num_comments": null, "score": 1, "selftext": "I hit the /api/info endpoint and ask for the id's directly and crawl upwards. PRAW may have updated their code since the last time when they were hitting /r/all/comments. The SSE Server is handling surprisingly well. With 15 connections, it's only using 15% of one CPU. Redis seems to be very efficient and I use pipelining mode to reduce the amount of back and forth between the Redis server. It looks like this VPS could support at least 100 streams. Bandwidth would eventually become an issue at that rate.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "GoldenSights", "created_utc": 1439371601, "gilded": 0, "name": "t1_cu079nz", "num_comments": null, "score": 3, "selftext": "The first thing I notice is that the request for /about/banned doesn't have the \"Substituting api.reddit.com for oauth.reddit.com\" message above it. This would lead me to believe that the oauth headers are not being included in the request, giving you the 403. I would download the .zip of the current praw repo [here](https://github.com/praw-dev/praw) and overwrite your installation, because this issue may have already been fixed. I'm guessing it has something to do with the restrict_access decorators, which I know have had a few changes since 3.1.0.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "created_utc": 1439444608, "gilded": 0, "name": "t1_cu1ayjj", "num_comments": null, "score": 3, "selftext": "Yeah, I understand that you didn't purposely change anything. I'm suggesting you download the GitHub version of PRAW because this problem may have been fixed since the 3.1.0 release.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "created_utc": 1439444853, "gilded": 0, "name": "t1_cu1b1nc", "num_comments": null, "score": 1, "selftext": "If you download the zip and open it up, you will see a folder called \"praw-master\", with a subfolder named \"praw\". This folder has \"\\_\\_init\\_\\_.py\", \"decorators.py\", and a few others. Extract these files to /usr/local/lib/python3.4/dist-packages/praw/ (I copied that path from your traceback), overwriting the ones that came from pip. Then you can re-run your script and see what happens.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "created_utc": 1439447054, "gilded": 0, "name": "t1_cu1bu8r", "num_comments": null, "score": -1, "selftext": "With this modification, it works perfectly $ ./demo.py substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json connexion\u2026 connect\u00e9. substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json Je suis dClauzel et j\u2019ai un karma de 15981 pour mes commentaires. Top 5 substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/Europe/about/.json substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/europe/.json - Immigration Megathread - Part VI \u2014 ModeratorsOfEurope \u2014 http://www.reddit.com/r/europe/comments/3frno2/immigration_megathread_part_vi/ - Access to the entire Reddit.com domain has been reportedly blocked in Russia \u2014 zurfer75 \u2014 https://meduza.io/news/2015/08/12/reddit-vnesli-v-reestr-zapreschennyh-saytov - Hey Europe, what are your country's top conspiracy theories? \u2014 herr_wildow \u2014 http://www.reddit.com/r/europe/comments/3gpgyx/hey_europe_what_are_your_countrys_top_conspiracy/ - EU rejects Eastern states' call to outlaw denial of crimes by communist regimes: Eastern European states wanted Soviet crimes 'treated according to the same standards' as those of Nazi regimes \u2014 nastratin \u2014 http://www.theguardian.com/world/2010/dec/21/european-commission-communist-crimes-nazism - Putin is actually in serious trouble \u2014 giggster \u2014 http://www.businessinsider.com/putin-is-actually-in-serious-trouble-2015-8 Liste des bannis substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/europe/about/banned/.json [Redditor(user_name='redacted'), Redditor(user_name='redacted'), Redditor(user_name='redacted'), Redditor(user_name='redacted'), Redditor(user_name='redacted'), Redditor(user_name='redacted')] fin sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty Many thanks for your time, you solved my problem. Do you know when this new version of praw will be available in pip?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "bboe", "created_utc": 1439451512, "gilded": 0, "name": "t1_cu1d7df", "num_comments": null, "score": 2, "selftext": "Just FYI, I absolutely recommend using the pip install from master approach over manual package updates: pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip It's a nice simple one-liner that's easy to undo.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "created_utc": 1439452339, "gilded": 0, "name": "t1_cu1dfft", "num_comments": null, "score": 2, "selftext": "Whenever I do that, I get this output: Collecting https://github.com/praw-dev/praw/archive/master.zip Downloading https://github.com/praw-dev/praw/archive/master.zip (1.8MB) 100% |################################| 1.8MB 161kB/s Collecting decorator>=3.4.2 (from praw==3.1.0) Using cached decorator-4.0.2-py2.py3-none-any.whl Requirement already up-to-date: requests>=2.3.0 in c:\\python34\\lib\\site-packages (from praw==3.1.0) Requirement already up-to-date: six>=1.4 in c:\\python34\\lib\\site-packages (from praw==3.1.0) Requirement already up-to-date: update-checker>=0.11 in c:\\python34\\lib\\site-packages (from praw==3.1.0) Installing collected packages: decorator, praw Found existing installation: decorator 3.4.2 Cannot remove entries from nonexistent file c:\\python34\\lib\\site-packages\\easy-install.pth and my installation remains unchanged. Google hasn't been very helpful with that error message, but to be fair I only looked at the first 2 pages of results since the manual install is good enough for me. Are you familiar with this error? And, since OP was wondering, when did you want to release the next version of PRAW? Is there anything on the table besides more test ports? I still need to make that next pr.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "created_utc": 1439452862, "gilded": 0, "name": "t1_cu1dk9i", "num_comments": null, "score": -2, "selftext": "I had to install decorator myself: `pip3 install decorator`. No problem after that. # pip3 install --upgrade https://github.com/praw-dev/praw/archive/master.zip Downloading/unpacking https://github.com/praw-dev/praw/archive/master.zip Downloading master.zip (unknown size): 1.8MB downloaded Running setup.py (path:/tmp/pip-ykthgbwi-build/setup.py) egg_info for package from https://github.com/praw-dev/praw/archive/master.zip Requirement already up-to-date: decorator>=3.4.2 in /usr/local/lib/python3.4/dist-packages (from praw==3.1.0) Requirement already up-to-date: requests>=2.3.0 in /usr/lib/python3/dist-packages (from praw==3.1.0) Requirement already up-to-date: six>=1.4 in /usr/lib/python3/dist-packages (from praw==3.1.0) Requirement already up-to-date: update-checker>=0.11 in /usr/local/lib/python3.4/dist-packages (from praw==3.1.0) Installing collected packages: praw Found existing installation: praw 3.1.0 Uninstalling praw: Successfully uninstalled praw Running setup.py install for praw Installing praw-multiprocess script to /usr/local/bin Successfully installed praw Cleaning up...", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "bboe", "created_utc": 1439476256, "gilded": 0, "name": "t1_cu1lgsf", "num_comments": null, "score": 3, "selftext": "Well that's interesting. I personally use virtual environments to avoid mucking around with the site packages thus whenever something like this happens I simply delete the virtual environment and start over. In your case you may simply be at a point where a virtual environment is a necessity as different packages certainly will require conflicting versions of the same dependency. As for releasing PRAW. There was a point where I would release a minor version every day. So feel free to do that as often as you'd like as long as it's not a backwards breaking change.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "created_utc": 1439586955, "gilded": 0, "name": "t1_cu38knz", "num_comments": null, "score": 1, "selftext": "Upgrading: # pip3 install --upgrade praw Downloading/unpacking praw from https://pypi.python.org/packages/3.4/p/praw/praw-3.2.0-py2.py3-none-any.whl#md5=6902294428b656c8535df5a636f1ad02 Downloading praw-3.2.0-py2.py3-none-any.whl (68kB): 68kB downloaded Requirement already up-to-date: decorator>=3.4.2 in /usr/local/lib/python3.4/dist-packages (from praw) Requirement already up-to-date: requests>=2.3.0 in /usr/lib/python3/dist-packages (from praw) Requirement already up-to-date: six>=1.4 in /usr/lib/python3/dist-packages (from praw) Requirement already up-to-date: update-checker>=0.11 in /usr/local/lib/python3.4/dist-packages (from praw) Installing collected packages: praw Found existing installation: praw 3.1.0 Uninstalling praw: Successfully uninstalled praw Successfully installed praw Cleaning up... No problem to report, the scripts run correctly.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": null, "created_utc": 1439438278, "gilded": 0, "name": "t1_cu189wj", "num_comments": null, "score": 1, "selftext": "Real men don't need PRAW: https://www.reddit.com/r/europe/about/banned.json I highly recommend https://snoocore.readme.io Good luck with your censorship.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": null, "created_utc": 1439445190, "gilded": 0, "name": "t1_cu1b60o", "num_comments": null, "score": 0, "selftext": "Snoocore works great with node at the command line. Seems like praw is broken unfortunately.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "created_utc": 1439261025, "gilded": 0, "name": "t1_ctyqdh2", "num_comments": null, "score": 2, "selftext": "When you make a comment, you should receive the comment object back as a response from Reddit. If you're using PRAW, it goes like this: comment = submission.add_comment('first message') reply = comment.reply('second message') You could then come up with a method that chains replies until the entire text is submitted.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gjm58/making_a_comment_chain_to_overcome_reddit_comment/"}, {"author": "GoldenSights", "created_utc": 1438767312, "gilded": 0, "name": "t1_cts53uo", "num_comments": null, "score": 2, "selftext": "This issue has been fixed, but not yet released. You can download the current praw source from [this page](https://github.com/praw-dev/praw) with the button on the right side. I'm not sure when the next praw release is going to happen. edit: Here's a little bit more [info](https://github.com/praw-dev/praw/issues/489#issuecomment-127091365)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "dClauzel", "created_utc": 1440616023, "gilded": 0, "name": "t1_cugkyo4", "num_comments": null, "score": -1, "selftext": "# Solved App lacked the right scope (Reddit added it recently), so a simple solution was to create a new app (as script). ----- Is this the same problem? With PRAW v3.2.1, when trying to ban a user I get this error: raise OAuthInsufficientScope('insufficient_scope', response.url) praw.errors.OAuthInsufficientScope: insufficient_scope on url https://oauth.reddit.com/api/friend/.json (ping /u/GoldenSights) ---- D\u00e9moBan.py #!/usr/bin/env python3 # -*- coding: utf-8 -*- import praw import OAuth2Util r = praw.Reddit(user_agent=\"posix:MonScript:v0 (by /u/MonIdentifiantReddit)\", site_name=\"Reddit\") o = OAuth2Util.OAuth2Util(r, print_log=False) o.refresh() sousjlailu = r.get_subreddit(\"Banquise\") utilisateur = r.get_redditor(\"TheUserToBan\") sousjlailu.add_ban(utilisateur, **{\"duration\": 30, \"note\": \"A sassy private comment\", \"ban_message\": \"A nice message telling to GTFO\"}) oauth.txt # Config scope=identity,account,edit,flair,history,livemanage,modcontributors,modconfig,modflair,modlog,modothers,modposts,modself,modwiki,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote,wikiedit,wikiread refreshable=True # Appinfo app_key=redacted app_secret=redacted # Token token=redacted refresh_token=redacted valid_until=redacted Running D\u00e9moBan.py $ ./D\u00e9moBan.py Traceback (most recent call last): File \"./D\u00e9moBan.py\", line 16, in sousjlailu.add_ban(utilisateur, **{\"duration\": 30, \"note\": \"A sassy private comment\", \"ban_message\": \"A nice message telling to GTFO\"}) File \"\", line 2, in do_relationship File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 268, in wrap return function(*args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 113, in do_relationship return session.request_json(url, data=data) File \"\", line 2, in request_json File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 605, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 438, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 193, in _raise_response_exceptions raise OAuthInsufficientScope('insufficient_scope', response.url) praw.errors.OAuthInsufficientScope: insufficient_scope on url https://oauth.reddit.com/api/friend/.json sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty sys:1: ResourceWarning: unclosed", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "GoldenSights", "created_utc": 1440627200, "gilded": 0, "name": "t1_cugsgam", "num_comments": null, "score": 1, "selftext": "Can you try doing this using a PRAW-only implementation of oauth? That is, r = praw.Reddit('posix:MonScript:v0 (by /u/MonIdentifiantReddit)') r.set_oauth_app_info(app_id, app_secret, app_uri) r.refresh_access_information(refresh_token) print(r._authentication, r.access_token) sousjlailu = r.get_subreddit(\"Banquise\") utilisateur = r.get_redditor(\"TheUserToBan\") sousjlailu.add_ban(utilisateur, **{\"duration\": 30, \"note\": \"A sassy private comment\", \"ban_message\": \"A nice message telling to GTFO\"}) Because I'm not having any issues with this: http://i.imgur.com/BtNKTGT.png", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "dClauzel", "created_utc": 1440628497, "gilded": 0, "name": "t1_cugt8cg", "num_comments": null, "score": 0, "selftext": "{'modposts', 'edit', 'subscribe', 'wikiread', 'privatemessages', 'modlog', 'history', 'modself', 'mysubreddits', 'vote', 'modwiki', 'livemanage', 'account', 'identity', 'modothers', 'save', 'wikiedit', 'modconfig', 'submit', 'modflair', 'read', 'flair', 'report'} redacted Traceback (most recent call last): File \"./D\u00e9moBanPur.py\", line 26, in sousjlailu.add_ban(utilisateur, **{\"duration\": 30, \"note\": \"A sassy private comment\", \"ban_message\": \"A nice message telling to GTFO\"}) File \"\", line 2, in do_relationship File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 265, in wrap raise errors.LoginOrScopeRequired(function.__name__, scope) praw.errors.LoginOrScopeRequired: ``do_relationship` requires a logged in session or the OAuth2 scope `modcontributors`` requires a logged in session sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty sys:1: ResourceWarning: unclosed Looks like the scope `modcontributors` is not present. I created the app as a \u201cscript\u201d, so all rights should be available; it was\u2026 3 weeks ago? Here is what I have for it in Reddit\u2019s pref: * Moderate Subreddit Configuration * Edit My Subscriptions * Vote * Wiki Editing * My Subreddits * Submit Content * Moderation Log * Moderate Posts * Moderate Flair * Save Content * Invite or remove other moderators * Read Content * Private Messages * Report content * My Identity * Manage live threads * Update account information * Read Wiki Pages * Edit Posts * Moderate Wiki * Make changes to your subreddit moderator and contributor status * History * Manage My Flair Obviously, as I am the creator of r/Banquise, I have full access on it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "GoldenSights", "created_utc": 1438758201, "gilded": 0, "name": "t1_cts2qwl", "num_comments": null, "score": 2, "selftext": "The best way to get around the 1,000 item cache, and collect all submissions to a subreddit, is to use the Cloudsearch timestamp query. I wrote a bit of a tutorial on how to use it [here](https://www.reddit.com/r/reddittips/comments/2ix73n/use_cloudsearch_to_search_for_posts_on_reddit/), and I have a praw script that collects submissions into an sqlite3 database, [here](https://github.com/voussoir/reddit/blob/master/Prawtimestamps/timesearch.py). You don't have to use it, but the main idea behind the get_all_posts function is to create a sliding window, performing the ts search from the subreddit's creation date to now.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "GoldenSights", "created_utc": 1439106975, "gilded": 0, "name": "t1_ctwpg4p", "num_comments": null, "score": 1, "selftext": "If you were to call the same function with the exact same parameters multiple times, you will get the same 100 results again. As a matter of fact, PRAW keeps a 30 second cache of the responses, so you'll get those items back without even making a request. Reddit doesn't \"know\" that I want the next 100 items, but since I'm specifying the lower and upper timestamps in the cloudsearch query, I will get what I was looking for. Does that help, or did I miss the point of your question?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "GoldenSights", "created_utc": 1438667403, "gilded": 0, "name": "t1_ctqtln3", "num_comments": null, "score": 2, "selftext": "Hmm, for me the `permalink` attribute is https. I'm using the github version rather than the pypi version of PRAW, so I don't know if that was a change introduced between 3.1.0 and now. By next release if you're still getting http then there is probably a config issue.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fpmx8/praw_get_submission_works_only_sometimes_with/"}, {"author": "GoldenSights", "created_utc": 1438597509, "gilded": 0, "name": "t1_ctprern", "num_comments": null, "score": 4, "selftext": "Woah, that's really bizarre. I've just tested this and found that the privatemessages scope allows the user to start new PM threads via api/compose, but not continue old ones via api/comment/. It appears that reddit is just [checking the endpoint itself](https://github.com/reddit/reddit/blob/c942cef02c71d3228cdaee234d3195aeec37e70e/r2/r2/controllers/api.py#L1785), and not whether the object you're replying to is a Message. I can change these praw docs for you, but I'd be more interested in seeing if the admins will consider reworking this. It's a very silly quirk. /u/gooeyblob?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "gooeyblob", "created_utc": 1438905509, "gilded": 0, "name": "t1_ctu84pk", "num_comments": null, "score": 3, "selftext": "Thanks for pointing this out! It's been [fixed](https://github.com/reddit/reddit/commit/9ccda2c49fe17321dad97e20bb6b2799e2e006d1). I've left the `submit` scope in there for backwards compatibility, but I'd love to get PRAW updated to use the new scope. u/bboe what would be the best way to do that? Thanks again!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "GoldenSights", "created_utc": 1438905587, "gilded": 0, "name": "t1_ctu86b0", "num_comments": null, "score": 2, "selftext": "Awesome! I'll go ahead and make a PR with PRAW to get that patched up. Thank you very much!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "SmBe19", "created_utc": 1438531585, "gilded": 0, "name": "t1_ctoy0sv", "num_comments": null, "score": 1, "selftext": "It looks like praw expects a logged in session (i.e. login via user/pw) and the oauth scope isn't correctly set in praw (i.e. praw doesn't know which scope is required). If that's the case, you can't do much except maybe telling /u/bboe.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "bboe", "created_utc": 1438535663, "gilded": 0, "name": "t1_ctp03dh", "num_comments": null, "score": 1, "selftext": "Can you file an issue please? https://github.com/praw-dev/praw/issues", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "GoldenSights", "created_utc": 1438377276, "gilded": 0, "name": "t1_ctn9iqk", "num_comments": null, "score": 1, "selftext": "Hmm, the first thing that comes to mind is that you aren't using the new token correctly. When you do the browser thing and get the \"code\" and plug it into PRAW, that will give you a new refresh token. This new token is completely separate from the older one, it doesn't update the older one with new scopes or anything like that. If you're sure that you're doing it correctly, maybe you can try updating praw with the version on github to see if something has been changed that would fix it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "created_utc": 1438378819, "gilded": 0, "name": "t1_ctnah16", "num_comments": null, "score": 1, "selftext": "Ha, that's weird as hell! Thanks for your help, bro. How the hell is anyone supposed to know that? And PRAW doesn't error on it except to tell you you aren't logged in (it should OAuth error IMO). /u/deimorz, might want to look into this.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "GoldenSights", "created_utc": 1438378899, "gilded": 0, "name": "t1_ctnairc", "num_comments": null, "score": 1, "selftext": "I'll check out the praw exceptions and see if that can be fixed. The oauth scope restriction decorators are supposed to get overhauled for PRAW4 and I think those are involved in the \"requires logged in session\" message.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "created_utc": 1438379743, "gilded": 0, "name": "t1_ctnb15u", "num_comments": null, "score": 1, "selftext": "OK, so is it possible I'm not logged in even if r._authentication shows my correct authentication? I commented out the access code generation lines, so this is what I have at the start of my script: refresh_token=\"string here\" r = praw.Reddit(\"gunnit mod agent\") r.set_oauth_app_info(client_id,client_secret,redirect_uri) r.refresh_access_information(refresh_token) print r._authentication", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "SmBe19", "created_utc": 1438373575, "gilded": 0, "name": "t1_ctn751p", "num_comments": null, "score": 2, "selftext": "If you do not want to handle oauth (the whole token retrieval) yourself there is a [small library](https://github.com/SmBe19/praw-OAuth2Util) available. Regarding your problem: it seems like you didn't fully understand how oauth works. Short summary: 1. Script wants to access reddit on behalf of a user. It sends the user to reddit to ask him whether he does want to grant access. (`r.get_authorize_url(...)`, `webbrowser.open(url)`) 1. The user clicks accept and reddit generates a code which it sends to the given callback url. Normally there would be a webserver listening to grab the code, but for small bots we just copy the code from the url ourselves. 1. This code is not to access reddit, it is only a \"one time use ticket\". You can trade it in and you receive a token and a refresh token. (`r.get_access_information(...)`) 1. The token is your \"ticket\" to access reddit, but it is only valid for one hour. The refresh token allows you to get a new ticket when the old one is expired. As you see you only need the code the first time you access reddit. Afterwards you have to provide the refresh token to retrieve a new token without the need to ask the user again. So you don't have to write the code permanently in your script, but you should save the refresh token and use this on a restart.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "SmBe19", "created_utc": 1438415586, "gilded": 0, "name": "t1_ctnr20i", "num_comments": null, "score": 1, "selftext": "Everytime you call `o.refresh` it checks how old the token is. If the token is too old, it will request a new one. So if you call this method everytime before a big chunk of praw things (e.g. at the beginning of a main loop), you don't need to keep track of the time yourself.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "erktheerk", "created_utc": 1438275208, "gilded": 0, "name": "t1_ctlpl87", "num_comments": null, "score": 2, "selftext": ">UserWarning: The keyword `bot` in your user_agent may be problematic. C:\\Python34\\lib\\site-packages\\praw\\decorators.py:88: DeprecationWarning: reddit intends to disable password-based authentication of API clients sometime in the near future. As a result this method will be removed in a future major version of PRAW. > For more information please see: > * Original reddit deprecation notice: https://www.reddit.com/comments/2ujhkr/ > * Updated delayed deprecation notice: https://www.reddit.com/comments/37e2mv/ So as soon as they update it most likely.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "avinassh", "created_utc": 1438521505, "gilded": 0, "name": "t1_ctotsqo", "num_comments": null, "score": 2, "selftext": "well, if you want a library which can do all the OAuth for you, then you can try using my library I wrote: https://github.com/avinassh/prawoauth2 it is almost similar as one written by /u/SmBe19", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "avinassh", "created_utc": 1438532873, "gilded": 0, "name": "t1_ctoyob1", "num_comments": null, "score": 1, "selftext": "yes. you don't need oauth token everytime. like I said earlier, when you pass `permanent` parameter, you get access permanently. you don't need to grab the code from URL or anything. it's one time process. check if this [readme](https://github.com/avinassh/prawoauth2#how-what-and-why) helps", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "powderblock", "created_utc": 1438277973, "gilded": 0, "name": "t1_ctlrmby", "num_comments": null, "score": 0, "selftext": "Yep. >DeprecationWarning: Password-based authentication will stop working on 2015/08/03 and as a result will be removed in PRAW4.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "zzpza", "created_utc": 1438344607, "gilded": 0, "name": "t1_ctmopgz", "num_comments": null, "score": 1, "selftext": "I'm using PRAW if that's what you're asking (sorry - n00b). I've read the section in the PRAW documentation about OAuth and it seemed very complex for my needs, especially as my scripts would need multiple scopes. It wasn't clear how I would handle that either - would I need to have a different token for each scope and swap between them before I did anything that requires a different scope?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "SmBe19", "created_utc": 1438350890, "gilded": 0, "name": "t1_ctmrode", "num_comments": null, "score": 2, "selftext": "I made a python module to help with OAuth for praw. Use `pip install praw-oauth2util` to install it. https://github.com/SmBe19/praw-OAuth2Util", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "gooeyblob", "created_utc": 1438345008, "gilded": 0, "name": "t1_ctmov0o", "num_comments": null, "score": 2, "selftext": "It's not really as complex as it seems! You create an app [here](https://www.reddit.com/prefs/apps/), then use that client ID, client secret, and redirect URI in your script (you can just use localhost for a redirect URI if it's just a local script). Set the code in PRAW, let it do the token exchange for you, and then you can use that access token and refresh token to make requests going forward. No - you can request all the scopes you need the first time and use those forever. If you request 4 different scopes, they all apply to that token, you don't get 4 different tokens.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "Spacedementia87", "created_utc": 1438934480, "gilded": 0, "name": "t1_ctulayk", "num_comments": null, "score": 1, "selftext": "How do you get the bot to do the token exchange? I was following the tutorial on the PRAW documentation but I don't have a webbrowser vis SSH so i got stuck", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "gooeyblob", "created_utc": 1438970673, "gilded": 0, "name": "t1_ctv1j5o", "num_comments": null, "score": 1, "selftext": "You don't need to open the authorize page in the same session that you're running PRAW, you just need to get the `code` query parameter after you open the URL and hit approve.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "GoldenSights", "created_utc": 1438122992, "gilded": 0, "name": "t1_ctjkbi0", "num_comments": null, "score": 1, "selftext": "This feature hasn't been released, so it is not included in the package that comes from pypi (and therefore pip). To download it, [go here](https://github.com/praw-dev/praw) and click \"Download Zip\", then extract the contents to overwrite your praw installation at C:\\python34\\lib\\site-packages\\praw (on linux I think it's /usr/local/lib/python3.4/lib or something like that).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eyfz9/praw_sticky_bottomtrue_function_not_being/"}, {"author": "TheEnigmaBlade", "created_utc": 1438140988, "gilded": 0, "name": "t1_ctjv2b8", "num_comments": null, "score": 2, "selftext": "Alternatively, you could use the command provided in the PRAW readme. Same thing, but pip does it for you: pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eyfz9/praw_sticky_bottomtrue_function_not_being/"}, {"author": "rtheunissen", "created_utc": 1438088570, "gilded": 0, "name": "t1_ctix2ir", "num_comments": null, "score": 1, "selftext": "Take a look if [Rockets](https://github.com/rtheunissen/rockets) could help you out? I realise it's not PRAW-related but it might suit your task. You can subscribe to `comments`, and use filters for `post` and `root`.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "rtheunissen", "created_utc": 1438094613, "gilded": 0, "name": "t1_ctj0g3q", "num_comments": null, "score": 1, "selftext": "Ah right, makes sense. Not sure how to do that with PRAW I'm afraid.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "GoldenSights", "created_utc": 1437953638, "gilded": 0, "name": "t1_cth656a", "num_comments": null, "score": 2, "selftext": "[PRAW will not use the 1 second ratelimit until password auth is removed.](https://github.com/praw-dev/praw/issues/319#issuecomment-120555369) In the meantime, you can do `r.config.api_request_delay = 1` to force it. Based on what I know, I'm willing to bet that multiprocess currently does not distinguish between multiple users' oauth sessions, but that it will in PRAW4. I've never so much as used multiprocess so I could be wrong.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3epjgl/praw_and_oauth_rate_limit/"}, {"author": "MycTyson", "created_utc": 1437777466, "gilded": 0, "name": "t1_ctf4i7p", "num_comments": null, "score": 2, "selftext": "You know, I have automoderator doing so much it hadn't occured to me to lean on that sucker to do this lifting for me. I just really wanted an excuse to learn both Python and PRAW haha. Thanks for pointing me in the direction of automod, though! I'll likely end up going that route, however if anyone has any suggestions relevant to Python and PRAW I am still listening :) Thanks /u/GoldenSights", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ehx0t/praw_automaticallly_assign_flair_to_posts_in_sub/"}, {"author": "bboe", "created_utc": 1436930630, "gilded": 0, "name": "t1_ct3rlwf", "num_comments": null, "score": 1, "selftext": "Sounds like it may be a bug. Care to file on github: https://github.com/praw-dev/praw/issues There is no ability to provide different cache lengths for different end points. However, you could change the default cache value to five minutes for all objects, and then add a unique parameter to bypass the cache for the listing requests. That might accomplish your task.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3db07x/praw_crashing_on_nonstandard_characters/"}, {"author": "GoldenSights", "created_utc": 1436862609, "gilded": 0, "name": "t1_ct2q1u4", "num_comments": null, "score": 3, "selftext": "/u/teaearlgraycold is working on a [pull request](https://github.com/praw-dev/praw/pull/467). There was something funny going on with the test suite but I'm sure it'll be resolved pretty quickly.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "teaearlgraycold", "created_utc": 1436886967, "gilded": 0, "name": "t1_ct2z95m", "num_comments": null, "score": 2, "selftext": "Not sure if this will upstage /u/bboe and /u/GoldenSights, but you can download from my repository and run setup.py to install a version that supports two stickies. https://github.com/teaearlgraycold/praw Syntax is sticky(1) or sticky(2). If you don't select either it chooses the second position.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "bboe", "created_utc": 1436927616, "gilded": 0, "name": "t1_ct3px5p", "num_comments": null, "score": 2, "selftext": "While I absolutely appreciate /u/teaearlgraycold for the pull request, I don't recommend updating the package via this approach. The changes are merged in master so the suggested way to run get the development version is: pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "GoldenSights", "created_utc": 1436829655, "gilded": 0, "name": "t1_ct2aj06", "num_comments": null, "score": 1, "selftext": "\"Forbidden\" should be pretty self explanatory -- you're not allowed to do whatever you're trying to do. PRAW gives you that exception any time you get a 403 error, which happens when you try to post in somewhere you're banned, or trying to access the config of somebody else's subreddit, etc.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d6p0y/my_bot_code_is_returning_a_rather_lengthy/"}, {"author": "teaearlgraycold", "created_utc": 1436581672, "gilded": 0, "name": "t1_cszc1vt", "num_comments": null, "score": 2, "selftext": "Here's the traceback: Traceback (most recent call last): File \"/home/pi/bots/TeaBot-OAuth/teaBot.py\", line 193, in message_commands resp = self.do_shadowban(subreddit, message, arguments) File \"/home/pi/bots/TeaBot-OAuth/teaBot.py\", line 282, in do_shadowban subreddit.un.add_note(n) File \"/home/pi/bots/TeaBot-OAuth/modules/puni.py\", line 206, in add_note self.set_json(notes, '\"create new note on user ' + note.username + '\" via puni') File \"/home/pi/bots/TeaBot-OAuth/modules/puni.py\", line 152, in set_json self.r.edit_wiki_page(self.subreddit, self.page_name, json.dumps(notes), reason) File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 1237, in edit_wiki_page return self.request_json(self.config['wiki_edit'], data=data) File \"/usr/local/lib/python3.2/dist-packages/praw/decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 423, in _request response = handle_redirect() File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 397, in handle_redirect url = _raise_redirect_exceptions(response) File \"/usr/local/lib/python3.2/dist-packages/praw/internal.py\", line 180, in _raise_redirect_exceptions raise RedirectException(response.url, new_url) The code: https://github.com/teaearlgraycold/TeaBot/blob/master/modules/puni.py#L152", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "GoldenSights", "created_utc": 1436583116, "gilded": 0, "name": "t1_cszcql8", "num_comments": null, "score": 2, "selftext": "Okay, looks like PRAW isn't sending the oauth headers for that request. I'm not quite sure why yet, but I'll let you know when I've got a fix.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "GoldenSights", "created_utc": 1436586852, "gilded": 0, "name": "t1_cszeg3c", "num_comments": null, "score": 2, "selftext": "Welp, I just went on a ridiculous goose chase only to find it was missing a single line :| Go to your python Lib\\site-packages\\praw\\\\\\_\\_init\\_\\_.py https://github.com/praw-dev/praw/blob/3d0be7a1697c90f715d4dd4bb73c63318810b14a/praw/__init__.py#L1225 Change that so it says: @decorators.restrict_access(scope='wikiedit') def edit_wiki_page(self, subreddit, page, content, reason=''): And that should do it for you. There's probably still a few functions missing their decorators.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "avinassh", "created_utc": 1436625958, "gilded": 0, "name": "t1_cszpvfp", "num_comments": null, "score": 2, "selftext": "Just a shameless plug, do check out [prawoauth2](https://github.com/avinassh/prawoauth2) for OAuth things. it will make lot of things easier.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "avinassh", "created_utc": 1436639380, "gilded": 0, "name": "t1_cszw85r", "num_comments": null, "score": 1, "selftext": "cool check the example here: https://github.com/avinassh/prawoauth2/tree/master/examples/halflife3-bot", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "jpfau", "created_utc": 1436546736, "gilded": 0, "name": "t1_csyqhuq", "num_comments": null, "score": 3, "selftext": "I'm pretty sure it doesn't read them all, but it does read them pretty fast. It'll fetch a batch of comments per API call, and then once you have the comments you want, reading them is as fast as your computer can do it. If you want to constantly read the newest comments in a subreddit, there is the [comment stream](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.helpers.comment_stream) helper function, and there is the [submission stream](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.helpers.submission_stream) for fetching posts. I would imagine many bots just loop through these in whatever subreddits they're interested in. Edit: I just noticed you aren't interested in using Python. Sorry! I don't know about any C# API, but the answer to your 'reading all comments' question still applies, I think.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "IAMA_YOU_AMA", "created_utc": 1436548576, "gilded": 0, "name": "t1_csyrqca", "num_comments": null, "score": 3, "selftext": "reddit bots should be able to read every single comment posted, unless there is an unusually high amount of comments per second. I don't have the API handy, but I think you are allowed 1 request every 2 seconds and each request can generate 50 comments (100 with gold, last time I checked). So there would have to be more than 1500 comments per minute for you to miss any. I really don't think that happens. Also keep in mind that PRAW has built in rate limiting, so you don't need to worry about upsetting reddit. If you write your bot outside of PRAW, then you need to make sure you don't exceed 30 requests/minute or you may have the account blocked.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "tappify", "created_utc": 1436792114, "gilded": 0, "name": "t1_ct1ncu1", "num_comments": null, "score": 1, "selftext": "PRAW stands for Python Reddit API Wrapper! (Ignore this, just testing my bot)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "diagonalfish", "created_utc": 1436547755, "gilded": 0, "name": "t1_csyr6gy", "num_comments": null, "score": 1, "selftext": "This is very cool, and it was working great last night, but I'm suddenly having problems - praw is throwing HTTP 400 errors when connecting. I have the latest version of your module and of PRAW. [Here is the traceback.](http://pastie.org/private/o7usngbgmhjaolhqphiakg) Code: app_key = \"J...\" app_secret = \"X...\" access_token = \"3...\" r = praw.Reddit(user_agent=\"Thingy by /u/diagonalfish\") try: oauth_helper = PrawOAuth2Mini(r, app_key=app_key, app_secret=app_secret, access_token=access_token, scopes=[\"read\", \"mysubreddits\", \"submit\", \"edit\", \"modlog\", \"modposts\", \"identity\"]) except praw.errors.HTTPException as err: print err._raw exit()", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "created_utc": 1436549536, "gilded": 0, "name": "t1_csysdlt", "num_comments": null, "score": 1, "selftext": "Hey, that traceback is not that helpful. However I suggest you to update the module and try again. if you stilll get error, then let me know. Also, tell me exact python and praw version. with exact name of exception. pip install --upgrade prawoauth2 Most probably your tokens are not correct", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "created_utc": 1436550650, "gilded": 0, "name": "t1_csyt4ec", "num_comments": null, "score": 1, "selftext": "All righty. Yeah, I had a feeling it wouldn't be, but I figured it was worth a try. :) Python 2.7.6 on Linux, PRAW 3.1.0. I turned on request logging in PRAW and I see the following (app key and secret subsituted - I checked them and they seem to be correct): substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json /usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py:198: DeprecationWarning: unicode for buf is no longer accepted, use bytes return self.connection.send(data) status: 401 POST: https://api.reddit.com/api/v1/access_token/ data: {u'grant_type': u'refresh_token', u'refresh_token': '', u'redirect_uri': 'http://127.0.0.1:9999/authorize_callback'} auth: ('', '') status: 400 I emphasize that this was working last night, with no changes, same tokens, everything. I successfully posted a thing with this script using the auth token I got from using a slightly hacked version example onetime.py script. Not sure what's up with that pyopenssl deprecation warning, but I don't think it is relevant since I was getting that before I switched this script over to using OAuth last night. ETA: I'm thinking the problem may be the blank refresh token?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "created_utc": 1436551849, "gilded": 0, "name": "t1_csytwjn", "num_comments": null, "score": 1, "selftext": "Hmm. Your [example bot](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/bot.py) doesn't use the refresh token, though. Doesn't seem to be passed into PrawOAuth2Mini anywhere. Did I miss something here? Is the refresh token passed to that API supposed to be the one you get from the one-time setup call?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "created_utc": 1436552112, "gilded": 0, "name": "t1_csyu30m", "num_comments": null, "score": 1, "selftext": "You can pass the refresh token, but it isn't mandatory. only access_token is enough. I do refresh thing here: https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/bot.py#L26", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "created_utc": 1436552459, "gilded": 0, "name": "t1_csyubob", "num_comments": null, "score": 1, "selftext": "Right, but you don't pass the refresh token into PrawOAuth2Mini anywhere, leaving it at the default empty string, which is why it's an empty string on the initial /access_token call in my log output there. I'm calling refresh later on but this error is happening during the initial setup (the constructor for the Praw2OAuthMini object). It seems like maybe passing the refresh token to it *is* necessary? (I don't have access to the original refresh token right now so I can't test this theory until later today.)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "created_utc": 1436553205, "gilded": 0, "name": "t1_csyuuju", "num_comments": null, "score": 1, "selftext": "Thank :) Please send a PR and add your bot to [the list](https://github.com/avinassh/prawoauth2#bots-using-prawoauth2)!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "MatchThreader", "created_utc": 1437400451, "gilded": 0, "name": "t1_ct9o8yi", "num_comments": null, "score": 1, "selftext": "I'm trying to login via OAuth2. I am fairly certain I have followed the instructions correctly and am running Python 2.7.10 and just upgraded PrawOAuth2. I have successfully used PrawOAuth2 to get the access token and refresh token and setup the server. user_agent = 'Testing OAuth access for me' r = reddit_client = praw.Reddit(user_agent) app_key = \"blah\" app_secret = \"blah blah\" access_token = '41036844-randomnumbers' refresh_token = '41036844-randomletters' scopes = 'flair history modconfig modflair modlog privatemessages read save submit wikiread' oauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key, app_secret=app_secret, access_token=access_token, refresh_token = \"\", scopes=scopes) sub = \"ussoccer\" oauth_helper.refresh() settings = r.get_settings(sub) oauth_helper.refresh() sidebar_contents = settings['description'] print sidebar_contents I get the error: Traceback (most recent call last): line 39, in settings = r.get_settings(sub) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py\", line 345, in wrapped raise errors.LoginOrScopeRequired(function.__name__, scope) LoginOrScopeRequired: 'get_settings' requires a logged in session or the OAuth2 scope 'modconfig' requires a logged in session The account I am using has mod powers and I'm pretty sure I have passed it the correct scopes --- Edit: I just tried changing the scopes to ['flair', 'history', 'modconfig', 'modflair', 'modlog', 'privatemessages', 'read', 'save', 'submit', 'wikiread'] and got this series of errors: Traceback (most recent call last): , line 40, in settings = r.get_settings(sub) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py\", line 348, in wrapped return function(cls, *args, **kwargs) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/__init__.py\", line 1487, in get_settings params=params)['data'] File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/__init__.py\", line 424, in _request _raise_response_exceptions(response) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/internal.py\", line 189, in _raise_response_exceptions raise OAuthInsufficientScope('insufficient_scope', response.url) OAuthInsufficientScope: insufficient_scope on url https://oauth.reddit.com/r/ussoccer/about/edit/.json", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "MatchThreader", "created_utc": 1437539775, "gilded": 0, "name": "t1_ctbs0rg", "num_comments": null, "score": 1, "selftext": "I think my issue is with scope and I'm about to pull out my hair. I understand what scope is but I don't understand how I am supposed to pass it to the 2Mini. If I create a `scopes` variable to pass as scopes in the PrawOAuth2Mini method, what form is `scopes` supposed to take? Do I need `scopes` = ['identity','privatemessages'] or 'identity privatemessages' or ['identity privatemessages']??", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "created_utc": 1437540253, "gilded": 0, "name": "t1_ctbs8hf", "num_comments": null, "score": 2, "selftext": "> `scopes = ['identity','privatemessages']` this is correct. I am sorry if this is not clear in docs. I will improve it. EDIT: The `readme` does not mention it clearly. I will make changes soon. however here's how it is done in [example](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/settings.py#L1)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "GoldenSights", "created_utc": 1436315173, "gilded": 0, "name": "t1_csvn08h", "num_comments": null, "score": 1, "selftext": "Are you using OAuth? If so, do you have the identity scope? Without it, `r.user` does not exist, which would explain this bug. I'll try making a PR to solve that, but in the meantime I think you'll need identity. Edit: I'm not sure if this scope thing is actually the reason you're having trouble, but if so, give this a try: Go to /usr/local/lib/python2.7/site-packages/praw/objects.py, Find [this line](https://github.com/praw-dev/praw/blob/master/praw/objects.py#L382) change to return self.reddit_session._mark_as_read([self.fullname]) so it bypasses the user stuff.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3chntm/praw_messagemark_as_read_and_mark_as_unread_broken/"}, {"author": "GoldenSights", "created_utc": 1436318031, "gilded": 0, "name": "t1_csvolp2", "num_comments": null, "score": 1, "selftext": "I should note that you need to give objects.Inboxable.mark_as_unread a similar treatment. I'll see to it that this is included in the next praw release.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3chntm/praw_messagemark_as_read_and_mark_as_unread_broken/"}, {"author": "GoldenSights", "created_utc": 1436316680, "gilded": 0, "name": "t1_csvnuye", "num_comments": null, "score": 1, "selftext": "When you were registering the OAuth app, and you got to the page that says >OauthApp requests to connect with your reddit account. > Allow OAuthApp to: > - Access my inbox and send private messages to other users. If it had a bullet point that said \"Access my reddit username and signup date.\", then the app should have the Identity scope. You can check what scopes your praw session has with `print(r._authentication)`. [Here's the praw docs](https://github.com/praw-dev/praw/blob/master/docs/pages/oauth.rst#oauth-scopes) for Oauth scopes, though the table is currently incomplete.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3chntm/praw_messagemark_as_read_and_mark_as_unread_broken/"}, {"author": "avinassh", "created_utc": 1436019575, "gilded": 0, "name": "t1_csrycbw", "num_comments": null, "score": 2, "selftext": "Existing solutions, have a look at them: - /u/SmBe19's [praw-OAuth2Util](https://github.com/SmBe19/praw-OAuth2Util) and [my fork](https://github.com/avinassh/praw-OAuth2Util). The way configs are handled is bit different in my fork - /u/KissTheBlade_ 's [script](https://github.com/x89/Shreddit/blob/master/get_secret.py)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "gschizas", "created_utc": 1436108008, "gilded": 0, "name": "t1_cssulgv", "num_comments": null, "score": 2, "selftext": "First of all, thank you for the recommendations :) /u/KissTheBlade_'s script is using tornado as a webserver. I tried to make do with as less external dependencies (especially ones that won't work in Python 3 - although it seems tornado is now supporting it) as possible. praw-OAuth2Util is very close to what I've done, but it seems it's about twice the size. Of course my own script isn't very configurable, but I was aiming for simplicity rather than configurability. Furthermore, OAuth2Util needs a manual refresh, while my own script handles the refresh automatically.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "gschizas", "created_utc": 1436290927, "gilded": 0, "name": "t1_csv70bn", "num_comments": null, "score": 1, "selftext": "Hold on, I don't think we have our terminology right. What I mean by a \"constantly running script\" is something like a service (Windows) or a daemon (Unix), where there is one long-running process. You can have a script that runs and exits and put it on Task Scheduler or cron, to be run periodically and unattended. The usage is that you run the script manually the first time (to open the browser, accept the permissions etc.), and *then* you put it on the Task Scheduler/cron so that it can run automatically e.g. every 5 minutes. This will run and exit in a very small period of time (so there is no need to do a new object per operation, or at least I don't think so), but it **will** have to do a refresh, preferably without waiting for the user to enter their credentials or press \"Accept\" on the browser. And I don't see any handling for *that* scenario in praw-OAuth2Util. You do it manually, by calling the `refresh` method of `OAuth2Util` class. I have the whole `refresh` logic inside the reddit_agent function call (I've since converted this to an object, that's why I mentioned the `RedditAgent` class).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "avinassh", "created_utc": 1436108560, "gilded": 0, "name": "t1_cssutlw", "num_comments": null, "score": 1, "selftext": "I agree about dependencies, however Tornado is very small and I don't think it really matters. Without Tornado or Flask, the code will be bit messy as you have to handle requests and status codes etc. I am aware of issues with Praw OAuth2Util thats why I stopped my fork and started working on something better: https://github.com/avinassh/prawoauth2 it's not complete yet, but it works. IMO, it is simple. And is also configurable.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "SandyRegolith", "created_utc": 1435983191, "gilded": 0, "name": "t1_csrohq7", "num_comments": null, "score": 2, "selftext": "I don't know python/praw but you're lowercasing the comment body while searching for `'I could care less'` which has an uppercase \"i\"...", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c2jeu/simple_noob_question_bot_not_finding_comments/"}, {"author": "armandg", "created_utc": 1435847346, "gilded": 0, "name": "t1_cspvg1x", "num_comments": null, "score": 3, "selftext": "Huh. Hopefully someone sees this and fixes the issue. Will also submit an issue on github. Edit: I am an idiot. I was not using a correct Boolean statement for triggering the function. By \"None\" I was telling PRAW that there were no trigger to send, so use default. Changing from \"None\" to \"False\" worked like a charm.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3buzu2/praw_send_replies_a_gold_only_feature/"}, {"author": null, "created_utc": 1435621149, "gilded": 0, "name": "t1_csmymz7", "num_comments": null, "score": 2, "selftext": "https://github.com/x89/Shreddit/blob/master/get_secret.py If you like you can just steal that. I threw it together to use myself. It's actually quite easy using PRAW. You fill in some details from the Reddit API in your praw.ini, open the given URL and voila!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": null, "created_utc": 1435940811, "gilded": 0, "name": "t1_csr40es", "num_comments": null, "score": 1, "selftext": "The one PRAW gets for you is permanent so you don't have to re-auth every 60 minutes. That part is just added to confuse! You only have to visit the Reddit API one time.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "avinassh", "created_utc": 1435941244, "gilded": 0, "name": "t1_csr48n3", "num_comments": null, "score": 1, "selftext": "but [docs](https://praw.readthedocs.org/en/v3.0.0/pages/oauth.html#step-6-refreshing-the-access-token) say otherwise: > An access token lasts for 60 minutes. To get access after that period, we\u2019ll need to refresh the access token.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "wanderingbilby", "created_utc": 1435615392, "gilded": 0, "name": "t1_csmvc79", "num_comments": null, "score": 1, "selftext": "OAuth is a little funky to wrap your head around, but once you get it things will be more smooth. [This tutorial](http://tsenior.com/2014-01-23-authenticating-with-reddit-oauth/) is pretty good and covers the concepts that PRAW is talking you through. Your initial Authorization string is a static URL, so if you don't have hosting anywhere you can link directly to it. What DOES need hosting is the callback URL which must accept and process a GET call. Once the user is redirected, you read the params from the GET call and use them to make a POST call back to reddit to get the token and appproved scopes. If your bot needs permanent (>1 hour) access to authentication, it will need to process that second part each time it needs to access data on the user's behalf to get a new token. You'll need a way to store and update both the current bearer token and the refresh token. Does your bot need users to authenticate to it, or do you just need to authenticate the bot to its reddit account? If you just need to authenticate the bot, you can throw a temporary page up somewhere, but if you need users to authenticate you'll need a page hosted to handle it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "joe-murray", "created_utc": 1435603132, "gilded": 0, "name": "t1_csmngb4", "num_comments": null, "score": 1, "selftext": "There's a good post here: https://www.reddit.com/r/redditdev/comments/2ujhkr/important_api_licensing_terms_clarified/coch3em If you're very inexperienced it might be hard to follow, but it's not too bad. The PRAW docs are kind of unbearable though, so don't feel bad for being confused by that one.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "bboe", "created_utc": 1435638734, "gilded": 0, "name": "t1_csn80oi", "num_comments": null, "score": 1, "selftext": "> The PRAW docs are kind of unbearable though, so don't feel bad for being confused by that one. We'd love to see any improvements you have to offer.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "bboe", "created_utc": 1435677109, "gilded": 0, "name": "t1_csnl8h5", "num_comments": null, "score": 2, "selftext": "Do you have any interest in opening a pull request for the changes? PRAW now more than ever relies on the community to help make the improvements necessary to access new and updates features, and to make improvements that will benefit newcomers. Here's a direct edit link to the page in question :) https://github.com/praw-dev/praw/edit/master/docs/pages/oauth.rst", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "Liudvikam", "created_utc": 1435679900, "gilded": 0, "name": "t1_csnn0nc", "num_comments": null, "score": 2, "selftext": "Sure, here it is: https://github.com/praw-dev/praw/pull/437", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "GoldenSights", "created_utc": 1435550063, "gilded": 0, "name": "t1_csm2ca7", "num_comments": null, "score": 1, "selftext": "I believe this feature is currently not implemented in praw. In the meantime, I may have a workaround for you. `multireddit.subreddits` should give you a list containing all the subreddits in the multi. Now, this feature appears to be slightly broken as well, but I'll try to fix that pretty soon here. It's supposed to return a bunch of Subreddit objects, but instead I'm getting a dict with {\"name\": \"redditdev\"}. So what you can do is: subs = multireddit.subreddits subs = [x['name'] for x in subs] subs = '+'.join(subs) r.search(query, subreddit=subs) Give that a try, and I'll see if I can help to fix whatever is currently broken.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "GoldenSights", "created_utc": 1435801377, "gilded": 0, "name": "t1_cspei2k", "num_comments": null, "score": 1, "selftext": "Well, in that case, it's no longer really a PRAW question, I can't even get this search to run in the browser: https://www.reddit.com/r/NamibiaPics+SouthKoreaPics+NorthKoreaPics+NationalPhotoSubs+PortugalPics/search?q=timestamp:0..1435829421&syntax=cloudsearch&restrict_sr=on&sort=new&period=year Using proper multireddits doesn't seem to solve the problem either: https://www.reddit.com/u/goldensights/m/test/search?q=timestamp:0..1435829421&syntax=cloudsearch&restrict_sr=on&sort=new&period=year I guess it's just too much for the server to do at once, you might have to cut this up into a couple simpler requests. To answer your question, the query I used was something simple like \"test\", I thought you meant it wasn't working at all.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "xfile345", "created_utc": 1435423046, "gilded": 0, "name": "t1_cskld95", "num_comments": null, "score": 1, "selftext": "I'm not familiar with praw or that specific API wrapper, but instead of requesting a temporary access token, request a permanent one by changing the `duration=permanent` during your authorization request. You will not only receive an `access_token`, but a `refresh_token` that you can save and use to obtain a new `access_token` each time without the need to re-authorize the app. When you have a `refresh_token`, provide it using the `grant_type=refresh_token` to `https://www.reddit.com/api/v1/access_token` and you will receive your `access_token` with that single request ([instructions here](https://github.com/reddit/reddit/wiki/OAuth2#refreshing-the-token)).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "irrational_function", "created_utc": 1435458657, "gilded": 0, "name": "t1_csl2cls", "num_comments": null, "score": 1, "selftext": "You report to the author that their wrapper is defective. The problem is that it supports refresh_tokens, but not correctly on startup. It saves your access_token and your refresh_token, but if your access_token is expired when you next create a wrapper object, it obtains completely new tokens instead of using the refresh_token. If you have unexpired tokens on startup or you obtain new tokens on startup, *then* it lets you refresh the tokens using the wrapper's `refresh` method. Here's the [documentation](https://praw.readthedocs.org/en/v3.0.0/pages/oauth.html) for using OAuth in PRAW. You can follow that directly, or you can use a wrapper without this problem. In particular, when using PRAW, you almost never ought to be bypassing it to access the reddit API directly. You use a refresh token in PRAW via the `refresh_access_information` method.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "irrational_function", "created_utc": 1435504117, "gilded": 0, "name": "t1_cslfu88", "num_comments": null, "score": 1, "selftext": "You're welcome! I can be a bit more explicit, actually. In the `_read_access_credentials` method, if you get an `OAuthInvalidToken` exception and have a refreshable token, you need to call `refresh_access_information`. PRAW doesn't use the refresh token automatically. Even passing it to `set_access_credentials`, it just remembers the refresh token so you can call `refresh_access_information` with no argument later.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "SmBe19", "created_utc": 1435511080, "gilded": 0, "name": "t1_csliyy1", "num_comments": null, "score": 1, "selftext": "import praw import OAuth2Util r = praw.Reddit(\"OAuth2Util Demo by /u/SmBe19\") o = OAuth2Util.OAuth2Util(r, print_log=True) o.refresh() print(\"Hi, {0}, you have {1} comment karma!\".format( r.get_me().name, r.get_me().comment_karma))", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "bboe", "created_utc": 1435205564, "gilded": 0, "name": "t1_cshwkwr", "num_comments": null, "score": 1, "selftext": "Unfortunately, there are so many things that could be going wrong here. (1) Can you provide the simplest bit of python code that reproduces the issue (e.g, the whole file with only necessary lines). (2) Can provide the version of PRAW and requests you have installed? import praw, requests print(praw.__version__) print(requests.__version__) (3) What is the output of the following? import requests requests.get('https://reddit.com/.json?limit=1').json()", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "nandhp", "created_utc": 1435373383, "gilded": 0, "name": "t1_csk5ct0", "num_comments": null, "score": 1, "selftext": "I am having this problem, too. So I went looking. First, my versions: Python 2.7.9 (default, Mar 1 2015, 12:57:24) [GCC 4.9.2] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import praw, requests >>> print(praw.__version__) 3.0.0 >>> print(requests.__version__) 2.4.3 And a minimal test case: import praw r = praw.Reddit(user_agent=\"Command-line test\") l = r.get_top(limit=5) print [str(x) for x in l] The exception occurs because `requests` (and/or the underlying `httplib`) is trying to write the HTTP request as a Unicode string. (Although it only complains when using OpenSSL, it's obviously a bug somewhere in there.) The request is in Unicode because of Unicode-ness propagated by `\"\\r\\n\".join(self._buffer)` -- well, it comes from the `method` provided by PRAW: # praw/internal.py from __future__ import print_function, unicode_literals def _prepare_request(..., method=None): # ... if method: pass elif data or files: method = 'POST' else: method = 'GET' # ... request = Request(method=method, ...) The method is Unicode because of the use of `unicode_literals`, and for some reason it's not being converted to a byte string by `requests`. The problem goes away if I use `method = b'GET'` or `Request(method=str(method), ...)`, though I've only tried it on Python 2.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "GoldenSights", "created_utc": 1435151006, "gilded": 0, "name": "t1_csgzttx", "num_comments": null, "score": 2, "selftext": "Check out [OAuth2Util, by /u/smbe19](http://www.reddit.com/r/botwatch/comments/38k30h/oauth2util_a_wrapper_around_praws_oauth2/). All it takes is a couple of text files and you're on your way", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ay1c9/praw_oauth_login/"}, {"author": "xoru", "created_utc": 1436268722, "gilded": 0, "name": "t1_csuuxhz", "num_comments": null, "score": 2, "selftext": "I made a small one-time script for easily setting up OAuth with praw. You should be able to easily modify the example at the bottom of the usage instructions to fit your code. Check it out [here](https://github.com/xoru/easy-oauth) and let me know if you need any more help.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ay1c9/praw_oauth_login/"}, {"author": null, "created_utc": 1435137000, "gilded": 0, "name": "t1_csgvwjy", "num_comments": null, "score": 1, "selftext": "It's `oauth_refresh_token` you need in your script (I put it in praw.ini) as well as your client_id and secret, then you just call `r.refresh_access_information()` and you're logged in. I did this yesterday https://github.com/x89/Shreddit/blob/master/get_secret.py and it works fine.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3amxbp/praw_and_refresh_tokens/"}, {"author": "GoldenSights", "created_utc": 1434733781, "gilded": 0, "name": "t1_csc19fa", "num_comments": null, "score": 3, "selftext": "Thanks a lot for the reply. The only part I'm not clear about yet is why Praw allowed me to submit more characters than chrome did, using nothing but the letter \"o\" five hundred thousand times. In fact, at one point I submitted the page via praw, using 511,891 characters, then I opened the page in chrome and clicked Save without making any changes at all, and it failed. What would explain this?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "xfile345", "created_utc": 1434735551, "gilded": 0, "name": "t1_csc2eva", "num_comments": null, "score": 2, "selftext": "> The only part I'm not clear about yet is why Praw allowed me to submit more characters than chrome did I have no idea what the answer is to this, but I know I've had a similar situation where I was editing my subreddit's CSS and it wouldn't update from the wiki editor because of file size, but if I copy/pasted the entire thing into the stylesheet editor (the config page which includes the image upload area), it would accept it just fine. There was roughly 1 KB of leeway between the two pages if I recall correctly, so for some reason, it *does* matter which method you're using to submit something.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "largenocream", "created_utc": 1434735793, "gilded": 0, "name": "t1_csc2khe", "num_comments": null, "score": 3, "selftext": "> The only part I'm not clear about yet is why Praw allowed me to submit more characters than chrome did, using nothing but the letter \"o\" five hundred thousand times. Ahhh yeah, [there's also a global limit on the size of POST bodies](https://github.com/reddit/reddit/blob/master/r2/r2/config/middleware.py#L323,) (which is lower than the wiki's rendered size limit, `512000`) so PRAW _could_ be sending a smaller body than the browser. If you're using OAuth with praw, you don't have to send the modhash. [www.reddit.com](https://www.reddit.com/)'s js always puts `&r=` on the end of the POST body, that can be avoided if you use URLs like `/r//api/` as PRAW might. It makes sense that you'd hit the POST data size limit instead of the wiki's size limit if you're sending something that doesn't expand in size much when rendered as markdown. If the HTTP response body for the \"too big\" error is HTML with a single script tag in it rather than JSON, you're hitting the global POST data limit rather than the wiki's page limit, and it looks like the wiki's JS doesn't know how to handle hitting that limit.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "GoldenSights", "created_utc": 1434737825, "gilded": 0, "name": "t1_csc3vi7", "num_comments": null, "score": 2, "selftext": "Hey, so I was right! At least, when it comes to a page full of the same letter. This also explains why PRAW gave me 500 errors instead of 403-Too-Big. Are comments and submissions also subject to this post-markdown-render size that you mentioned? I always thought they were capped at 10k and 40k input characters, but wiki pages seem to be different in that way. Anyways, thank you. I'll be able to make this work. Happy cake day!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "GoldenSights", "created_utc": 1434750671, "gilded": 0, "name": "t1_cscbm6n", "num_comments": null, "score": 2, "selftext": "Only in PRAW: >>> s = r.get_subreddit('goldtesting') >>> s.edit_wiki_page('75000', 'o'*511891) {} >>> try: ... s.edit_wiki_page('75000', 'o'*511892) ... except Exception as e: ... a=e ... >>> a._raw.status_code 500 >>> a._raw.reason 'Internal Server Error' >>> Chrome does 413 as expected: http://i.imgur.com/1ZFhJyO.png", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "bboe", "created_utc": 1434235113, "gilded": 0, "name": "t1_cs5ncnf", "num_comments": null, "score": 1, "selftext": "http://praw.readthedocs.org/en/latest/pages/oauth.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "MrJohz", "created_utc": 1434294336, "gilded": 0, "name": "t1_cs671kd", "num_comments": null, "score": 1, "selftext": "Well that's the point, I am the user (or at least the account the script will use is owned by me), so there shouldn't be a need to ask for permission each time. Hence why I wanted to use script-based oauth - given that I have the login details and that the script's account is a registered developer account, that seems most appropriate to me. Tbh, I've been working against PRAW for a few things, it'll probably be easier at this point to put together my own module, but thanks nonetheless.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "bboe", "created_utc": 1434599052, "gilded": 0, "name": "t1_csaci6a", "num_comments": null, "score": 1, "selftext": "You might find some help in this thread where the documentation link is, and a little tool I wrote to help obtain refresh tokens: https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "bboe", "created_utc": 1434641479, "gilded": 0, "name": "t1_csarwll", "num_comments": null, "score": 1, "selftext": "Once you have a refresh token you can (re)authenticate via: self.r.refresh_access_information(REFRESH_TOKEN) Your PRAW instance will likely need to have the right OAuth parameters configured. You are probably already doing that, but for completeness: r.set_oauth_app_info(CLIENT_ID, CLIENT_SECRET, REDIRECT_URI)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "totallyknowyou", "created_utc": 1433789155, "gilded": 0, "name": "t1_crzphnv", "num_comments": null, "score": 2, "selftext": "Is it possible? Yeah. You'd need PRAW and also an API to parse webpages (I use Requests to get the page and BeautifulSoup4 to parse them). However, having it so that it constantly posts a comment when a story is posted on your site could possibly be considered spam, but I think that would be more of a subreddit issue than anything else. I'd ask the community of that sub and private message the mods if it would be okay to do that. I'll help if you have any more questions. I'm currently working on my own bot(a bit complicated heh heh...) And already see a couple methods we could get yours rolling with.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "wanderingbilby", "created_utc": 1433798866, "gilded": 0, "name": "t1_crzvmeh", "num_comments": null, "score": 2, "selftext": "Yes, you can use PRAW or any other tool to access the API. You need to do an OAUTH request with the correct scopes, then you can post as the user. TOS-wise, what's the spirit of the tool? If you're helping people submit to /r/writingprompts (formatting helper, something like that) you should be fine - you aren't causing any more or less traffic to the sub than there would be anyway. If your site is a platform itself and offers a \"cross-post to /r/writingprompts\" option, you might want to check with the mods there before putting it in place. Hypothetically it could cause unwanted traffic. An example of a similar application: I made a page for /r/randomactsofvroom that allowed users to fill out a series of questions and submit to the sub. This created a pre-formatted self post or image post, made by the user. An associated bot also used the data to set flair on the post and update an index in a stickied post. It also handled marking posts filled or removed as needed.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "xfile345", "created_utc": 1433432145, "gilded": 0, "name": "t1_crvdym6", "num_comments": null, "score": 2, "selftext": "I don't use PRAW, but the API will return `after` with any list of items. You'll have to make a second request (and 3rd, etc) sending the previous `after` parameter to get a new list starting where the first list left off. Once `after` contains `null`, you've exhausted everything modmail will show you. To my knowledge there is no limit on how many modmail pages you can view as opposed to normal listing pages which typically maxes out at 1,000.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "russellvt", "created_utc": 1433444589, "gilded": 0, "name": "t1_crvm02m", "num_comments": null, "score": 1, "selftext": "I believe you're looking for the parameters to [get_content()](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit.get_content), which are passed as additional arguments via get_mod_mail().", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "bboe", "created_utc": 1433402798, "gilded": 0, "name": "t1_crv3lhb", "num_comments": null, "score": 1, "selftext": "There's a very strong likeliness that if PRAW returns 6668 items, that your web browser would also only show you 6668 items. I am not aware of any item listings in PRAW that return less data than obtainable via the browser.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "GoldenSights", "created_utc": 1433384439, "gilded": 0, "name": "t1_cruwa76", "num_comments": null, "score": 3, "selftext": "It appears that in the [unreleased version of PRAW](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L2391) this has been updated to if text is None and bool(text) == bool(url): So `text=''` will make an empty post.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "GoldenSights", "created_utc": 1433388737, "gilded": 0, "name": "t1_cruyiwq", "num_comments": null, "score": 1, "selftext": "This is making me feel very silly. Apparently boolean logic is my weakness. Here's what I came up with that passes all the tests I feel are appropriate: def check(url=None, text=None): print(repr(url), repr(text), end=' ') if (text != None and url != None) or (url == None and text == None) or (url != None and not isinstance(url, str)): print('Bad') else: print('Good') check(None, None) # Bad check('hi', None) # Good check(None, 'hi') # Good check(None, '') # Good check(None, 0) # Good check(0, None) # Bad check('hi', '') # Bad check('', 'hi') # Bad The problem with your test is it passes `check('hi', '')` which should fail because praw will interpret `text=''` as making a blank selfpost when you might also be intending to submit a url string.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "thorarakis", "created_utc": 1433362762, "gilded": 0, "name": "t1_crujpez", "num_comments": null, "score": 2, "selftext": "Looks like praw isn't handling wiki read oauth access correctly. I have [opened a PR](https://github.com/praw-dev/praw/pull/421) to resolve this.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "habnpam", "created_utc": 1435525253, "gilded": 0, "name": "t1_cslq2i8", "num_comments": null, "score": 1, "selftext": "/u/thorarakis [opened a PR](https://github.com/praw-dev/praw/pull/421) to fix it. It still doesn't work for me, though. Although the 403 client error doesn't show up any more. Now I get a \"praw.errors.Forbidden\" error ( from line 194 internal.py). It seems the PR added a \"wikiread\" refresh token, but we still can't edit because there's no \"wikiedit\" refresh token yet. But that's just a guess. I don't really know how it all works.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "boib", "created_utc": 1435526842, "gilded": 0, "name": "t1_cslqx7v", "num_comments": null, "score": 2, "selftext": "The PR fixed get_wiki_page() but not edit_wiki_page(). I've never done a PR but I did edit my local copy of PRAW to add a decorator to edit_wiki_page(). 1) Find the file `__init__.py` under PRAW 2) Search for `def edit_wiki_page` and on the line just above it, add `@decorators.restrict_access(scope='wikiedit', login=False)` 3) Voila! Maybe /u/thorarakis could submit a PR for this as well :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "thorarakis", "created_utc": 1435632504, "gilded": 0, "name": "t1_csn4y2f", "num_comments": null, "score": 1, "selftext": "My bad, probably should have caught that too. But sounds like you've got the fix already written, which means you've done the hard part and might as well get the credit for the PR yourself :) You just need to fork a copy of [PRAW in github](https://github.com/praw-dev/praw), commit then push the changes in your fork and [create the PR](https://help.github.com/articles/creating-a-pull-request/) from your fork to PRAW. Shouldn't take much more then 10 minutes if you are familiar with git. And if you aren't I highly recommend that you start! If you have any questions I'd be happy to help. But the community really appreciates having more people contributing, and small fixes like these make a big difference.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "GoldenSights", "created_utc": 1432880600, "gilded": 0, "name": "t1_crok6wa", "num_comments": null, "score": 2, "selftext": "The function appears to be called `r.get_domain_listing`, defined [here](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L747). But you don't stack the get_hot() function on top of it, it has a parameter for sort.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ow7j/how_to_get_domain_instead_of_get_subreddit/"}, {"author": "GoldenSights", "created_utc": 1432616125, "gilded": 0, "name": "t1_crl1l76", "num_comments": null, "score": 2, "selftext": "The PRAW Comment object includes the subreddit_id, but also the subreddit name. Try `comment.subreddit.display_name`. If you *had* to get the name for a given ID, you'd want to use `r.get_info(thing_id='t5_2r8lo')` to get the Subreddit, and use its display_name", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "jbw976", "created_utc": 1432618042, "gilded": 0, "name": "t1_crl2bgt", "num_comments": null, "score": 2, "selftext": "nice, using comment.subreddit.display_name worked great. i'm new to python, so i was having a hard time inspecting objects and reading the PRAW docs for useful properties...thanks for the help!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "fragmede", "created_utc": 1432651469, "gilded": 0, "name": "t1_crlcnq1", "num_comments": null, "score": 2, "selftext": "`dir(thing)` is your friend. As in, run python, then paste in import praw r = praw.Reddit(user_agent='/u/fragmede') thing = r.get_info(thing_id='t5_2r8lo') dir(thing) # prints out big long list of attributes on thing. If dir(thing) is too long, I'll type out a quick list comprehension to print a shorter list. Something like: [x for x in dir(thing) if 'name' in x.lower()] That'll print out any attribute with the word 'name' on thing; in this case, display_name.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "phil_s_stein", "created_utc": 1432591431, "gilded": 0, "name": "t1_crkop0g", "num_comments": null, "score": 1, "selftext": "Why use php? Take a look at the first comment on your link. Just use PRAW (python). Much more simple for a first project. Don't use Karmalytics. You can write a simple bot that responds to user mentions, grabs the text pf the post to scan it for keywords. If found respond to the post. [PRAW documentation on writing a simple bot.](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/378zz9/reply_to_parent_comment/"}, {"author": "AnneBancroftsGhost", "created_utc": 1432592014, "gilded": 0, "name": "t1_crkozqu", "num_comments": null, "score": 1, "selftext": "I was playing around with that, too. I just thought the php/karmalytics route would be better because I don't need to run it locally. But if python/praw is the way to go then I'll happily do that. Can you offer any advice though for my original question? Even using python, I still haven't been able to figure out a way to reply to the parent of the comment that calls the bot...", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/378zz9/reply_to_parent_comment/"}, {"author": "bboe", "created_utc": 1432174737, "gilded": 0, "name": "t1_crfxy3k", "num_comments": null, "score": 3, "selftext": "Thanks for asking. There really should be an example on the page you linked to. The following should explain it: https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html?highlight=site#praw.__init__.BaseReddit And here is an example to use the 'bboe' site. r = praw.Reddit('USER AGENT', site_name='bboe')", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36pfcm/praw_how_is_the_correct_site_selected_in_prawini/"}, {"author": "bboe", "created_utc": 1432176700, "gilded": 0, "name": "t1_crfz2nk", "num_comments": null, "score": 1, "selftext": "Yes it has an affect. However, reddit also has a cache that you're likely hitting. There are some tricks you can do to bypass the cache. You should check out [comment_stream](https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html?highlight=comment_stream#praw.helpers.comment_stream) if you're trying to get new comments for a subreddit regularly.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36pfcm/praw_how_is_the_correct_site_selected_in_prawini/"}, {"author": "GoldenSights", "created_utc": 1432093053, "gilded": 0, "name": "t1_creuuww", "num_comments": null, "score": 3, "selftext": "praw.Reddit is a class, representing a session with reddit. It is defined [here](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L2269). It looks pretty barren, but that's because all of its methods come from the different mixins that it inherits from, each of which inherit from AuthenticatedReddit, which inherits from UnauthenticatedReddit, which inherits from BaseReddit, which is where your useragent ends up when you instantiate it. You can consider pretty much the entire \\_\\_init__.py file as the definition of the Reddit object, as far as I understand it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36kwbq/praw_where_is_the_prawreddit_method_defined/"}, {"author": "TheEnigmaBlade", "created_utc": 1431833369, "gilded": 0, "name": "t1_crbmo7z", "num_comments": null, "score": 2, "selftext": "You can add extra parameters using the `params` keyword argument, and more information on the available parameters can be found on the [reddit API page](https://www.reddit.com/dev/api#POST_api_friend). Something like this (although I've never banned anyone using PRAW, so it's untested): sub.ban_user(user, params={\"duration\": 7, \"note\": mod_reason, \"ban_message\": user_reason})", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/367w2j/praw_ban_reason_and_length/"}, {"author": "Eathed", "created_utc": 1431519454, "gilded": 0, "name": "t1_cr7nm5s", "num_comments": null, "score": 1, "selftext": "\"thing\" is a comment object. It doesn't just contain the message, it contains the time posted, author, points, all of that type of stuff. In your case, use thing.body for the text of a comment. [Here](https://praw.readthedocs.org/en/v2.1.21/pages/comment_parsing.html) is a guide on comment parsing from the docs.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35tfe9/praw_noob_issue_while_retrieving_comments_from_a/"}, {"author": "GoldenSights", "created_utc": 1431237185, "gilded": 0, "name": "t1_cr4abb1", "num_comments": null, "score": 2, "selftext": "Hot damn. I'm gonna go ahead and say that this is not a praw issue. When I view the thread normally I get a \"You broke reddit\" screen, but the page actually loads if I use .compact. When I click the button at the bottom, only 1 set of comments comes up, exactly as you describe. So it's not PRAW's fault, reddit is just not serving that content. Luckily, all of those comments should fit in your subreddit's cache. If you do: comments = subreddit.get_comments(limit=None) keep = [] for comment in comments: if comment.parent_id == 't1_cr3etpa': keep.append(comment) keep.sort(key= lambda x: x.created_utc) Now the list `keep` *should* have all of the comments on the thread, in order of creation.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "hizinfiz", "created_utc": 1431053934, "gilded": 0, "name": "t1_cr25q3a", "num_comments": null, "score": 2, "selftext": "Thanks for the response :) I actually did some searching of my own after posting here and [followed this](http://stackoverflow.com/a/27957073) pip3 install praw worked flawlessly for me.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358zpt/having_trouble_getting_praw_to_work_with_python_3/"}, {"author": "GoldenSights", "created_utc": 1431045714, "gilded": 0, "name": "t1_cr21bv1", "num_comments": null, "score": 1, "selftext": "Look for phrases like \"origin error\", \"origin down\" or \"service unavailable\". The codes that I see most are 502, 503, and 520, though there may be another. You can also check out the notes about Exceptions in [praw's documentation](https://praw.readthedocs.org/en/v2.1.21/pages/exceptions.html?highlight=exceptions)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358lu6/what_exception_may_i_expect_when_reddit_is_busy/"}, {"author": "redalastor", "created_utc": 1431046073, "gilded": 0, "name": "t1_cr21isr", "num_comments": null, "score": 1, "selftext": "Damn... I was on an older praw's doc page... Thanks.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358lu6/what_exception_may_i_expect_when_reddit_is_busy/"}, {"author": "kemitche", "created_utc": 1431375729, "gilded": 0, "name": "t1_cr5uxh1", "num_comments": null, "score": 1, "selftext": "Generally, an error code from 501-599 indicates a server-side reddit error that you can safely wait and retry again later. As the praw docs that GoldenSights linked mention, 500 is often transient, but it can be persistent and may indicate a bug on the reddit server side. If you notice a persistent 500 error that lasts more than a few minutes (and reddit is [not currently experiencing elevated error rates](https://www.redditstatus.com)), then you should post here with as much info as possible.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358lu6/what_exception_may_i_expect_when_reddit_is_busy/"}, {"author": "WheelsOnPavement", "created_utc": 1431041635, "gilded": 0, "name": "t1_cr1z50b", "num_comments": null, "score": 1, "selftext": "I wouldn't be surprised if I did base it off of one of your templates- if so, thanks, big help. EDIT: I guess it's also worth mentioning that if I log in as the bot in a browser I can post fine in any sub, albeit with the new account restrictions. Here's my traceback: Traceback (most recent call last): File \"C:\\Users\\Sam Hosmer\\Desktop\\Bot 2\\bot3export.py\", line 60, in run_bot() File \"C:\\Users\\Sam Hosmer\\Desktop\\Bot 2\\bot3export.py\", line 47, in run_bot comment.reply('You have summoned the bagel gods! Bask in their glory!') File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 384, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 338, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 2173, in _add_comment retry_on_error=False) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 561, in request_json retry_on_error=retry_on_error) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 403, in _request _raise_response_exceptions(response) File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 178, in _raise_response_exceptions response.raise_for_status() File \"C:\\Python34\\lib\\site-packages\\requests\\models.py\", line 851, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 403 Client Error: Forbidden", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "WheelsOnPavement", "created_utc": 1431042833, "gilded": 0, "name": "t1_cr1zssv", "num_comments": null, "score": 1, "selftext": "I lengthened the useragent by quite a bit and got the same error. r = praw.Reddit(user_agent=\"BB by /u/WheelsOnPavement- created out of boredom for no real purpose. Made in Python using Reddit's PRAW API.\")", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "GoldenSights", "created_utc": 1430814001, "gilded": 0, "name": "t1_cqywffv", "num_comments": null, "score": 1, "selftext": "This is definitely a Python question over a praw question. Essentially the goal is to take a large string, locate \"@Search:\", and take the next word. I think every person's solution to this would look slightly different, and some would probably go the regex route, but here's the first thing that comes to my mind: def findterm(body): # If the user does \"@search:heyo\" then the following split won't separate the two words # So just replace the colon with the space which will be split. # If this causes problems you'll have to find an alternative body = body.replace(':', ' ') words = body.split(' ') # This assumes that body is already .lower() but you could move that here instead. index = words.index('@search') try: term = words[index+1] # Remove any \\n if this was at the end of a line. term = term.strip() return term except IndexError: return None cterm = findterm(cbody) if cterm: Do your stuff If you want to allow the user to search multiple words by encasing them in quotes, you'll have to make it a little more fancy, but it shouldn't be too bad. This was off the top of my head so I'm sure there are some critiques to be made here, but I hope this gets you on the right track.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34x4ri/parsing_comments/"}, {"author": "demonlordghirahim", "created_utc": 1430799872, "gilded": 0, "name": "t1_cqysdgq", "num_comments": null, "score": 1, "selftext": "you rule I really appreciate everything. One last question - should I uninstall PRAW (using pip) and only have it in a venv?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "GoldenSights", "created_utc": 1430301055, "gilded": 0, "name": "t1_cqsgt85", "num_comments": null, "score": 2, "selftext": "It's because either you didn't log in to the PRAW session, or you did log in but your account does not have the Over 18 checkbox set. PRAW is expecting to be redirected to a submission, but instead it's getting the \"are you over 18\" page. You can either do r.login with an account that has the checkbox set, or you can do r.http.cookies.set('over18', '1') before asking for a random submission.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/349c0d/praw_cant_get_nsfw_submission/"}, {"author": "bboe", "created_utc": 1430251434, "gilded": 0, "name": "t1_cqrtcbm", "num_comments": null, "score": 1, "selftext": "You are looking at the docs for an old version of PRAW. This is the current release version doc: https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html?highlight=get_comments#praw.objects.Subreddit.get_comments There is not an explicit \"sort\" parameter to `get_comments` because the `subreddit/comments` endpoint does not provide different sort methods. You may be looking for the comment sort order on a single submission's comments. You can do that via `comment_sort` on the `get_submission` method: https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html?highlight=get_comments#praw.__init__.UnauthenticatedReddit.get_submission", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/345x51/praw_subredditget_comments_does_not_take_sort/"}, {"author": "GoldenSights", "created_utc": 1430017557, "gilded": 0, "name": "t1_cqoujos", "num_comments": null, "score": 0, "selftext": "As long as you're only running a single praw bot, you don't have to worry because praw handles the timer for you. If you were writing your own wrapper, you would have to use those ratelimit headers that are returned by reddit.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "sunbolts", "created_utc": 1430017645, "gilded": 0, "name": "t1_cqoul5z", "num_comments": null, "score": 1, "selftext": "So just to confirm, I can just run my PRAW bot and it'll automatically stop sending requests if I've hit the limit until the timer resets?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "bboe", "created_utc": 1430069975, "gilded": 0, "name": "t1_cqpccls", "num_comments": null, "score": 1, "selftext": "Almost correct. PRAW waits up to two seconds before making the next request. It's actually 2 - (current time - last request time). So if your script does some other busy work between requests it may not need to wait at all. For a long time I had been meaning to write something that would support bursty-requests, for long-lived processes (wouldn't work for scripts that are started every few seconds via cron), but I never followed through. With the rate limit changes for OAuth, and the OAuth requirement, PRAW4 will probably provide a burst-mode and utilize the rate-limit headers.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "GoldenSights", "created_utc": 1430018495, "gilded": 0, "name": "t1_cqouz5b", "num_comments": null, "score": 1, "selftext": "Hmm, that's a good question. Praw puts the requests behind a lot of layers, but as far as I can tell, the actual requests happen [here](https://github.com/voussoir/praw/blob/master/praw/handlers.py#L101). (C:\\python34\\lib\\site-packages\\praw\\handlers.py) So if you were to intercept it with a= self.http.send(request, proxies=proxies, timeout=timeout, allow_redirects=False) print(a) print(a.headers) print(a.cookies) return a You still don't get to play with the actual response, but you can at least read it. Do a couple of requests and you'll see all their headers.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "JustAnotherUser_1", "created_utc": 1429962366, "gilded": 0, "name": "t1_cqo5knh", "num_comments": null, "score": 1, "selftext": "Not a python/praw dev - But is it possible to obtain the username? I know there are bots out there that don't reply to themselves, so either they check the username they're replying to, or only reply per 1 parent. If it's possible to obtain the username, then maybe do a bit of POC on a private subreddit? * create own subreddit * install bot * post under your username with keyword * bot replies with \"debug mode\" on\\* * configure your bot, to stop after x comments (to prevent the bot spamming itself) *` keywork matched: . Username is bot? true/false` Hope this helps. ###Edit [Stackoverflow thread might help](http://stackoverflow.com/questions/20822808/praw-comment-submitters-username) >Anyway, a Comment has a author attribute, which is a Redditor instance of the author.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33t71g/praw_how_to_make_a_bot_ignore_certain_users/"}, {"author": "radd_it", "created_utc": 1429834904, "gilded": 0, "name": "t1_cqmmhqx", "num_comments": null, "score": 2, "selftext": "This has less to do with PRAW and more to do with reddit's requested 2-second delay between calls. Can you speed it up? Sure, you can: * Be smarter about what you're loading. Do you really need *all* the comments? Many of those \"load more\"s are just a handful of downvoted comments. Maybe you only load one \"more\" for each top-level comment. * Bypass the 2-second delay. If you're just getting *all the comments* from a single post, it's ok to bypass the rate limit in short bursts. Careful, too many pings and reddit will cut you off and/ or ban your IP.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33nfe0/prawreplace_more_comments_is_bottlenecking_my/"}, {"author": "bboe", "created_utc": 1429843761, "gilded": 0, "name": "t1_cqmrl54", "num_comments": null, "score": 1, "selftext": "I suppose the `replace_more_comments` [documentation](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.replace_more_comments) could be a bit more clear. If you set `limit=0`, then all `MoreComments` objects will be removed without making a single additional request.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33nfe0/prawreplace_more_comments_is_bottlenecking_my/"}, {"author": "PantlessKitten", "created_utc": 1429798378, "gilded": 0, "name": "t1_cqlzo49", "num_comments": null, "score": 2, "selftext": "For what's worth, I'm having the exact same issue. Was kinda waiting to see if what /u/bboe ^(ping) suggested worked for you. Tried different logins (with different passwords), no dice. Python 2.7.6 & praw 3.0a1, testing with: import praw r = praw.Reddit(user_agent=\"Quick Login Test\") r.login(\"username\", \"password\", disable_warning=True)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "PantlessKitten", "created_utc": 1429799602, "gilded": 0, "name": "t1_cqm0cv7", "num_comments": null, "score": 2, "selftext": "We're noobs! :D Nevermind. Different error. ~~Try adding ````api_domain=\"https://api.reddit.com\"````to praw.Reddit. It should get you past the login. Like so:~~ ~~````r = praw.Reddit(\"Your user agent here\", api_domain=\"https://api.reddit.com\")````~~", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "created_utc": 1429799247, "gilded": 0, "name": "t1_cqm05nj", "num_comments": null, "score": 2, "selftext": "Unfortunately I am unable to reproduce. You could try outputting the raw response by adding some debugging lines around: https://github.com/praw-dev/praw/blob/master/praw/handlers.py#L147 There might be some interesting information in the response headers: from pprint import pprint pprint(vars(result.headers)) or in the response body: print(result.text) Edit: `result` not `response` Edit2: `text` not `body`", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "created_utc": 1429799693, "gilded": 0, "name": "t1_cqm0eqy", "num_comments": null, "score": 2, "selftext": ":-/ You have to modify the line after the one I linked to in your local copy of the PRAW source files. Unfortunately, I presently don't have the time to walk you through it but maybe someone else can. Good luck!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "PantlessKitten", "created_utc": 1429842656, "gilded": 0, "name": "t1_cqmqymt", "num_comments": null, "score": 2, "selftext": "It makes two of us, never touched python before. (And I'm really not that good when it comes to JSON). I just wanted a \"bot\" to go through posts of an old account and wipe them out. Anyway, I'm going to bed (again) in a bit so I guess it's your turn! :P After you went to bed I tried to follow the code (starting with ``__init__.py``). So, it starts in ``__init__``, then do do ``login``. From there it jumps to ``request_json`` with the user and pwd. Right [here](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L551-L552) it sends the login info to the server that replies back with a response. So I sent a json request with ``curl`` to the server and, as expected, got a json response: {\"json\": {\"errors\": [], \"data\": {\"need_https\": true, \"hsts_redir\": \"https://reddit.com/modify_hsts_grant?dest=\", \"modhash\": \"ihzmt8xcpj37498d7ad6c74ae848b80a899fb826d7db82eac8\", \"cookie\": \"16367057,2015-04-23T19:02:44,5ec90f403d58ef96b0cc805e8af0d885082b3a3c\"}}} I tried to print the ````response```` right after ([this here](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L551-L552)) (``self._request(yada yada`` ) and here's what comes back: u'\\x1f\\ufffd\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x1c\\ufffd\\ufffdN\\ufffd \\x14\\x05\\x7f\\ufffd\\ufffdu\\ufffd\\x05J\\ufffd\\ufffd\\x18\\x7f\\x9d1\\ufffd-\\ufffd\\nO[\\x0c`\\ufffdy\\u9fdbv{\\ufffd\\ufffd", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "created_utc": 1429842958, "gilded": 0, "name": "t1_cqmr4vu", "num_comments": null, "score": 2, "selftext": "Well that certainly does not look like JSON. My guess is something is not installed properly on your machine. How did you install PRAW?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "created_utc": 1429843569, "gilded": 0, "name": "t1_cqmrh8m", "num_comments": null, "score": 1, "selftext": "pip install praw like the instructions told me to. How exactly should I do this? I can uninstall everything and retry.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "GoldenSights", "created_utc": 1429660665, "gilded": 0, "name": "t1_cqk8l3c", "num_comments": null, "score": 3, "selftext": "To see the attributes of a submission or comment, I would recommend visiting the /api/info link for that item, like this: http://www.reddit.com/api/info.json?id=t3_33eill http://www.reddit.com/api/info.json?id=t1_cqk8l3c On the thread, you're looking for \"selftext\", \"created_utc\", and \"score\". On the comments it's called \"body\" instead of selftext. Upvote ratio is not available for comments. upvote_ratio is available for submissions, but you have to go to it's specific json page instead of api/info for some reason. http://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/.json The 1,000 item limit only applies to item listings, not comment trees. If you're getting comments by /r/redditdev/comments then yes, you're limited to 1,000, but if you're getting the comments off a specific submission you aren't. However, any time there is a \"load 7 more\" button in the browser, that's a spot where PRAW has to make a separate request to fetch those items. This means that large threads can take a very long time to fill in all the comments. To do this, you'll want to call `submission.replace_more_comments`. [Docs](https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html#praw.objects.Submission.replace_more_comments)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/"}, {"author": "GoldenSights", "created_utc": 1429686373, "gilded": 0, "name": "t1_cqklmbh", "num_comments": null, "score": 1, "selftext": "You're forgetting about the upvote_ratio attribute [here](http://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/.json). It's still fuzzed, and you can't extrapolate to get true ups-downs, but it's better than nothing. For comments, there is only \"controversiality\", which is not very useful.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/"}, {"author": "GoldenSights", "created_utc": 1429420267, "gilded": 0, "name": "t1_cqh7t3u", "num_comments": null, "score": 1, "selftext": "Hmm, at first I thought I was getting the same error as you, but now I can't reproduce it. Here's what I wrote: def print_comments(comment): print(comment.body) for child in comment.replies: try: print_comments(child) except: pass a=r.get_submission(submission_id='333jjs') print_comments(a.comments[0]) original = praw.helpers.flatten_tree(a.comments) print(len(original)) print([x.body for x in original[:-1]]) print([x.replies for x in original[:-1]]) a.replace_more_comments(limit=None, threshold=0) print_comments(a.comments[0]) now = praw.helpers.flatten_tree(a.comments) print(len(now)) print([x.body for x in now]) print([x.replies for x in now]) results: one two three four five six seven eight nine ten 11 ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'] [[], [], [], [], [], [], [], [], [], []] one two three four five six seven eight nine ten eleven twelve thirteen 13 ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen'] [[], [], [], [], [], [], [], [], [], [], [], [], []] What happens if you run this snippet?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "GoldenSights", "created_utc": 1429426331, "gilded": 0, "name": "t1_cqh9sjn", "num_comments": null, "score": 1, "selftext": "I wonder if it's a version thing? I'm using Python 3.4.2, PRAW 2.1.21. I can see that you're using Py2, so if that's not the issue then praw will need some changes.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "scandinavian-", "created_utc": 1429434657, "gilded": 0, "name": "t1_cqhbqqy", "num_comments": null, "score": 2, "selftext": "You code works fine here, Python 2.7.8 and Praw 2.1.21. So it must be Praw version or some weird sort of caching issue with reddit?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "Amablue", "created_utc": 1429471001, "gilded": 0, "name": "t1_cqhpdrc", "num_comments": null, "score": 3, "selftext": "Logging does not seem to make a difference. I get the same output when logged in. However, I did try updating my version of praw like /u/scandinavian- suggested and now I seem to be getting correct results. Yay! Thanks /u/scandinavian-.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "bboe", "created_utc": 1429280529, "gilded": 0, "name": "t1_cqfh5kz", "num_comments": null, "score": 2, "selftext": "The problem is that the comment represented by `\"id\": \"cqewoy7\"` is appearing twice consecutively ([see here](https://api.reddit.com/r/HelpMeFind/comments.json?limit=50)). The `stream_generator` in PRAW decides when to fetch stop yielding 'new' comments based on encountering duplicate IDs, and unfortunately will not respond well to this invalid data.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "Icnoyotl", "created_utc": 1429290207, "gilded": 0, "name": "t1_cqfn689", "num_comments": null, "score": 1, "selftext": "It still isn't working for me; I noticed that I still needed to update praw so I did that, (it didn't work before or after the update) and now I get a new error: :0: UserWarning: The keyword `bot` in your user_agent may be problematic. C:\\Users\\MyPC\\Anaconda\\lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:79: InsecurePlatformWarning: A true SSLContext ob ject is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For m ore information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning. InsecurePlatformWarning But, it does still run after that, it just only loads in 43. Is yours successfully pulling in the 1,000?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "habnpam", "created_utc": 1429290934, "gilded": 0, "name": "t1_cqfnmn0", "num_comments": null, "score": 1, "selftext": "Yeah, Reddit doesn't want people using the word \"bot\" anymore. Go to line 48. It should look like this: self.reddit = praw.Reddit(SUBREDDIT_NAME + ' bot') The convention that I use to name my bots is: self.reddit = praw.Reddit(\"/u/my_username short-description-of-what-my-bot-does-goes-here\") d --- This is the line I used: for c in comment_stream(r, \"HelpMeFind\", limit=10000): The names of the variables are a little different,but it's all the same. And yes I do get the 1000 comments.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "GoldenSights", "created_utc": 1429150973, "gilded": 0, "name": "t1_cqdwglj", "num_comments": null, "score": 1, "selftext": "You'll want to use submission.comments, but on large threads you'll also have to fill in the comments that are behind \"load x more\" buttons. To get all the comments on a thread would be like this: submission = r.get_submission(submission_id='32rc86') submission.load_more_comments(limit=None, threshold=1) And now `submission.comments` contains all the comments in a tree structure. Trees aren't as easy to iterate through, so if you want a list of all comments, commentlist = praw.helpers.flatten_tree(submission.comments)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32rc86/praw_how_do_i_get_the_comments_to_a_particular/"}, {"author": "scarface1993", "created_utc": 1429151555, "gilded": 0, "name": "t1_cqdwuoh", "num_comments": null, "score": 1, "selftext": "So, if i want the comments i do submission= r.get_submission(submission_id=\"\") submission.load_more_comments(limit=None,threshold=1) commentList=praw.helpers.flatten_tree(submission.comments) Will that give me all the comments in the post ?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32rc86/praw_how_do_i_get_the_comments_to_a_particular/"}, {"author": "kemitche", "created_utc": 1428598451, "gilded": 0, "name": "t1_cq6pbi4", "num_comments": null, "score": 1, "selftext": "For now, keep doing what you're doing. I'm hoping to work with PRAW to seamlessly support username/password auth for simple scripts/bots by pulling the stuff from https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example into PRAW", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31z8g6/questions_regarding_converting_from_userpass_to/"}, {"author": "IAMA_YOU_AMA", "created_utc": 1428526076, "gilded": 0, "name": "t1_cq5rlem", "num_comments": null, "score": 0, "selftext": "I don't think it's an internal problem with praw. Reddit's servers have been a little less than stellar lately and just simple browsing has returned me more 500 errors than usual. Also last week when reddit released \"the button\" for April Fools, I'm pretty sure that caused a lot more traffic than normal. This is purely anecdotal, though.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/"}, {"author": "GoldenSights", "created_utc": 1428450973, "gilded": 0, "name": "t1_cq4qfyu", "num_comments": null, "score": 2, "selftext": "What you're looking for is `comment.delete()` Also, it's good to encase your While loop inside a try-except so if reddit times out your script won't crash. That would just be: import traceback try: while True: ... except: traceback.print_exc() print('Resuming in 10...') time.sleep(10) You can catch individual praw exceptions, but this simplified version will catch *any* error and just ignore it. You may also prefer to just build this loop into your regular bot script, maybe making it only run after 10 usual loops, or something. That way you don't have to have two separate bots with separate praw instances.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31t6py/autodelete_downvoted_bots_comments/"}, {"author": "TomSparkLabs", "created_utc": 1428451232, "gilded": 0, "name": "t1_cq4qljc", "num_comments": null, "score": 2, "selftext": "Thanks! I figured it would be something as simple as that but, as I said, the PRAW documentation is confusing to me. Also, thanks for the try-except suggestion, will be adding that shortly!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31t6py/autodelete_downvoted_bots_comments/"}, {"author": "hansolo669", "created_utc": 1428394740, "gilded": 0, "name": "t1_cq3x7m4", "num_comments": null, "score": 2, "selftext": "Make sure to take a good read through the [PRAW docs](http://praw.readthedocs.org/en/latest/), and also the [reddit API docs](https://github.com/reddit/reddit/wiki/API), specifically the [JSON section.](https://github.com/reddit/reddit/wiki/JSON) I assume you're doing something along the lines of: for comment in submission.comments: print comment When you need to do (something along the lines of): for comment in submission.comments: print comment.body When you get a submission PRAW returns an object containing all the comments, each of those comments itself is an object containing attributes that correspond 1 to 1 with what's outlined in the reddit JSON docs.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31pxl5/praw_get_comments_without_character_limit/"}, {"author": "shaggorama", "created_utc": 1427773630, "gilded": 0, "name": "t1_cpwbs8a", "num_comments": null, "score": 2, "selftext": "FYI: This tip actually comes from the [praw documentation](https://github.com/reddit/reddit/wiki/API). Their full user-agent advice: >Change your client's User-Agent string to something unique and descriptive, including the target platform, a unique application identifier, a version string, and your username as contact information, in the following format: >:: (by /u/) > >* Example: User-Agent: android:com.example.myredditapp:v1.2.3 (by /u/kemitche) >* Many default User-Agents (like \"Python/urllib\" or \"Java\") are drastically limited to encourage unique and descriptive user-agent strings. >* Including the version number and updating it as your build your application allows us to safely block old buggy/broken versions of your app. >* NEVER lie about your user-agent. This includes spoofing popular browsers and spoofing other bots. We will ban liars with extreme prejudice.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/"}, {"author": "exoendo", "created_utc": 1427242870, "gilded": 0, "name": "t1_cpppedw", "num_comments": null, "score": 2, "selftext": ">I run multiple bots . >keeps throwing the HTTP 429 There is your problem. 429 means you are being rate limited by reddit, which means you are making too many requests at once. Typically you are only allowed 30 requests a minute. The reason you aren't having problems when you are running it locally is because you are using a different IP address from the one your server has. If this is only happening to this one specific bot, my guess is it probably makes far more requests than your other bots, or does things more frequently. OR your other bots are already at the tipping point for total requests being made and starting this bot up is putting you over the rate-limiting edge. Your solutions are, get additional hosting (or run it locally) or write your script in a way that either doens't go beast mode with the requests, or you can just wrap everything in a try...except and catch the 429 errors and just sleep for a few minutes before continuing on with your script. --- edit: one last thing, if your user agent actually has the word 'bot' in it, I have heard/seen that this could lead to all sorts of problems with running praw scripts. So try taking that word out of your user agent.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "AnSq", "created_utc": 1427263766, "gilded": 0, "name": "t1_cppznbn", "num_comments": null, "score": 4, "selftext": "[praw-multiprocess](https://praw.readthedocs.org/en/v2.1.20/pages/multiprocess.html) can handle proper rate limiting of multiple bots running at the same time.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "noobit", "created_utc": 1431654107, "gilded": 0, "name": "t1_cr9k511", "num_comments": null, "score": 1, "selftext": "Interesting, thanks for the link. I had a question on this line, though - > All requests made through the same praw-multiprocess server will respect reddit\u2019s API rate-limiting rules When it says \"same\" praw-multiprocess server - is there some unique string or something I need to pass the handler to ensure it's using the same one as all my other scripts...? Or does using `MultiprocessHandler()` without arguments always generate the same one or something? --- (I only ask because right after that it has this example for how to connect to a \"specific\" praw-multiprocess server: > If instead, you wish to connect to a praw-multiprocess server running at address 10.0.0.1 port 65000 then you would create the PRAW instance via > `handler = MultiprocessHandler('10.0.0.1', 65000)` Which makes it sound like Reddit has more than one or like I can create my own, and I need to specify which one I'm connecting to or something)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "AnSq", "created_utc": 1431672652, "gilded": 0, "name": "t1_cr9sbpo", "num_comments": null, "score": 1, "selftext": "`MultiprocessHandler()` with no arguments will connect to a praw-multiprocess server running on localhost port 10101, i.e., one running on the same computer as the script, and started with no arguments. If you want, you can start the multiprocess server on another port, like `praw-multiprocess --port 22233`, and then connect to it using `MultiprocessHandler(port=22233)`. If you have multiple computers on a LAN all running praw scripts (i.e., they're all accessing reddit from the same public IP), then you'll need to have a centralized praw-multiprocess server for the whole LAN. Start praw-multiprocess on one of the computers, and have all the others connect to it with `MultiprocessHandler(host='server_LAN_IP_here')`. For example, say you have a LAN of two computers with local addresses 192.168.1.2 and 192.168.1.3 named Two and Three respectively. On Two you might run `praw-multiprocess --port 23456`. On Three you would connect to it using `MultiprocessHandler('192.168.1.2', 23456)`. On Two you can connect using the same thing, or with `MultiprocessHandler(port=23456)`, since the multiprocess server is on the same computer. To actually make the connection, you need to pass the result of `MultiprocessHandler()` to `Reddit()`. A complete example might look like this: handler = praw.handlers.MultiprocessHandler('192.168.1.2', 23456) r = praw.Reddit(user_agnet, handler=handler) In practice I have a function in each of my scripts to pick the right handler and make the connection: def reddit_connect(useragent, multi=False): \"\"\"connect to reddit\"\"\" handler = None if multi: handler = praw.handlers.MultiprocessHandler() print \"Connecting to praw-multiprocess\" else: handler = praw.handlers.DefaultHandler() r = praw.Reddit(useragent, handler=handler) return r and then I have a little extra around where I call it to make it a simple command line option: multi = False if \"--multi\" in sys.argv or \"-m\" in sys.argv: multi = True r = reddit_connect(user_agent, multi) Reddit doesn't provide praw-multiprocess servers for you to use. You have to start your own. If you have any more questions, feel free to ask.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "noobit", "created_utc": 1431761053, "gilded": 0, "name": "t1_craun02", "num_comments": null, "score": 1, "selftext": "> `MultiprocessHandler()` with no arguments will connect to a praw-multiprocess server running on localhost port 10101 I see - so it's something I have to have running before invoking `MultiProcessHandler()` from any scripts. > Reddit doesn't provide praw-multiprocess servers for you to use. You have to start your own. Ohh - I was thinking 'server' meant it was already communicating with reddit or something, to get my PRAW usage or such. This makes a lot more sense. --- > If you have any more questions, feel free to ask. Thanks! I have a couple more general questions. 1. What's the best way to handle bots with different timings - say I want bot 1 to run once every 2 minutes, and bot 2 to run every 10 seconds. Is this something `praw-multiprocess` can understand, or do I do something else to set that up? 2. What do you use to run `praw-multiprocess` or your bots on startup? I looked into adding them to `.bashrc` (am on linux) but think that only runs when I log in.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "AnSq", "created_utc": 1431792920, "gilded": 0, "name": "t1_crb4348", "num_comments": null, "score": 1, "selftext": "> I see - so it's something I have to have running before invoking `MultiprocessHandler()` from any scripts. Yeah, basically. *** >What's the best way to handle bots with different timings - say I want bot 1 to run once every 2 minutes, and bot 2 to run every 10 seconds. Is this something praw-multiprocess can understand, or do I do something else to set that up? praw-multiprocess basically just handles rate limiting while proxying requests to reddit, so yes it can understand it, although it can't set it up. In that case I'd just have two different scripts that connect to reddit with `MultiprocessHandler()`, and then have infinite loops that sleep for 10 or 120 seconds after each iteration, where the main body of the loop is whatever you want the bot to do (or more likely, a function call to it). >What do you use to run praw-multiprocess or your bots on startup? I looked into adding them to .bashrc (am on linux) but think that only runs when I log in. [I have a script set to run through mate-session-properties (also on Linux) when I log in](https://gist.github.com/AnSq/41aab9531e98de3bc518) (The reddit stuff is at the bottom). My setup is pretty specific to me though. You could put something in .bashrc (or [.bash_profile](http://www.joshstaiger.org/archives/2005/07/bash_profile_vs.html) more likely), but that might lead to strange effects if you log in twice. If you want to literally start it on boot, before you even log in, there are also ways to do that, such as [rc.local](https://www.raspberrypi.org/documentation/linux/usage/rc-local.md) (it will run your programs as root though, so be careful).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "GoldenSights", "created_utc": 1427002526, "gilded": 0, "name": "t1_cpmpt48", "num_comments": null, "score": 3, "selftext": "If you only need to deflair the easily visible posts -- those found in the /new or /top queues, it's just a matter of using `subreddit.get_new(limit=None)` and [each variation of get_top](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.objects.Subreddit.get_top_from_all) to get the posts, and doing `submission.set_flair('', '')` to clear the text and css. If you want a deep cleanse, we'll have to do something different. I have a script that gets every post on the subreddit, which I can use to build a file of post IDs for you, and then you can use those IDs to get the Submissions and deflair them. Let me know which route you want to take, and if I can provide any snippets for you.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "GoldenSights", "created_utc": 1427007337, "gilded": 0, "name": "t1_cpmr8xf", "num_comments": null, "score": 2, "selftext": "That would be [this one](https://github.com/voussoir/reddit/blob/master/Prawtimestamps/timesearch.py), but I haven't updated PRAW in so long I don't know if it works without modifying anything. If it doesn't work I'd be happy to run it for you if you tell me the subreddit name.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "5loon", "created_utc": 1427011966, "gilded": 0, "name": "t1_cpmsbla", "num_comments": null, "score": 2, "selftext": "Now I'm getting Traceback (most recent call last): File \"C:\\Users\\me\\Desktop\\timesearch.py\", line 172, in main() File \"C:\\Users\\me\\Desktop\\timesearch.py\", line 144, in main sub = input() File \"\", line 1, in NameError: name 'subredditnamehere' is not defined I even updated to 3.4 and installed praw under it. Same error.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "GoldenSights", "created_utc": 1427013016, "gilded": 0, "name": "t1_cpmsj5j", "num_comments": null, "score": 2, "selftext": "When you launch the script it will ask you to type in a name, then a time to start searching from, which you will leave blank. Of course, make sure it launches using the correct version of Python as well. When it's done, you'll have a file called \"subredditname.db\", which contains all of the posts. Then, you'll need to create another script that does the deflairing. I think this should work: import sqlite3 import praw sql = sqlite3.connect('subredditname.db') cur = sql.cursor() r=praw.Reddit('useragent stuff') r.login('username', 'password') cur.execute('SELECT * FROM posts WHERE flair_text IS NOT NULL OR flair_css_class IS NOT NULL') fetch = cur.fetchall() fetch = ['t3_' + i[1] for i in fetch] while len(fetch) > 0: submissions = r.get_info(thing_id=fetch[:100]) for submission in submissions: submission.set_flair('', '') fetch = fetch[100:] but I haven't actually tested that.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "dakta", "created_utc": 1426488247, "gilded": 0, "name": "t1_cpgd2lv", "num_comments": null, "score": 1, "selftext": "For that function, it's a wrapper on `get_content()` so you can find the accepted args under that function: http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit.get_content", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2z6rzn/praw_where_do_we_find_the_list_of_kwargs_for_a/"}, {"author": "Eathed", "created_utc": 1426306403, "gilded": 0, "name": "t1_cpeekv7", "num_comments": null, "score": 1, "selftext": "It does not appear to be possible with praw. [The API does allow you to edit those settings](https://www.reddit.com/dev/api#POST_wiki_settings_{page}), but it appears that praw has not implemented it yet. I may be wrong so you are more than welcome to investigate further. [Here](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_wiki_page) is the code for getting a wiki page in praw. [Here](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.objects.WikiPage) is a description of the WikiPage object. It appears that you can only create a page/update a page's content.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yys8k/praw_making_a_bot_change_the_who_can_edit_this/"}, {"author": "zathegfx", "created_utc": 1426265401, "gilded": 0, "name": "t1_cpdsxlt", "num_comments": null, "score": 7, "selftext": "Ok - I am retarded. To find the flair of the link, just use: `submission.link_flair_css_class` Hope this helps anyone with the same issues Also, this helped me: http://stackoverflow.com/questions/12719542/decoding-json-from-reddit-api-in-python-using-praw", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yx828/praw_find_the_current_flair_for_a_specific_link/"}, {"author": "GoldenSights", "created_utc": 1426044822, "gilded": 0, "name": "t1_cpb2q9e", "num_comments": null, "score": 3, "selftext": "The problem you're running into is different than what you think it is. It's not happening because you've gotten too many submissions, it's happening because one of the submissions has a large number of comments. I don't know what post it is because you're using `get_top` which is ambiguous and will behave differently based on whatever mode you used last. I always prefer to be explicit and use `get_top_from_all` or one of [the others](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.objects.Multireddit.get_top_from_all), so it will always act the same. *Anyway*, the MoreComments object represents the button at the bottom of the page that says \"Load x more comments\". Those objects don't have a body, obviously, and that's why `writer.writerow` is crashing when it gets there. Option 1: before doing `comments = praw.helpers....` , do `submission.replace_more_comments(limit=None)` so praw will automatically replace each one of those with the proper comments. This is astronomically slow on large threads so you might want to use a real limit if it's taking too long Option 2: Put an if statement before the writer that checks `if not isinstance(comment, praw.objects.MoreComments):` so the MoreComments just get ignored. If you're only looking for high-scoring comments this could work just fine. Hope that wasn't too wordy.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "tooproudtopose", "created_utc": 1426057978, "gilded": 0, "name": "t1_cpb7mu4", "num_comments": null, "score": 2, "selftext": "Thank you so much! This is exactly what I needed. Especially for the part about all comments versus only high scoring comments, as I'll eventually need both! As a note to anyone that might need this, you also need to put `f.close()`at the end (outside the loop). EDIT: This actually creates an assertion error. This problem is fixed in the changelog here: https://github.com/praw-dev/praw/blob/master/CHANGES.rst", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "Pathogen-David", "created_utc": 1425939603, "gilded": 0, "name": "t1_cp9lr84", "num_comments": null, "score": 2, "selftext": "It looks like PRAW is still using HTTP for most API endpoints still, unfortunately: https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L253 You could probably modify [line 197](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L197) to use https instead and it'd work just fine, but I've not tested it myself.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yhhm3/prawpythonare_requests_encrypted/"}, {"author": "largenocream", "created_utc": 1425950689, "gilded": 0, "name": "t1_cp9s9h5", "num_comments": null, "score": 2, "selftext": "Here's a monkey patch I use for forcing HTTPS with PRAW: https://gist.github.com/JordanMilne/04c161a5b66a087619ed . I haven't tested it very much, so you might want to!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yhhm3/prawpythonare_requests_encrypted/"}, {"author": "JBHUTT09", "created_utc": 1425906748, "gilded": 0, "name": "t1_cp92l22", "num_comments": null, "score": 1, "selftext": "Thanks. I have 2 more questions: 1. This is what I have for getting a token: def get_new_token( self ): current_time = time( ) client_auth = requests.auth.HTTPBasicAuth( self.client_settings[ 'client_id' ], self.client_settings[ 'client_secret' ] ) headers = { 'user-agent': self.client_settings[ 'user_agent' ] } post_data = { \"grant_type\": \"password\", \"username\": self.client_settings[ \"username\" ], \"password\": self.client_settings[ \"password\" ] } response = requests.post( \"https://www.reddit.com/api/v1/access_token\", auth = client_auth, data = post_data, headers = headers ) token_data = response.json( ) self.token_expiration = current_time + token_data[ 'expires_in' ] self.reddit.set_access_credentials( token_data[ 'scope' ], token_data[ 'access_token' ] ) I'm now getting this error from the set_access_credentials() call: >praw.errors.OAuthAppRequired: The OAuth app config parameters client_id, client_secret and redirect_url must be specified to use this function. I gather this means that I need to include those three parameters, but I'm not quite sure where to squeeze them in. My knee-jerk reaction would be to add them as parameters to the set_access_credentials() call, but, based on the [documentation](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.AuthenticatedReddit.set_access_credentials), it doesn't look like they are parameters. 2. What actions does the token need to be valid for? Should I check the token validity before every action I take with praw, like looking at new posts, or just things that only a user can do, like sending PMs or removing posts? Also, can I request a new token before my current one expires? I only ask because I'd like to throw in a several second grace period so that I can be sure that a nearly expired token doesn't expire while I'm doing something. Thanks for all your help so far.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yekdx/how_do_i_get_an_oauth2_refresh_token_for_a_python/"}, {"author": "kemitche", "created_utc": 1425919819, "gilded": 0, "name": "t1_cp998gu", "num_comments": null, "score": 1, "selftext": "1 Ah. Didn't catch that you were using `praw` at first. You need to make a call to `set_oauth_app_info`: http://praw.readthedocs.org/en/latest/pages/oauth.html Last I checked, `praw` didn't handle the username/password flow for scripts, so you may have some slight oddities to handle when the initial token expires depending on how `praw` tries to renew it. 2 By default, the script tokens have access to all scopes; i.e., any documented reddit API is accessible. No need to check validity before requests, just use the token. I actually recommend writing your code to let the tokens expire, because it forces you to write-in handling of the 401 case (or deal with however `praw` ends up handling it), which is an important part of a well-written OAuth app - though admittedly less important for script-apps.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yekdx/how_do_i_get_an_oauth2_refresh_token_for_a_python/"}, {"author": "shaggorama", "created_utc": 1425762660, "gilded": 0, "name": "t1_cp7jwxy", "num_comments": null, "score": 3, "selftext": "I'm going to hazard a guess that you have multiple versions of python running simultaneously. I'm guessing you had CPython installed already and then separately installed anaconda (or some other distribution that comes with spyder). When you ran `pip install praw` to install praw, the module got downloaded to the CPYthon installation instead of the anaconda installation. Spyder is connected to the anaconda distribution so doesn't see pacakges attached to the other installation. Does that sound like it might fit your situation? If so: poke around the files in your anaconda folder and look for the version of pip in there. Call pip using the absolute path to this version of pip and it'll install praw into your anaconda installation. Also, it's worth noting that this is really a *python* issue and not really a *reddit/praw* issue. In the future, you should post this sort of question to /r/learnpython.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "iforgotmylegs", "created_utc": 1425762781, "gilded": 0, "name": "t1_cp7jz27", "num_comments": null, "score": 3, "selftext": "Ah yes thank you this is exactly my situation. I thought maybe it was a praw issue because I didn't seem to have any trouble with other imports.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "kemitche", "created_utc": 1425755400, "gilded": 0, "name": "t1_cp7gcoo", "num_comments": null, "score": 1, "selftext": ">>> import praw >>> print praw.__file__ /some/path/to/praw", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "iforgotmylegs", "created_utc": 1425756679, "gilded": 0, "name": "t1_cp7gzpc", "num_comments": null, "score": 1, "selftext": "The path returned is C:\\Python34\\lib\\site_packages\\praw\\__init__.py In the PYTHONPATH manager is Spyder, I add the folder C:\\Python34\\lib\\site-packages\\praw then restart, but it still doesn't work.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "GoldenSights", "created_utc": 1425598362, "gilded": 1, "name": "t1_cp5p4vm", "num_comments": null, "score": 3, "selftext": "Okay, this is possible and I've just done it, but you'll have to edit your PRAW installation because PRAW does not yet have anything for the /edited page. 1. Go to C:/python34/scripts/site-packages/praw 2. open objects.py in a text editor 3. Near lines 1318, you should find yourself inside the Subreddit object, looking at `get_top = _get_sorter('top')` 4. Beneath that, add a line: `get_edited = _get_sorter('about/edited')` Now, in your script's code, you can do this: subreddit = r.get_subreddit('redditdev') stream = praw.helpers._stream_generator(subreddit.get_edited, r) for item in stream: #Do your stuff Of course, using 'redditdev' gives you a 403 because it only works on subreddits you moderate. Edit: Wait, this will give you submissions and comments. The API has a parameter to return comments only, but if you just want to be simple I would do a check for t1_ or t3_ in the objects' fullname when in the stream.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y2rvj/praw_comment_stream_search_subredditaboutedited/"}, {"author": "joe-murray", "created_utc": 1425593467, "gilded": 0, "name": "t1_cp5mbi0", "num_comments": null, "score": 1, "selftext": "Try this: from pprint import pprint pprint(vars(comment_object_here)) You might have seen this before -- it's in the PRAW docs -- but it will show you the variables available for the comment object. If \"likes\" isn't in there, then it's just not there. Reasons for this could include, among other things, 1. Your authentication scheme isn't working properly (you should be able to easily test this with a couple commands or print statements though, so this isn't likely to be the source of the error) 2. It was removed, for whatever reason (possibly related to the change in the voting system)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "Mekhami", "created_utc": 1425593642, "gilded": 0, "name": "t1_cp5mff8", "num_comments": null, "score": 1, "selftext": "Authentication is definitely working. All sorts of other auth-scope features are working as intended. It's not just comment objects, but all voteable objects, and the PRAW docs don't make any mention of the likes attribute. There's this github issue which says that it existed at one point: https://github.com/praw-dev/praw/issues/208 but looking through the source code itself, I don't see anything that would point to a likes attribute.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "joe-murray", "created_utc": 1425593807, "gilded": 0, "name": "t1_cp5misd", "num_comments": null, "score": 2, "selftext": "Oh wait, you are just looking for it in the docs? That's a completely different story -- the PRAW docs don't have everything in them. But the \"likes\" variable is shown in part of the PRAW docs (specifically, [where I code that code snippet from](https://praw.readthedocs.org/en/v2.1.20/pages/writing_a_bot.html)), so it definitely exists, or at least used to. In order to verify that is still exists, just do the test I mentioned or do something like this: print comment_object.likes Edit: it might actually be this: print comment_object['likes']", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "joe-murray", "created_utc": 1425598211, "gilded": 0, "name": "t1_cp5p1tp", "num_comments": null, "score": 1, "selftext": "Yea, I've had issues with this in the past -- PRAW doesn't record every little detail in the docs unfortunately, so you just have to kind of hack it. Just use the vars() function as an easy way to see the full depth of what attributes exist, and if you don't know what they do, try printing the variable itself to see what it looks like.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "radd_it", "created_utc": 1425157129, "gilded": 0, "name": "t1_cp07l2d", "num_comments": null, "score": 1, "selftext": "I'm not familiar with PRAW-- but is message actually in messages? Or is it in ``messages.data.children.data`` or somesuch?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xhrkc/iterating_through_unread_messages_isnt_working_as/"}, {"author": "PixelDJ", "created_utc": 1425157602, "gilded": 0, "name": "t1_cp07t9h", "num_comments": null, "score": 2, "selftext": "Using PRAW, the messages should be stored in the \"messages\" variable as a generator, as far as I understand. If I run: messages = r.get_unread(limit = None) print messages I get the following: I also tried using messages = r.get_unread(limit = None) print list(messages) and get the following: [, ] Another version: print list(messages)[0] prints the message body: > From: PixelDJ > > Subject: Another message! > > > Hey, I'm sending another message for... So I know the message is there somewhere.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xhrkc/iterating_through_unread_messages_isnt_working_as/"}, {"author": "GoldenSights", "created_utc": 1424565326, "gilded": 0, "name": "t1_cosysdu", "num_comments": null, "score": 2, "selftext": "I haven't tested this, but I think that's only relevant if you're also sorting by /top. Then you can choose top by day, month, alltime, etc. It's not a time range like choosing specific dates. comments = user.get_comments(limit=100, sort='top', time='day') [Notice that it says](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.objects.Redditor.get_comments) \"time period to return submissions **if applicable**\". And it gives you the different strings you can choose from. The parameter wouldn't work on subreddits because subreddits don't allow sorted comment listings like user pages do.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wpkzd/how_does_time_work_for_get_comments_and_can_it_be/"}, {"author": "GoldenSights", "created_utc": 1424549205, "gilded": 0, "name": "t1_cosqsha", "num_comments": null, "score": 3, "selftext": "Unless there's a particular reason for using Py2.7, I would highly recommend choosing Python 3.4 instead. It even comes with pip pre-packaged. You need to type `pip install praw` into a commandline while in whatever directory you installed pip to. My Pip is in C:/python34/scripts, so I do cd C:/python34/scripts pip install praw When praw receives an update, you can do `pip install praw --upgrade` to download it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "Jinbuhuan", "created_utc": 1424575841, "gilded": 0, "name": "t1_cot3tac", "num_comments": null, "score": -4, "selftext": "Praw? You add two letters, as it is spelled Prawns, and you cook or bar-b-que them and eat them! Hyuk hyuk!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "RudyH246", "created_utc": 1424531757, "gilded": 0, "name": "t1_cosi57c", "num_comments": null, "score": 1, "selftext": "Thanks! I definitely want this to run at all times. How's this? import time import praw r = praw.Reddit(user_agent='RudyH246 big example user agent thingy') r.login('user', 'pass') while True: print 'loop' for comment in r.get_unread(): if comment.author() == r.get_redditor('username'): comment.reply('Reply.') comment.mark_as_read() time.sleep(60)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "mgrieger", "created_utc": 1424540325, "gilded": 0, "name": "t1_cosm6bs", "num_comments": null, "score": 2, "selftext": "Cool! Yeah, python is pretty fun to use. PRAW is a great API wrapper.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "RudyH246", "created_utc": 1424573703, "gilded": 0, "name": "t1_cot2sx0", "num_comments": null, "score": 1, "selftext": "Hm, I got home after work today and found this in my terminal window: Traceback (most recent call last): File \"stupid.py\", line 9, in for comment in r.get_unread(): File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 504, in get_conten t page_data = self.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 557, in request_js on retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 399, in _request _raise_response_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 178, in _raise_res ponse_exceptions response.raise_for_status() File \"C:\\Python27\\lib\\site-packages\\requests\\models.py\", line 831, in raise_fo r_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 521 Server Error: Origin Down Looks like some kind of exception was thrown? After some short googling, it looks like I need to alter my code to look more like: import time import praw r = praw.Reddit(user_agent='RudyH246 big example user agent thingy') r.login('user', 'pass') while True: try: for comment in r.get_unread(): if comment.author == r.get_redditor('username'): comment.reply('Reply.') comment.mark_as_read() time.sleep(60) except: I'm not entirely sure what to put inside the except: bit. If I get an exception, I guess I just want to start the script over again after sleeping for maybe 5 minutes? Not entirely sure how I'd do that. I guess I'd just need to put \"time.sleep(300)\" inside the \"except:\" bit, and nothing else? Or would I need like, a goto line to make the script go back to the original \"try\" bit?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "kemitche", "created_utc": 1423239001, "gilded": 0, "name": "t1_cod5eav", "num_comments": null, "score": 1, "selftext": "PRAW is definitely the way you want to start! reddit's API has its fair share of oddities; PRAW wraps those up fairly nicely (and also keeps you from having to learn a bunch of HTTP semantics on top of python). > the top 10 hottest reddits for a subreddit The top 10 posts? Yes, that's possible. Item 6: https://praw.readthedocs.org/en/v2.1.20/", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2uzqy7/first_timer_here_looking_for_guidance/"}, {"author": "Marorin", "created_utc": 1423240472, "gilded": 0, "name": "t1_cod69fg", "num_comments": null, "score": 1, "selftext": "Thing is my professor wants a presentation on the API and it's use, so I was thinking that. He didn't want something that was 10 lines though so I'm trying to think of creative things to do with PRAW.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2uzqy7/first_timer_here_looking_for_guidance/"}, {"author": "bboe", "created_utc": 1422341496, "gilded": 0, "name": "t1_co25uio", "num_comments": null, "score": 1, "selftext": "PRAW is not thread safe. Remove your threading code and you'll be good to go.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tnplt/comment_stream_error/"}, {"author": "bboe", "created_utc": 1422342389, "gilded": 0, "name": "t1_co2653y", "num_comments": null, "score": 2, "selftext": "Thanks for reporting. Both the errors should be resolved. I won't be making a release just yet but you can jump onto the master branch via: pip install -U https://github.com/praw-dev/praw/archive/master.zip", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "advocat3", "created_utc": 1426150975, "gilded": 0, "name": "t1_cpcdv49", "num_comments": null, "score": 1, "selftext": "Hi really don't mean to hijack or anything but I found this thread via my google search of: \"assert len(self.children) == 1 and self.id == self.children[0]\". I'm working on my first bot and trying to parse the comments on [this thread](http://www.reddit.com/r/TopGear/comments/2yq388/prime_minister_of_the_united_kingdom_hails_jeremy/) I'm super super new to PRAW but loving it so far. Do I have the same issue as OP? Here's a [pastebin](http://pastebin.com/jLNjrd9m).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "gavin19", "created_utc": 1422029638, "gilded": 0, "name": "t1_cnydz1k", "num_comments": null, "score": 1, "selftext": "Do you not get any errors in the console/terminal when you run it? I use a very similar script that works fine import praw r = praw.Reddit('some useragent') r.login('user', 'password') with open('subs.txt', 'r') as e: subs = e.read().split('\\n') for sub in subs: r.get_subreddit(sub).subscribe()", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tevir/problems_subscribing_to_a_list_of_subreddits_from/"}, {"author": "letgoandflow", "created_utc": 1421957735, "gilded": 0, "name": "t1_cnxitax", "num_comments": null, "score": 1, "selftext": "Thanks so much! How can I incorporate your change? Do I have to wait until the next version of praw is released?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "kemitche", "created_utc": 1421962377, "gilded": 0, "name": "t1_cnxlrir", "num_comments": null, "score": 1, "selftext": "Uh, I'm not sure. /u/bboe, can you fill this person in on how PRAW releases work?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "kemitche", "created_utc": 1421953295, "gilded": 0, "name": "t1_cnxg11e", "num_comments": null, "score": 1, "selftext": "/u/bboe merged [the fix I wrote](https://github.com/praw-dev/praw/pull/361) - sorry for the trouble!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "nath_schwarz", "created_utc": 1421898527, "gilded": 0, "name": "t1_cnwuru8", "num_comments": null, "score": 2, "selftext": "To indent your log please put four spaces in front of it, like so: Traceback (most recent call last): File \"redacted\", line 5, in r.login('redacted', 'redacted') File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/init.py\", line 1263, in login self.requestjson(self.config['login'], [...] Or upload it to a nopaste site like http://nopastie.info/ or http://pastebin.org/. For the error: Do you have https on those account activated?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "nath_schwarz", "created_utc": 1421905197, "gilded": 0, "name": "t1_cnwy159", "num_comments": null, "score": 1, "selftext": "Mhm, in that case I'd suggest asking for help in the [official repo](https://github.com/praw-dev/praw) via an issue. Don't forget to mention that you tried it with and without https.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "nemec", "created_utc": 1421700599, "gilded": 0, "name": "t1_cnu6czd", "num_comments": null, "score": 1, "selftext": "1. OAuth support is meant to allow users to authenticate with reddit through an application and allow that application to perform API calls *on their behalf*. This is slightly different from normal logins, because some other user could log into *your* PRAW instance without having to provide you with their password. 2. Now that we've established that OAuth is meant to let others log in to your PRAW application (which is typically hosted on a server somewhere), I can also assume that you want multiple users to be able to use PRAW commands *at the same time*. So, for instance, two users getting the contents of their inbox simultaneously. 3. Now, the warning is simply a reminder that a single instance of PRAW (e.g. `my_praw_instance = praw.Reddit(...)`) can only manage authentication for a single user. If you want multiple users, *each one* must have their own PRAW instance. In environments like a web server, requests may share the exact same set of global variables. So if you store a PRAW instance somewhere that every user request can access, some users will be performing queries on someone else's logged-in PRAW instance. Example: import praw r = praw.Reddit(\"...\") r.set_oauth_app_info(...) def perform_request(**access_information): # Method is called when a user connects to the web page # Pretend the rest of authorization with user was done before this point r.set_access_credentials(**access_information) # Now authenticated as user print(r.get_me()) I'm not really familiar with the PRAW OAuth flow, but imagine you had the above code and two users visited your site at the exact same time. The `perform_request` method would be called twice and set the `r` object's credentials to each user's account. Imagine if user 1 authenticated, then user 2 authenticated *before* user 1 could call `r.get_me()`. Now both user 1 and user 2 are calling that method using user 2's credentials which is very bad. However, it sounds to me like you're not yet at the stage where you're interested in running PRAW on a server for multiple other users to access, so feel free to keep on using the `r` variable as you go through learning to use PRAW.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2syxjd/not_sure_what_to_do_about_warning_in_the_praw/"}, {"author": "letgoandflow", "created_utc": 1421707394, "gilded": 0, "name": "t1_cnuantd", "num_comments": null, "score": 1, "selftext": "Thanks for the info. Sorry I sounded like such a n00b, but I'm already serving an app that uses PRAW and OAuth. I do have some code that is similar to what you posted. I'm wondering what the alternative would be so that you don't run into the issue you outlined. Taking your code, would switching it to something like this work? import praw def perform_request(**access_information): # Method is called when a user connects to the web page r = praw.Reddit(\"...\") r.set_oauth_app_info(...) # Pretend the rest of authorization with user was done before this point r.set_access_credentials(**access_information) # Now authenticated as user print(r.get_me())", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2syxjd/not_sure_what_to_do_about_warning_in_the_praw/"}, {"author": "nemec", "created_utc": 1421708654, "gilded": 0, "name": "t1_cnubfip", "num_comments": null, "score": 3, "selftext": "Ah, okay. Yes, your alternative looks correct. Basically, just make sure that a single PRAW instance isn't accessible to more than one user.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2syxjd/not_sure_what_to_do_about_warning_in_the_praw/"}, {"author": "wscottsanders", "created_utc": 1421361669, "gilded": 0, "name": "t1_cnq9pkt", "num_comments": null, "score": 2, "selftext": "BTW Just curious...What are they trying to explain in the praw documentation? Thanks for the help.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "GoldenSights", "created_utc": 1421366557, "gilded": 0, "name": "t1_cnqcjdq", "num_comments": null, "score": 2, "selftext": "Well I haven't had to deal with captchas since I started PRAW so I don't remember how it all works. I remember that when I was testing things from the commandline, I would receive a CaptchaError, and it would print the url to a captcha. I'd paste that into Chrome, and then type the letters back into my CMD and the post would go through. If you were using PRAW to build a gui program, you could use the provided captcha iden to get an image from http://www.reddit.com/captcha/EBG2KudNny2pB6o5x2VTOIeyjT9n92SH.png and the user could solve it within your application. For the record, the solution to that image is different on my browser than what you put on the post, so maybe they expire or reset after a single use, I don't know. tl;dr There is a way to do captchas with PRAW but I don't know how. I'm guessing your hard-coded solution doesn't work because they only want each captcha to work once.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "bboe", "created_utc": 1421426574, "gilded": 0, "name": "t1_cnr0gwc", "num_comments": null, "score": 3, "selftext": "As /u/GoldenSights has suggested, by default PRAW will prompt and hang waiting for a captcha response to be typed into the shell where the PRAW program is running. By adding `raise_captcha_exception=True` a PRAW user can catch and subsequently write their own logic for fetching and obtaining the captcha response bypassing PRAW's built-in handler.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "GoldenSights", "created_utc": 1421293161, "gilded": 0, "name": "t1_cnpft6i", "num_comments": null, "score": 1, "selftext": "It will receive the comment exactly as it was typed, with no sense of formatting. To see for yourself, just use PRAW from the commandline, retrieve a comment, and `print(comment.body)` Also, http://www.reddit.com/api/info.json?id=t1_cnpfqpr", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sh6sp/how_can_i_use_reddit_formatting_with_a_bot_using/"}, {"author": "hassanchug", "created_utc": 1421101404, "gilded": 0, "name": "t1_cnmx0n2", "num_comments": null, "score": 2, "selftext": "I see, thank you. I have a Raspberry Pi laying around that I haven't done much with, so I might consider just doing away with the App Engine altogether and running straight from the Pi, because after quite a bit of googling it doesn't seem like there's any way of getting GAE and PRAW to work together easily. Ah well.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2s7mcc/using_praw_with_google_app_engine/"}, {"author": "damontoo", "created_utc": 1421181671, "gilded": 0, "name": "t1_cnnx8sn", "num_comments": null, "score": 2, "selftext": "For whatever reason PRAW uses some less popular dependencies that require compiling, so it will never work on GAE. I gave up trying and just wrote a simple module to replace it that basically just handles before/after, setting UA string, parsing the JSON results and rate limiting. My use didn't require OAuth support or anything so it was pretty easy.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2s7mcc/using_praw_with_google_app_engine/"}, {"author": "kemitche", "created_utc": 1420569016, "gilded": 0, "name": "t1_cngdoly", "num_comments": null, "score": 3, "selftext": "Hello there! I'll try and help out as best as I can. First, looking at your code, you seem to be mixing OAuth access and cookie based access. Mixing those two authentication mechanisms won't work. I recommend dropping the login() and getModhash() functions, and using an OAuth \"script\" client: https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example Also or alternatively, I recommend looking up and using PRAW (Python Reddit API Wrapper), a 3rd party SDK for accessing reddit's API from python scripts. I'm also going to try and answer your specific questions, but bear the above in mind. > Why isn't https://ssl.reddit.com/api/v1/access_token in any documentation? It's documented under https://github.com/reddit/reddit/wiki/OAuth2. Apologies, reddit's documentation is split and incomplete. > Why the hell am I getting so many 429 errors? You need to set your custom `User-Agent` on ALL requests you make to the reddit servers (based on your code, you're only doing it in the `authorize()` function). > I also sometimes get 401, 403, and 404 errors with login() and getModhash() as well. 401: Most likely means your access token is expired (attempting to access oauth.reddit.com without an access token or with an expired token) 403: You don't have permission to access the resource 404: The URL you're attempting to access doesn't exist. > Even after login has returned a 200, getModhash() will still give me this: {\"json\": {\"errors\": [[\"USER_REQUIRED\", \"please sign in to do that\", null]]}} The way you're using the python `requests` lib is resulting in the result of your login calls to not be saved. You need to use a [session](http://docs.python-requests.org/en/latest/user/advanced/#session-objects) if you want to use cookie authentication with `requests` (but again, I recommend using OAuth - in which case, you need to be sending an `Authorization` header with an access token). > Am I making this much more complicated than it needs to be? A little bit, but mostly you're stumbling across a combination of (from what I can see) being new to HTTP, being new to APIs, being new to reddit's API, and reddit's API documentation being less than ideal. So don't feel bad about it! A lot of people run into similar problems.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "kemitche", "created_utc": 1420658087, "gilded": 0, "name": "t1_cnhiz0d", "num_comments": null, "score": 1, "selftext": "> 1. To set the User-Agent on all requests, would I just add it to the end of listthing to everything I send over? You'd need to pass in a `headers` dict with every request (PRAW, if you use that, manages this for you) > 2. How do I know which functions work with oauth and which functions work for cookie based? [The API list](/dev/api) indicates which API endpoints are OAuth accessible. Note: There are separate \"/api/me\" and \"/api/v1/me\" endpoints for cookies and OAuth respectively. > 3. What's the difference between the two ways? Cookie authentication is the \"old\" way. It's non-ideal because it encourages 3rd party developers to directly ask for a reddit user's password to access their account. OAuth 2 authentication allows a redditor to temporarily or permanently grant a 3rd party app or website access to their account (and to limit what actions the 3rd party can do with their credentials). > 4. How do I know which addresses should have ssl, oauth, or www in front of them? ssl is no longer needed (www can and SHOULD be accessed over https for all API access). Use `www.reddit.com` for cookie-authenticated requests. Use `www.reddit.com` when RETRIEVING an access token for use in OAuth. Use `oauth.reddit.com` when USING the access token to make API requests. > 5. What is the redirect uri, why do I need it, and how do I make one that's useful? The redirect URI is used in some OAuth 2 flows to return the user back to your website or app. For example, say I made \"supercool.com\" and I want to let redditors use my website to do something supercool with their reddit accounts. The redditor would do the following: 1. Go to supercool.com, and click a button that says \"Do the supercool thing\". 2. The button sends me to reddit.com, and the redditor sees a page that says \"Hey, supercool.com wants to use your reddit account to do X Y and Z. Is that ok?\" 3. If the user clicks the \"Allow\" button on that page, they are *redirected* back to supercool.com - how and where exactly? That's what the redirect URI is for - you register with reddit to tell us where on your site to send users after they click allow. The redirection URL will include information that allows supercool.com to create an access token, and that access token will let supercool.com's back-end web servers use the reddit API as if they were \"logged in\" as the redditor. > 6. How was I able to get a token at all? I think I did everything wrong, but it still worked sometimes. You used the `password` grant_type perfectly - that's how! (See, you didn't do everything wrong!) All you needed to do from there to get your script working (and really, I'm sorry, I should have *started* with this) is to SAVE the access token somewhere, and SEND it with each of your requests; e.g.: def get_my_info(access_token, user_agent): headers = {\"User-Agent\": user_agent, \"Authorization\": \"bearer \" + access_token} r = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=headers) print r.json()", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "nath_schwarz", "created_utc": 1420436575, "gilded": 0, "name": "t1_cnev5os", "num_comments": null, "score": 3, "selftext": ">File \"C:\\Users\\Steam\\Desktop\\sendStylesheet.py\", line 6, in r.login('jakethespectre','[redacted]'); ... Also take a look at [this](https://github.com/nathschwarz/rwikibot) - it lets you push and pull the stylesheet including wiki, sidebar, etc pp. The error itself looks like that either reddit sent back invalid data or your praw-package is outdated. Edit: No, its actually your first line. `r = praw.Reddit(user_agent = 'here goes what you want')`. `user_agent` is an optional argument, not positional. Edit 2: you also might want to take a password that is a bit stronger. Also, take a look at KeePass and/or lastpass.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "jakethespectre", "created_utc": 1420437637, "gilded": 0, "name": "t1_cnevkyp", "num_comments": null, "score": 1, "selftext": "Whoops! I thought I checked it! Also, I'm not sure what you're saying I should do? I replaced my r=praw.Reddit() line with what you have, and I changed the text too, but it still gives me the same error.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "nath_schwarz", "created_utc": 1420437983, "gilded": 0, "name": "t1_cnevpre", "num_comments": null, "score": 2, "selftext": "Then your praw package is probably out of date and you have to update it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "nath_schwarz", "created_utc": 1420439373, "gilded": 0, "name": "t1_cnew8g0", "num_comments": null, "score": 2, "selftext": "Mhm, I just tried - same outcome as yours, yesterday it worked fine. I guess reddit changed the API. So we have to wait for a praw update.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "nath_schwarz", "created_utc": 1420441308, "gilded": 0, "name": "t1_cnewvtm", "num_comments": null, "score": 2, "selftext": "Aye, as long as this isn't fixed either by reddit or by praw there is nothing that can be done - aside from changing the praw code yourself, but that causes more trouble and takes longer than waiting.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "nath_schwarz", "created_utc": 1420568114, "gilded": 0, "name": "t1_cngd4nc", "num_comments": null, "score": 3, "selftext": "https://github.com/praw-dev/praw/issues/358 The issue is enabled https, you have to [disable it](https://www.reddit.com/prefs/security/) to use username/password auth with your account.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "bboe", "created_utc": 1420317246, "gilded": 0, "name": "t1_cnde01q", "num_comments": null, "score": 2, "selftext": "Use the submission stream as it orders them in the correct order. https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html?highlight=stream#praw.helpers.submission_stream", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2r8974/pushing_submissions_to_twitter_but_not_correct/"}, {"author": "exoendo", "created_utc": 1420010695, "gilded": 0, "name": "t1_cna5m5b", "num_comments": null, "score": 2, "selftext": "moderation queue contains comments, but **unmoderated links** is just submissions. If you are pretty confident that 99%+ isn't spam, it's probably more efficient to just auto approve everything in your unmoderated links queue (or just camp in the new queue) import praw import time user_agent = ('Automoderator for GAPINGCUNT') r = praw.Reddit(user_agent=user_agent) r.login(username,password) subreddit = reddit.get_subreddit('TestSubreddit') while True: for link in subreddit.get_unmoderated(limit=None): link.approve() time.sleep(1800) # sleep 30 minutes", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qw7gu/check_if_item_in_modqueue_is_a_flagged_post_or_a/"}, {"author": null, "created_utc": 1419967283, "gilded": 0, "name": "t1_cn9k6wg", "num_comments": null, "score": 1, "selftext": "Unless you're running multiple bots, praw automatically inserts enough sleep() time in between api requests to reddit to conform to the rules. Is this the only bot that's running?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": null, "created_utc": 1419981880, "gilded": 0, "name": "t1_cn9skio", "num_comments": null, "score": 1, "selftext": "What I mean to say is, praw does all timing automatically. No need to worry about it unless you're running multiple scripts.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": "GoldenSights", "created_utc": 1419886425, "gilded": 0, "name": "t1_cn8mmbw", "num_comments": null, "score": 1, "selftext": "If you have HTTPS enabled on your account, try disabling that and see if it goes through. PRAW3 will have https support, and you can download the repo in development https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/ If https isn't the problem, I'm not sure.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qqm9j/cant_log_in_with_praw_any_more/"}, {"author": "bboe", "created_utc": 1421515361, "gilded": 0, "name": "t1_cns13ia", "num_comments": null, "score": 1, "selftext": "Have you tried either PRAW3 or the latest master branch?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qqm9j/cant_log_in_with_praw_any_more/"}, {"author": "rhiever", "created_utc": 1421517808, "gilded": 0, "name": "t1_cns29vq", "num_comments": null, "score": 1, "selftext": "I've updated to the latest version of PRAW with: >sudo pip install --upgrade praw Unless now I need to install praw3 or something instead?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qqm9j/cant_log_in_with_praw_any_more/"}, {"author": "minlite", "created_utc": 1419372111, "gilded": 0, "name": "t1_cn3najh", "num_comments": null, "score": 1, "selftext": "Hey thanks. I am testing a new system now. I ditched praw and used reddit API directly. If that didn't work out, I will really appreciate your help! Thanks again..", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "bboe", "created_utc": 1419151422, "gilded": 0, "name": "t1_cn16jl5", "num_comments": null, "score": 1, "selftext": "https://www.reddit.com/r/redditdev/comments/2pumjt/issues_with_praw/", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pyi07/value_error_when_trying_to_login/"}, {"author": "GoldenSights", "created_utc": 1419101574, "gilded": 0, "name": "t1_cn0mxnu", "num_comments": null, "score": 2, "selftext": "You want all of the post's comments sorted by score, regardless of their depth in any comment chain? How about flat_comments = praw.helpers.flatten_tree(submission.comments) flat_comments.sort(key=lambda comment: comment.score, reverse=True) reverse=True puts the highest rated at the front of the list. Also, change your useragent!! Include a username, give detail.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "GoldenSights", "created_utc": 1419106920, "gilded": 0, "name": "t1_cn0pacu", "num_comments": null, "score": 2, "selftext": "You'll need to do some experimenting from the commandline to figure out exactly how this mechanic works. Here's what I'm seeing in the PRAW docs: get_submission(url=None, submission_id=None, comment_limit=0, comment_sort=None, params={}) Return a Submission object for the given url or submission_id. Parameters: comment_limit \u2013 The desired number of comments to fetch. If", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "AnalyseReddit", "created_utc": 1422432840, "gilded": 0, "name": "t1_co3bwg4", "num_comments": null, "score": 1, "selftext": "I figured I just did not understand the API and made a mistake but it seems that this is a real issue. I think this is a big weakness of the API, because bots cannot easily accses old content for analysis. I wonder if there is anything we can do about it. Maybe if I have the time I will look at the praw-tools code, but thats probably not gonna happen. I hope that we will find a solution.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2p48qb/praw_getting_all_posts_to_a_subreddit_within_a/"}, {"author": "bboe", "created_utc": 1418138152, "gilded": 0, "name": "t1_cmprcl7", "num_comments": null, "score": 2, "selftext": "If you want praw to actually make requests turn the generator into a list: submissions = list(r.get_subreddit('videos').get_hot(limit=30)) This will result in some sort of exception if you don't have an Internet connection.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "bboe", "created_utc": 1418138732, "gilded": 0, "name": "t1_cmprm8l", "num_comments": null, "score": 2, "selftext": "PRAW doesn't currently support the \"continue this thread\" links: https://github.com/praw-dev/praw/issues/321", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "howbigis1gb", "created_utc": 1422569375, "gilded": 0, "name": "t1_co52f4y", "num_comments": null, "score": 1, "selftext": "If I'm reading correctly - the issue seems to be fixed https://github.com/praw-dev/praw/commit/12623275d9061e754f74dd05276aa71c7025edfd", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "bboe", "created_utc": 1422584066, "gilded": 0, "name": "t1_co5ara5", "num_comments": null, "score": 1, "selftext": "Yes, it _should_ have been. But there was a bug which requires an unreleased version. Information here: https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "howbigis1gb", "created_utc": 1422569402, "gilded": 0, "name": "t1_co52fpu", "num_comments": null, "score": 2, "selftext": "bboe seems to indicate that the issue has been fixed https://github.com/praw-dev/praw/commit/12623275d9061e754f74dd05276aa71c7025edfd", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "hooked_dev", "created_utc": 1422571859, "gilded": 0, "name": "t1_co53x2p", "num_comments": null, "score": 1, "selftext": "Thanks for the headup! I wrote my own website scraper when I detected these special cases but it would be great to keep everything in praw.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "bboe", "created_utc": 1418222961, "gilded": 0, "name": "t1_cmqsfei", "num_comments": null, "score": 3, "selftext": "Maybe you could try adding the support to PRAW since it's something you'd like to have in the library. (_wishful thinking_)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "bboe", "created_utc": 1416988420, "gilded": 0, "name": "t1_cmdf0i7", "num_comments": null, "score": 1, "selftext": "If you're using PRAW you don't need to deal with the modhash. It's all taken care of for you.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nfe4j/voting_by_api_cant_find_examples_of_mejson_with/"}, {"author": "beardgoggles", "created_utc": 1416991621, "gilded": 0, "name": "t1_cmdfqc7", "num_comments": null, "score": 1, "selftext": "OK, so that's what I'm asking for examples of. The Reddit API had an example of voting, and it says it requires a modhash. I can't find any examples of voting using Praw. Can you be more specific about how the syntax would change using Praw?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nfe4j/voting_by_api_cant_find_examples_of_mejson_with/"}, {"author": "GoldenSights", "created_utc": 1416951645, "gilded": 0, "name": "t1_cmczdw5", "num_comments": null, "score": 3, "selftext": "One problem lies in `flat_comments`. Since you're only looking for top-levels, we only want the roots of the tree, and therefore have no interest in a flattened set. Try the following from the command line: comments = submission.comments for x in comments: print(x) and flat = praw.helpers.flatten_tree(submission.comments) for x in flat: print(x) In the first case, you'll see a single More Comments object at the bottom, and in the second you'll see it all over the place. Secondly, you may want to replace Mores with a custom function instead of the usual `replace_more_comments` which will replace as many as possible, which we don't want. You may want something along these lines comments = submission.comments while isinstance(comments[-1], praw.objects.MoreComments): comments = comments[:-1] + comments[-1].comments() As we saw earlier, the last item in the non-flat comment list will be a More object. This loop will take it out and replace it with the comment data. When finished, ~~everything in `comments` will be a root-level Comment~~ you should have every root-level comment, and you could run it through another filter to get rid of any sneaky Mores. Furthermore, 1. Testing `is_root` does not require additional calls because it is included when you retrieve the comment, so don't worry about using that. 2. You may also want to check for username duplicates before writing to the file so nobody has an unfair advantage. Also double-check that they are all roots just to be safe. 3. Reddit is not returning 200 at a time. In my cmd testing, it appears that my while loop will produce 20 items at a time, and some of those are the sneaky Mores. This *will* be slow. I would get generous with the print statements so you can see the progress and it doesn't feel stuck. edit: It appears that some Mores have snuck into the list after running the While loop, but they shouldn't cause problems because the loop will not expand them. Just make sure to do some type-checking before trying to print it to the file. Could also do something more sophisticated that checks what's in the More data before adding it to the `comments` list. ____ I'm not at home at the moment, so please excuse minor errors.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nesds/make_praw_only_replace_top_level_comments_not_the/"}, {"author": "bboe", "created_utc": 1416980432, "gilded": 0, "name": "t1_cmdcnax", "num_comments": null, "score": 2, "selftext": "No need to write you own replacement, as you can use `replace_more_comments` to simply remove all `More` objects without a single extra request: submission.replace_more_comments(limit=0) http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.replace_more_comments", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nesds/make_praw_only_replace_top_level_comments_not_the/"}, {"author": "kemitche", "created_utc": 1416951553, "gilded": 0, "name": "t1_cmczbsg", "num_comments": null, "score": 2, "selftext": "You're developing an Android app, so I'm a little confused - how does PRAW come into play here? What is your back-end server for? If you're just \"forwarding\" requests to reddit via your server, you don't need to do that. You can use OAuth2 to make the requests directly from the Android device. If you do in fact need a back end, then yes, you do need some concept of user's authenticating *to your server*, separate from OAuth-ing to reddit. You can do that and still be effectively REST-ful; the \"statelessness\" means more that each individual request is \"independent\"\\*, not that you don't have cookies/sessions. \\* For example, a \"stateful\" request would be a form with 4 pages, where the server tracks which page your on and your existing requests in the session.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nblsd/praw_oauth_and_replying_to_comments/"}, {"author": "bboe", "created_utc": 1416459650, "gilded": 0, "name": "t1_cm7pwph", "num_comments": null, "score": 3, "selftext": "The problem is that you have https only redditing enabled. That functionality is not compatible with PRAW2. I suppose I could be more explicit about that in PRAW. Your options: * Disable the HTTPS only requirement * Use the beta version of [PRAW3](https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/) (there are no known bugs)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "boibtest", "created_utc": 1416525437, "gilded": 0, "name": "t1_cm8ffs1", "num_comments": null, "score": 1, "selftext": "Thanks for the reply - I'll try PRAW3 - but... Where do I have https only enabled? Is that under the reddit account settings or somewhere in praw? I just looked under the reddit settings and I don't see anything about https there. Regardless, thanks for the info.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "chrisarr", "created_utc": 1416411606, "gilded": 0, "name": "t1_cm70fp3", "num_comments": null, "score": 2, "selftext": "The r.get_submission you are calling is a type of .get_content call in PRAW. The docs for that are found on the [praw docs](https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html?highlight=get_submission#praw.__init__.BaseReddit.get_content) What I like to do though is to just grab the JSON off of reddit, by appending \".json\" to a submission URL in the web browser. You can copy and paste that into python scripts and pprint it to see all of the fields you have access to. import pprint data = [copy/pasted JSON from reddit] pprint(data) Anything you see listed in the JSON for the top level comment post can be referenced within your Submission objects. e.g.: import praw r = praw.Reddit(\"App name /u/MoarMag\") r.config.store_json_result = True thread = r.get_submission(\"http://www.reddit.com/r/redditdev/comments/2mrcb8/\") description = thread.selftext.encode('utf-8') print description Probably other ways to do that, but hands on is easy enough! Cheers.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "letgoandflow", "created_utc": 1416443768, "gilded": 0, "name": "t1_cm7idsw", "num_comments": null, "score": 1, "selftext": "I see the \"upvote_ratio\" attribute when I append .json to a submission, but I'm not getting that value when I pull the submission with PRAW.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "letgoandflow", "created_utc": 1416453707, "gilded": 0, "name": "t1_cm7nav2", "num_comments": null, "score": 1, "selftext": "> r.config.store_json_result = True Is this a critical component? I just assumed that PRAW would give you all of the attributes (or at least note that they aren't).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "chrisarr", "created_utc": 1416454536, "gilded": 0, "name": "t1_cm7np2i", "num_comments": null, "score": 1, "selftext": "> .store_json_result I think it is all a matter of how you plan to reference the data after you request it. See [this answer on stack overflow](http://stackoverflow.com/a/24749142/3399542) which helped me get my PRAW JSON working correctly.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "letgoandflow", "created_utc": 1416454853, "gilded": 0, "name": "t1_cm7nuh8", "num_comments": null, "score": 1, "selftext": "Hmm, well I'm stumped. I'm pulling a post in and pprinting the data and it doesn't include \"upvote_ratio\". You can see in the PRAW docs and the upvote_ratio attribute isn't included there either - https://praw.readthedocs.org/en/v2.1.19/pages/writing_a_bot.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "letgoandflow", "created_utc": 1416371404, "gilded": 0, "name": "t1_cm6p5ex", "num_comments": null, "score": 2, "selftext": "Alright, I'll skip the PM and just comment here so you can reply in public and others can learn as well. I'm really just wondering if there are any other guides or info out there about how to navigate PRAW in general.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "zer0t3ch", "created_utc": 1416375817, "gilded": 0, "name": "t1_cm6qxiv", "num_comments": null, "score": 2, "selftext": "Ok, well I'm on computer now, so here goes: First of all, look at other people's code, it will help you. Now, onto this specific problem, here's a little code snippet that should help you figure out whatever your problem is. import praw # Import the praw library agent = \"testScript/0.1 by zer0t3ch\" # Set a string for our user-agent variable r = praw.Reddit(user_agent=agent) # Create a new instance of a reddit connection (called r) r-slash-funny = r.get_subreddit(\"funny\") # Create a new instance of the funny sub (called r-slash-funny) top-10-funny = r-slash-funny.get_top(limit=10) # Gets the current top 10 (in the week, I think) top-10-all-time = r-slash-funny.get_top_from_all(limit=10) # Gets the top 10 of ALL TIME Now remember, your extra parameters are passed to [get_content()](https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html#praw.__init__.BaseReddit.get_content), so you have to go see what can be passed to it. In my example I showed a couple that you might need. Did that answer your question/problem? If not, let me know, and try to go into more detail, and I'll lend a hand where I can.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "letgoandflow", "created_utc": 1416407723, "gilded": 0, "name": "t1_cm6ypdc", "num_comments": null, "score": 1, "selftext": "Thanks for responding. I guess it was just confusing that the \"parameters get passed to get_content()\". Why not just list those parameters out again so the user knows what they can pass to this function? Also, I'm really just looking for more general advice on how to use PRAW. I'm not saying you have to provide it, but I'm more looking for I want to do \"X\" with PRAW, where do I start in terms of finding the right functions that will accomplish \"X\"?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "hooked_dev", "created_utc": 1416423055, "gilded": 0, "name": "t1_cm76u4x", "num_comments": null, "score": 1, "selftext": "Thanks for the response. Is the rate limiting factor PRAW itself, or the download restrictions it places on requests (as governed by the Reddit API)? Wouldn't `r.request_json` still end up having to send just as many requests (sorry if this is an obvious question)?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mpeqt/praw_why_does_it_take_an_hour_to_download_a_large/"}, {"author": "chrisarr", "created_utc": 1416431152, "gilded": 0, "name": "t1_cm7bjeo", "num_comments": null, "score": 1, "selftext": "Reddit API will throttle you around 60 requests a minute. PRAW keeps a cadence of 2 seconds per request, so you don't get throttled. r.request_json is itself 1 request. .replace_more_comments will make a series of additional requests each time it encounters a entity. For very large threads, it is these additional requests being made that slows the process. Being said, .request_json will encounter a limit of its own, around 1000 (or 1500 if you are a reddit gold member, and your app is accessing the API with your credentials). At the tail of the JSON string you will see a bunch of id's for additional comments. These are the comments that occurred past the limit your account encountered. If speed is what you truly need, explore implementing JSON. otherwise, you will have to make due with the API limiting your requests speeds.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mpeqt/praw_why_does_it_take_an_hour_to_download_a_large/"}, {"author": "letgoandflow", "created_utc": 1421364887, "gilded": 0, "name": "t1_cnqblkc", "num_comments": null, "score": 1, "selftext": "See [this post in the praw docs](https://praw.readthedocs.org/en/v2.1.19/pages/faq.html#how-can-i-handle-captchas-myself) which will tell you how to get the captcha ID. Once you have that, you can build URL for the captcha image by following this format: http://www.reddit.com/captcha/insert_captcha_id_here.png Then you can use that URL in an tag to display the captcha image to the user.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mm9sd/praw_how_to_get_a_captcha_image_url_after_you/"}, {"author": "rhiever", "created_utc": 1416177316, "gilded": 0, "name": "t1_cm4feob", "num_comments": null, "score": 1, "selftext": "sudo pip install praw If that doesn't work, paste the error message that comes from that.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mht2a/how_to_install_praw_in_anaconda_in_mac/"}, {"author": "igeorgetaylor", "created_utc": 1416180128, "gilded": 0, "name": "t1_cm4guwo", "num_comments": null, "score": 1, "selftext": "your anaconda don't PRAW unless you got buns hun", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mht2a/how_to_install_praw_in_anaconda_in_mac/"}, {"author": "PasDeDeux", "created_utc": 1416086196, "gilded": 0, "name": "t1_cm3i26m", "num_comments": null, "score": 2, "selftext": "Reading through the source for praw's get_content() method, I wonder if this is my problem: objects_found = 0 params = params or {} fetch_all = fetch_once = False if limit is None: fetch_all = True params['limit'] = 1024 # Just use a big number elif limit > 0: params['limit'] = limit else: fetch_once = True However, when I use very large limits (like 10,000), I still hit the ~1000 wall. After my program terminates, I get: sys:1: ResourceWarning: unclosed C:\\Users\\####\\Anaconda3\\lib\\importlib\\_bootstrap.py:2150: ImportWarning: sys.meta_path is empty sys:1: ResourceWarning: unclosed IIRC this is what I usually get when a download terminates early.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2merm5/praw_hitting_a_limit_with_get_commentslimit_none/"}, {"author": "archon_rising", "created_utc": 1416020699, "gilded": 0, "name": "t1_cm2ybh6", "num_comments": null, "score": 2, "selftext": "Thank you for your ideas. ~~I made the change you suggested, and it prints some of the comments but not all. Some of the others still print as . Why is that?~~ Your critiques are most welcome :). I'll retry the IDs part. Thanks! The list approach didn't work too well for me, I'm not sure why. I could give that another look. The useragent is temporary. I set that originally, but I've been modifying my search term to improve the processing speed. Can you suggest a more thorough documentation than praw.readthedocs? I'm sort of struggling with the level of detail in it(not enough for me) EDIT: the first problem was because of the repr() method, which I removed.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "GoldenSights", "created_utc": 1416023587, "gilded": 0, "name": "t1_cm2zh5b", "num_comments": null, "score": 2, "selftext": "Hmm, I'm not sure why the list method isn't working for you. Try this, and let's ditch that `break` of yours. results = list(r.search('subreddit:gunners', sort='new', period='day', limit=5)) for submission in results: submission.replace_more_comments(...) Boom. Cuts out like 9 lines of code. The only more thorough documentation on PRAW would be [the source code](https://github.com/praw-dev/praw/tree/master/praw) itself.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "6086555", "created_utc": 1415828995, "gilded": 0, "name": "t1_cm0rnza", "num_comments": null, "score": 2, "selftext": "An useful thing to do with praw is to use dir on any object. I had forgotten the answer to your question but: import praw r = praw.Reddit('test') r.get_subreddit('redditdev') sub = r.get_subreddit('redditdev') print dir(sub) In the results you can find 'get_top_from_week' so sub.get_top_from_week works. Pass it limit = None as argument if you want to have all results (up to 1000)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2m3qxk/praw_how_to_pull_all_top_rated_posts_from_last/"}, {"author": "aksios", "created_utc": 1415050114, "gilded": 0, "name": "t1_cls1nlq", "num_comments": null, "score": 2, "selftext": "Because of line 7: link_id = r.get_info(thing_id=link_id) and line 51: parent_id = r.get_info(thing_id=parent_id) You're pinging reddit's API agin here, which means that praw officially waits 2 seconds, but is usually anywhere between 1-3 seconds and can even be more. So since you're doing it twice, it adds up to about 5 seconds.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "fragmede", "created_utc": 1415047409, "gilded": 0, "name": "t1_cls053m", "num_comments": null, "score": 1, "selftext": "Is the included code inside a larger for loop? Without looking at the rest of your code, I'd be that the slow piece is that first line. How long does the script take if you take out everything except the first 4 lines? comments = praw.helpers.comment_stream(r, subreddit, limit=500) for comment in comments: comment_id = comment.id", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "gavin19", "created_utc": 1414440741, "gilded": 0, "name": "t1_cllfxj7", "num_comments": null, "score": 1, "selftext": "I believe it's due to the bot account being brand new so it gets hit with a captcha. https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html?highlight=send_message#praw.__init__.PrivateMessagesMixin.send_message Rather than try to figure out how to handle captchas and respond to each one you should just use the bot account to build up some link karma so you bypass them. I can't recall exactly how much you need to accumulate and if it's also tied to the account age. If you log into the bot using your own account (to temporarily test), then it should work fine.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kho9h/praw_error_when_sending_pm_valueerror_dictionary/"}, {"author": "bboe", "created_utc": 1414469020, "gilded": 0, "name": "t1_cllv2zk", "num_comments": null, "score": 1, "selftext": "Your send message syntax is incorrect for the user-specified function call. You either want to do: r.get_user('icedvariables').send_message('Pug Posts\", 'pugPosts) or r.send_message('icedvariables', 'Pug Posts\", 'pugPosts) Messages are always sent from the logged in account. The way you are calling it, PRAW thinks `pugPosts` is a captcha which it is not.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kho9h/praw_error_when_sending_pm_valueerror_dictionary/"}, {"author": "Planecrazy1191", "created_utc": 1414455677, "gilded": 0, "name": "t1_cllnys9", "num_comments": null, "score": 1, "selftext": "PRAW automatically limits your requests so that they fit within the API access rules. So you only need the sleep timers if you want your bot to periodically rest. As for the PMs, is it possible your bot is failing because of a captcha?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "Planecrazy1191", "created_utc": 1414457449, "gilded": 0, "name": "t1_cllox8o", "num_comments": null, "score": 1, "selftext": "Huh, I wouldn't have expected that. Looking at the PRAW documentation it seems you'd have to re-initialize the praw.reddit object after sleeping as best I can tell thats where the connection is opened.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "teaearlgraycold", "created_utc": 1414459615, "gilded": 0, "name": "t1_cllq485", "num_comments": null, "score": 2, "selftext": "It wasn't a consistent problem. But it might depend on the version of Python/urllib/PRAW - the Keep Alive value isn't something *I* hard coded in. It's part of one of those modules. I'll let you know if the bot still doesn't work as it hasn't ran for as long in its current state as it did with the code I posted in this thread.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "bboe", "created_utc": 1414381986, "gilded": 0, "name": "t1_clkupbr", "num_comments": null, "score": 2, "selftext": "Yes, in the development version: https://github.com/praw-dev/praw/blob/master/CHANGES.rst#unreleased", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kex5m/does_praw_support_multiple_things_for_get_info/"}, {"author": "GoldenSights", "created_utc": 1414382230, "gilded": 0, "name": "t1_clkut58", "num_comments": null, "score": 1, "selftext": "Oh good! Does this mean I can simply copy-paste your [\\_\\_init__](https://github.com/praw-dev/praw/blob/88c037f70742d198530500bd60db265761eef7bb/praw/__init__.py#L777) file over my local one, or will this cause any problems?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kex5m/does_praw_support_multiple_things_for_get_info/"}, {"author": "GoldenSights", "created_utc": 1414127630, "gilded": 0, "name": "t1_clia2bj", "num_comments": null, "score": 1, "selftext": "Try catching `praw.requests.exceptions.HTTPError` instead. I remember having a lot of trouble with errors until I used this. For testing, you could induce an error intentionally by fetching info about a private sub.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2k6668/praw_504_httperror_exception_not_being_caught/"}, {"author": "bboe", "created_utc": 1414129768, "gilded": 0, "name": "t1_cliapqh", "num_comments": null, "score": 2, "selftext": "> The problem is when I go to PRAW (with the domain changed to the appropriate value), the generator get_subreddit returns 'dicts' instead of praw.objects.Submission. Your object to `thing prefix` mapping is probably wrong: > xxx_kind: A string that maps the type returned by json results to a local object. xxx is one of: comment, message, more, redditor, submission, subreddit, userlist. This variable is needed as the object-to-kind mapping is created dynamically on site creation and thus isn\u2019t consistent across sites. You need to figure out the prefix for each object in your install and update you configuration so that the mapping matches.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2k5ui0/figuring_out_the_api_on_a_reddit_clone/"}, {"author": "doob10163", "created_utc": 1413904209, "gilded": 0, "name": "t1_clfllnh", "num_comments": null, "score": 1, "selftext": "So you mean reset my computer, essentially? I'll try it again when I get home. I was frustrated last night and did not try that. Also, praw is working [nicely](http://www.reddit.com/r/CompetitiveHS/comments/2jw3oc/anyone_have_a_good_database_for_card_images/) on this machine", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": null, "created_utc": 1413860070, "gilded": 0, "name": "t1_clf8myh", "num_comments": null, "score": 2, "selftext": "download it from [here](https://pypi.python.org/pypi/praw) unzip it to C:\\Python27\\lib open command prompt. cd C:\\Python27\\lib python setup.py", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "xcombelle", "created_utc": 1413663101, "gilded": 0, "name": "t1_cld4v88", "num_comments": null, "score": 2, "selftext": "you can use ssl_domain in one of your config file like explained https://praw.readthedocs.org/en/latest/pages/configuration_files.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "catzhoek", "created_utc": 1413631411, "gilded": 0, "name": "t1_clctw2r", "num_comments": null, "score": 1, "selftext": "That's not the case, user and pswd are entered via terminal if left empty, but i tried it in all variations. That's why i mentioned that it reacts with proper exceptions if i provide incorrect login credentials. (praw.errors.InvalidUserPass)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "bboe", "created_utc": 1413341737, "gilded": 0, "name": "t1_cl9sznm", "num_comments": null, "score": 3, "selftext": "Initialize praw via: r = praw.Reddit(user_agent='...', disable_update_check=True)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j8dsv/using_praw_with_the_google_app_engine_no_such/"}, {"author": "GoldenSights", "created_utc": 1413174450, "gilded": 0, "name": "t1_cl7za5s", "num_comments": null, "score": 1, "selftext": "Comments only have controversiality measurements, not upvote ratios In fact, I can't even do `upvote_ratio` on submissions with praw, and I dont see it on the [code overview](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html). Has the name of it changed?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j364y/praw_getting_downvotes_for_comments/"}, {"author": "aksios", "created_utc": 1413036268, "gilded": 0, "name": "t1_cl6ilak", "num_comments": null, "score": 2, "selftext": "PRAW doesn't use dicts, it uses objects, so instead of accessing fields like `submission['field']` you have to do `submission.field`. It's the same for comments and redditors: >>> comment = r.get_info(thing_id='t1_c9ehbkl') >>> comment.body \"Thank you! I'll have to play around with praw and see what it can do, but it does sound very fascinating!\" >>> s = comment.submission >>> s.score 11 >>> op = comment.submission.author >>> op.link_karma 4309 Now, the reason why your code above doesn't work is because `r.get_submission()` returns a submission object, which is not iterable. An iterable is something that you can go through one by one, like a list or a generator. Going through a submission one by one doesn't make sense, which is why python raises this error. Really, you can just do this: submission = r.get_submission(url='http://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/') print(submission.title) Here's something neat that's helpful for debugging; you can use `var()` to get a list of the fields of an object: >>> vars(submission) {'media': None, 'created': 1365765708.0, 'downs': 0, 'author': Redditor(user_name='aphexcoil'), 'likes': None, 'ups': 12, 'saved': False, '_underscore_names': None, 'url': 'http://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/', 'author_flair_text': None, 'subreddit': Subreddit(display_name='redditdev'), 'reddit_session': , '_comments_by_id': {'t1_c9ehbkl': , 't1_c9dvbqu': , ... And another tip: when you want to display nice-looking code on reddit, indent all your lines by 4 spaces, or surround the text in backticks. (Look at the source of this comment to see examples of their usage).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ix3aj/how_do_i_read_the_fields_of_praw_submission/"}, {"author": "bboe", "created_utc": 1413051335, "gilded": 0, "name": "t1_cl6oahl", "num_comments": null, "score": 2, "selftext": "> Is this the best way to use PRAW to exceed the non-oauth rate limit? The only way? For the time being, if you ensure you _only_ make OAuth requests then it's probably safe to change the request delay. You should be aware that some actions in PRAW when using OAuth are sent unauthenticated which may put you over the rate limit (I'm not sure) so try enabling request logging (http://praw.readthedocs.org/en/latest/pages/configuration_files.html [see log requests]) to ensure all the requests are going to the 'oauth' domain. You'll have to modify PRAW to access the response headers: https://github.com/praw-dev/praw/blob/5e8abf3fd7f08b5f27031b419defc44d43169048/praw/__init__.py#L382", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2isomp/praw_using_oauth_with_reddit_api_ratelimits/"}, {"author": "bboe", "created_utc": 1413050866, "gilded": 0, "name": "t1_cl6o3fc", "num_comments": null, "score": 1, "selftext": "If you want to use HTTPS, give PRAW3 a try: https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iqen5/keep_getting_redirectexception_using_set_flair/"}, {"author": "GoldenSights", "created_utc": 1412807597, "gilded": 0, "name": "t1_cl47ed8", "num_comments": null, "score": 3, "selftext": "- My comment rate is severely limited on this bot You should be seeing an error along the lines of \"Please wait 9 more minutes\" or something, right? The only real way to avoid these is to gain karma, either at /r/FreeKarma or by some other means. Otherwise, you could store comments that you know you want to reply to, and then keep work through them over time. PRAW will automatically limit your requests to comply with reddit rules. This is 1 call per 2 seconds. If you're seeing rates any slower than that, there are other restrictions on your account because of karma. ____ - How does the bot know how many comments to pull Well, you've done it in a bit of a round-about way, it looks like you were testing the bot on a specific submission just to see how it works. Normally you'd use `subreddit.get_comments(limit=100)` which will get the 100 newest comments to the subreddit. Instead, what you've done is grab a certain post and you're only looking at its comments using the flattened tree. This is comparable to simply opening your browser to the thread and reading comments as you scroll down the page. That means that the comments you're reading are sorted by score and do not necessarily represent the order they were posted. Furthermore, any time you see the \"Load more comments\" button in your browser, PRAW will automatically put a [MoreComments](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.MoreComments) object. If you pass over this object with `if '[[' in comment.body`, your bot will crash because those items don't have bodies. ____ - Ensuring recent posts Well as I said, you're not fetching recent posts right now, you're only fetching 2ioe5e. When you start using `subreddit.get_comments(limit=100)`, then you'll need to start keeping track of ID numbers (with `comment.id`) that you have already scanned and replied to. I use SQL databases to store these numbers. You could also use .txt files but I think that's clunky. [Here is an example of a bot that uses SQL](https://github.com/voussoir/reddit/blob/master/ReplyBot/replybot.py). In line 40-45 I open the file and make sure the table is in place. Line 62 checks for membership, 64 adds new members, and 72 saves the file, all very easy after some Googling ____ You've got quite a lot of questions going on at once, and your code is incomplete. If you want to get specific feel free to ask away.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ipcik/completely_new_to_reddit_api_and_praw_with_python/"}, {"author": "cylindrical418", "created_utc": 1412740003, "gilded": 0, "name": "t1_cl3hel1", "num_comments": null, "score": 2, "selftext": "Let me check that. **Edit**: It's true. Something happened on my comment builder class. Thanks for the help. Where do I get a list of all `praw.APIException` and details about them?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2immwi/praw_apiexception_no_text/"}, {"author": "GoldenSights", "created_utc": 1412740598, "gilded": 0, "name": "t1_cl3hng4", "num_comments": null, "score": 2, "selftext": "https://praw.readthedocs.org/en/v2.1.16/pages/exceptions.html That list isn't *super* detailed. You can read the source code for errors at `C:\\python34\\lib\\site-packages\\praw\\errors.py`, accommodating for your file path Most of the time you'll get HTTPErrors instead of APIErrors, and you can google those to see what they mean.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2immwi/praw_apiexception_no_text/"}, {"author": null, "created_utc": 1412608620, "gilded": 0, "name": "t1_cl1wwa6", "num_comments": null, "score": 1, "selftext": "I don't think PRAW supports giving gold. if you have creddits, you should be able to gild a comment or submission by creating a POST request to [this](https://www.reddit.com/dev/api#POST_api_v1_gold_gild_{fullname}) enpoint. https://www.reddit.com/api/v1/gold/gild/[fullname] where [fullname] is of the form t1\\_[6 numbers/leters] for comments and t3\\_[6 numbers/letters] for posts. sending a POST request to [https://www.reddit.com/api/v1/gold/gild/t3_2ig2ci] would give gold to your submission.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": "F3AR3DLEGEND", "created_utc": 1412610649, "gilded": 0, "name": "t1_cl1xv0n", "num_comments": null, "score": 1, "selftext": "Can you give me a basic tutorial on how to do this? Here is what I've tried so far: import praw import requests reddit = praw.Reddit(\"/u/f3ar3dlegend test\") reddit.login(\"f3ar3dlegend\", \"MYPASSWORD\") client = requests.session() client.headers = {\"User-Agent\": \"/u/f3ar3dlegend test\"} resp = client.post(\"https://www.reddit.com/api/v1/gold/give\", data = {\"uh\": reddit.modhash, \"id\": \"t3_2ig2ci\"}) print resp.text # this seems to be the Reddit front page print resp.ok # False I'm not sure what \"uh\" is supposed to represent. I modified the [dictionary supplied here](http://stackoverflow.com/questions/12936274/reddit-api-and-voting-not-accepting-modhash-cookie-error-user-required).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": "GoldenSights", "created_utc": 1412453062, "gilded": 0, "name": "t1_cl0f9ry", "num_comments": null, "score": 1, "selftext": "Yes, I wrote the comment just minutes after trying it. Is \"Test mention agent\" the actual useragent you're submitting or is that a placeholder text to make this post? That useragent is very nondescript and I'd change it to `\"/u/phil_s_stein commandline praw testing: fetching username mentions with praw\"`, that's generally how mine look. Your problem is probably something very simple, because there's really [no tricks](http://i.imgur.com/Ii27fLW.png) with this. [Useragents are known to cause trouble](http://www.reddit.com/r/redditdev/comments/2i6gcs/praw_httperror_when_trying_to_login/)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "GoldenSights", "created_utc": 1412361215, "gilded": 0, "name": "t1_ckzkriw", "num_comments": null, "score": 1, "selftext": "This is just a shot in the dark, but what is your Useragent? Also, if you try doing praw stuff from the command line, does it behave differently?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2i6gcs/praw_httperror_when_trying_to_login/"}, {"author": "GoldenSights", "created_utc": 1412009986, "gilded": 0, "name": "t1_ckvomrd", "num_comments": null, "score": 1, "selftext": "You're trying to remove every single post on the subreddit? PRAW is probably your best bet, since there is no reddit function to do this.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hsvhj/easiest_way_to_delete_all_posts_for_a_particular/"}, {"author": "nj47", "created_utc": 1411842620, "gilded": 0, "name": "t1_cku39fo", "num_comments": null, "score": 1, "selftext": "NOTE: Everything I said below is still applicable, but there is a more pressing issue. The gold api requires you to be using oAuth - that is why the code you posted is not working - you need to authenticate with oauth to use that endpoint I _think_ (I'm not too familiar with praw, but looking through it's documentation and the reddit api docs, this is what seems most likely to me) Are you wanting to gild a post/comment? You want to use /v1/gold/gild, not the api endpoint you posted. The fullname parameter is a reference to the id of post/comment in question (poorly named imo, fullname could be confused with far to many other things). Read more here: https://www.reddit.com/dev/api#fullnames Also, here is a link to the actual source in question: https://github.com/reddit/reddit/blob/master/r2/r2/controllers/apiv1/gold.py#L101 If you are wanting to give gold as a gift directly to a user and not to a specific post, I apologize as what I posted is irrelevant to you.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "nj47", "created_utc": 1411843580, "gilded": 0, "name": "t1_cku3omw", "num_comments": null, "score": 2, "selftext": "After digging a little more, I'm not 100% sure I'm correct. I would wait for someone else to reply and see what they have to say. Do other functions marked as oauth on this page (https://www.reddit.com/dev/api) work with praw when you login as opposed to using oauth? As for actually using oauth: https://praw.readthedocs.org/en/v2.1.16/pages/oauth.html https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "F3AR3DLEGEND", "created_utc": 1411843958, "gilded": 0, "name": "t1_cku3uhc", "num_comments": null, "score": 1, "selftext": "Upvoting via PRAW works by just logging with PRAW (voting requires oAuth according to the docs).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "Mustermind", "created_utc": 1411145496, "gilded": 0, "name": "t1_ckmvp2f", "num_comments": null, "score": 2, "selftext": "Most PRAW methods have a `limit` keyword. If you set run the method with `limit = None`, PRAW will keep going until it reaches the end of a listing. I made a bot that did that; my attempt looked something like this: user = r.get_user(\"b0wmz\") new = user.get_new(limit = None) posts_in_sub = (link for link in new if link.subreddit == \"redditdev\")", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "Mustermind", "created_utc": 1411149402, "gilded": 0, "name": "t1_ckmxpa3", "num_comments": null, "score": 2, "selftext": "Nope, PRAW would keep going and making multiple requests to get all the content.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "b0wmz", "created_utc": 1411150943, "gilded": 0, "name": "t1_ckmyi5t", "num_comments": null, "score": 2, "selftext": "I was unable to find the `limit=None` parameter in the [documentation](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.search), that did the trick. Thanks a lot", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "Mustermind", "created_utc": 1411152616, "gilded": 0, "name": "t1_ckmze6d", "num_comments": null, "score": 2, "selftext": "Nope, it's there in [`get_content`](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.__init__.BaseReddit.get_content), an internal method.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "DAMN_it_Gary", "created_utc": 1410831737, "gilded": 0, "name": "t1_ckjgzbn", "num_comments": null, "score": 1, "selftext": "this fixed it. should try to contact praw so they can fix this.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "bboe", "created_utc": 1410959310, "gilded": 0, "name": "t1_ckksavn", "num_comments": null, "score": 1, "selftext": "See this post for a current solution: https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/ I appreciate any feedback you have.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "bboe", "created_utc": 1410890371, "gilded": 0, "name": "t1_ckk1ssx", "num_comments": null, "score": 1, "selftext": "Unfortunately I might not get to it today. Follow this bug for status updates: https://github.com/praw-dev/praw/issues/318", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "bboe", "created_utc": 1410828627, "gilded": 0, "name": "t1_ckjfey1", "num_comments": null, "score": 1, "selftext": "Not really. You can write your own function that alternates requests between the submission and comment listings. However, I personally think it would be simpler and cleaner to have two separate programs, one for comments, one for submissions, and use those in combination with [praw-multiprocess](http://praw.readthedocs.org/en/latest/pages/multiprocess.html) and some method of communicating between the two processes if necessary, e.g., a database.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gihdt/is_it_possible_to_run_a_submission_and_comment/"}, {"author": "spinnelein", "created_utc": 1410844785, "gilded": 0, "name": "t1_ckjmu5s", "num_comments": null, "score": 1, "selftext": "I tried creating a self post and editing it in praw, same error message submission.edit(text=posttext) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 285, in edit response = self.reddit_session.request_json(url, data=data) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 177, in wrapped raise error_list[0] praw.errors.APIException: (TOO_LONG) `this is too long (max: 15000.0)` on field `text`", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gfnzr/15000_character_limit_on_self_posts_in_praw/"}, {"author": "bboe", "created_utc": 1410749218, "gilded": 0, "name": "t1_ckikffe", "num_comments": null, "score": 1, "selftext": "You can ignore the ResourceWarnings. They only appears because PRAW inadvertently enables the output of warnings, they otherwise occur but are simply ignored. Aside from the presence of warnings (out of our control) there is no difference in behavior with PRAW on python2 or python3 and if happens to be we'll fix it. I don't really understand your first question. If your program loops forever then you need someway to manage its process. If it only runs for a finite amount of time on each run, then you need some way to periodically execute the program.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gezwe/two_simple_questions_from_a_praw_noob/"}, {"author": "GoldenSights", "created_utc": 1410645031, "gilded": 0, "name": "t1_ckhgu4d", "num_comments": null, "score": 1, "selftext": "That's kinda strange. Sometimes it's nice to see the json information layed out like that, instead of having to launch Python+Praw and do vars(). Unless it's particularly taxing for their servers, I don't really get why this wouldn't be supported. Anyway, thanks", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2g9rig/why_cant_i_get_info_with_an_account_fullname/"}, {"author": "bboe", "created_utc": 1409984998, "gilded": 0, "name": "t1_ckalink", "num_comments": null, "score": 6, "selftext": "It wasn't supported. I just added support. https://github.com/praw-dev/praw/commit/0fabfa930fc0a8d9ba1dcaf16794dbbac1f06d79", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2fle0e/praw_userget_liked_only_returns_50_items_does_not/"}, {"author": "Mustermind", "created_utc": 1409499136, "gilded": 0, "name": "t1_ck5gmhl", "num_comments": null, "score": 2, "selftext": "AFAIK, there's no explicit `rank` attribute on a submission, but a listing of a subreddit returns submissions in order, so you can infer the submission's rank from there. import praw r = praw.Reddit(user_agent = \"Rank Finder\") submission = r.get_info(thing_id = \"t3_2f265d\") listing = r.get_subreddit(\"redditdev\").get_hot() # The best thing to do is to iterate through the generator # and return the index of the submission, but I'm lazy, so # here's a shortcut. rank = list(listing).index(submission) + 1 print(rank)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2f265d/any_way_to_get_the_rank_of_a_submission_in_a/"}, {"author": "_Daimon_", "created_utc": 1409082488, "gilded": 0, "name": "t1_ck16gj8", "num_comments": null, "score": 4, "selftext": "And if you *do* use PRAW, please say so in your post. Sometimes it can be hard to tell the difference.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2enl27/help_us_help_you_include_important_debugging_info/"}, {"author": "aakilfernandes", "created_utc": 1409087786, "gilded": 0, "name": "t1_ck19e69", "num_comments": null, "score": 2, "selftext": "Not sure about praw, but it looks like you can hit http://www.reddit.com/r/AskReddit/about/traffic/.json", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2enkr2/prawapi_is_there_a_way_to_pull_traffic_stats_from/"}, {"author": null, "created_utc": 1408539436, "gilded": 0, "name": "t1_cjvhifc", "num_comments": null, "score": 7, "selftext": "As mentioned, reddit not longer publicly shows the number of downvotes, but it does show the ratio of upvotes to downvotes, and you can use that to get an estimate of the number of upvotes & downvotes: import praw r = praw.Reddit('downs test') subreddit = 'programming' for submission in r.get_subreddit(subreddit).get_hot(): ratio = r.get_submission(submission.permalink).upvote_ratio ups = round((ratio*submission.score)/(2*ratio - 1)) if ratio != 0.5 else round(submission.score/2) downs = ups - submission.score The only problem is that this is quite slow, since it needs to make a separate API call for each submission. Still, it's the best you can do for now.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": "Dobias", "created_utc": 1408539952, "gilded": 0, "name": "t1_cjvhotl", "num_comments": null, "score": 2, "selftext": "Awesome, thank you very much. The calculated count seems to fit quite OK to the normal one. import praw r = praw.Reddit('downs test') subreddit = 'programming' for submission in r.get_subreddit(subreddit).get_hot(): ratio = r.get_submission(submission.permalink).upvote_ratio ups = int(round((ratio*submission.score)/(2*ratio - 1)) if ratio != 0.5 else round(submission.score/2)) downs = ups - submission.score print submission.ups, ups, downs Output: 203 220 17 1278 1480 202 23 31 8 17 23 6 80 93 13 31 43 12 24 33 9 55 65 10 5 5 0 160 201 41", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": "bboe", "created_utc": 1408514636, "gilded": 0, "name": "t1_cjvbxca", "num_comments": null, "score": 1, "selftext": "If you can follow the comment tree IDs deterministically via the json responses then it's a PRAW bug. It was at one point an issue on reddit's end, but that may be resolved.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2191/praw_how_to_retrieve_replies_to_a_comment_past_10/"}, {"author": "_Daimon_", "created_utc": 1408123145, "gilded": 0, "name": "t1_cjr5hnj", "num_comments": null, "score": 1, "selftext": "If you did not update PRAW, then PRAW has not changed in the past day. If it's stopped working then it's due to something else, such as a change to your code or Reddits. What excatly is happening, are you getting an exception or something like this? Do you have a code piece that replicates the issue?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "snaysler", "created_utc": 1408123480, "gilded": 0, "name": "t1_cjr5ohn", "num_comments": null, "score": 1, "selftext": "What's happening is that all of my bots have a connection timeout exception thrown every time they try to edit the sidebar. And now they are throwing random connection errors left and right for trying to edit or load several random wiki pages. It's like my subreddit is slowly taking less and less requests from PRAW. Everything was working perfectly yesterday with no problems, and nothing has changed since then...", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "tst__", "created_utc": 1408125230, "gilded": 0, "name": "t1_cjr6oj9", "num_comments": null, "score": 2, "selftext": "If you run a lot of bots from one server I would definitely check out [PRAW's multiprocess architecture](https://praw.readthedocs.org/en/v2.1.16/pages/multiprocess.html). Otherwise good debugging should help you isolate the problem. Ideas top of the head: * Check the bot from your connection instead of the server * Use links (or something equivalent) to browse reddit from your server * Try out different accounts / bots", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "snaysler", "created_utc": 1408132122, "gilded": 0, "name": "t1_cjrakxa", "num_comments": null, "score": 1, "selftext": "I think it is standard, but I'm not sure. Here's what it looks like: r = praw.Reddit('/r/ludobots TreeBot UVM') r.login(\"mcLudobot\", \"mypassword\")", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "GoldenSights", "created_utc": 1407971941, "gilded": 0, "name": "t1_cjpj2hr", "num_comments": null, "score": 2, "selftext": "[PRAW Code Overview](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html) You're looking for `comment.score`", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "GoldenSights", "created_utc": 1407974960, "gilded": 0, "name": "t1_cjpkh1x", "num_comments": null, "score": 2, "selftext": "Well, no, but if you do a ctrl+f for \"score\" one of the results you'll find is tied to [praw.Objects.Comment](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.Comment). The code overview is layed out by objects, and each object has its own attributes and functions.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "_Daimon_", "created_utc": 1407972022, "gilded": 0, "name": "t1_cjpj3wf", "num_comments": null, "score": 1, "selftext": "You should use python introspection to find attributes and methods of the objects. See [this tutorial](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) in the PRAW documentation. For your specific question, both Comment and Submission objects have the 'score' attribute which is the score of the comment/submission.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "Mustermind", "created_utc": 1407757366, "gilded": 0, "name": "t1_cjn009t", "num_comments": null, "score": 1, "selftext": "Me and /u/SavinaRoja actually compiled a file of subreddits where bots are not allowed or only partly allowed at /r/Bottiquette/wiki/robots_txt_json. I know that your idea is different and is spread out over various subreddits; but many bots look for submissions or comments in /r/all and querying each subreddit's robots_txt is very expensive (in terms of time) for the bot, so gathering all the rules first and then checking it within the logic of the program is much simpler. I've also never seen PRAW implement non-API features like that. I'm not much of a python person, but it might be possible to build a plugin for PRAW that does that.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "Mustermind", "created_utc": 1407726163, "gilded": 0, "name": "t1_cjmrfh3", "num_comments": null, "score": 2, "selftext": "In my experience building the picturegame bot, [get_new()](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_new) reliably returns results from newest to oldest every time. By the way, to avoid hitting submission you've already seen, have you considered using the slightly obscure but nonetheless cool [`praw.helpers.submission_stream`](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.helpers.submission_stream)? That way, you can avoid working with databases and keep the code simple. But keep in mind: you'll need to escape the bot from common errors ([see lines 547-574 to see how I did it](https://github.com/RequestABot/picturegamebot/blob/master/picturegamebot/bot.py#L457)) or you'll lose your users.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "_Daimon_", "created_utc": 1407726375, "gilded": 0, "name": "t1_cjmriuq", "num_comments": null, "score": 2, "selftext": "[get_new](http://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_new) returns the [/new](http://www.reddit.com/r/redditdev/new/) listing for the subreddit. Just as on the webend it is returned new to old. The order is fully dependable. The time of creation is available as a unix timestamp in the created_utc attribute for the Submission object. Do not use created, as this timestamp is based on the server which is completely unreliable.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "bboe", "created_utc": 1407399558, "gilded": 0, "name": "t1_cjjccko", "num_comments": null, "score": 1, "selftext": "The data obtained when fetching a message through `get_info` does not return any children objects. It looks like a message endpoint now exists which does, however, though PRAW does not currently have support. http://www.reddit.com/message/messages/ID.json", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cv1ua/how_do_i_get_this_praw_object_to_populate/"}, {"author": "bboe", "created_utc": 1406820341, "gilded": 0, "name": "t1_cjd1wha", "num_comments": null, "score": 3, "selftext": "Having reddit gold on your account will maximize the number of comments you can fetch along with a submission. Otherwise, no, this is simply not possible on reddit (not a PRAW limitiation, nor API limitation).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2c8lem/is_there_a_way_to_get_all_comments_in_a_thread/"}, {"author": "_Daimon_", "created_utc": 1406373251, "gilded": 0, "name": "t1_cj89jyp", "num_comments": null, "score": 2, "selftext": "I've seen others with the same question, so having access to such methods would be useful. You could create a github project, similar to replybot, that would help others with the same usecase. We won't accept a PR to add methods to parse content for a specific purpose inside the core PRAW, as that lies beyond the scope of a wrapper. However I think it would be fine to include it in the [helpers](https://github.com/praw-dev/praw/blob/master/praw/helpers.py) file which provide a few commonly needed methods that lies beyond PRAWs normal scope.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2brrf9/praw_how_do_i_extract_links_from_a_comment/"}, {"author": "naiyt", "created_utc": 1406301421, "gilded": 0, "name": "t1_cj7hlys", "num_comments": null, "score": 1, "selftext": "Good thoughts, thanks! It should perform fine under load as it's light and just sits on to of PRAW. I've set it watching /r/all without a problem, but I haven't done extensive testing for more popular keywords. And I've mainly used it in debug mod.e which just prints the reply rather than making it. Should be fine in a thread, although if you're running bots in the other threads you'll need to read the PRAW recommendations for they so you don't hit API limits. That's a good idea about expanding it to support other types of bots (moderation bots, bots that can PM, etc.).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/"}, {"author": "bboe", "created_utc": 1406232907, "gilded": 0, "name": "t1_cj6sfbz", "num_comments": null, "score": 1, "selftext": "While reasonable, this is not a use-case I have previously encountered. If you look at the source for [Comment.reply](https://github.com/praw-dev/praw/blob/master/praw/objects.py#L341) you will see: response = self.reddit_session._add_comment(self.fullname, text) Thus, while I haven't tested it, you should be able to do: r_ss._add_comment(comment.fullname, \"COMMENT MESSAGE\") Note that `_add_comment` is currently a non-public method so expect that it may not exist, or behave the same in a future version of PRAW. For example, if this works for you, I just may decide to remove the `_` prefixing the function name, document the function, and provide this example in the documentation.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bl8ew/praw_loading_with_one_instance_replying_with/"}, {"author": "bboe", "created_utc": 1406352196, "gilded": 0, "name": "t1_cj854lc", "num_comments": null, "score": 1, "selftext": "Great! Glad it worked. > Any idea where I should drop some example-code? If you mean in the PRAW docs, I would suggest under the [few short examples](https://praw.readthedocs.org/en/v2.1.16/index.html#a-few-short-examples) section. Thanks!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bl8ew/praw_loading_with_one_instance_replying_with/"}, {"author": "BurnBabyBurn71", "created_utc": 1405773795, "gilded": 0, "name": "t1_cj1qnqp", "num_comments": null, "score": 2, "selftext": "I know you've already got it installed but here is a link to a step-by-step simple tutorial that I wrote that someone might find useful. http://www.reddit.com/r/burnbabyburn71/comments/2b4lst/tutorial_how_to_install_python_33_with_praw/ I could post the tutorial on this sub too if anyone wants me to.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "bboe", "created_utc": 1405658661, "gilded": 0, "name": "t1_cj0nnyp", "num_comments": null, "score": 1, "selftext": "Confirmed as a python3 bug: https://github.com/praw-dev/praw/issues/311", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b0f51/praw_is_get_mod_mail_broken/"}, {"author": "bboe", "created_utc": 1405659241, "gilded": 0, "name": "t1_cj0nvkp", "num_comments": null, "score": 2, "selftext": "I just fixed it, but won't make an immediate release. Here's the fix: https://github.com/praw-dev/praw/commit/e8b1dd848fffa86f28feba159fe3ef63babe89b3", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b0f51/praw_is_get_mod_mail_broken/"}, {"author": "elihusmails", "created_utc": 1405593649, "gilded": 0, "name": "t1_cizur0j", "num_comments": null, "score": 1, "selftext": "I am on Mac 10.7.5. Ultimately I would like to transition this to a BeagleBone, which is where I originally found the problem. I'm using PRAW, so I don't specify a domain. I just have the code: r = praw.Reddit(user_agent=username) r.config._ssl_url = None r.login(username, password) I will check in to the PRAW lib, maybe there's a way to specify the URL I log in to.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2atnid/how_to_install_reddit_ssl_cert/"}, {"author": "IAmAnAnonymousCoward", "created_utc": 1404420143, "gilded": 0, "name": "t1_cinuvrt", "num_comments": null, "score": 1, "selftext": "Yeah, [same problem](http://www.reddit.com/r/redditdev/comments/28qbgd/praw_impossible_to_set_link_flair_unless/). I think the API doesn't allow it for some reason.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29isgn/using_praw_to_set_submission_flair_without/"}, {"author": "djimbob", "created_utc": 1404106247, "gilded": 0, "name": "t1_cikml31", "num_comments": null, "score": 3, "selftext": "In [1]: import praw In [2]: r = praw.Reddit('Testing v1.0 for /u/Cuisinier_Microsoft') In [3]: sub = r.get_submission(submission_id='29g2sd') In [4]: sub.created Out[4]: 1404129928.0 In [5]: sub.created_utc Out[5]: 1404101128.0 Note they are [unix timestamps](http://en.wikipedia.org/wiki/Unix_time) (basically # of seconds since Jan 1 1970), but its trivial to convert them to other formats. E.g., In[6]: import datetime In [7]: datetime.datetime.utcfromtimestamp(sub.created_utc) Out[7]: datetime.datetime(2014, 6, 30, 4, 5, 28) which matches the `Mon Jun 30 04:05:28 2014 UTC` displayed if I hover over the in \"submitted by Cusisinier_Microsoft\"", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "okmkz", "created_utc": 1404101504, "gilded": 0, "name": "t1_cikkzjn", "num_comments": null, "score": 2, "selftext": "Not familiar with praw, but in the json data for a link, there is a created_utc value.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "Cuisinier_Microsoft", "created_utc": 1404101797, "gilded": 0, "name": "t1_cikl3ha", "num_comments": null, "score": 1, "selftext": "I see, if I can't find anything with praw I'll check it out. Thank you !", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "ThreeCorners", "created_utc": 1404233404, "gilded": 0, "name": "t1_ciltq1u", "num_comments": null, "score": 1, "selftext": "Actually, the data returned by PRAW is the JSON data, which I think is why the PRAW documentation there is a little light. So for example, if p is a comment object, just do print vars(p) to look at all of the data associated with that comment that you can access.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "gavin19", "created_utc": 1403888512, "gilded": 0, "name": "t1_ciim9sc", "num_comments": null, "score": 1, "selftext": "Use `get_comments` instead of `get_hot` and use `body` instead of `selftext`. Also, this msg = '[PRAW related thread](%s)' % submission.short_link won't work because comment objects don't have that attribute. You could use `link_title` instead of `short_link` so that you can at least identify the source thread.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2991wo/praw_writing_a_questiondiscover_program_for/"}, {"author": "_Daimon_", "created_utc": 1403735095, "gilded": 0, "name": "t1_cih2uhb", "num_comments": null, "score": 2, "selftext": "You want [update_settings](http://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.__init__.ModConfigMixin.update_settings). subreddit = r.get_subreddit(YOU_SUBREDDIT) subreddit.update_settings()", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/290jf7/praw_equivalent_of_clicking_the_save_button_on/"}, {"author": "tst__", "created_utc": 1403591966, "gilded": 0, "name": "t1_cifk6h8", "num_comments": null, "score": 4, "selftext": "[Non-obvious behavior](https://praw.readthedocs.org/en/latest/pages/faq.html#non-obvious-behavior-and-other-need-to-know) > We can at most get 1000 results from every listing, this is an upstream limitation by reddit. There is nothing we can do to go past this limit. But we may be able to get the results we want with the search() method instead.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "ThreeCorners", "created_utc": 1403592405, "gilded": 0, "name": "t1_cifka9j", "num_comments": null, "score": 2, "selftext": "I was concerned that could be it. For my own user account, however, I only have 18 submissions and 143 comments, and yet it is still quite different: Reddit gives: 185 link karma, 332 comment karma PRAW gives: 276 link karma, 455 comment karma", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "_Daimon_", "created_utc": 1403739803, "gilded": 0, "name": "t1_cih4xof", "num_comments": null, "score": 2, "selftext": "The user listings will return content that is visible to the viewer. So you won't see submissions/comments made to private subreddit or stuff that's been removed. This will obviously skrew the combined results. Additionally you can only go 1000 elements back, so for prolific users not everything will be returned. I'd also like to point out that PRAW doesn't manipulate the results from reddit in any way. What you see is what reddit gave PRAW. If you go to your own [submitted](http://www.reddit.com/user/ThreeCorners/submitted/) page and manually combined the score, then you will arrive roughly at what PRAW gave you. The differences are caused by reddits anti-spam closed-source algorithms. Not that I'm aware of no. Sounds like you want [get_submissions](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_submissions) which fit's your usecase pretty well.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "TerraMaris", "created_utc": 1403477566, "gilded": 0, "name": "t1_ciedklv", "num_comments": null, "score": 1, "selftext": "Have you tried \"sudo pip install praw\"?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": "nandhp", "created_utc": 1403633541, "gilded": 0, "name": "t1_cify2v3", "num_comments": null, "score": 1, "selftext": "You're typing \"pip install praw\" into Python. You need to type it into your shell instead. Ordinarily, the shell would be the thing you run Python from, but on Windows you're probably just clicking an icon and going directly to a Python interpreter, rather than using the Command Prompt (the command-line shell for Windows). A command prompt window will look like this: Microsoft Windows [Version 6.3] C:\\WINDOWS> Try opening Command Prompt and you should see a prompt like the one above (although you it may start in a folder (directory) other than C:\\WINDOWS). Then try `pip install praw` in there. If `pip` doesn't work, try `python`, which should give you a Python interpreter like you had before. If `python` works, but not `pip` there may be a problem with the way `pip` is installed, but try using the `cd` commands below first. If you get an error like `not recognized as an internal or external command, operable program, or batch file`, then you need to tell the Command Prompt where your Python is installed. You can try changing the current directory with the `cd` command: `cd C:\\Python33` or maybe `cd C:\\Python33\\scripts` and then try the `pip` command again. However, it's recommended that Python be in your system's PATH environment variable (so that that it always knows which directory to look in); I believe the Python installer offers to do this for you, you can also add it manually. There's a bit more information about all this here: https://docs.python.org/3/faq/windows.html#how-do-i-run-a-python-program-under-windows", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": "gavin19", "created_utc": 1403046098, "gilded": 0, "name": "t1_cia8izw", "num_comments": null, "score": 5, "selftext": "https://praw.readthedocs.org/en/latest/pages/code_overview.html?highlight=submit#praw.__init__.SubmitMixin.submit", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28etuk/submit_a_post_with_praw/"}, {"author": "bboe", "created_utc": 1402928384, "gilded": 0, "name": "t1_ci8w1yw", "num_comments": null, "score": 1, "selftext": "Just FYI this only looks at top-level comments. If you want lower level comments, you may want to look at the `flatten_tree` helper function as covered here: http://praw.readthedocs.org/en/latest/pages/comment_parsing.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2880cz/comment_bot_not_commenting_what_have_i_done_wrong/"}, {"author": "gavin19", "created_utc": 1402676096, "gilded": 0, "name": "t1_ci6n9fm", "num_comments": null, "score": 3, "selftext": "Something like import praw def main(): approved = [] flaired = [] target_sub = 'some_sub' # migrating to this sub current_sub = 'another_sub' r = praw.Reddit(user_agent=\"/u/someone for /r/somesub\") r.login('user', 'password') target = r.get_subreddit(target_sub) current = r.get_subreddit(current_sub) target_approved = target.get_contributors(limit=None) current_flaired = current.get_flair_list(limit=None) for i in target_approved: approved.append(i.name) for i in current_flaired: flaired.append(i['user']) to_approve = [i for i in flaired if i not in approved] print(len(to_approve), \"to approve\") for i in to_approve: try: target.add_contributor(i) print(\"Added\", i) except praw.errors.InvalidUser: pass if __name__ == '__main__': main() It could do with a review (or two), but it worked fine in a couple of subs I tested. I noticed an issue with users who had flair at one time, but since deleted their account. The name of the user is still returned when fetching the flair, so trying to add them as an approved submitter raises `praw.errors.InvalidUser`, hence the try/except.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/281u72/syncing_a_flaired_user_list_to_approved_submitter/"}, {"author": "6086555", "created_utc": 1402613932, "gilded": 0, "name": "t1_ci63fr3", "num_comments": null, "score": 1, "selftext": "[This](http://www.reddit.com/r/redditdev/comments/191bn8/does_praw_support_the_before_field_for_getting/c8k4cxd) should help you", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28037u/praw_cant_get_prawobjectsubredditget_new_to_honor/"}, {"author": "bboe", "created_utc": 1402498237, "gilded": 0, "name": "t1_ci4t83f", "num_comments": null, "score": 2, "selftext": "Unfortunately, without a single reproducible request there isn't much that we (PRAW devs) can do. From what you've written it seems like the response from reddit is being truncated due to some sort of network issue. If that is the case, raising an exception is the appropriate action to take so you can handle it as you've done. Restarting the stream is not that inelegant / inefficient.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/27vebn/praw_valueerror_in_prawhelperssubmission_stream/"}, {"author": "VoterBot", "created_utc": 1402535305, "gilded": 0, "name": "t1_ci5asya", "num_comments": null, "score": 1, "selftext": "Thanks for the response. Just knowing that it's not crashing because of my clumsiness is good. PRAW is solid as a rock on my local machine and it's only started playing up on Heroku in the last few days.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/27vebn/praw_valueerror_in_prawhelperssubmission_stream/"}, {"author": "bboe", "created_utc": 1402066004, "gilded": 0, "name": "t1_ci0pg4v", "num_comments": null, "score": 1, "selftext": "It sounds like you are running into a caching issue where the second execution of a similar command within a single execution doesn't receive the updated parameters. As per reddit's API guidelines that specify not to make the same request within 30 seconds, PRAW maintains a cache of the same request for 30 seconds. You can either wait 30 seconds, or you can add an arbitrary query parameter to a GET request to bypass the caching functionality.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/27ey83/im_designing_online_class_with_reddit_praw_has/"}, {"author": "_lowell", "created_utc": 1401972755, "gilded": 0, "name": "t1_chzrg0t", "num_comments": null, "score": 1, "selftext": "I found [this](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.Subreddit.add_contributor). Sounds like what you're looking for. Maybe try it, if you haven't already?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/279qlu/adding_a_user_as_an_approved_submitter_with_praw/"}, {"author": "DrJosh", "created_utc": 1401284229, "gilded": 0, "name": "t1_cht30lu", "num_comments": null, "score": 1, "selftext": "As a lot of people have proposed improvements, please find the full code for CommentTreeBot below. Feel free to improve it and launch your own CommentTreeBots: import matplotlib.pyplot as plt import networkx as nx import random import praw G = nx.Graph(); def Parse_Forest(replies,tabAmount,parentID): if ( replies == [] ): return; for comment in replies: isComment = type(comment) is praw.objects.Comment if ( isComment ): myID = comment.id; G.add_node(myID); G.add_edge(parentID,myID); Parse_Forest(comment.replies,tabAmount+' ',myID); r = praw.Reddit('testing') submission = r.get_submission(submission_id='26lrs8') submission.replace_more_comments(limit=None,threshold=0); forest_comments = submission.comments G.add_node(0); runningID = 0; Parse_Forest(forest_comments,'',0); pos=nx.graphviz_layout(G,prog='twopi',args='') nx.draw(G,pos,node_size=20,alpha=0.5,node_color=\"blue\", with_labels=False) plt.show();", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "gavin19", "created_utc": 1401019068, "gilded": 0, "name": "t1_chqn7wa", "num_comments": null, "score": 1, "selftext": "Sorry for the delay. GMT. Nodded off. The four scripts are a [flair bot](https://dl.dropboxusercontent.com/u/2552046/_scripts/py/reddit/t.py) that checks for new PMs and updates the flair based on that info. Two others are like [this](https://dl.dropboxusercontent.com/u/2552046/_scripts/py/reddit/top3.py) and request the top 50 posts from new and update the sidebar contents based on that. The last is very similar except it only updates the sidebar. No other requests are made to reddit. Then there are the two AutoMod instances. Adding the second instance is where it seems the problems began. The UA isn't unique for those run via the same account, like /u/RFootballBot. They're all just '/u/someone for /r/somesub'. DO faq states that the IP is unique per server (droplet). I'm unsure how to fetch the header info. All I can relay at the minute is that PRAW tells me praw.errors.RateLimitExceeded: `you are doing that too much. try again in X hours.` on field `vdelay`", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26ex2a/rate_limit_exceeded_errors_with_praw/"}, {"author": "bboe", "created_utc": 1401040711, "gilded": 1, "name": "t1_chqtu2x", "num_comments": null, "score": 3, "selftext": "If you're running separate instances of PRAW behind the same IP address you should use the multiprocess proxy server to manage the rate limiting: http://praw.readthedocs.org/en/latest/pages/multiprocess.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26ex2a/rate_limit_exceeded_errors_with_praw/"}, {"author": "kemitche", "created_utc": 1400866976, "gilded": 0, "name": "t1_chpeawj", "num_comments": null, "score": 2, "selftext": "~~It doesn't look like PRAW exposes the \"subreddit_type\" JSON field directly, however, you can access it if needed like so:~~ /u/bboe has the proper solution below. sr = r.get_subreddit(\"redditdev\") sr._get_json_dict()['subreddit_type']", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "bboe", "created_utc": 1400877224, "gilded": 0, "name": "t1_chpiyl3", "num_comments": null, "score": 3, "selftext": "PRAW exposes the parameters on demand so the following will work just fine: r.get_subreddit('redditdev').subreddit_type Running the following does not actually make any request as fetching the subreddit about page is not often required. r.get_subreddit('redditdev')", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "bboe", "created_utc": 1401089236, "gilded": 0, "name": "t1_chravm7", "num_comments": null, "score": 1, "selftext": "I cannot reproduce your error: import praw r = praw.Reddit('bboe test') print(r.get_subreddit('askreddit').subreddit_type) print(r.get_subreddit('gaming').subreddit_type) print(r.get_subreddit('pyongyang').subreddit_type) Its output is: public public restricted", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": null, "created_utc": 1400785188, "gilded": 0, "name": "t1_chom5b6", "num_comments": null, "score": 2, "selftext": "Still messing about in praw. Ill see if I can get a decent function for you.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2678bh/re_praw_is_there_a_function_for_getting_unread/"}, {"author": "jsmcgd", "created_utc": 1400755360, "gilded": 0, "name": "t1_choadxl", "num_comments": null, "score": 5, "selftext": "Ah, I believe this is what I'm looking for: https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26762p/is_there_a_documented_list_of_praws_api_functions/"}, {"author": "spike77wbs", "created_utc": 1400470727, "gilded": 0, "name": "t1_chli8fy", "num_comments": null, "score": 1, "selftext": "So it looks to me looking at the documentation that PRAW doesn't address the resubmit option of the API, am I reading that correctly?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "bboe", "created_utc": 1400631632, "gilded": 0, "name": "t1_chn3iev", "num_comments": null, "score": 1, "selftext": "Support will be added in the next version of PRAW: https://github.com/praw-dev/praw/commit/24ac157d369fd9c54615e2f19a1f2f6b92f8f9b4", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "spike77wbs", "created_utc": 1400564193, "gilded": 0, "name": "t1_chmf9ov", "num_comments": null, "score": 1, "selftext": "Here is the full error sequence: Version 2.1.11 of praw is outdated. Version 2.1.16 was released 7 days ago. Traceback (most recent call last): File \"C:/Documents and Settings/Family/Desktop/Python/reddpost3.py\", line 62, in redd.submit('borntoday', title, url=url, params={'resubmit': True}) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 320, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 222, in wrapped return function(obj, *args, **kwargs) TypeError: submit() got an unexpected keyword argument 'params'", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "ubccompscistudent", "created_utc": 1399483660, "gilded": 0, "name": "t1_chbz0fo", "num_comments": null, "score": 2, "selftext": "Sorry, but every time I searched for pip/praw, the threads were all in this subreddit, so I assumed it was the right one. My bad", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "_Daimon_", "created_utc": 1399409429, "gilded": 0, "name": "t1_chb8wgg", "num_comments": null, "score": 2, "selftext": "Attributes of objects are dynamically determined at runtime based on what is exposed by Reddits API. This is a really cool feature that allos us to instantly integrate new features of the API to all versions of PRAW. No upgrade necessary. But it does mean, that there is no way of knowing what attribute are available on what objects at the time of compiling the documentation. This is why [PRAW's documentation](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) details how to use standard python introspection tools to discover what attributes are available on an object.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "_Daimon_", "created_utc": 1399409305, "gilded": 0, "name": "t1_chb8u7j", "num_comments": null, "score": 6, "selftext": "PRAW is open source. You're welcome to contribute..", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "vicstudent", "created_utc": 1399397639, "gilded": 0, "name": "t1_chb3114", "num_comments": null, "score": 3, "selftext": "You can start by doing something like this and move forward. import praw import time r = praw.Reddit('Blah') u = r.get_redditor('vicstudent') timestamp = u.created_utc # Convert the timestamp date = time.ctime(timestamp) # date = 'Thu Jul 25 22:08:04 2013'", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "OnceAndForever", "created_utc": 1399353663, "gilded": 0, "name": "t1_chaqqs0", "num_comments": null, "score": 2, "selftext": "You are not actually logging in with a user account. `r.login()` takes the username and password of a reddit account as two parameters. Your error message says this: `praw.errors.NotLoggedIn: 'please login to do that' on field 'None'.` Keep in mind that you can only delete comments and submissions that your account has posted.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ua4d/cant_get_praw_to_delete_a_post/"}, {"author": "ScredditAttackBot", "created_utc": 1399179191, "gilded": 0, "name": "t1_ch91p6l", "num_comments": null, "score": 3, "selftext": "I'm new. It looks like you have praw installed. What makes you think you don't? Make sure you are running just python in the command prompt and not python3.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "pietrod21", "created_utc": 1399203982, "gilded": 0, "name": "t1_ch96kdy", "num_comments": null, "score": 1, "selftext": "or in python 3 IDLE or in the command line by writing \"python\" (and the default it's python 3), it tells me: Traceback (most recent call last): File \"\", line 1, in import praw ImportError: No module named 'praw'", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "_Daimon_", "created_utc": 1399242897, "gilded": 0, "name": "t1_ch9kunz", "num_comments": null, "score": 2, "selftext": "The output of your command says Requirement already satisfied (use --upgrade to upgrade): praw in c:\\users\\admin \\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages It means that pip thinks praw is installed and in the correct path. The likely cause of your problem is that the site-packages directory, where PRAW is located, isn't on your PYTHONPATH. Which is basically a list of directories where Python looks for libraries when you have an import statement in your code. So Python cannot find the module, even though it actually is on your system. I haven't used python in a windows environment in many years. So this is moving out of my area of expertise. You might be able to solve the problem by adding to your pythonpath as mentioned in this [stackoverflow answer](http://stackoverflow.com/a/4855685/1368070). You might also consider starting a new question on SO or /r/learnpython as this is a more general question, that has little to do with PRAW and a lot to do with running python on windows. The guyes at those two places will know much more about this subject, and will be better able to help you. Best of luck!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": null, "created_utc": 1399254175, "gilded": 0, "name": "t1_ch9pits", "num_comments": null, "score": 1, "selftext": "it's a **very** hacky workaround start your script with: import sys sys.path.append('c:\\users\\admin\\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages') import praw for some reason PRAW was installed in a directory that wasn't in your path.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "actual_factual_bear", "created_utc": 1405713541, "gilded": 0, "name": "t1_cj172rx", "num_comments": null, "score": 1, "selftext": "my machine rebooted the other day and i just got around to re-starting my PRAW client and found it had the same error I mentioned in this post two months ago. I have no idea how I originally fixed it, but this time I did an `easy_install -U requests` and it updated to 2.3 and the problem was fixed.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": "bboe", "created_utc": 1398980196, "gilded": 0, "name": "t1_ch77gaa", "num_comments": null, "score": 2, "selftext": "This isn't the place for feature requests. https://github.com/praw-dev/praw/issues", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24gpae/feature_request_add_reason_to_get_banned_in_praw/"}, {"author": "0x14", "created_utc": 1398962031, "gilded": 0, "name": "t1_ch6ynfk", "num_comments": null, "score": 1, "selftext": "Do Amazon expect you to hand over any payment details, personal information? How easy is registration? So they use a fedora/redhat/centos system (i had to look up yum, i use debian) Do you get a pretty basic user account in this linux environment (CLI and GUI?) and you can easily install with a package manager... sounds good can you install pip nice and easy with yum? How does it handle praw/reddit logins, i know pythonanywhere doesnt like praw going through SSL. Finally are redditbots pretty stable on it? Pythonanywhere will switch your bot off after a few days if its going over its CPU allowance, is EC2 more generous?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "RemindMeBotWrangler", "created_utc": 1398963481, "gilded": 0, "name": "t1_ch6zchi", "num_comments": null, "score": 2, "selftext": ">Do Amazon expect you to hand over any payment details, personal information? How easy is registration? Credit card info >So they use a fedora/redhat/centos system (i had to look up yum, i use debian) https://aws.amazon.com/marketplace/b/2649367011?page=1&category=2649367011 for a list but free only get to pick like 8. There are debian systems you can use though I believe. >can you install pip nice and easy with yum? How does it handle praw/reddit logins, i know pythonanywhere doesnt like praw going through SSL. Same as apt-get really. sudo yum install pip. sudo pip install praw. It works fine on there for me. >Finally are redditbots pretty stable on it? Pythonanywhere will switch your bot off after a few days if its going over its CPU allowance, is EC2 more generous? I've only been using it for a few days but so far so good. http://aws.amazon.com/free/faqs/ There are the limitations you get for free. If you go over you pay for that month but I believe you still get the 12 months for free. But I don't think you'll go over if you are only searching and replying. I read that if you are always at 100% CPU they will throttle you but you won't notice as they just lower your speed, but you'll still be at 100%. Not sure if it really matters, I'll notice or if it's true.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "damontoo", "created_utc": 1398950688, "gilded": 0, "name": "t1_ch6tx4p", "num_comments": null, "score": 1, "selftext": "Google app engine also has a free tier but you can't use PRAW I don't think. I just wrote my own pure python API wrapper. It's not fancy but it works.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "bboe", "created_utc": 1398980608, "gilded": 0, "name": "t1_ch77n8f", "num_comments": null, "score": 1, "selftext": "PRAW should work on GAE.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "vicstudent", "created_utc": 1398736813, "gilded": 0, "name": "t1_ch4onq7", "num_comments": null, "score": 2, "selftext": "The msg argument you are passing to is used for the 'subject' parameter. [Here's the docs](https://praw.readthedocs.org/en/latest/pages/code_overview.html?highlight=send_message#praw.__init__.PrivateMessagesMixin.send_message). Passing print(r.user.send_message('MyUserName', 'subject', msg)) should work.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "created_utc": 1398736923, "gilded": 0, "name": "t1_ch4opq3", "num_comments": null, "score": 1, "selftext": "Did you read the pastebin? Line 17, `msg = '[PRAW related thread](%s)' , submission.short_link` shouldn't this fill both?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "vicstudent", "created_utc": 1398737749, "gilded": 0, "name": "t1_ch4p4tl", "num_comments": null, "score": 2, "selftext": "From what I understand, this won't work. However, I don't really understand what you're trying to get at with that variable. Is that an attempt at string formatting? Because that won't work with Py3.4. Otherwise, what you're doing is sending send_message a tuple, which won't work. This is the method according to the docs: send_message(recipient, subject, message, captcha=None). If you're trying to format the string just do: msg = '[PRAW related thread]({}).format(submission.short_link) #Then pass: print(r.user.send_message('MyUserName', 'Subject Title', msg)) Edit: The point is that you're sending the msg argument to the subject title parameter which does not have a default argument.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "created_utc": 1398738381, "gilded": 0, "name": "t1_ch4pgio", "num_comments": null, "score": 1, "selftext": "Okay, I changed it to `print(r.user.send_message('HellFireKoder', 'From Your Reddit Bot', msg, None))` and now it says Traceback (most recent call last): File \"C:\\Users\\w_000\\Desktop\\questionsearch.py\", line 22, in print(r.user.send_message('HellFireKoder', 'From Your Reddit Bot', msg, None)) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 58, in wrapped return function(self.reddit_session, self, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 225, in wrapped return function(obj, *args, **kwargs) TypeError: send_message() takes from 4 to 5 positional arguments but 6 were given Any clues? I don't get it because I only count 4 arguments... Thank you for your help and patience so far.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "created_utc": 1398739935, "gilded": 0, "name": "t1_ch4q8hp", "num_comments": null, "score": 2, "selftext": "It returns Traceback: Traceback (most recent call last): File \"C:\\Users\\w_000\\Desktop\\questionsearch.py\", line 22, in print('HellFireKoder', r.send_message('HellFireKoder', 'From Your Reddit Bot', msg, None)) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 225, in wrapped return function(obj, *args, **kwargs) TypeError: send_message() got multiple values for argument 'captcha' Edit: I am going to try again with `None` in there... Edit 2: It worked without `None` in it, still asking for captcha though :(", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "CastleCorp", "created_utc": 1398644523, "gilded": 0, "name": "t1_ch3pqj0", "num_comments": null, "score": 1, "selftext": "Yes, did login, at least I think I did it correctly. It is logging in under my account, so it does have subscribed subreddits. As for the second part, I am running on like 2 hours of sleep here, and I am not really processing what the heck you are talking about. I don't think I am but I could be very wrong. Here is what I have: user_agent = (\"PlaceHolder bot by /u/CastleCorp\" \"github link\") # Should this be open source? r = praw.Reddit(user_agent = user_agent) # Leave on this line. r.login(username,password) already_done = [] # Submissions already checked user = r.get_redditor(username) subscribed_reddits = r.get_my_subreddits(limit=500) # get all subscribed reddits, up to 500 subbed_reddits = [] # Array to hold all subscribed subreddits hot_posts= [] # Array to hold all the hot posts def getSubscribedReddits(): # store each subscribed reddit in array, print the array for subreddit in subscribed_reddits: subbed_reddits.append(subreddit) # add the subreddit's name to the array print subreddit.display_name print len(list(subscribed_reddits)) return subbed_reddits # return for function call def getHotPosts(): # Get the top \"Hot\" post from each of the user's subscribed reddits hot_posts = [next(x.get_hot(limit=1)) for x in subscribed_reddits] for post in hot_posts: print(post.title) Thank you very much for all your help, it is really awesome!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "27oz_is_800ml", "created_utc": 1398375979, "gilded": 0, "name": "t1_ch16opg", "num_comments": null, "score": 3, "selftext": "It's worth noting that [snudown](https://github.com/reddit/snudown) has some differences to markdown. PRAW has submission.selftext_html which should be easier to use.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23w0jq/is_there_a_library_to_translate_markdown_to_html/"}, {"author": "_Daimon_", "created_utc": 1398323594, "gilded": 0, "name": "t1_ch0nn2s", "num_comments": null, "score": 3, "selftext": "Yes. They are in the attributes `ups` for upvotes and `downs` for downvotes. You can use [standard python introspection](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) to see what attributes an object has, sometimes you might think the name is a bit strange. But it's always easy to find when you know what value the value the attribute you're looking for should be.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23u4eh/getting_the_number_of_upvotes_and_downvotes_of_a/"}, {"author": "Doctor_McKay", "created_utc": 1398356891, "gilded": 0, "name": "t1_ch0ww7t", "num_comments": null, "score": 1, "selftext": "I've never used praw, but try setting the limit to 100, as that's the most reddit will return anyway. Passing None might result in praw sending no limit parameter to reddit, making it default to 25.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bboe", "created_utc": 1398357758, "gilded": 0, "name": "t1_ch0xbw7", "num_comments": null, "score": 2, "selftext": "`limit=None` here passes in an appropriate no-restriction limit. The issue as I believe is there is a certain amount of error in relying on real-time listings. `comment_stream` does not keep a huge cache of seen comments, and stops yielding new comments (in that iteration) as soon as it finds one it's already seen: https://github.com/praw-dev/praw/blob/master/praw/helpers.py#L108 If you play around with the parameters in the _stream_generator, you may be able to make it more sound.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "linuxydave", "created_utc": 1398370927, "gilded": 0, "name": "t1_ch144r8", "num_comments": null, "score": 1, "selftext": "The API guidelines say that you shouldn't be making more than 30 requests per minute. If you make more you're liable to get limited. Now, this means that even if you made a request every 2 seconds you'll miss a lot of comments gives then size of Reddit. Consider pushing your requests through a praw multiprocess handler. That way you'll be able to see the actual requests being made which will help with debugging :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bboe", "created_utc": 1398642134, "gilded": 0, "name": "t1_ch3oqrz", "num_comments": null, "score": 1, "selftext": "Gotcha. The PRAW docs do not really discuss the relationship between a function call and its underlying requests so I understand the confusion.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "JackOfCandles", "created_utc": 1398374979, "gilded": 0, "name": "t1_ch16799", "num_comments": null, "score": 1, "selftext": "> The API guidelines say that you shouldn't be making more than 30 requests per minute. If you make more you're liable to get limited. But the praw documentation says that it handles all the reddit API rules internally. Should this still be an issue?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bobdammit", "created_utc": 1397940068, "gilded": 0, "name": "t1_cgwrvfy", "num_comments": null, "score": 1, "selftext": "ahhh, I haven't tested this out, yet, but I bet I have to add the 'submit' scope to the get_authorize_url(). Let me know if I am wrong here. Edit: Yep, another case of RTFM. I was looking at the source code, instead of the praw docs. Example usage, just in case someone gets stuck in the future. scopes = ['identity', 'submit'] # and whatever else you want to add r.get_authorize_url('uniqueKey', scopes, True)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23gl7i/submitting_comments_with_oauth2/"}, {"author": "_Daimon_", "created_utc": 1397941257, "gilded": 0, "name": "t1_cgwsdlm", "num_comments": null, "score": 1, "selftext": "Yep, sounds about right. If you get an error about insufficient permissions while using OAuth, the required scope should be listed in the exception. For most functions, the Oauth scope required is stored in the docstring of each function. For instance [sticky](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.sticky) requires modposts scope to use via OAuth.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23gl7i/submitting_comments_with_oauth2/"}, {"author": "_Daimon_", "created_utc": 1397900129, "gilded": 0, "name": "t1_cgwfl7r", "num_comments": null, "score": 1, "selftext": "Yeah, Reddit recently changed the [permissions to use get_flair](http://www.reddit.com/r/redditdev/comments/1xreor/has_there_been_a_change_to_the_permissions/) to moderators only. There's no way via the API (which is what PRAW uses ) to directly retrieve a users flair on a subreddit. If you have a submisson/text, then you can still retrieve the authors flair. import praw r = praw.Reddit(UNQUE_AND_DESCRIPTIVE_USERAGENT) s = r.get_submission(\"http://www.reddit.com/r/DotA2/comments/23f929/live_discussion_sltv_star_series_season_9_day_3/\") print(s.author_flair_css_class)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23es5v/getting_the_flair_of_a_comment/"}, {"author": "testingpraw", "created_utc": 1397748619, "gilded": 0, "name": "t1_cguw5kr", "num_comments": null, "score": 0, "selftext": "Okay, so if I do something like: credentials = {'user': username, 'passwd': password, 'api_type': 'json',} headers = {'user-agent': '/u/testingpraw comment stream',} session_client = requests.session() r = session_client.post('http://www.reddit.com/api/login', data = credentials, headers=headers) the_json = json.loads(r.text) session_client.modhash = j['json']['data']['modhash'] ...grab the data with credentials session_client.get('http://reddit.com/r/subreddit/12345.json', headers=headers) Reddit will not send cached results?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2370hr/comment_stream_is_there_anyway_to_bypass_reddits/"}, {"author": "_Daimon_", "created_utc": 1397483623, "gilded": 0, "name": "t1_cgs2jiq", "num_comments": null, "score": 2, "selftext": "Why don't you just use [OAtuh2](http://praw.readthedocs.org/en/latest/pages/oauth.html)? In this way users can give you permission to do some stuff on your behalf. It can be limited, so users know for certain that you won't say delete their old submissions or pm girls on /r/gonewild from their account. Plus the access can last forever, which is nice, but the user can also revoke your authorization at any point they want. Which is even nicer. Also. PRAW has excellent support of OAuth and nobody seems to be really using it, which kinda annoys me since we spent quite some time last year refactoring the library to allow OAuth to be easily used. So as a bonus you'll make me happier :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "testingpraw", "created_utc": 1397502174, "gilded": 0, "name": "t1_cgsbbv9", "num_comments": null, "score": 1, "selftext": "Follow up question. I got everything working, however, is there some sort of boolean that I can check to see if the user has allowed the use of oauth? For logins, PRAW has is_logged_in, is there anything similar to that? I want users that aren't logged in to be able to read the feed, however, if users want to comment on a thread they must be logged in.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "_Daimon_", "created_utc": 1397502743, "gilded": 0, "name": "t1_cgsbm79", "num_comments": null, "score": 2, "selftext": "Yep. It is called [is_oauth_session](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.AuthenticatedReddit.is_oauth_session). You can also use [get_me](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L1136) which returns the user currently authenticated via oauth login. It is a `LoggedInRedditor` object, so it works just like `r.user` which returns the user/passwd authenticated user. The methods that deal with uauthentication are located in [AuthenticatedReddit](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.AuthenticatedReddit). This object is a subset of the main `Reddit` object, mainly seperated out for clarity and maintainability.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "OnceAndForever", "created_utc": 1397203409, "gilded": 0, "name": "t1_cgpnxi8", "num_comments": null, "score": 3, "selftext": "Are you getting any exceptions when you try to run the script? I notice a couple things in your snippet that could cause problems. On line 5, you have written `submission_id = \"id\"`. Since `id` is in quotations, python interprets that as a string and does not actually use the value of the `id` variable on line 4. You are also missing a quotation mark after output.csv on line 14. I don't think that line 6 is necessary. You could include the comment limit as a parameter to the `get_submission` method on line 5, which would look like this: submission = r.get_submission(submission_id=id, comment_limit=None) Also, line 8 would throw an exception because there is no `subm` object in your given snippet. That line should instead read `submission.replace_more_comments()`. To flatten the comment tree, use `praw.helpers.flatten_tree()`. Line 9 would then look like this: comments = praw.helpers.flatten_tree(submission.comments) As for you for loop from lines 11 to 13, you do not need to use `enumerate` because you do not actually use the index (`idx`) for anything within the loop so there is no need to keep track of it. You could simply loop through them by using `for comment in comments:`. You have also already `replaced_more_comments()` on line 8, so there is no need to check the comment type against `praw.objects.MoreComments` on line 12. If you wish to grab a comments body, author, and author comment karma, you would just use the various attributes of the `comment` object. This section may look like this: for comment in comments: users.append([comment.author.name, comment.body, comment.author.comment_karma]) It may be easier to use python's `csv` module to write the csv file. It is part of the standard library and you could read more about it [here](https://docs.python.org/2/library/csv.html). You can use it select your delimiter and quotation handling among other things, but a simple example could look like this: with open(\"output.csv\", \"wb\") as output: writer = csv.writer(output) writer.writerows(users) Here is everything in it's entirety to make it easier to see: import praw import csv r = praw.Reddit(user_agent='RaffleCommentBot by /u/alexratman') r.login('RaffleCommentBot', '##Password##') id = \"22p30f\" submission = r.get_submission(submission_id=id, comment_limit=None) print \"submission get!\" submission.replace_more_comments() comments = praw.helpers.flatten_tree(submission.comments) users = [] for comment in comments: users.append([comment.author.name, comment.body, comment.author.comment_karma]) with open(\"output.csv\", \"wb\") as output: writer = csv.writer(output) writer.writerows(users) Keep in mind that praw sends another request to the API every time it accesses a users comment karma. There is a 2 second delay between requests, so this script could take a while to run depending on how many comments there are in a submission. You may also want to handle the formatting of `comments.body`s because new line characters could mess with the formatting of your csv file. You may also want to handle unicode characters that may appear in comments because they could throw `UnicodeEncodeError` exceptions when trying to write them to the csv. The csv module does not support unicode input, but there is some information on the csv documentation page for handling this. If it is too much of a problem, you could also install other csv writing/reading modules that are not part of the standard lib. Finally, if you are trying to select a random comment from a submission based on certain criteria for a raffle, you may find this tool helpful: http://redditraffle.com/ Hopefully this is clear. I had a slightly more detailed reply typed but I somehow lost it when I tried to comment the first time.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ql4t/is_there_a_way_to_pull_comments_and_additional/"}, {"author": "linuxydave", "created_utc": 1396884543, "gilded": 0, "name": "t1_cgmafrn", "num_comments": null, "score": 2, "selftext": "PRAW is ideal for this. It's a powerful library for interacting with the Reddit API and that's why it's so popular with bot creators (and probably why so many bots are written in Python, haha).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22fhy9/praw_or_just_do_direct_harvesting_for_collecting/"}, {"author": "zynix", "created_utc": 1396885526, "gilded": 0, "name": "t1_cgmaw74", "num_comments": null, "score": 1, "selftext": "Hmm, I guess I will need to pull praw into a ipython notebook and do some experimentation. As a sanity check, I didn't see anywhere where I could use my raw username/password credentials ( if my computer is compromised I'm screwed anyway, my reddit accounts being the least of my concern ). edit: Also through the oauth privileges request API, I saw that these endpoints `get_comments, get_new_by_date (and the other listing funcs), get_submission, get_subreddit, get_content, from_url` are available which is 50-60% of what I want, but does that include harvesting my (up|down)vote's? Additional: http://www.reddit.com/dev/api#GET_user_{username}_disliked This is the other endpoint I need but I don't see what oauth privilege I need to provide to get access to it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22fhy9/praw_or_just_do_direct_harvesting_for_collecting/"}, {"author": "linuxydave", "created_utc": 1396885879, "gilded": 0, "name": "t1_cgmb298", "num_comments": null, "score": 2, "selftext": ">As a sanity check, I didn't see anywhere where I could use my raw username/password credentials Most of the stuff can be found in the [code overview](https://praw.readthedocs.org/en/latest/pages/code_overview.html) import praw r = praw.Reddit(user_agent='Bot UserAgent') r.login(username='', password='') Hope that helps :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22fhy9/praw_or_just_do_direct_harvesting_for_collecting/"}, {"author": "_Daimon_", "created_utc": 1396806057, "gilded": 0, "name": "t1_cgljukl", "num_comments": null, "score": 2, "selftext": "The method [get_unread](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.PrivateMessagesMixin.get_unread) returns a listing of unread messages. If it's empty, then there's no unread orangereds. You can also use the `has_mail` attribute of the authenticated user object `r.user`. But this is a bit uncertain, as those attributes are set on logging in and not updated whenever the user object is accessed.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22crls/praw_check_for_new_mail/"}, {"author": "Mustermind", "created_utc": 1396904434, "gilded": 0, "name": "t1_cgmkiey", "num_comments": null, "score": 3, "selftext": "This should work: import praw r = praw.Reddit(user_agent=\"parentfinder or something\") comment = r.get_info(thing_id=\"t1_cgmdnwg\") # or some other comment if not(comment.is_root): parent_author = r.get_info(thing_id=comment.parent_id).author print(str(parent_author))", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ap4k/author_of_parent_comment/"}, {"author": "_Daimon_", "created_utc": 1396458026, "gilded": 0, "name": "t1_cgi9duk", "num_comments": null, "score": 1, "selftext": "> I have written some code with Praw that uploads (overwrites) some already existing images in the stylesheet. This image is coded into the sidebar. The problem that I have is if I upload the the new image to the stylesheet, it won't be updated in the sidebar until I save the stylesheet. This is how reddit rolls and is the same no matter how you change an image. What's the problem with saving the stylesheet after updating the image?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/220tbz/update_image_in_sidebar/"}, {"author": "_Daimon_", "created_utc": 1396457930, "gilded": 0, "name": "t1_cgi9c5m", "num_comments": null, "score": 2, "selftext": "Yes you should. Basically before \"get_new\" returned *either* the new listing or the reddit listing depending on your reddit preference, which caught quite a few people unaware. This wasn't a PRAW thing, it was how the Reddit API worked. If you wanted to be sure you actually got the new listing, you had to send a argument with the request. Which is what get_new_by_date did. Then Reddit decided it would be a good idea to split new and rising into separete listings (and it is a good idea). Which made get_new_by_date obsolete, because now get_new always returned the new listing. What specific problem are you encountering in your transition?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/220p0f/question_about_praws_get_new_by_date_vs_get_new/"}, {"author": "SOTB-human", "created_utc": 1396051542, "gilded": 0, "name": "t1_cgekghl", "num_comments": null, "score": 1, "selftext": "I can't say for certain without seeing your code, but I think your issue might be that you're trying to reply to a comment that was fetched with an unauthenticated session. In PRAW, reddit \"things\" (like comments, submissions, etc.) are associated with the session in which you fetched them, which is not going to be the same as the `self.r` session attached to the bot objects. In order to handle the comment with a different session, you have to re-fetch it with `get_url` or whatever it is.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/21mtuq/praw_multiple_bots_multiple_threads/"}, {"author": "linuxydave", "created_utc": 1395651355, "gilded": 0, "name": "t1_cgafyuw", "num_comments": null, "score": 1, "selftext": ">I installed praw (i guess) What steps did you do? From the error message I take it you're running Mac OS?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/217ivx/im_a_noob_that_cant_seem_to_be_able_to_install/"}, {"author": "nissoPT", "created_utc": 1395678959, "gilded": 0, "name": "t1_cganmn3", "num_comments": null, "score": 1, "selftext": "It's working now, i think i installed praw in the wrong folder.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/217ivx/im_a_noob_that_cant_seem_to_be_able_to_install/"}, {"author": "kemitche", "created_utc": 1395590931, "gilded": 0, "name": "t1_cg9t7cv", "num_comments": null, "score": 1, "selftext": "Hrm. PRAW docs aren't super clear, but if the arguments map the way I think they do, you need to send your query like so: query = scraper.search(query=\"(and text:'FOTD' %s\" % postperiod, subreddit='makeupaddiction',sort='new',syntax='cloudsearch') I *believe* the `period` parameter maps to the \"today/this week/this month/this year/all time\" drop down, not the semi-hidden timestamp stuff.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "makeupdev", "created_utc": 1395591990, "gilded": 0, "name": "t1_cg9tlvq", "num_comments": null, "score": 1, "selftext": "Thanks... this seems to return no submissions unfortunately, and only one if I pass in 'all time' or 'today' instead of the timestamp. I was basing the post period on [this](https://github.com/praw-dev/praw/issues/231) GitHub issue.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "OnceAndForever", "created_utc": 1395563277, "gilded": 0, "name": "t1_cg9njgj", "num_comments": null, "score": 1, "selftext": "I don't think you need to bother with the `datetime` unless you need a specific time period. Otherwise sorting by new and setting your limit to 3000 will give you the latest posts that match your search terms. I got it to work using this code: import praw r = praw.Reddit('YOUR USER AGENT') subreddit = r.get_subreddit('MakeupAddiction') search = subreddit.search('FOTD', sort='new', limit=3000) for submission in search: print submission This returned 998 results, which is the same amount I got when using the [reddit search](http://www.reddit.com/r/MakeupAddiction/search?q=fotd&sort=new&restrict_sr=on). I don't believe reddit allows you look further back than 1000 posts, so if you needed to get 3000 entries you would have to run this search every few days and add the new entries to your list. Otherwise, you could try using the [after_field parameter](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit.get_content) and pass the thing_id of the oldest available entry to see if you can restart the search with the oldest entry as your starting point. Here is a small sample of my results: 3 :: [FOTD] I was feeling a little red today. First daytime outing without my... 47 :: I've been doing my makeup Korean style for a long time. I wanted to sho... 40 :: The lighting was too good to not take a bunch of pictures! [FOTD] 17 :: [FOTD] Haven't posted recently, and wanted to do something bright and f... 6 :: Tried something new, FOTD using MAC Dreaming Dahlia. CCW! 7 :: [FOTD] Standard Red Lip + Wings. CCW 18 :: Saturday FOTD. A little more dramatic eye than I'm used too! CCW 16 :: My FOTD, running around Manhattan. It finally feels like spring in NYC!... 320 :: FOTD: Using my Too Faced Natural Eye palette! I don't know what data you need for your project, but in the `for` loop you can type `print vars(submission)` to see the attributes available to you. I would set the limit on line 6 to a much smaller number when you check out the attributes though. There is also no need to do `time.sleep()` when using praw because praw handles that for you and already delays every request by 2 seconds. Hope this helps and leads you in the right direction.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "_Daimon_", "created_utc": 1394744573, "gilded": 0, "name": "t1_cg1wbha", "num_comments": null, "score": 2, "selftext": "Update your version of PRAW to 2.1.13 or later and it will work like the other listings like get_hot. pip install praw -U There was a [recent change to banlist](http://www.reddit.com/r/changelog/comments/1ydu2d/reddit_change_paginated_ban_lists/) that fundamentally changed how the data for banlists are returned, which means older versions of PRAW doesn't really understand these listings anymore. Shoutout to **Andre-d** for sending the [pull request](https://github.com/praw-dev/praw/pull/280) that updated PRAWs functionality to handle the new userlistings.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20a9hl/how_to_iterate_the_ban_list_with_praw/"}, {"author": "LungFungus", "created_utc": 1394520516, "gilded": 0, "name": "t1_cfzo741", "num_comments": null, "score": 3, "selftext": "You are looping over comments of r/all, the only time your bot takes action is if one of those comments is from your friend. The problem is that you are not exhaustively looping over all comments made to r/all, so the likelihood of finding a comment from your friend is low. For a better way of searching comments for your friend, why not just periodically grab the comments he actually makes? Alternatively, maybe look at something like praw's comment stream helper.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2032ul/what_is_wrong_with_this_bot/"}, {"author": "LungFungus", "created_utc": 1395540147, "gilded": 0, "name": "t1_cg9gzmm", "num_comments": null, "score": 1, "selftext": "It looks like you're already doing it somewhat with user.get_comments, I believe user.get_comments only returns comments for that user. I would do something like: import time import praw from random import randint r=praw.Reddit(\"Annoybot\") r.login(\"annoysterninator\",\"secret\") responses = [\"lick\", \"I found the vegan!\", \"DdddddddDrop the bass\", \"twerk eth twerk eth twerk\", \"what nice leg muscles you have\", \"The qua-a-adratic formula\", \"it is ethjck twerk music swag money\"] already_seen = set() while True: user = r.get_redditor(\"bassguitarman\") user_comments = user.get_comments(limit=25) # tweak this for comment in user_comments: if comment.id not in already_seen: already_seen.add (comment.id) reply = responses[randint(0, len(responses) - 1)] comment.reply(reply) time.sleep(60 * 60) # sleep for an hour. likely no need to constantly check I didn't test this, I essentially just tweaked your code. So if there was some other bug in there, it will still be there. If you get errors, you can post them. Food for though: if your bot crashes, and you restart it, you will very likely re-reply to some comment. I recommend storing the already_seen set to disk somehow, perhaps just a file with a comment id per line for something really simple.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2032ul/what_is_wrong_with_this_bot/"}, {"author": "_Daimon_", "created_utc": 1394217512, "gilded": 0, "name": "t1_cfwvuhn", "num_comments": null, "score": 1, "selftext": "PRAW doesn't inhibit the rate you can submit stuff, only reddit does that. There are some lower limit on how often you can submit stuff and this is based on your karma both total and on a subreddit level. So there's fewer restrictions on submissions in a subreddit where you've gained loads of karma, then in one where you've never posted before like r/test or other testing subreddits.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "ravik78c", "created_utc": 1394226910, "gilded": 0, "name": "t1_cfx0b8u", "num_comments": null, "score": 1, "selftext": "According to my information RATE_LIMIT exception is thrown by the server if the something has happened too frequently. My error which is being thrown is : praw.errors.APIException: (QUOTA_FILLED) `You've submitted too many links recently. Please try again in an hour.` on field `None` If it can be fixed by RATE_LIMIT then I can change the respective tag in the configuration file (development.ini) in my reddit source code. But I think the problem is somewhere else. Please correct me if I am wrong.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "chpwssn", "created_utc": 1394155555, "gilded": 0, "name": "t1_cfwc9hu", "num_comments": null, "score": 1, "selftext": "I actually asked this 35 days ago. http://www.reddit.com/r/redditdev/comments/1wkn3s/praw_on_a_reddit_clone/ Edit: sorry for the lazy link I'm on mobile", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zroo7/praw_using_praw_for_local_reddit_instance/"}, {"author": "ravik78c", "created_utc": 1394156697, "gilded": 0, "name": "t1_cfwcrj8", "num_comments": null, "score": 1, "selftext": "I am able to able to submit links now to my local instance. Thanks for the help but I know about the link you are referring to. 'READTHEDOCS' site is not clear as what to do exactly to do this. However, I can submit the links after logging into my local instance but after submitting the link it is giving this error - >>> r.submit('REDDIT_TEST1', 'REDDIT_API_TEST2', text=None, url='www.google.com', captcha=None, save=True, send_replies=None) Traceback (most recent call last): File \"\", line 1, in File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 225, in wrapped return function(obj, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 2029, in submit return self.get_submission(url) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 891, in get_submission params=params) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 851, in from_url submission.comments = c_info['data']['children'] AttributeError: 'dict' object has no attribute 'comments' Link is getting submitted though. But I suppose I can add only one link in one hour which is not good for my project as I need to add unlimited links to my local instance.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zroo7/praw_using_praw_for_local_reddit_instance/"}, {"author": "_Daimon_", "created_utc": 1402771404, "gilded": 0, "name": "t1_ci7iyz3", "num_comments": null, "score": 1, "selftext": "That's because of lazy objects, which PRAW utilizes to greatly optimize the run time of projects. See this article https://praw.readthedocs.org/en/v2.1.16/pages/lazy-loading.html in our documentation for more details.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "gavin19", "created_utc": 1393962606, "gilded": 0, "name": "t1_cfubh4d", "num_comments": null, "score": 2, "selftext": "From the [tutorial](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html#finding-what-we-need). See the *over18* property.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "saltyjohnson", "created_utc": 1393984365, "gilded": 0, "name": "t1_cfulyk5", "num_comments": null, "score": 1, "selftext": "For your reference, https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html Scroll down a few paragraphs and you'll see all the attributes and their values when you look at a specific submission in PRAW. ^(I may have posted this from the wrong account at first. My apologies.)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zgt0d/basic_praw_question_getting_comment_posted_time/"}, {"author": "_Daimon_", "created_utc": 1393278239, "gilded": 0, "name": "t1_cfnptyn", "num_comments": null, "score": 1, "selftext": "What version of python are you using? EDIT: Also, what's your version of six? EDIT2: Problem found. PRAW now requires version 1.4 or later of the six module. To fix this problem run pip install six==1.4 or pip install six -U if you want the latest version. Which is better ff you don't have any need for a specific version of six, say you only have it installed due to PRAW. I'm running the test suite with six 1.4 installed now, if no problems occur I'll push another version out tonight. Thanks for bringing this to my attention.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ytpmx/latest_praw_not_working/"}, {"author": "_Daimon_", "created_utc": 1392929135, "gilded": 0, "name": "t1_cfkeznc", "num_comments": null, "score": 1, "selftext": "Here is the [issue raised with urllib3](https://github.com/shazow/urllib3/issues/238) for your problem. PRAW uses requests which uses urllib3, so the warning will go away when you have a version of urllib3 with the merged fix. However, from what I can see it *should* be in the latest version of requests. Which it obviously isn't. The fix was included in version 1.71 of urllib3, which was created 5 months ago and the version of urllib included within requests is 1 month old, or more precisely [this commit](https://github.com/shazow/urllib3/commit/9346c5c8b97ccd06d4210401f3fdb9ed71ed1cd3). Looking at the [release history of requests](https://pypi.python.org/pypi/requests/2.2.1) there should have been several releases after the fix was included in urllib3. Either I misread something and your issue is related to something non fixed or there's something else I've misunderstood about the inclusion of urllib3 into requests and their release schedule. If you want more info, then you should raise an issue with urllib3/requests or send them a mail. Either way. This is a *warning*, not an *error* and it is harmless with python version below", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "markerz", "created_utc": 1392753656, "gilded": 0, "name": "t1_cfik168", "num_comments": null, "score": 2, "selftext": "Interesting. With just the math, 4500 comments / (25 minutes * 30 requests / minute) makes for 6 comments per request. I don't use PRAW and I believe the it should already employ these optimizations but you can specify the limit query parameter which allows you to pull more comments per request. For example, http://reddit.com/?limit=100 gives you the first 100 links from the front page. Alternatively, I believe the max for comments is 500 and even higher if you have reddit gold. Edit: just did a quick test and got this message: Sorry, the maximum number of comments is 500. (However, if you subscribe to reddit gold, it goes up to 1500.) The more children api should support it all the same.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y9lu7/praw_replace_more_comments_rate_limiting/"}, {"author": "bboe", "created_utc": 1392761711, "gilded": 0, "name": "t1_cfioa13", "num_comments": null, "score": 2, "selftext": "If you spend some time with the \"load more comments\" links on the site you'll notice that for many of those links, the browser will only pull in 1 additional comment. The API suffers the same limitation. If you want a speed improvement, use a reasonable threshold for `replace_more_comments` so that it only makes requests that yield a reasonable number of comments. Edit: For posterity, when I wrote the more comments handling in PRAW I tried to group all the unfetched comment ids together to reduce the number of requests. However, reddit's API, does not work in that way, thus if you're trying to fetch a large comment tree it's going to take some time.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y9lu7/praw_replace_more_comments_rate_limiting/"}, {"author": "_Daimon_", "created_utc": 1392736987, "gilded": 0, "name": "t1_cficacy", "num_comments": null, "score": 1, "selftext": "You can't. It's not a feature in reddit so PRAW cannot implement it. See your own inbox http://www.reddit.com/message/inbox/ and notice there's no way to delete it in the webview. If this is the case then it would be extremely unlikely to be possible via the API which PRAW uses.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y8gt3/what_is_the_proper_way_to_delete_a_message/"}, {"author": "_Daimon_", "created_utc": 1392738970, "gilded": 0, "name": "t1_cfid2s7", "num_comments": null, "score": 1, "selftext": "[`get_unread`](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.PrivateMessagesMixin.get_unread) is like `get_inbox` except it only returns unread messages. You can use that instead to easily poll for unread messages :) On a sidenote. It would be great if you *could* download messages.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y8gt3/what_is_the_proper_way_to_delete_a_message/"}, {"author": "bboe", "created_utc": 1392762983, "gilded": 0, "name": "t1_cfiox5t", "num_comments": null, "score": 2, "selftext": "There's a bug in how reddit's json for a submission returns the equivalent of \"load more comments\" and \"continue this thread\" as they both appear as a \"More\" type in the json. Take a look here for instance: http://www.reddit.com/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/c5w6n63 Notice the \"continue this thread\" link which takes you to: http://www.reddit.com/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/c64b5qb First of all, even through the web interface, when you specify to limit the number of comments, you will be oblivious to the fact that the above comment exists (after loading more comments): http://www.reddit.com/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/c5w6n63?limit=14 The primary issue, however, is that the \"continue this thread\" in the json view is not correct as it appears like: \"kind\":\"more\", \"data\":{ \"count\":0, \"parent_id\":\"t1_c64b5qb\", \"children\":[ ], \"name\":\"t1__\", \"id\":\"_\" } PRAW ignores such requests which means all \"continue this thread\" comments are not being included. If you find a nice way to handle that problem, please consider a PRAW pull-request. Regarding the sort order, you can specify the comment sort order in `get_submission`: https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_submission", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y7pq7/help_with_retrieving_all_comments_for_a_given/"}, {"author": "_Daimon_", "created_utc": 1392564212, "gilded": 0, "name": "t1_cfgpdxa", "num_comments": null, "score": 2, "selftext": "Generally you want to replace all MoreComment objects with the Comment objects they represent. It's detailed in [the Comment Parsing](http://praw.readthedocs.org/en/latest/pages/comment_parsing.html#the-number-of-comments) page in the documentation. If you just want to replace a single MoreComment object, then see the link kemitche provided,", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/"}, {"author": "kemitche", "created_utc": 1392529119, "gilded": 0, "name": "t1_cfgiaib", "num_comments": null, "score": 1, "selftext": "http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.MoreComments", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/"}, {"author": "thunder_afternoon", "created_utc": 1392304212, "gilded": 0, "name": "t1_cfecdkd", "num_comments": null, "score": 1, "selftext": "Why don't you just [search](https://praw.readthedocs.org/en/latest/pages/code_overview.html?highlight=search#praw.objects.Subreddit.search) for the URL in question? You can use the URL query. For example: http://www.reddit.com/r/nfl/search?q=url%3Ahttp%3A%2F%2Fwww.nola.com%2Fsaints%2Findex.ssf%2F2014%2F02%2Fnew_orleans_saints_will_smithr.html&restrict_sr=on", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xrkhi/get_a_submission_object_based_on_the_url_of_the/"}, {"author": "OnceAndForever", "created_utc": 1392026749, "gilded": 0, "name": "t1_cfblhs8", "num_comments": null, "score": 3, "selftext": "I've been playing around with this a bit, and I think I got it working. If you look look at the permalink of an inbox message, it looks something like this: `http://www.reddit.com/message/messages/1jkdxs`. You can use praw's `get_content()` method to fetch this url. For example: import praw r = praw.Reddit(USER AGENT) r.login(USERNAME, PASSWORD) message_permalink = r.get_content(url='http://www.reddit.com/message/messages/1jkdxs') for message in message_permalink: message.reply('This is your reply to the message') In this example, `message_permalink` is a [get_content generator](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit.get_content), and message is a [praw.objects.Message object](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Message), which is based off of the [Inboxable object](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Inboxable). As for storing the message ids, use the `get_inbox()` ( or `get_unread()` depending on what you're doing) and then loop through all your messages. The `id` attribute is the one you're looking for. messages = r.get_inbox() for message in messages: print message.id print message.body >>> 1jkdxs >>> this is a test You can use the `id` to build the url you will use for `get_content()` because it will always be `'http://www.reddit.com/message/messages/' + id`. Hope this helps.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xi8ol/praw_how_to_store_an_inbox_message_in_a_database/"}, {"author": "zd9", "created_utc": 1391900340, "gilded": 0, "name": "t1_cfaholo", "num_comments": null, "score": 1, "selftext": "Okay, thanks! I was looking through the PRAW docs, and didn't see `.subreddit` as a property of `comment`", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xe0fw/praw_check_what_subreddit_a_comment_is_from/"}, {"author": "_Daimon_", "created_utc": 1391786414, "gilded": 0, "name": "t1_cf9euze", "num_comments": null, "score": 1, "selftext": "It's a warning in a dependency of a dependency of PRAW that it's doing something that isn't supported anymore. What's your version of `requets`? Run this command in the terminal `pip freeze | grep requests`. You should try upgrading it `pip install requests -U` and see if that helps. That said, it looks like a non-serious warning so you probably won't encounter problems related to it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9wzh/having_trouble_logging_in_with_praw/"}, {"author": "Deimorz", "created_utc": 1391808387, "gilded": 0, "name": "t1_cf9p1sj", "num_comments": null, "score": 3, "selftext": "> In the praw \"tutorial\" for writing a bot, they store a list of comments ids for each time an action is performed. What if the bot needs to be restarted? If it's not a quick script and a record of what it's already done is something that you need to store over a longer period, you should be persisting it across runs somehow by using something like a sqlite database (or just a text file, if your needs are basic). > What about checking the replies to a comment to see if the bot has already replied? Is there anything that can go wrong there? The biggest problem with this is that it will probably require far more requests (which slows your bot down a lot, since it can only make one request every 2 seconds). Most bots that are replying to things are using an \"out-of-context\" listing, like http://www.reddit.com/r/all/comments, or the /comments page for a particular multireddit/subreddit. You won't have any way to tell which comments you've replied to from there except by loading the comment thread for each comment, which will require an additional request each.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "decho", "created_utc": 1391878285, "gilded": 0, "name": "t1_cfa8zuo", "num_comments": null, "score": 1, "selftext": "Oh yeah, when you \"scan\" an exact thread it will not come with replies, unlike when you scan a subreddit if that's what you meant. But you can use the praw helpers. I just tested it and returned both comments and replies. submission = r.get_submission(submission_id='1wya5l') comments = submission.comments comments_and_replies = praw.helpers.flatten_tree(comments) for x in comments_and_replies: print x ## it should return everything in the thread. Try it out, it think this might be exactly what you need.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "_Daimon_", "created_utc": 1391764606, "gilded": 0, "name": "t1_cf9a6cb", "num_comments": null, "score": 2, "selftext": "Do you know those problems where you struggle for a long time only to realise it's something trivial? This is one of those problems, [get_mod_queue](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModOnlyMixin.get_mod_queue) has a space in the name :P", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x8q1b/am_i_not_using_get_modqueue_correctly/"}, {"author": "Plague_Bot", "created_utc": 1391395266, "gilded": 0, "name": "t1_cf5lkdc", "num_comments": null, "score": 2, "selftext": "[set_flair(subreddit, item, flair_text='', flair_css_class='')](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModFlairMixin.set_flair) is what you want. To clear the previous one, just have a variable that tracks the last post that the flair was assigned to. When you update the new post, check that variable, and call `set_flair()` on the old post.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "mattster42", "created_utc": 1391407772, "gilded": 0, "name": "t1_cf5qnmn", "num_comments": null, "score": 1, "selftext": "Thank you! I'm assuming I can immediately set that flair when the submission is posted, but I'm having trouble referencing it. From what I understand, praw returns the submission object immediately with a message like: yet when I try to immediately implement: reddit.set_flair('subredditredacted', submission.id, flair_text=\"Flair text\", flair_css_class=\"test\") Traceback (most recent call last): File \"\", line 1, in NameError: name 'submission' is not defined So obviously my assumption that praw automatically converts my submission into a \"submission\" object is flawed. How can I call the latest submission to use in a function like set_flair?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "_Daimon_", "created_utc": 1391109549, "gilded": 0, "name": "t1_cf2wv73", "num_comments": null, "score": 3, "selftext": "Sure :) Look at the [configuration files](https://praw.readthedocs.org/en/latest/pages/configuration_files.html) page and the documentation for [BaseReddit](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit) for details on how to implement this.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wkn3s/praw_on_a_reddit_clone/"}, {"author": "gyroda", "created_utc": 1391175310, "gilded": 0, "name": "t1_cf3k1oj", "num_comments": null, "score": 1, "selftext": "This doesn't seem to be working for me, I'm likely doing something wrong here. Python 2.7.3 (default, Sep 26 2013, 21:37:06) [GCC 4.6.3] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import praw >>> print dir(comment) Traceback (most recent call last): File \"\", line 1, in NameError: name 'comment' is not defined >>> It also doesn't work if I try from praw import * Any idea what's not right?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "Plague_Bot", "created_utc": 1391214188, "gilded": 0, "name": "t1_cf40xbt", "num_comments": null, "score": 2, "selftext": "The problem is you haven't actually defined `comment` yet. `comment` was just an example variable name. You want to do something like this: import praw r = praw.Reddit(user_agent='UNIQUE USER AGENT') comments = praw.helpers.comment_stream(r, 'all', limit=None, verbosity=0) for comment in comments: print dir(comment) break", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "gabrieldain", "created_utc": 1391005611, "gilded": 0, "name": "t1_cf1sis5", "num_comments": null, "score": 2, "selftext": "+1 I learnt praw by using `dir()` and `help()`. You can learn most simple libraries by following a pattern of `dir(library)`, `help(library.element)`, `dir(library.element)` and so on.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "bboe", "created_utc": 1390622719, "gilded": 0, "name": "t1_ceyb8ku", "num_comments": null, "score": 2, "selftext": "In python 3.2+ unclosed file descriptors (those that are not explicitly closed) raise warnings. By default warnings are not logged anywhere, however, upon importing PRAW warnings begin being logged to the console. PRAW probably shouldn't interfere with the console logging settings, nevertheless, even without PRAW the program still has the ResourceWarning as the file descriptor requests uses is not explicitly closed. If I recall correctly, the error will go away if you do: resp = requests.get('http://example.com') resp.close() return 'idk'", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "_Daimon_", "created_utc": 1390590555, "gilded": 0, "name": "t1_cexx9bg", "num_comments": null, "score": 1, "selftext": "It's a python 3.3 warning for unclosed resouce. It got opened and never closed. At first glance id guess it may be a problem with PRAWs `requests` session that's never closed and I cannot see how to do that without magic. Anyway, this is not a critical problem indeed you are unlikely to see any bad effects of this. I have no idea why importing praw would cause this effect. We only do imports and and setting of global variables at startup, no establishing of session or any or any other resource.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "FramesPerSushi", "created_utc": 1390591440, "gilded": 0, "name": "t1_cexxpq2", "num_comments": null, "score": 1, "selftext": "Yeah praw must have messed something up recently (even if it was my fault), because I wasn't getting this error yesterday. For this reason I'm confident that this won't be a problem once I reboot my computer, but it would be nice to know how/what happened so I can prevent it in the future. Sure it may not me fatal to my program, but it hurts internally every time an error pops up.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "_Daimon_", "created_utc": 1390601017, "gilded": 0, "name": "t1_cey2i2h", "num_comments": null, "score": 1, "selftext": "Have you updated your version of PRAW between yesterday and today? AFAIK resource warning is not a warning that will consistently pop up, so it may simple be due to random chance. To be fair it's not an error, it's a warning. But yeah, that's something I'd like to get fixed as well if possible.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "bboe", "created_utc": 1391312581, "gilded": 0, "name": "t1_cf4v161", "num_comments": null, "score": 2, "selftext": "For now, you can disable the reporting of warnings. PRAW _shouldn't_ effect that setting, but for some reason it currently does.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "_Daimon_", "created_utc": 1390668721, "gilded": 0, "name": "t1_ceylc8l", "num_comments": null, "score": 1, "selftext": "No. Neither on PRAW nor requests page. You're welcome to file one if you want to. I'm not sure where it should be filed, probably on the requests page with another issue on the PRAW page linking to the requests page.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "FramesPerSushi", "created_utc": 1390632950, "gilded": 0, "name": "t1_ceyeack", "num_comments": null, "score": 1, "selftext": "No I haven't updated PRAW. I also rebooted my computer and am still getting the warning. This is strange, because the same code yesterday was working fine. I'm not sure what I've done in the meantime to invoke it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "OnceAndForever", "created_utc": 1390297654, "gilded": 0, "name": "t1_ceuyqke", "num_comments": null, "score": 2, "selftext": "How often are you trying to fetch new comments? Most pages on reddit are cached for 30 seconds, so you won't get new data if you keep checking all that frequently. From the [API wiki](https://github.com/reddit/reddit/wiki/API#rules): >* Most pages are cached for 30 seconds, so you won't get fresh data if you request the same page that often. Don't hit the same page more than once per 30 seconds. When using PRAW, the cache would be the same. The other issue you may have is that the cache is longer for unauthenticated requests, so logging in may allow you to get fresh comments more frequently.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vqywg/praw_comment_fetching_problem/"}, {"author": "gabrieldain", "created_utc": 1390342337, "gilded": 0, "name": "t1_cevewor", "num_comments": null, "score": 1, "selftext": "Have you tried using comment_stream? reddit = praw.Reddit(\"app\") counter = 0 for post in praw.helpers.comment_stream(reddit, 'all'): counter += 1 if counter > 100: break else: [code] It reads a bit more hacky, but if it works...", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vqywg/praw_comment_fetching_problem/"}, {"author": "_Daimon_", "created_utc": 1390207095, "gilded": 0, "name": "t1_ceu347h", "num_comments": null, "score": 3, "selftext": "Hey, you're probably looking for `get_unread` and [`set_flair`](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModFlairMixin.set_flair). import praw r = praw.Reddit(UNIQUE_AND_DESCRIPTIVE_USERAGENT_HERE) r.login() # As the moderator that will be receiving the mails sub = r.get_subreddit(YOUR_SUBREDDIT) for message in r.get_unread(): # Parse the message to get the username and css class sub.set_flair(USERNAME, flair_css_class=THE_CSS_CLASS)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "_Daimon_", "created_utc": 1390299543, "gilded": 0, "name": "t1_ceuz179", "num_comments": null, "score": 0, "selftext": "Have you read the [writing a bot](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) guide? (After [getting started](http://praw.readthedocs.org/en/latest/pages/getting_started.html) obviously) It talks about standard python introspection and how to find the methods and attributes you need.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "_Daimon_", "created_utc": 1390206352, "gilded": 0, "name": "t1_ceu2yds", "num_comments": null, "score": 1, "selftext": "Looks like something has gone wrong on Reddit's end. Because accessing the 'alaska88' userpage via the webend results in a 404, but the [json version of the page works fine](http://www.reddit.com/user/Alaska88/about.json). This is why you don't get a 404 in the `get_redditor`line, because everything looks good to PRAW.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vn25q/praw_how_to_do_a_tryexcept_for_a_lazy_object/"}, {"author": "_Daimon_", "created_utc": 1390048277, "gilded": 0, "name": "t1_cesn6ie", "num_comments": null, "score": 1, "selftext": "Sounds like a problem with `pip` that has nothing to do with PRAW or the reddit API. I don't know what the problem is and cannot help you and this seem like the wrong place to ask your question. I'm sure you'll have more luck by asking this this question in r/python, /r/learnpython or on stackoverflow.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": "SeaCowVengeance", "created_utc": 1389764345, "gilded": 0, "name": "t1_cepy4pc", "num_comments": null, "score": 3, "selftext": "I don't know if there is a testing framework for PRAW, if there is, my life could've been much easier. I did manage to make a workaround. I recently developed a [bot](https://github.com/renfredxh/compilebot) and ran into a similar issue to yours. I mocked PRAW functionality by creating inner classes that are identical to the classes in PRAW that my bot needed to use for testing. Check out the [checkProcessUnread class](https://github.com/renfredxh/compilebot/blob/master/compilebot/tests/test_reply.py#L147) in the unit test module for examples of this. The first inner classes are meant to model PRAW classes and have some of the attributes and methods needed for them. In the `setUp` method that is run before each test, you can see `self.r = self.Reddit()` is called, which creates a new \"fake\" reddit object that was defined locally that can be used for testing without actually logging into reddit via the \"real\" Reddit object in the praw module. Similarly, in the actual tests when I need to pass a comment into a function I pass in a new `self.Comment` instead of the comment object from the praw module. This way I have complete control over what all of these objects do during the tests, and additionally none of these tests require the PRAW module to make any requests to reddit.com. Depending on how your bot is set up, it might be best to actually reassign those new \"fake\" classes to the corresponding class in the praw module. So if you make your own reddit Submission class you can say `praw.objects.Submission = self.Comment`. Hopefully this makes sense. Let me know if you have any other questions. **EDIT:** Just realized from the other comments there's actually a library called [mock](http://www.voidspace.org.uk/python/mock/) that pretty much does this for you. Wish I knew about it earlier.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v8sbx/using_praw_with_a_unit_testing_framework_and_mocks/"}, {"author": "idProQuo", "created_utc": 1389780864, "gilded": 0, "name": "t1_ceq1s9v", "num_comments": null, "score": 1, "selftext": "I'm going to try using mock, but I wanted to see if anyone had a solution tailored to this domain (reddit bots) and this framework (praw).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v8sbx/using_praw_with_a_unit_testing_framework_and_mocks/"}, {"author": "LungFungus", "created_utc": 1389800323, "gilded": 0, "name": "t1_ceq69kg", "num_comments": null, "score": 1, "selftext": "There was a feature recently added to praw that allows praw objects to export themselves into JSON format. Later, you can reinstantiate the object using the json. This could be useful for testing if you want to have the same objects each time. (Sorry I just woke up)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v8sbx/using_praw_with_a_unit_testing_framework_and_mocks/"}, {"author": "_Daimon_", "created_utc": 1389583851, "gilded": 0, "name": "t1_ceo6q0r", "num_comments": null, "score": 2, "selftext": "Try [standard introspection](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) on a `Comment` object.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "PointsOutTrains", "created_utc": 1389740353, "gilded": 0, "name": "t1_cepnpxz", "num_comments": null, "score": 1, "selftext": "Also, I didn't realize until now that you are THE praw guy. Subtle", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "_Daimon_", "created_utc": 1389745755, "gilded": 0, "name": "t1_cepq7ex", "num_comments": null, "score": 2, "selftext": "I'm not. There's no \"THE\" PRAW guy. I'm just a maintainer who has written some of the documentation. /u/mellort is the original author and /u/bboe is the other maintainer. He has contributed more commits than me and was the architect of the version 2.0 redesign.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": null, "created_utc": 1389105154, "gilded": 0, "name": "t1_cejk1cy", "num_comments": null, "score": -1, "selftext": "you try RTFM? https://praw.readthedocs.org/en/latest/pages/getting_started.html >Next we can use the functions get_comments() and get_submitted() to get that redditor\u2019s comments and submissions. Both are a part of the superclass Thing as mentioned on the reddit API wiki page. Both functions can be called with the parameter limit, which limits how many things we receive. As a default, reddit returns 25 items. When the limit is set to None, PRAW will try to retrieve all the things.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1umg87/how_to_get_all_posts_from_a_user/"}, {"author": "SwedishBoatlover", "created_utc": 1389109683, "gilded": 0, "name": "t1_cejlkg4", "num_comments": null, "score": 1, "selftext": "Sorry, no I didn't find the correct page. I'm completely new to python as well as PRAW (installed both yesterday), and the documentation is a complete djungle to me thus far. But thank you for helping despite my lack of RTFM *embarrased* A followup question: I'm having trouble finding how to format the time-argument. \"All\" is kind of self-explanatory, but where can I find other ways to format this argument? Can I for example choose to only retrieve submissions/comments posted between two dates? Or say the last six months? I'm used to documentation showing how to actually use the different arguments, so I'm assuming it's just that I can't find it (yes, I have searched for it both on praw.readthedocs.org and google). Edit: I'm also trying to figure out if I can retrieve only unread submissions/comments (from a certain user), I can't find that in the documentation either.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1umg87/how_to_get_all_posts_from_a_user/"}, {"author": "_Daimon_", "created_utc": 1389124776, "gilded": 0, "name": "t1_cejshuf", "num_comments": null, "score": 1, "selftext": "Have you looked at the [writing a bot](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) article? It talks about using python introspection to learn more about objects, what arguments methods can be called with and so forth.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1umg87/how_to_get_all_posts_from_a_user/"}, {"author": "_Daimon_", "created_utc": 1388651877, "gilded": 0, "name": "t1_cefd42r", "num_comments": null, "score": 4, "selftext": "In [PRAW 2.0.12](https://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-0-12) functionality was added to decode &,. By default it is disabled, as it may break some clients. To fix your issue simply switch this feature on. See the [configuration files](https://praw.readthedocs.org/en/latest/pages/configuration_files.html) in PRAW's documentation for more details on this.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "_Daimon_", "created_utc": 1388695251, "gilded": 0, "name": "t1_cefpw05", "num_comments": null, "score": 1, "selftext": "I assumed you were speaking of PRAW which is a client of reddits API since we have the issue (with default settings) and you used us in your example. Maybe those client should use PRAW then ;)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "releasebot", "created_utc": 1388447091, "gilded": 0, "name": "t1_cedpkjl", "num_comments": null, "score": 7, "selftext": "I just use this: r = praw.Reddit(user_agent='releasebot') pw = getpass(prompt='releasebot login: ') r.login('releasebot', pw) It means I have to enter the password each time, but on the other hand I don't have to keep the credentials on file and can publish the script on GitHub.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "_Daimon_", "created_utc": 1388445642, "gilded": 0, "name": "t1_cedoy8a", "num_comments": null, "score": 2, "selftext": "It's a bit complicated, so I'll just copy paste from the docstring of `login` > Look for username first in parameter, then praw.ini and finally if both were empty get it from stdin. Look for password in parameter, then praw.ini (but only if username matches that in praw.ini) and finally if they both are empty get it with getpass. Add the variables user (username) and pswd (password) to your praw.ini file to allow for auto- login. I use it without arguments mainly because this makes it harder for me to accidentally post my access credentials. That's the simple truth :) Thanks for your kind word of the tutorials. Really mean a lot.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "pachufir", "created_utc": 1388490062, "gilded": 0, "name": "t1_cee13d8", "num_comments": null, "score": 1, "selftext": "So, if I could ask some follow up questions, where is the praw.ini located? Should I just make a file in the same directory named praw.ini? And how would I specify the credentials in that file?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "_Daimon_", "created_utc": 1388495882, "gilded": 0, "name": "t1_cee1z0p", "num_comments": null, "score": 2, "selftext": "We have a page on that. http://praw.readthedocs.org/en/latest/pages/configuration_files.html If you have any questions to the content there then I'll be happy to answer them :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "pachufir", "created_utc": 1389003898, "gilded": 0, "name": "t1_ceim0ee", "num_comments": null, "score": 1, "selftext": "Sorry about the late response, but I have been traveling so I haven't had consistent internet access. Anyway i tried to make a local configuration file in my working directory, named it praw.ini and saved the credentials in this format: [bboe] domain: www.reddit.com ssl_domain: ssl.reddit.com user: bboe pswd: this_isn't_my_password however, when i run my script it still prompts me for login info in the terminal. I'm not sure what I'm doing wrong? Anyway, thanks for your support, it has been incredibly helpful!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "_Daimon_", "created_utc": 1389523755, "gilded": 0, "name": "t1_cenlwny", "num_comments": null, "score": 1, "selftext": "The word inside the hard brackets is the name of the reddit instance you want to connect to or DEFAULT for default settings that may be overriden by specific instance settings. If you create a praw.ini file as below (obviously changing to your real pswd). [DEFAULT] user: pachufir pswd: your_password", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "pachufir", "created_utc": 1388489896, "gilded": 0, "name": "t1_cee12fy", "num_comments": null, "score": 1, "selftext": "So, if I could ask some follow up questions, where is the praw.ini located? Should I just make a file in the same directory named praw.ini? And how would I specify the credentials in that file?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "pachufir", "created_utc": 1388350388, "gilded": 0, "name": "t1_cecrm7k", "num_comments": null, "score": 2, "selftext": "Ah, okay, I guess this makes sense. I was just hoping that praw would be able to recognize hyperlinks within a text. This will work though, so thanks!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "DarkAutumn", "created_utc": 1388052886, "gilded": 0, "name": "t1_ceaerbu", "num_comments": null, "score": 3, "selftext": "The reddit API is a simple GET/POST http/json API that doesn't require PRAW to use. The easiest way to do what you are looking to accomplish is to run something like Fiddler to monitor HTTP requests of a PRAW application (like the collect_data.py you linked), then replicate those raw http requests/json parsing into your new language. The reddit API documentation similarly can help resolve questions when you don't exactly know what data is being passed around. You don't have to rewrite praw in R, but a few helper functions to pull and parse the data you need is really not that difficult (assuming R has usable HTTP and JSON libraries...I've never used the language). I've done this same thing in C# when I couldn't find a suitable C# Reddit library for some side projects.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tq6kg/any_documentation_or_tutorials_for_using_the_api/"}, {"author": "_Daimon_", "created_utc": 1387655774, "gilded": 0, "name": "t1_ce77tf2", "num_comments": null, "score": 2, "selftext": "I'm not quite sure if you're also asking when `get_subreddit_recommendations` was added to PRAW. Just in case you were, it was added in 2.1.8. You can read [it's changelog entry](http://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-1-8) to see more details about it and the other new features/changes that have been made to PRAW since.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tay79/noticed_new_api_call_get_subreddit_recommendations/"}, {"author": "_Daimon_", "created_utc": 1386875248, "gilded": 0, "name": "t1_ce06anq", "num_comments": null, "score": 2, "selftext": "What's your version of PRAW? Prior to 2.1.6 you could experience multiple postings if reddit failed after inserting the comment into the database and returning a successful response to PRAW. See the [changelog](http://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-1-6) for more details.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "_Daimon_", "created_utc": 1387654705, "gilded": 0, "name": "t1_ce77fv3", "num_comments": null, "score": 1, "selftext": "I think the error is in your code, maybe in the way you handle exceptions. Either that or it's a uncovered corner case of the bug that was fixed in 2.1.6. If there was a a bug in the latest version of PRAW that consistently caused double postings then there would have been a lot more posts about it. I'll need to see your bots code and dig into that before I can provide any more help.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "Plague_Bot", "created_utc": 1385917954, "gilded": 0, "name": "t1_cdqzv7f", "num_comments": null, "score": 2, "selftext": "So in the context of PRAW, how do I catch that? My googling is coming up with [urllib2](http://docs.python.org/2/howto/urllib2.html), but from what I can tell, since PRAW deals with fetching data, that module is redundant. Sorry I'm a bit new to python and haven't done much error catching before.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "Pathogen-David", "created_utc": 1385939720, "gilded": 0, "name": "t1_cdr9bs8", "num_comments": null, "score": 1, "selftext": "No problem, In a nutshell, you can catch an exception in Python like this: try: r.somePrawCall() r.orMaybeABunchOfThem() except HTTPError: print \"An HTTP error happened! Oh no!\" You might want to read up on exceptions in Python in general: http://docs.python.org/2/tutorial/errors.html Exceptions in general (in Python and otherwise) are thrown/raised to signal that the program is in an inconsistent state, meaning that something bad happened and the program can't recover from it or thinks you should decide how to recover from it. When you make a try-catch block, you are basically giving Python instructions on how to recover from the event.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "thunder_afternoon", "created_utc": 1385831001, "gilded": 0, "name": "t1_cdqabh3", "num_comments": null, "score": 1, "selftext": "I tried your code and it works as expected for me. Do you have a config file? Maybe something is configured incorrectly there. Also change your user agent just in case. I don't think that's the case but maybe \"testing_praw\" is too common or something.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rs00u/praw_retrieving_all_comments_works_once_then/"}, {"author": "umop_aplsdn", "created_utc": 1385587543, "gilded": 0, "name": "t1_cdogeks", "num_comments": null, "score": 5, "selftext": "Try using this: https://praw.readthedocs.org/en/latest/", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rlhzs/praw_retrieving_the_newest_10_submissions_in_a/"}, {"author": "gosslot", "created_utc": 1385575383, "gilded": 0, "name": "t1_cdoaxmc", "num_comments": null, "score": 2, "selftext": "In your Python installation there should be a folder named \"Scripts\". If you have successfully installed pip, there should be a pip.exe. The command pip install praw should be issued from Windows command prompt. To install PIP on Windows is ironically quite complicated itself. I myself just went with [WinPython](http://code.google.com/p/winpython/), a python installation that comes with many useful packages (like PIP) pre-installed.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": "_Daimon_", "created_utc": 1385576653, "gilded": 0, "name": "t1_cdobihg", "num_comments": null, "score": 1, "selftext": "Have you tried following the guides we listed in the installation step? If you did, I'd love to hear what was unclear/confusing to you. Because rather than try to walk you through it personally, I'd prefer figuring out how to improve the installation guide we point to. It just scales better :) [Our installation instructions](https://praw.readthedocs.org/en/latest/#installation) > If you don\u2019t have pip installed, then the Hitchhiker\u2019s Guide to Python has a section for setting it up on Windows, Mac and Linux. There is also a Stack overflow question on installing pip on Windows that might prove helpful.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": null, "created_utc": 1385618012, "gilded": 0, "name": "t1_cdorf5b", "num_comments": null, "score": 1, "selftext": "the easiest way I found was to download/unzip the files from [github](https://github.com/praw-dev/praw). click the '.zip' download on the right hand side. Unzip it to a directory you can easily navigate to. Open up a Powershell and navigate to that directory once you are in that directory type 'python setup.py' and it should install PRAW", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": "rcxdude", "created_utc": 1385685079, "gilded": 0, "name": "t1_cdp925n", "num_comments": null, "score": 3, "selftext": "I think this can be fixed in PRAW. It looks like the 'r' value needs to be set in the POST request otherwise reddit treats the post as a post to the frontpage for the purposes of validation. Currently it looks like it is only set on certain requests. This is based on reading the code on github, not experimenting because I'm lazy. Relevant code: [validation code](https://github.com/reddit/reddit/blob/f7c2ebb6efbfd172a22a2c249160c2e61293ef05/r2/r2/lib/validator/validator.py#L624) Note that it reads from c.site [part which sets c.site](https://github.com/reddit/reddit/blob/ac25e39dda668e4260e76085fee1b18222ede837/r2/r2/controllers/reddit_base.py#L335) the only [two](https://github.com/praw-dev/praw/blob/19ddbd04541ac372b8c7c3c35d0630ec4da126e3/praw/objects.py#L618) [places](https://github.com/praw-dev/praw/blob/19ddbd04541ac372b8c7c3c35d0630ec4da126e3/praw/internal.py#L89) where PRAW sets 'r' in the POST request that I can find.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkmao/praw_im_getting_apiexception_too_long_when_trying/"}, {"author": "bboe", "created_utc": 1385327524, "gilded": 0, "name": "t1_cdm30wp", "num_comments": null, "score": 1, "selftext": "> https://praw.readthedocs.org/en/PRAW-__1.0.9__/praw.html#praw.objects.LoggedInRedditor.my_reddits My guess is you're running a different version of PRAW than the docs you're looking at. Try `r.user.get_my_reddits()`", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rcl4u/praw_praw_throwing_an_attributeerror_when_trying/"}, {"author": "NotSinceYesterday", "created_utc": 1384782565, "gilded": 0, "name": "t1_cdh48py", "num_comments": null, "score": 1, "selftext": "Another minor issue: I can't seem to get py2exe to work with praw. Is this a known issue? A search only brought up one other instance of this and it was not resolved in the thread. It's not a major issue, but I wanted to give my other mods an .exe of the bot so they didn't need to install python and praw. Is there another method for producing an executable?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "_Daimon_", "created_utc": 1384812414, "gilded": 0, "name": "t1_cdhgh8o", "num_comments": null, "score": 1, "selftext": "Unknown issue. What error are you getting that's preventing you from using PRAW with py2exe and what was the other thread where this issue was raised?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "_Daimon_", "created_utc": 1384813257, "gilded": 0, "name": "t1_cdhgv1e", "num_comments": null, "score": 3, "selftext": "Looks like py2exe modifies the path to the system packages, causing PRAW to crash as it cannot find a configuration file. Try copying [praw.ini](https://github.com/praw-dev/praw/blob/master/praw/praw.ini) from the github repo and place it in the same directory as the script you're trying to compile. This should make PRAW use this as the configuration and being unable to find the global configuration file, which is usually used as a basis, won't matter.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "NotSinceYesterday", "created_utc": 1384852741, "gilded": 0, "name": "t1_cdhuz7d", "num_comments": null, "score": 1, "selftext": "I now have a new error: >C:\\Python27\\dist>porygon2.exe >Traceback (most recent call last): > File \"porygon2.py\", line 3, in > File \"praw\\__init__.pyc\", line 42, in > File \"six.pyc\", line 84, in __get__ > File \"six.pyc\", line 103, in _resolve > File \"six.pyc\", line 74, in _import_module >ImportError: No module named htmlentitydefs", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "reseph", "created_utc": 1384799907, "gilded": 0, "name": "t1_cdhamfd", "num_comments": null, "score": 1, "selftext": "Is your praw up to date? This wasn't added until v2.1.5", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "_Daimon_", "created_utc": 1384771288, "gilded": 0, "name": "t1_cdh2czq", "num_comments": null, "score": 3, "selftext": "Read the [getting started](http://praw.readthedocs.org/en/latest/pages/getting_started.html) page in our documentation. It shows how to get a `Redditor` object, which you can then use to get an overview, a listing of their comments or whatever it is you want.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qvzkd/praw_opening_a_user_page/"}, {"author": "NEBRASSKICKER", "created_utc": 1384427546, "gilded": 0, "name": "t1_cde3iue", "num_comments": null, "score": 1, "selftext": "i'm 100% new to programing so go easy on me.. I need a bot for my sub /r/huskies that will submit games threads [like this] (http://www.reddit.com/r/huskies/comments/1q9zp4/game_thread_colorado_washington_500pm_pst/) one hour before the game. The task has proven much harder than I anticipated and people I try to get help with assume I already very familiar with programing. I've downloaded python and praw and I need help with getting python to post the thread at a certain time..I've been going at this for about 7 hours... Any Help?? Edit: I posted the question to stack overflow [here's the question] (http://stackoverflow.com/questions/19973888/how-to-schedule-a-python-script-to-run-at-a-certain-time). I haven't really gotten anywhere.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "AndrewNeo", "created_utc": 1384375812, "gilded": 0, "name": "t1_cddm2t6", "num_comments": null, "score": 3, "selftext": "Here's a quick example: import praw r = praw.Reddit(user_agent=\"UA goes here\") r.login(\"username\", \"password\") sub = r.get_subreddit(\"Subreddit name goes here\") sub.submit(\"Title goes here\", text=\"Post text goes here\") Cron is the name of the scheduler. Check the man page or search for it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "davidystephenson", "created_utc": 1383842090, "gilded": 0, "name": "t1_cd8wl20", "num_comments": null, "score": 1, "selftext": "Thanks for the note! However, this still does not seem to solve the problem. Changing the code to: import os import praw r = praw.Reddit('aegis 1.0 by /u/davidystephenson') terms = ['scrum', 'fight'] if os.path.exists('last.txt'): with open('last.txt', 'r') as file: last = file.read() last_created_utc = r.get_submission(submission_id=last).created_utc print('last test', last, last_created_utc) else: print('Warning: no last.txt file') last = None subreddits = ['rugbyunion', 'hockey'] submissions = [] for subreddit in subreddits: submissions += list( r.get_subreddit(subreddit).get_new(limit=20, place_holder=last) ) print('length test', len(submissions)) for submission in submissions: text = submission.selftext.lower() title = submission.title.lower() matches = [term for term in terms if term in text or term in title] print( 'after test', submission.created_utc, last_created_utc, submission.created_utc > last_created_utc, submission.short_link, ) if matches and submission.id != last: message = ('matches: ' + str(matches) + ' ' + submission.short_link) print(message) else: pass # no matches if submission.created_utc > last_created_utc: with open('last.txt', 'w') as file: file.write(submission.id) Still repeatedly returns a list full of old values: last test 1q3sd0 1383837889.0 length test 24 after test 1324653662.0 1383837889.0 False http://redd.it/nnz15 after test 1324350637.0 1383837889.0 False http://redd.it/njcky after test 1324339049.0 1383837889.0 False http://redd.it/nj4fz after test 1324258832.0 1383837889.0 False http://redd.it/nhwp0 after test 1323190963.0 1383837889.0 False http://redd.it/n2nyi after test 1323123716.0 1383837889.0 False http://redd.it/n1ne5 after test 1323092218.0 1383837889.0 False http://redd.it/n12ts after test 1322913039.0 1383837889.0 False http://redd.it/mynkf after test 1322750462.0 1383837889.0 False http://redd.it/mw5bs after test 1321621476.0 1383837889.0 False http://redd.it/mgvc5 after test 1321585437.0 1383837889.0 False http://redd.it/mgh5e after test 1321581807.0 1383837889.0 False http://redd.it/mgf12 after test 1321530988.0 1383837889.0 False http://redd.it/mflcn after test 1321446493.0 1383837889.0 False http://redd.it/mebag after test 1321255851.0 1383837889.0 False http://redd.it/mbmhy after test 1320730653.0 1383837889.0 False http://redd.it/m4ich after test 1320705280.0 1383837889.0 False http://redd.it/m41qo after test 1320520377.0 1383837889.0 False http://redd.it/m1mrt after test 1320399238.0 1383837889.0 False http://redd.it/m04bg after test 1319990581.0 1383837889.0 False http://redd.it/lu5au after test 1383838500.0 1383837889.0 True http://redd.it/1q3t6e after test 1383838498.0 1383837889.0 True http://redd.it/1q3t6a after test 1383838040.0 1383837889.0 True http://redd.it/1q3skh after test 1383837889.0 1383837889.0 False http://redd.it/1q3sd0", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1q13hd/praw_using_place_holder_still_returns_posts_older/"}, {"author": "SOTB-human", "created_utc": 1383674240, "gilded": 0, "name": "t1_cd7dyys", "num_comments": null, "score": 1, "selftext": "If I recall correctly, `flat_comments` becomes a \"generator object\" that queries reddit every time you iterate through it. The way I usually do it is: flat_comments = [x for x in praw.helpers.flatten_tree(submission.comments)] Then, it will make all of the queries at that point, and store the results in memory.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pyczr/store_api_call_and_code_cleanup_help/"}, {"author": "AdamJacobMuller", "created_utc": 1383703506, "gilded": 0, "name": "t1_cd7qjbj", "num_comments": null, "score": 1, "selftext": "This is correct, but, assuming you're doing \"simple\" conversion of generator -> list you can do it much more simply than a comprehension, just flat_comments = list(praw.helpers.flatten_tree(submission.comments)) Will convert a generator to a list. Obviously anything more complex than a totally flat/simple conversion is going to (probably) be well done with a comprehension.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pyczr/store_api_call_and_code_cleanup_help/"}, {"author": "The_Gingey", "created_utc": 1383598236, "gilded": 0, "name": "t1_cd6q7au", "num_comments": null, "score": 1, "selftext": "Can you please tell me what traceback is? This is the full error: Traceback (most recent call last): File \"/Users/The_Gingey/Desktop/praw.py\", line 3, in import praw File \"/Users/The_Gingey/Desktop/praw.py\", line 5, in r = praw.Reddit(user_agent='my_cool_application') AttributeError: 'module' object has no attribute 'Reddit' That is the full error.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "The_Gingey", "created_utc": 1383598700, "gilded": 0, "name": "t1_cd6qeub", "num_comments": null, "score": 1, "selftext": "Thank you! I think I seriously messed something up though... When I try to import praw, idle says there is no module is named praw. But when I use the command line, it works. Would you happen to know what I'm doing wrong there?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "_Daimon_", "created_utc": 1382873653, "gilded": 0, "name": "t1_cd0ki91", "num_comments": null, "score": 1, "selftext": "If you're learning to use PRAW then IMO the best way is with the official [getting started](http://praw.readthedocs.org/en/latest/pages/getting_started.html) tutorial. I can't see what goes wrong for you with this script, because that line is not part of the linked code and you didn't post a traceback.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1paezw/help_with_a_keyerror_using_praw/"}, {"author": "_Daimon_", "created_utc": 1383865843, "gilded": 0, "name": "t1_cd9792h", "num_comments": null, "score": 1, "selftext": "You are encountering a `RateLimitError` which comes from exceeding reddits API limit. PRAW protects you against the regular limit, but for certain actions such as logging in, messaging or posting content this limit is lower. Since it's also dependent on super secret anti-spamming code, we cannot know in advance whether something will exceed the API limit for those methods. So it's generating a `RateLimitError`. That error contains a 'ratelimit' argument that tells you how long to wait before you can go again. It's the x in the \"you're doing that too much, please wait x time\" you might have seen on webend if you're exceeding the ratelimit there. Somehow though, that argument isn't included in the json returned by reddit. I'm not sure why that is. It could be a temporary thing or that for some cases it doesn't tell us how much. If you go on the webend and try logging in, then it should tell you how long to wait for. In general, when you're writing code that requires logging in. Try postponing adding the log in code until the very last time, as running code that logs in in quick succession will prompt the `RateLimitError`.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1paezw/help_with_a_keyerror_using_praw/"}, {"author": "kirbz1692", "created_utc": 1382905559, "gilded": 0, "name": "t1_cd0tq4c", "num_comments": null, "score": 1, "selftext": "Oh wow are you Daimon the developer, thanks for even looking at my code. The problem fixed itself somehow, idk but it works now so thanks anyway! **EDIT:** Just so you know, here's the traceback: Traceback (most recent call last): File \"C:/Users/ME/PycharmProjects/RedditBot/tutorial\", line 9, in r.login(\"MYUSERNAME\",\"MYPASSWORD\"); File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\__init__.py\", line 1157, in login self.request_json(self.config['login'], data=data) File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\decorators.py\", line 172, in wrapped return_value)) File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\errors.py\", line 325, in __init__ self.sleep_time = self.response['ratelimit'] KeyError: 'ratelimit'", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1paezw/help_with_a_keyerror_using_praw/"}, {"author": "_Daimon_", "created_utc": 1382739412, "gilded": 0, "name": "t1_cczo8k4", "num_comments": null, "score": 3, "selftext": "Try import pprint import praw r = praw.Reddit(UNIQUQ_AND_DESCRIPTIVE_USERAGENT) s = r.get_subreddit('redditdev', fetch=True) pprint.pprint(vars(s)) See the [writing a bot](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) page in PRAW's documentation for more information on python introspection and [lazy loading](http://praw.readthedocs.org/en/latest/pages/lazy-loading.html) of objects for why the `fetch=True` is included.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "_Daimon_", "created_utc": 1382787099, "gilded": 0, "name": "t1_cczyhlt", "num_comments": null, "score": 1, "selftext": "There isn't a method via PRAW to get it in bulk. You'll need to request each subreddit separately.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "bboe", "created_utc": 1382638523, "gilded": 0, "name": "t1_ccysabr", "num_comments": null, "score": 1, "selftext": "The docs list dynamically generated properties of objects: https://python-reddit-api-wrapper.readthedocs.org/en/latest/praw.html#praw.objects.Comment", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "bboe", "created_utc": 1382658977, "gilded": 0, "name": "t1_ccz0go8", "num_comments": null, "score": 1, "selftext": "Yes, you can build the comment manually if you don't require the _actual_ permalink. For instance the permalink to your comment is: http://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/ccz05xk which includes the submission title (the submission must be fetched for that which is why `permalink` is slow. You can, however, actually reduce that url just to: http://www.reddit.com/comments/1p3qau/_/ccz05xk You can actually replace the `_` with anything between the two `/`. The first base36 number is the submission id, and the second is the comment id.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "nareik15", "created_utc": 1382362140, "gilded": 0, "name": "t1_ccwahvw", "num_comments": null, "score": 1, "selftext": "So for the purposes of fetching these comments using the API/PRAW, is there any way to request a page with 1500 comments. As I had said, I get an error when adding \"?limit=1500\" to the end of urls (which I think may be something to do with dynamic pages). Given that it is possible to retreive 1500 comments for a page through the browser, surely there should be a working PRAW function that could retrieve that page? Again, thanks for the help.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ovi6a/just_bought_reddit_gold_how_do_i_change_default/"}, {"author": "bboe", "created_utc": 1382467028, "gilded": 0, "name": "t1_ccx98k3", "num_comments": null, "score": 1, "selftext": "Have you tried `get_submission('regular url', limit=1500)`? Edit, sorry it's: get_submission('regular url', comment_limit=1500) The documentation is usually pretty helpful for the functions you are using: [get_submission](https://praw.readthedocs.org/en/latest/pages/code_overview.html?highlight=get_submission#praw.__init__.UnauthenticatedReddit.get_submission)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ovi6a/just_bought_reddit_gold_how_do_i_change_default/"}, {"author": "_Daimon_", "created_utc": 1382011907, "gilded": 0, "name": "t1_cctir6g", "num_comments": null, "score": 1, "selftext": "I don't know. This question is about importing and running a 3rd party library, it has nothing to do with PRAW specifically. So you should ask it in r/learnpython.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1omaox/installing_praw_into_python/"}, {"author": "JBHUTT09", "created_utc": 1381670682, "gilded": 0, "name": "t1_ccqs1d6", "num_comments": null, "score": 1, "selftext": "Sorry. Here it is: Traceback (most recent call last): File \"C:\\Python27\\botTest.py\", line 34, in main() File \"C:\\Python27\\botTest.py\", line 27, in main r.send_message(author,subLine,msg) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 303, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 205, in wrapped return function(obj, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 1909, in send_message retry_on_error=False) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 141, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 479, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 348, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 321, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\adapters.py\", line 354, in send raise ConnectionError(e) ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /api/compose/.json (Caused by : [Errno 10054] An existing connection was forcibly closed by the remote host)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "_Daimon_", "created_utc": 1381659336, "gilded": 0, "name": "t1_ccqqjfk", "num_comments": null, "score": 1, "selftext": "If the submission has been either deleted or inaccessible. Which in this case can only happen if the post becomes removed and the authenticated account doesn't have access to removed submissions. Then an exception will happen when trying to access the `link_flair_text` as only at this time does PRAW send an API request to reddit to return information about the submission. If the information cannot be returned then an error will occur. EDIT: That's my guess at least. Just like /u/hiles I think a crash message would be most helpful.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "created_utc": 1381670782, "gilded": 0, "name": "t1_ccqs20d", "num_comments": null, "score": 1, "selftext": "The bot is a moderator on the sub, and the posts had not been removed. Here's the crash message: Traceback (most recent call last): File \"C:\\Python27\\botTest.py\", line 34, in main() File \"C:\\Python27\\botTest.py\", line 27, in main r.send_message(author,subLine,msg) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 303, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 205, in wrapped return function(obj, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 1909, in send_message retry_on_error=False) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 141, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 479, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 348, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 321, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\adapters.py\", line 354, in send raise ConnectionError(e) ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /api/compose/.json (Caused by : [Errno 10054] An existing connection was forcibly closed by the remote host)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "sixteenmiles", "created_utc": 1381595895, "gilded": 0, "name": "t1_ccqa0gi", "num_comments": null, "score": 1, "selftext": "Well what I mean is, by using get_new(limit=None) it gets everything, but only as far back as reddit displays 'new' posts, which seems to be about 28 pages. This isn't exclusive to using PRAW or anything, even just browsing in a browser, the 'new' page (at least of the specific sub I am looking at) goes back 28 pages and then stops. I just tried it manually and it eventually just hits a \"There are no more pages to load\" message. The sub I am looking at has been around for 4 years, but only seems to display about a month worth of content. Is this what you are talking about? Even browsing manually, reddit only displays a certain number of posts and all posts prior to that are... what, lost? Is there a way to use PRAW to look at all of a subs content, rather than what I am doing right now which is only looking at the 'new' content?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": "gavin19", "created_utc": 1381602203, "gilded": 0, "name": "t1_ccqbuja", "num_comments": null, "score": 1, "selftext": "At 100 items shown per page, 10 pages is as far back as you can go. The listings will show 1000 different posts, depending on sort type, but they all max out at 1000. Any other older posts aren't lost, it's just that they can't be found via those listings, only search. > which is only looking at the 'new' content You can use any of the other sort types like get_hot() etc. All found [here](https://praw.readthedocs.org/en/latest/pages/code_overview.html#module-praw.objects).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": "_Daimon_", "created_utc": 1381267693, "gilded": 0, "name": "t1_ccnqgac", "num_comments": null, "score": 1, "selftext": "You set flair with the [`set_flair`](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModFlairMixin.set_flair) method. Use standard introspection to see the attributes of Submission objects. It should be pretty obvious which ones are flair related. We have a [page about that](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) in our documentation if you're unsure on how to use introspection :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "_Daimon_", "created_utc": 1384991951, "gilded": 0, "name": "t1_cdj7sgm", "num_comments": null, "score": 1, "selftext": "I don't have access to a machine running PRAW atm, but the following should work. submission = r.get_submission('http://www.reddit.com/r/CoolWall/comments/1m3b0p/volkswagen_golf_gti_mk6/.json') submission.link_flair_text If the above doesn't work, post code showing what you're doing.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "_Daimon_", "created_utc": 1380803639, "gilded": 0, "name": "t1_cck62p3", "num_comments": null, "score": 3, "selftext": "The `update_settings` basically runs `get_settings` and then updates the existing settings with those parameters you've given and use that to run `set_settings`. The problem is that the recent addition of [new spam levels](http://www.reddit.com/r/changelog/comments/1krtu4/reddit_change_new_spam_filter/) didn't add them to the json about the page. Go to r/YOUR_SUBREDDIT/about/edit/.json to see what PRAW sees. Notice there is nothing about spam. So when PRAW sends the settings, the spam settings are not included because PRAW doesn't know about them, which means the spam settings become unset. I've made [a comment](http://www.reddit.com/r/changelog/comments/1krtu4/reddit_change_new_spam_filter/cck61hp) on the spam filter post. Since it's more of a forgotten change that belonged with this rather than a bug or a new feature. Ideally this will be fixed upstream by adding said fields to the json version of about/edit. If not, then I'll find a workaround for you.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "rreyv", "created_utc": 1380807101, "gilded": 0, "name": "t1_cck6vyw", "num_comments": null, "score": 1, "selftext": "Thank you. So when you say 'unset', does this mean that they're changed back to default of high, high, low? Or are they turned off? I feel like they would be changed back to defaults and I went through the reddit code to see if this is actually true. I thought I found the block of code that changes the spam settings to default if they're ever set to null, but I'm new to Python and not confident in my answer. Could you help me answer this? Thanks again for the prompt response. Really appreciate it and your work with praw.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "_Daimon_", "created_utc": 1380755058, "gilded": 0, "name": "t1_ccjuoju", "num_comments": null, "score": 2, "selftext": "The refresh_token is permanent. Store it. No, except obviously ``client_id``, ``client_secret`` and ``redirect_uri`` need to be set before OAuth can be used. See [PRAW's OAuth section 2](http://praw.readthedocs.org/en/latest/pages/oauth.html#step-2-setting-up-praw) on setting that up. Send us a link once your app is done, especially if it's opensource, and we might link to it from our documentation. :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nll3d/saving_oauth_codes_in_db/"}, {"author": "_Daimon_", "created_utc": 1380756962, "gilded": 0, "name": "t1_ccjvbnm", "num_comments": null, "score": 1, "selftext": "Sounds interesting :) The method you're looking for is [refresh_access_token](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.AuthenticatedReddit.refresh_access_information). Pass the new `refresh_token` as an argument and it will refresh the OAuth information based on that.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nll3d/saving_oauth_codes_in_db/"}, {"author": "_Daimon_", "created_utc": 1380223210, "gilded": 0, "name": "t1_ccfy0k4", "num_comments": null, "score": 1, "selftext": "Read the [comment parsing](http://praw.readthedocs.org/en/latest/pages/comment_parsing.html) article in PRAW's documentation.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "_Daimon_", "created_utc": 1380224018, "gilded": 0, "name": "t1_ccfyb2f", "num_comments": null, "score": 2, "selftext": "Getting a specific users comments is demonstrated in [getting started](http://praw.readthedocs.org/en/latest/pages/getting_started.html).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "_Daimon_", "created_utc": 1380224514, "gilded": 0, "name": "t1_ccfyhp8", "num_comments": null, "score": 1, "selftext": "I'm having a bit of a bad day. Sorry if I'm being a bit short with you. it has nothing to do with you. It is better if you could find the answer from the documentation, in that way I save time by not giving the same answer repeatedly. If that doesn't help you, then the best outcome would be to get great feedback from you so the documentation can be improved for future users. But here is a demonstration so you don't have to look longer. import praw r = praw.Reddit(UNIQUE_AND_DESCRIPTIVE_USER_AGENT) user = r.get_redditor('im14') for comment in user.get_comments(limit=None): print comment.body **EDIT:** Somehow I totally missed the 1000 limit part. Yeah /u/bboe is correct. The 1000 limit is a max due to Reddits cache, it doesn't matter whether you access it directly, via PRAW or any other wrapper. The max is the same.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "_Daimon_", "created_utc": 1379874082, "gilded": 0, "name": "t1_ccdagai", "num_comments": null, "score": 2, "selftext": "No (as of PRAW 2.1.7). I think the simplest solution for you would be to patch your version of PRAW to store the json_dict as an attribute upon instantiation. A `reddit_session` is an instance of a `Reddit` object. It is mandatory because it contains configuration information that is needed for the `Comment`object to work. So pass it an instance of `Reddit` with the configuration you need.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1mtvu2/praw_saving_json_response_and_using_that_to/"}, {"author": null, "created_utc": 1378972655, "gilded": 0, "name": "t1_cc6r9lr", "num_comments": null, "score": 1, "selftext": "I haven't looked at your code closely but i do know that praw does some level of caching. Could that account for some of your sporadic result times? Edit: Perhaps (and this is just a guess) praw goes to grab a new copy of the page after the normal cache expire time, gets a bad response from reddit, the falls back to using the now outdated cached version? That might explain why your bot is replying to the same comment multiple times.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "created_utc": 1378978034, "gilded": 0, "name": "t1_cc6rx81", "num_comments": null, "score": 6, "selftext": "For the people who are having this issue. Can you tell me whether you use PRAW? What methods you use to find out where to comment/submit/respond and what (if anything) you do in case of an exception. I'll use that information to check whether my hunch of a 1 year old bug is correct. If there is a bug, then I won't be able to fix it until I get home which will be in about 8-10 hours.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "created_utc": 1378997962, "gilded": 0, "name": "t1_cc6vuv1", "num_comments": null, "score": 1, "selftext": "I use PRAW for my bot. [The code is here on Github](https://github.com/matthieugrieger/versebot), but I have made some changes to the code since my last commit (invalid comment ids aren't stored in the database anymore, some attempts at fixes for this spamming problem, etc). Here's where the comment is made in my current code: botComment = False while True: try: botComment = comment.reply(currentComment).id except: if botComment != False: print('Comment posted on ' + ctime() + '.') return True else: continue I haven't really tested it that much so I'm not 100% sure if it works correctly. I have a hunch that it may not work right, as some others have said that reddit has been returning incomplete JSON data.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "created_utc": 1379083760, "gilded": 0, "name": "t1_cc7kq87", "num_comments": null, "score": 1, "selftext": "I've made a fix in PRAW that should fix the problem. See [this comment](http://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/cc7kn2q) for more info.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "avery_crudeman", "created_utc": 1378996995, "gilded": 0, "name": "t1_cc6vivl", "num_comments": null, "score": 1, "selftext": "Using PRAW. [Here's](https://github.com/avery-crudeman/Baseball-GDT-Bot/tree/master/src) the github page of the bot I'm using. It posts threads and updates them with statistics. When it posts multiple threads I just delete all but one and switch the sub = r.submit part with sub = r.get and use the ID of the remaining post. Here's the part of the code that does the posting. for d in directories: timecheck.gamecheck(d) title = editor.generatetitle(d) if not timecheck.ppcheck(d): while True: try: print \"Submitting game thread...\" sub = r.submit('SUBREDDITNAME', title, editor.generatecode(d)) #sub = r.get_submission(submission_id='xxxxxx') print \"Game thread submitted...\" break except Exception, err: print err time.sleep(300) while True: str = editor.generatecode(d) while True: try: sub.edit(str) break except Exception, err: print \"Couldn't submit edits, trying again...\" time.sleep(10) if \"|Decisions|\" in str: print \"Submitting postgame thread...\" posttitle = posteditor.generatetitle(d) sub = r.submit('SUBREDDITNAME', posttitle, posteditor.generatecode(d)) print \"Postgame thread submitted...\" break elif \"###POSTPONED\" in str: break time.sleep(10) [**EDIT:** Just to be clear, I have a simplified version of this I use for testing that doesn't have any \"try except\" bits and should just submit a post without retrying or anything, but that also posts a random amount of multiple submissions before it finally realizes it's posted something and stops running.]", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "created_utc": 1379083755, "gilded": 0, "name": "t1_cc7kq5j", "num_comments": null, "score": 1, "selftext": "I've made a fix in PRAW that should fix the problem. See [this comment](http://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/cc7kn2q) for more info.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "nandhp", "created_utc": 1378994409, "gilded": 0, "name": "t1_cc6urom", "num_comments": null, "score": 1, "selftext": "I'm using PRAW. Here's my code: post = praw.objects.Submission.from_id(self.reddit, postid) try: comment = post.add_comment(comment_text) comment_id = comment.id except praw.errors.APIException, exception: if exception.error_type in ('TOO_OLD','DELETED_LINK'): print \"[Can't post comment: archived by reddit]\" else: raise # Try posting again later [The full code is on Github](https://github.com/nandhp/movieguide/blob/master/movieguide.py#L261). I'm using PRAW 2.0.15, not 2.1; but I can try upgrading if it will help. (I tried back when it came out and had trouble, so I've still been using 2.0.15.)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "created_utc": 1379083738, "gilded": 0, "name": "t1_cc7kpxr", "num_comments": null, "score": 1, "selftext": "I've made a fix in PRAW that should fix the problem. See [this comment](http://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/cc7kn2q) for more info.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "created_utc": 1379083503, "gilded": 0, "name": "t1_cc7kn2q", "num_comments": null, "score": 6, "selftext": "PRAW automatically retries requests that fails, if the error is a temporary one. Like a 502 status. This makes you see fewer exceptions and ensures that exceptions are actually exceptions rather than temporary glitches in Reddit or the internet infastructure. In the past few days Reddit has been under heavier stress than normal and been throwing more errors than normal. I wasn't running any bot at the time of the problem, but according to /u/offtherocks a lot of requests were timing out impartial json. This mean a 504, which PRAW retries for. So what happened was that the request was made, the data was added to Reddit's database and Reddit then tried to return a succesful response. But ended up timing out so PRAW received incomplete data. Which then prompted it to resend the request causing the data to be added multiple times to the database. I've fixed the bug in PRAW, so now methods that may spam reddit will not retry failed requests. This bugfix is included in PRAW 2.1.6. So I recommend upgrading asap. $ pip install praw -U Remember that PRAW has a [changelog](https://praw.readthedocs.org/en/latest/pages/changelog.html) so you can easily see what changes have been made that affect your applications. Finally if you still have the problem after upgrading then let me know as quickly as possible. I've done the best I can with this solution, but testing handling of errors on a machine you don't control is quite hard.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "created_utc": 1378960158, "gilded": 0, "name": "t1_cc6oiep", "num_comments": null, "score": 2, "selftext": "Yeah, I can't figure it out either. The behavior is pretty strange and unpredictable. Maybe it's just something that we have to wait for reddit/PRAW to fix.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "nandhp", "created_utc": 1379038951, "gilded": 0, "name": "t1_cc7b17t", "num_comments": null, "score": 3, "selftext": "I think I found a workaround. --------------------- Using http://api.reddit.com seems to help, both with 502/503 errors and with [page load times](http://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/cc6qgaz). You can do this in PRAW by setting the environment variable `REDDIT_SITE=reddit_bypass_cdn`or by using the constructor `praw.Reddit(site_name='reddit_bypass_cdn')`. I suggest you don't leave this set on a long-term basis, it probably causes more load on their servers.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "created_utc": 1378976269, "gilded": 0, "name": "t1_cc6rpza", "num_comments": null, "score": 2, "selftext": "You should always first store the ID of the thing you're replying to, before actually commenting on it. Otherwise commenting on the same thing more than once is bound to happen eventually. There's simply too many places between adding it to Reddits database and your end where it can go wrong. Using PRAW an error can happen in 5 different libraries `Reddit`, `Urllib3`, `requests`, `PRAW` and your own. And in the internet infastructure as well obviously. Currently it seems Reddit is under heavier stress than normal and throwing more errors than usual. Which obviously would make this problem happen more frequently. Store the id first, then make the comment. If the comment-making process fails, then first check whether a comment has been made. Either manually or via PRAW. Then make the comment again if it hasn't. Remember that there is a 30 second cache in both Reddit and PRAW. The above also applies if you're responding to a message or similar processes.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "m1ss1ontomars2k4", "created_utc": 1377581909, "gilded": 0, "name": "t1_cbw49pk", "num_comments": null, "score": 3, "selftext": "Maybe it's just a bug in PRAW then?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "_Daimon_", "created_utc": 1377601016, "gilded": 0, "name": "t1_cbw74og", "num_comments": null, "score": 3, "selftext": "Fetching contributors always work when you are a moderator of the subreddit, which is why the method is currently restricted to moderators. I've been looking at a subreddit I mod, and I cannot find any way of making the contributor list public. If there isn't an option, then I can only assume that users having access to the list of contributors of accesable subreddits are what is intended. And that the 404s are the bugs. In which case I'll file a bug report on Reddits end and remove the requirement on PRAW's end when Reddit fixes the bug. So is there a way of making the contributor list public?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "_Daimon_", "created_utc": 1377453922, "gilded": 0, "name": "t1_cbv377q", "num_comments": null, "score": 1, "selftext": "https://praw.readthedocs.org/en/latest/pages/faq.html#when-i-print-a-comment-only-part-of-it-is-printed-how-can-i-get-the-rest", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l185p/printing_submission_comments_doesnt_print_entire/"}, {"author": "isolani", "created_utc": 1377103779, "gilded": 0, "name": "t1_cbsd5ym", "num_comments": null, "score": 2, "selftext": "Use `sticky()` and `unsticky()` on a Submission object. Source: https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.sticky", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kt95a/using_praw_to_set_subreddit_sticky/"}, {"author": "Squidifier", "created_utc": 1377103910, "gilded": 0, "name": "t1_cbsd7x5", "num_comments": null, "score": 1, "selftext": "Whoops, I feel dumb now. Searching ['sticky' in readthedocs](https://readthedocs.org/search/project/?q=sticky&selected_facets=project_exact%3Apraw) didn't turn up with anything, but it seems [Google does](https://www.google.com.ph/search?q=site%3Ahttps%3A%2F%2Fpraw.readthedocs.org+sticky&oq=site%3Ahttps%3A%2F%2Fpraw.readthedocs.org+sticky&aqs=chrome..69i57j69i58.3155j0&sourceid=chrome&ie=UTF-8). Thanks again for your help!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kt95a/using_praw_to_set_subreddit_sticky/"}, {"author": "_Daimon_", "created_utc": 1377107687, "gilded": 0, "name": "t1_cbserch", "num_comments": null, "score": 1, "selftext": "A [recent commit](https://github.com/praw-dev/praw/commit/cd9f5ce005dd59cbee85301c2d15c46001cc27c8) added the functionality. Which you can also see in [PRAW's changelog](https://praw.readthedocs.org/en/latest/pages/changelog.html). But what you can also see there is that it's part of a version that hasn't yet been released. It is up to bboe to decide when the next version of PRAW is deployed, but I think it will be soon.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kt95a/using_praw_to_set_subreddit_sticky/"}, {"author": "OnceAndForever", "created_utc": 1377053308, "gilded": 0, "name": "t1_cbs10jf", "num_comments": null, "score": 1, "selftext": "What do you mean your code is making too many requests? Reddit only lets you make 30 api calls per minute, and praw handles all of that for you. I don't think get_submitted() and get_comments() are slower, but praw will only let you call that method every 2 seconds.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "MrFanzyPanz", "created_utc": 1377057040, "gilded": 0, "name": "t1_cbs2b7n", "num_comments": null, "score": 2, "selftext": "get_submitted() and get_comments() both return get_content() objects through PRAW. get_content() objects return batches of at the most 100 items within 1 call. For example: for post in r.get_subreddit('pics').get_hot(limit=500): This code would get posts from the 'pics' subreddit in batches of 100 posts, sorted by the get_hot() algorithm. If you asked for attributes of posts, say post.score or post.fullname, those attributes would be placed in the call along with the 99 other posts and their attributes. There is an attribute, however, that requires its own call: the post.comments attribute. It returns a list of the root comments of the given post. If you're trying to get the comments of all the posts, instead of getting your posts in batches of 100, post.replies will slow your code down to one post every two seconds. I want to know if this is inherent or if I've simply messed something up. **Edit:** This is also a problem for when pulling comments. comment.replies requires its own call as well.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "OnceAndForever", "created_utc": 1377066489, "gilded": 0, "name": "t1_cbs50gi", "num_comments": null, "score": 1, "selftext": ">There is an attribute, however, that requires its own call: the post.comments attribute. It returns a list of the root comments of the given post. > >If you're trying to get the comments of all the posts, instead of getting your posts in batches of 100, post.replies will slow your code down to one post every two seconds. I want to know if this is inherent or if I've simply messed something up. I see what you mean. No you haven't messed anything up. The two second delay is inherent. When you use praw, you can't make requests any faster than once every two seconds. It is part of reddit's API limitations. reddit only allows 30 api requests per minute, which ends up being one every two seconds. It is explained in more detail in the praw documentation: >Another thing you may have noticed is that retrieving a lot of elements take time. reddit allows requests of up to 100 items at once. So if you request", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "Bruin116", "created_utc": 1377067433, "gilded": 0, "name": "t1_cbs57v4", "num_comments": null, "score": 1, "selftext": "Right, but I think his problem is that while most of the praw calls get data in batches of 100, this particular post.comments call only gets 1 entry at a time, making it run 100x slower than the rest of his code. It is effectively making 100 times more requests to get the same amount of information as all of the other batch pulls.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "_Daimon_", "created_utc": 1376933145, "gilded": 0, "name": "t1_cbqyyeh", "num_comments": null, "score": 2, "selftext": "That sounds interesting. Be sure to post it here to /r/redditdev when you're done. Would love to read it :) Btw, it would be really great if you would add PRAW to your citations. Like for [IPython](http://ipython.org/citing.html). [Bryce](http://cs.ucsb.edu/~bboe/), the main committer, is getting a Ph.D. in Computer Science so citations matter for him. It doesn't much for me professionally, I'd just think it would be awesome :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "CMThF", "created_utc": 1376953337, "gilded": 0, "name": "t1_cbr5x8n", "num_comments": null, "score": 1, "selftext": "thank you for mentioning this, I will do that! A colleague of mine also writes his master thesis, and I will recommend him to cite PRAW too. Also, I am going to post the resulting paper; I hope it will result in a worthwhile read... we will see about that. :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "bboe", "created_utc": 1377121363, "gilded": 0, "name": "t1_cbski9x", "num_comments": null, "score": 1, "selftext": "Yes, it would be awesome if you cite PRAW. I suppose something like the following would be a good citation: > PRAW Development Team (2013). Python Reddit API Wrapper (PRAW), GNU General Public License. https://github.com/praw-dev/praw Edit: Please sent us a link to whatever work you create using PRAW as we'll add a section of PRAW-related publications. Thanks!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "MrFanzyPanz", "created_utc": 1376639325, "gilded": 0, "name": "t1_cbovrdr", "num_comments": null, "score": 1, "selftext": "Also, I have code that looks like this: r = praw.Reddit(user_agent=dict['user_agent']) h_index_list = list() for comment in r.get_redditor(account).get_comments(limit = 300): comment_citations = 0 for reply in comment.replies: comment_citations = comment_citations + 1 h_index_list.append(comment_citations) print(comment_citations) This counts each comment in comment.replies as a request. This means that counting the replies to each comment takes at least 2 seconds. However, I have other code like this: for post in r.get_subreddit(subreddit).get_hot(limit=500): # write to database # data listed as needed is in order: unique_id = post.name #reddit_id subred = post.subreddit.display_name that processes 100 posts at once, along with their post.name and post.subreddit.display_name requests. When I built the account code, I was told it should pull data in 100 blocks like the second set of code. Is this true? Have I done something wrong?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kgauh/in_praw_is_there_a_way_to_request_an_entire/"}, {"author": "_Daimon_", "created_utc": 1376656288, "gilded": 0, "name": "t1_cboyak6", "num_comments": null, "score": 1, "selftext": "Are you interested in all the comments in a Submission or only a small subset? EDIT: PRAWs ``MoreComments`` represent stubbed out comments. The same you see on the webend, when you use the \"see more comments\" button. For comments that are hidden due to being on a low layer, there's no method of preventing them from being ``MoreComments`` AFAIK. My testing says that it's the 11 layer that becomes a ``MoreComment``. Do you have a link to where it happens with 5?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kgauh/in_praw_is_there_a_way_to_request_an_entire/"}, {"author": "_Daimon_", "created_utc": 1376376558, "gilded": 0, "name": "t1_cbmpfow", "num_comments": null, "score": 2, "selftext": "Seems ``update_checker`` tries to create a temporrary directory and that functionality isn't implemented by GAE's version of Python 2.7. You can try purging references to update_checker from PRAW, which won't have any effect on the functioning of PRAW. Except obviously update_checker will no longer work.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "_Daimon_", "created_utc": 1377794728, "gilded": 0, "name": "t1_cbxrpp5", "num_comments": null, "score": 1, "selftext": "I don't use GAE, so I wouldn't be much help. I would either contact OP of this thread, who got PRAW running on GAE, or checkout the [Pull Request](https://github.com/praw-dev/praw/pull/239) he sent us to add GAE compatibility.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "pipsqueaker117", "created_utc": 1376411642, "gilded": 0, "name": "t1_cbmwu5f", "num_comments": null, "score": 3, "selftext": "Followed your advice, and managed to get the script working. Thanks! Also, I want to post a .zip containing the dependencies for praw and the modded version I had to use (help other newbs avoid the trouble I went through), any idea where I would post such a thing?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "_Daimon_", "created_utc": 1376433196, "gilded": 0, "name": "t1_cbn5cs9", "num_comments": null, "score": 2, "selftext": "If you can write a guide to getting PRAW running on GAE, then I will happily include it as a page in [PRAW's documentation](https://praw.readthedocs.org/en/latest/). But I don't want zip files, because it is static format. Meaning that any bugfixes, new features or other improvements made to PRAW or any of it's dependencies will not be included in that zip Worst case, a backwards incompatible change to the API would make it impossible for users of that zip folder to follow the tutorials or run any of the example code. I don't want any PRAW user to experience that, and especially not newcomers.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "pipsqueaker117", "created_utc": 1376501866, "gilded": 0, "name": "t1_cbnnzna", "num_comments": null, "score": 1, "selftext": "Good point. Do you maintain praw, btw? If you do, I've put in a pull request for a change which would allow stickying of posts through the library Also, I changed some things around in init.py to make it compatible with Google App Engine: would you want me to submit a pull request for those changes as well- if they're accepted then users should be able to use praw w/ GAE w/o having to mod it (after they downloaded all its dependencies, of course)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "IAmAnAnonymousCoward", "created_utc": 1375571274, "gilded": 0, "name": "t1_cbge66a", "num_comments": null, "score": 2, "selftext": "Thanks. I was hoping I could use the API / PRAW, so this will be new territory for me.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "_Daimon_", "created_utc": 1375661486, "gilded": 0, "name": "t1_cbgzzx4", "num_comments": null, "score": 1, "selftext": "Possible yes, implemented in PRAW no. I have some other coding matters to attend to first, but I should be able to add this functionality within the week. I'll be sure to add another comment to this post when I do.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jklbu/praw_is_there_a_function_for_getting_unread_mod/"}, {"author": "_Daimon_", "created_utc": 1375232697, "gilded": 0, "name": "t1_cbdq9mh", "num_comments": null, "score": 2, "selftext": "That method was intentionally broken in version 2.0.9. Because it uses a SSL endpoint and Reddits SSl endpoints are not meant for heavy traffic. See [Issue 175 on github.](https://github.com/praw-dev/praw/issues/175)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jcgsw/unexpected_redirect_getting_friends_list/"}, {"author": "MonkeyNin", "created_utc": 1375122901, "gilded": 0, "name": "t1_cbcq43x", "num_comments": null, "score": 1, "selftext": "I can't seem to find a function named `get_liked` anywhere? Normally I'm good at reading documentation but I'm having trouble with PRAW. And it doesn't match the built in docstrings, which is weird.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "nemec", "created_utc": 1375129920, "gilded": 0, "name": "t1_cbcszzi", "num_comments": null, "score": 1, "selftext": "It [should](https://github.com/praw-dev/praw/blob/master/praw/objects.py#L684). Maybe you're using an older version or misspelling the name?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "xTristan", "created_utc": 1375229367, "gilded": 0, "name": "t1_cbdp4ms", "num_comments": null, "score": 1, "selftext": "It is simpler than that... Once you've logged in, you can get your likes with `r.user.get_liked()` which returns a [RedditContentObject](http://python-reddit-api-wrapper.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.RedditContentObject) that you can iterate through to get your liked posts as [Submission](http://python-reddit-api-wrapper.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission) objects. Example: import praw r = praw.Reddit(user_agent='example') r.login('userfoo', 'passwordbar') for post in r.user.get_liked(): if post.subreddit.display_name == 'EarthPorn' or post.subreddit.display_name == 'spaceporn': print post.url", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "bboe", "created_utc": 1375123674, "gilded": 0, "name": "t1_cbcqfkz", "num_comments": null, "score": 2, "selftext": "I [filed a bug](https://github.com/praw-dev/praw/issues/235) for this issue. You should be able to extract the url from the exception object, and then call `r.get_submission` on that URL, but you shouldn't have to. Not sure when the issue will be fixed though.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j1j6q/praw_redirectexception_when_searching_for_url_if/"}, {"author": "IAmAnAnonymousCoward", "created_utc": 1375124334, "gilded": 0, "name": "t1_cbcqpfu", "num_comments": null, "score": 1, "selftext": "Thanks! Yeah it's annoying but manageable. Any idea how to handle [my other issue](http://www.reddit.com/r/redditdev/comments/1hvgjf/praw_actually_getting_the_top_200_submissions/)?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j1j6q/praw_redirectexception_when_searching_for_url_if/"}, {"author": null, "created_utc": 1374723077, "gilded": 0, "name": "t1_cb9rakn", "num_comments": null, "score": 3, "selftext": "I literally had this problem 15 minutes ago :) What operating system are you using? I know this is a problem, but personally I didn't even use virtualenv and just used pip to install praw easily. http://www.tylerbutler.com/2012/05/how-to-install-python-pip-and-virtualenv-on-windows-with-powershell/ This was a miracle. I don't use PowerShell, but if you ignore those sections of the tutorial you can easily get pip. Even if you use a Unix based OS or somethng else, this tutorial also works because it just relies on python scripts to install everything.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": null, "created_utc": 1374727612, "gilded": 0, "name": "t1_cb9ssjm", "num_comments": null, "score": 2, "selftext": "I would say anything you don't immediately need. I'm not familiar with virtualenv, but I realize I'll need to learn it eventually. However I pretty much just did everything up to and including installing pip and virtualenv through it, then just installed praw. Feel free to explore and try things out though!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "created_utc": 1374727760, "gilded": 0, "name": "t1_cb9su71", "num_comments": null, "score": 4, "selftext": "Wow. I *just* found a shortcut. All I had to do to install PRAW was download distribute_setup.py and get-pip.py, then type: python distribute-setup.py python get-pip.py pip install praw", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "mrwazsx", "created_utc": 1377188920, "gilded": 0, "name": "t1_cbt2v6e", "num_comments": null, "score": 1, "selftext": "Hey i just found this through a google search and it looks like you have successfully managed to install PRAW; anyway i'm up to having installed pip and in your shortcut you say type \"pip install praw\" would you mind expanding on this i'm confused about how to direct this to the .zip file from github", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "created_utc": 1377193885, "gilded": 0, "name": "t1_cbt4xcz", "num_comments": null, "score": 1, "selftext": "PRAW installation is included in pip. Literally just execute \"pip install praw\" in cmd.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "created_utc": 1377212344, "gilded": 0, "name": "t1_cbtci8m", "num_comments": null, "score": 1, "selftext": "1. Open cmd (Start button > \"cmd\" > Hit Enter) 2. Type \"pip install praw\" 3. ???? 4. PROFIT!!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "PM_ME_YOUR_TITS_GIRL", "created_utc": 1375883963, "gilded": 0, "name": "t1_cbirbki", "num_comments": null, "score": 1, "selftext": "Sorry to butt in on your thread but I found this via google search as I am looking to install PRAW. Your solution seems very easy to follow but I am curious what folder do you put the distribute_setup.py and get-pip.py files? I too am using Windows.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "untrusted_wifi", "created_utc": 1374684742, "gilded": 0, "name": "t1_cb9cqix", "num_comments": null, "score": 4, "selftext": "I'm using a bottom tier VPS to run multiple projects. Praw's included praw-multiprocess has been a lifesaver. Resource consumption is minimal, but will depend on what you're doing. Other projects on the same VPS include a LAMP stack and various bits and pieces.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "untrusted_wifi", "created_utc": 1374674654, "gilded": 0, "name": "t1_cb992ov", "num_comments": null, "score": 5, "selftext": "Arduino? While PyMite will run on an Arduino Mega the work it'd take to get PRAW to work makes it an illogical choice. **Edit**: Instead of replying to u/isolani or myself, u/bennythomson edited their response. The original text was \"Raspberry ip [sp] or arduino\"", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "SuperSeriouslyUGuys", "created_utc": 1374434551, "gilded": 0, "name": "t1_cb7bv7z", "num_comments": null, "score": 3, "selftext": "It looks like if your image comes from somewhere other than imgur or is already in used it's going to leave the current one there for another sleep cycle. You could use python's built in `filter` or a list comprehension to strip all the non-imgur links from submissions before going into the loop. And use else continue If it's already in used. Also, I'm not experienced with praw, so this might not be the case, but I wouldn't expect to see the same submission id multiple times so you could probably take the whole used array and check out.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "xiggy", "created_utc": 1374437674, "gilded": 0, "name": "t1_cb7culb", "num_comments": null, "score": 1, "selftext": "Like I said, I am quite the novice when it comes to Python and PRAW. This is literally the first thing I've written. I do think I understand what you're saying about leaving the current image for another cycle. I believe I experienced that when I was testing with only grabbing 5 submissions. In that case, only 3 of the 5 images where from i.imgur.com. After it got to the last image, it just kept repeating it over and over. Although, one time once it looped though all the submissions, it just started writing blank CSS files, which, in turn, displayed a blank HTML page. I wrote in the part with the used list thinking that once it's grabbed that submission, it will add it to a list and skip over it every time afterwards. It, however, doesn't do that, to my knowledge. I still have a lot to learn about Python and programming in general. I will definitely research what you've mentioned in your reply.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "ddxxdd", "created_utc": 1374211272, "gilded": 0, "name": "t1_cb5s0zv", "num_comments": null, "score": 2, "selftext": "I'm not gonna lie, I could not get the MoreComments object to work in any way, shape, or form. So I basically used a workaround: the _request() function. Daimon, one of the praw developers, told me not to use such low-level functions, but I figured \"why not? It works, and it abides by the 2-second API call limit\". So basically, my code looks like: r=praw.Reddit(user_agent='u/ddxxdd') after='' q=r._request('http://www.reddit.com/r/test/comments.json?after=\" + after) after=q['data']['after'] q=r._request('http://www.reddit.com/r/test/comments.json?after=\" + after) And that's effectively how I get more comments.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ijb3m/error_when_running_a_praw_script/"}, {"author": "yesorknow", "created_utc": 1374258049, "gilded": 0, "name": "t1_cb63w5f", "num_comments": null, "score": 2, "selftext": "You don't need to worry about MoreComments as you come across them. There is a simple method you can call at the beginning of your script which automatically replaces all MoreComment objects with Comment objects. r = praw.Reddit('foo') submission = r.get_submission(url=url_you_want) submission.replace_more_comments(limit=None, threshold=0) This last line will do what you want. `limit` is the maximum number of MoreComment objects to replace. `threshold` is the minimum number of replies a comment must have for it to be replaced. `(None, 0)` will replace everything. Note that if you chance `limit` and/or `threshold` to avoid certain MoreComment objects, these obecjts are simply removed, not turned into Comments.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ijb3m/error_when_running_a_praw_script/"}, {"author": "_Daimon_", "created_utc": 1374147660, "gilded": 0, "name": "t1_cb55m3e", "num_comments": null, "score": 1, "selftext": "That's weird. What's the code you're using? Are you running the script from your local machine? Does the following work? import praw r = praw.Reddit(user_agent='experimenting with PRAW by u/ _Skrillex_') submissions = r.get_subreddit('opensource').get_hot(limit=5) [str(x) for x in submissions]", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iirrw/just_installed_praw_keep_getting_error_when/"}, {"author": "vaetrus", "created_utc": 1373926978, "gilded": 0, "name": "t1_cb3c5t1", "num_comments": null, "score": 5, "selftext": "[[runs praw module]] >>> ===== RESTART ===== >>> 2013-07-15 18:16:19,111 - Assigning user_agent. 2013-07-15 18:16:19,114 - Creating reddit object. >>> bot >>> bot.logIn() 2013-07-15 18:16:28,128 - Logging in. >>> bot.getUnread() 2013-07-15 18:16:32,250 - Getting unread messages. >>> bot.unread >>> unread = [i for i in bot.unread] >>> unread [] >>> unread = unread[0] >>> unread >>> dir(unread) ['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattr__', '__getattribute__', '__hash__', '__init__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_get_json_dict', '_info_url', '_populate', '_populated', '_underscore_names', 'author', 'body', 'body_html', 'context', 'created', 'created_utc', 'dest', 'first_message', 'first_message_name', 'from_api_response', 'fullname', 'id', 'mark_as_read', 'mark_as_unread', 'name', 'new', 'parent_id', 'reddit_session', 'replies', 'reply', 'subject', 'subreddit', 'was_comment'] >>> unread.body u'test' This is typically how I deal with all of reddit: I dir() it. Hope it helps.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "NH4ClO4", "created_utc": 1373932062, "gilded": 0, "name": "t1_cb3dvuo", "num_comments": null, "score": 2, "selftext": "Ok, thanks. I was sorta frustrated that PRAW's documentation doesn't include variables, more me used to non-python languages.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "_Daimon_", "created_utc": 1373928194, "gilded": 0, "name": "t1_cb3cl50", "num_comments": null, "score": 2, "selftext": "Use the standard Python tools for introspection. [See this page in PRAWs documentation for more details](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "killver", "created_utc": 1373667593, "gilded": 0, "name": "t1_cb1h2w6", "num_comments": null, "score": 1, "selftext": "just the specific praw call that causes the problems, you can then repeat the call or ignore it if it is not important", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "MrFanzyPanz", "created_utc": 1373155926, "gilded": 0, "name": "t1_caxa45f", "num_comments": null, "score": 1, "selftext": "That's okay! We only wanted to peruse the first 500 submissions under the 'hot' section of each subreddit we analyze. We figured we'd rather get less submissions and more subreddits, especially because some of the subreddits we will scrape have less than 10,000 subscribers, but they offer the most interesting communities. For the limit parameter, we just eat the time delay. It takes roughly 9.3 seconds to scrape a subreddit of its first 500 submissions using praw. We're fine with this. Thank you, though! :D", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "bboe", "created_utc": 1373065052, "gilded": 0, "name": "t1_cawog1j", "num_comments": null, "score": 3, "selftext": "As long as the VMs have different IPs you shouldn't run into any problems with the 30 requests a minute rule. If they do not, then you'll probably have some issues and might as well run only a single instance (or use praw's multiprocessing feature) to handle the rate-limiting.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "MrFanzyPanz", "created_utc": 1373065784, "gilded": 0, "name": "t1_cawonxi", "num_comments": null, "score": 1, "selftext": "We are using dedicated proxies for each VM. I may be wrong, but it seemed to me that dedicated proxies are more stable for a month-long unbroken datascrape across multiple VMs, considering that multi-processing is still rather new. Besides, I already wrote the code to handle the data without multi-processing. Each VM is respecting the 30 requests/minute rule that is a default for praw.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": null, "created_utc": 1372700325, "gilded": 0, "name": "t1_catvray", "num_comments": null, "score": 3, "selftext": "The reason it asks you for a captcha is because your bot hasn't got enough karma to post without doing it. The best thing to do would be to post an image in the subreddit asking people to upvote it for the new bot. As for the sidebar thing, I don't think so. I am also new to PRAW and nothing came up after a long, hard google search. Sorry about that, you'll have to do it manually.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/"}, {"author": "_Daimon_", "created_utc": 1372703714, "gilded": 0, "name": "t1_catx0tc", "num_comments": null, "score": 3, "selftext": "The method you're looking for is [update_settings](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModConfigMixin.update_settings). It takes the same arguments as [set_settings](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModConfigMixin.set_settings) the diference is that with ``update_settings`` only the arguments explicitly given are changed. With ``set_settings`` it's all. I *believe* the argument you want to change is description. But I don't have access to a Python terminal with PRAW atm, so I cannot confirm.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/"}, {"author": "bboe", "created_utc": 1372650142, "gilded": 0, "name": "t1_catkgts", "num_comments": null, "score": 3, "selftext": "That feature is not currently supported. You can request it on [PRAW's github](https://github.com/praw-dev/praw) page if you'd like.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1has5q/praw_how_do_you_add_moderators_with_nonfull/"}, {"author": "vacantmentality", "created_utc": 1372389500, "gilded": 0, "name": "t1_carqwpw", "num_comments": null, "score": 5, "selftext": "In your example, \"t\" is a list of submission objects. In order to see the kinds of things you can get from those objects visit the URL of a submission, like the one in your example, except add \".json\" to the end, Like [this](http://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/.json). That will show you the JSON data for the page that can be accessed with the reddit API. It's a pretty good bet that PRAWs variable names are the same. If you wanted to get the actual text of the submission, 'selftext' seems to be what you need (from looking at the JSON link). This would mean the way to get that information would be: >>> t = list(r.get_content('http://www.reddit.com/r/redditdev', limit = 10)) >>> print(t) [, , , , , , , , , ] >>> t[0].selftext \"Hi, i'm very interested in using PRAW and i think its great, but i can't figure out how to get the actual text out of a submission. For example, if i do \\n\\n >>>t = list(r.get_content('http://www.reddit.com/r/redditdev', limit = 10))\\n >>>str(t[0])\\n '1:: [Praw] [Request] Use timestamps in submission query'\\n\\nhowever, i don't want the title of the post, i want the text which is \\n\\n> I recently had good luck using timestamps in an undocumented manner. It would be awesome to be able to use them in Praw and it might even get around the 1k assuming 'new' sorts chronologically (?).\\n\\nI've searched through the documentation and have read through a good portion of it, but i still haven't figured out how to do this, if it's possible. Can anyone help? Thank You!\\n\\nexample post from here: http://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/\\n\\nIf it makes any difference i'm trying using this with a subreddit where all posts are text posts\\n \" As you can see, it's not a matter of 'turning the objects into strings', each submission object has a range of variables like 'subreddit', 'banned_by' etc. that can be accessed by object(dot)variable_name, eg. \"t[0].selftext\". Hope that helps :) I'm still learning to use reddit bots myself but feel free to ask me any questions.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7yno/how_to_get_text_of_a_submission/"}, {"author": "_Daimon_", "created_utc": 1372409621, "gilded": 0, "name": "t1_carvlxq", "num_comments": null, "score": 2, "selftext": "Read [this tutorial in PRAWs documentation](https://praw.readthedocs.org/en/v2.0.15/pages/writing_a_bot.html) that talks about introspecting. It contains your answer and how to figure it out for yourself.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7yno/how_to_get_text_of_a_submission/"}, {"author": "bboe", "created_utc": 1372649918, "gilded": 0, "name": "t1_catke88", "num_comments": null, "score": 2, "selftext": "Requests for features should be made on [github](https://github.com/praw-dev/praw). If you do add a request, I suggest filling in a little more detail about what you want specifically along with how to accomplish those things outside of PRAW.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/"}, {"author": "bboe", "created_utc": 1372649740, "gilded": 0, "name": "t1_catkc5u", "num_comments": null, "score": 2, "selftext": "You can tell PRAW to decode them. See: https://github.com/praw-dev/praw/issues/186", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7722/praw_html_entities_and_special_characters_in/"}, {"author": "SOTB-human", "created_utc": 1372363173, "gilded": 0, "name": "t1_carhqz6", "num_comments": null, "score": 1, "selftext": "I noticed that special characters are sometimes handled inconsistently in PRAW; they appear as HTML entities in some places and as normal Python characters in other places. To demonstrate this, I'll execute some code to use PRAW to repost the selftext of this submission as a comment on the submission. > quoted text \u0ca0_\u0ca0 < >", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7722/praw_html_entities_and_special_characters_in/"}, {"author": "SOTB-human", "created_utc": 1372363359, "gilded": 0, "name": "t1_carhtnh", "num_comments": null, "score": 1, "selftext": "Notice how the signs are changed, and the \"quoted text\" indicator becomes a literal > sign. Here is the code used to generate the comment: >>> import praw >>> r = praw.Reddit(\"commenting tester by /u/SOTB-human\") >>> r.login(\"SOTB-human\",\"\") >>> submission = r.get_submission(submission_id = \"1h7722\") >>> submission.selftext u\"I noticed that special characters are sometimes handled inconsistently in PRAW; they appear as HTML entities in some places and as normal Python characters in other places. To demonstrate this, I'll execute some code to use PRAW to repost the selftext of this submission as a comment on the submission.\\n\\n> quoted text\\n\\n \\u0ca0_\\u0ca0\\n < >\" >>> comment = submission.add_comment(submission.selftext) >>> comment.body u\"I noticed that special characters are sometimes handled inconsistently in PRAW; they appear as HTML entities in some places and as normal Python characters in other places. To demonstrate this, I'll execute some code to use PRAW to repost the selftext of this submission as a comment on the submission.\\n\\n&gt; quoted text\\n\\n \\u0ca0_\\u0ca0\\n &lt; &gt;\" Is there any way to prevent this, short of manually replacing all `&` entities with their equivalents?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7722/praw_html_entities_and_special_characters_in/"}, {"author": "_Daimon_", "created_utc": 1372326310, "gilded": 0, "name": "t1_car6xfy", "num_comments": null, "score": 1, "selftext": "Reddit uses Markdown to parse the text uses post into pretty looking text. Making a comment via PRAW i.e. the Reddit API is no different. The text gets formatted in the same way. Go to /r/test and try posting that line. You'll notice that it doesn't have a newline either. For more info on Markdown (and how to do what you want) check out this [older post](http://www.reddit.com/r/reddit.com/comments/6ewgt/reddit_markdown_primer_or_how_do_you_do_all_that/c03nik6), which IIRC is still the most accurate. If you're feeling adventurous, then you can look at the [source code for Reddits markdown parser](https://github.com/reddit/snudown) on github.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "dakta", "created_utc": 1372312679, "gilded": 0, "name": "t1_car4khp", "num_comments": null, "score": 1, "selftext": "This is a Python question, not a PRAW question. In python, string literals including line break can be enclosed in double or single quote strings. Quotes are interchangeable, and thus a matter of style. You want the string literal for new line: `\\n` AFAIK that pseudocode is totally valid, at least it's valid Python. Unless you're asking if you can spread that string concatenation over multiple lines, in which case the answer is 'yes, you can'. To split pretty much arbitrary lines, use this: http://stackoverflow.com/questions/53162/how-can-i-do-a-line-break-line-continuation-in-python", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "_Daimon_", "created_utc": 1372206785, "gilded": 0, "name": "t1_caq8qev", "num_comments": null, "score": 2, "selftext": "Yes it does. Try using the `vars` method on a Comment object. For more details see [this tutorial on introspection in PRAW's documentation](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h2p60/does_praw_provide_access_to_traits_of_comments/"}, {"author": "_deffer_", "created_utc": 1372174210, "gilded": 0, "name": "t1_capwfnx", "num_comments": null, "score": 2, "selftext": "Okay - so I downloaded Notepad++ and everything looks great. All I have to do is save the file as a `.py` file? Then in the cmd.exe, I just type `python filename.py' and it will run, or do I have to put the file somewhere specific? I've already added python to the environment variables, so when I open the cmd prompt I just have to type python and it's ready. Would I have to manually run it every so often, or is there something I can put in the code to tell it to run itself every 10 minutes (600 seconds)? I've been reading [this page](https://praw.readthedocs.org/en/latest/#a-few-short-examples) and think I've found the few lines that I would need to put in to grab the information I want. I want to try it out before moving on to the (assumingly) more difficult part of posting that information in a new subreddit, unless you know how and can explain like I'm an idiot.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "Lh2You", "created_utc": 1372175384, "gilded": 0, "name": "t1_capwv9u", "num_comments": null, "score": 1, "selftext": "No you do not have to put it anywhere specific. Yes it will run if you just type in \"python filename.py\" when in the same directory. You would need to manually run itself after you make a change to the code but if you wanted it to loop infinitely after running it you could do something like this: import praw import time while 1: time.sleep(seconds) Note: This requires importing time. Doing this would infinitely loop forever and you could change the variable to something else in order to allow the program to stop itself if a certain condition is met such as import praw import time runcount = 0 runrep = 1 while runrep == 1: time.sleep(seconds) runcount = runcount + 1 if runcount >= 20: runrep = 0 What this will do is it will run the code 20 times, gradually counting the amount of times it has been run. When it reaches 20 runs it will stop. I haven't really experimented with posting posts to a specific subreddit (I can take a guess at it but I have no clue on how to format the text posted) but just a warning. If you try to do something such as message someone with less than ~10 link karma it will give you a CAPTCHA to fill out (it will give you a URL to enter which will link you to a CAPTCHA). In the docs it says that the code to post a SELF post to a specific subreddit is r.submit('reddit_api_test', 'submission title', text='body'). As far as I can tell, the first one is the subreddit, the second the title, and the third the text in the body. This will need a login to complete though which is also mentioned on the page at #2.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "_deffer_", "created_utc": 1372177475, "gilded": 0, "name": "t1_capxns1", "num_comments": null, "score": 1, "selftext": "Can I avoid the captcha if I make the bot an approved submitter or moderator to the subreddit? So, I can't figure out how to just open cmd.exe and type `python defferbot.py` to make it run, it says errno 2 cannot find file or directory or something similar. I can change the directory to C:\\python27 and then type `defferbot.py` and it will accept it, but it won't do anything. The cursor moves to the next link, the underscore blinks, but nothing ever runs. Here is my code: import praw import time while 1: r = praw.Reddit(user_agent='_deffer_') submissions = r.get_subreddit('gameswap').get_new(limit=5) [str(x) for x in submissions] Am I messing something up?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "Lh2You", "created_utc": 1372178203, "gilded": 0, "name": "t1_capxxiz", "num_comments": null, "score": 1, "selftext": "I don't know if you can avoid the CAPTCHA by being a moderator (I am not one). You may not have been in the right directory at the time when you tried to run it. Your code has a flaw as far as I can tell. You put your str(x) code outside of the while loop which means that it will not print until it exits the while loop. Fixed code is below. Also, you can leave your user agent before the while loop as you only need to declare it once. import praw import time while 1: r = praw.Reddit(user_agent='_deffer_') submissions = r.get_subreddit('gameswap').get_new(limit=5) [str(x) for x in submissions]", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "Lh2You", "created_utc": 1372180489, "gilded": 0, "name": "t1_capyt3p", "num_comments": null, "score": 1, "selftext": "I have absolutely no idea why it is not working. Based on the result that I got by typing it in in Python, I assumed that you were going for the title. This rewritten code achieves basically the same result. Sorry, I have literally no idea why it is not working when it is put into a file. import praw import time while 1: r = praw.Reddit(user_agent='_deffer_') submissions = r.get_subreddit('gameswap').get_new(limit=5) for submission in submissions: print(submission.title) Since the only difference is when it is typed in (and the >>> is just the signal to tell you that you can type something in) I really have no clue. My only guess is that for some reason, x is defined as having no value when you run the program but does gain a value when you type it in. I assume this because when I run the code without the while loop, there is a line without any information on it and then it ends. Once again, sorry.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "vacantmentality", "created_utc": 1371967201, "gilded": 0, "name": "t1_caog6qi", "num_comments": null, "score": 2, "selftext": "I think what you're looking for is: if isinstance(item, praw.objects.Submission): # stuffs elif isinstance(item, praw.objects.Comment): # other stuffs I got that from the source code for [AutoModerator](https://github.com/Deimos/AutoModerator/blob/master/modbot.py), written by /u/Deimorz. I've never used [isinstance](http://docs.python.org/2/library/functions.html#isinstance) before (I'm still learning python myself) but I think this is the way you would do it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "brucemo", "created_utc": 1371981561, "gilded": 0, "name": "t1_caoiflk", "num_comments": null, "score": 2, "selftext": "I haven't explored this, in part since I didn't really get an answer. They are different, but this would be another inelegant way to do any of this stuff. A submission: http://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/ A comment: http://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/caoidsy If you do `rh.get_submission` on either URL, obj.permalink will equal the first URL.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "RandomIndianGuy", "created_utc": 1377278039, "gilded": 0, "name": "t1_cbtugnh", "num_comments": null, "score": 1, "selftext": "I was wondering about the same thing and found an answer. If you haven't found it yet, then, [here](http://stackoverflow.com/a/12738719/2711873) is a stack overflow answer by /u/bboe who is (i think) one of the devs working on praw.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "_Daimon_", "created_utc": 1370976304, "gilded": 0, "name": "t1_cagruqw", "num_comments": null, "score": 2, "selftext": "You can use the `place_holder` attribute to only have reddit return results until it meets that submission. import praw r = praw.Reddit('placeholder example by u/_daimon_') subreddit = r.get_subreddit('redditdev') for submission in subreddit.get_new(limit=100, place_holder='1bu7ak'): print submission.title This will return all results younger than the id **on the assumption that a submission with that id can be found**. This is important. If it cannot be found, say because the submission was deleted, then this script will return the last 100 submissions. So you need an additional test to see when you've reached an \"old\" submisison. There are three different ways of doing this. * Keep the id of the of the last 5 submissions you've handled and test if the id matches one of them. * Use the ``created_utc`` value to see when you reach posts as old as your last processed post or older, watch out for submissions at exactly the same time. * All Reddit ids are in base36 and sequential after creation time. So you've reached an already processed post when you encounter a id with a lower or equal base36 value to your last processed post. Hope this helped.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/"}, {"author": "stickytruth", "created_utc": 1370921408, "gilded": 0, "name": "t1_cagd5wo", "num_comments": null, "score": 1, "selftext": "Edit: Now using this comment and submission in the example Getting the submission of a comment >>> import praw >>> r = praw.Reddit(user_agent=YourUserAgentHere) >>> comment = r.get_info(thing_id='t1_cagd5wo') retrieving: http://www.reddit.com/api/info/.json params: {'id': 't1_cagd5wo'} data: None >>> comment.name u't1_cagd5wo' >>> comment.parent_id u't3_1g308t' >>> submission = comment.submission retrieving: http://www.reddit.com/comments/1g308t.json params: None data: None >>> submission.name u't3_1g308t'", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/"}, {"author": "brucemo", "created_utc": 1370922700, "gilded": 0, "name": "t1_cagdp0p", "num_comments": null, "score": 1, "selftext": "Mine then becomes: def ParentObj(self, obj): assert type(obj) == praw.objects.Comment if obj.is_root: return obj.submission return self.rh.get_info(thing_id=obj.parent_id) This code works, and addresses the concerns I had. Thank you very much.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/"}, {"author": "jerenept", "created_utc": 1370834690, "gilded": 0, "name": "t1_cafnqbw", "num_comments": null, "score": 1, "selftext": "When you get more karma in a particular subreddit your timeout will go down. Just hang in there, and maybe use the message from the praw.RateLimitExceeded exception to wait for the correct amount of time?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/"}, {"author": "bboe", "created_utc": 1370482842, "gilded": 0, "name": "t1_cad1uqp", "num_comments": null, "score": 1, "selftext": "PRAW's test suite is usually a good place to look for how to use something: https://github.com/praw-dev/praw/blob/master/praw/tests/__init__.py#L1239", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "brucemo", "created_utc": 1370501281, "gilded": 0, "name": "t1_cad7iq7", "num_comments": null, "score": 1, "selftext": "In case anyone comes along and sees this, here is a useful link: https://github.com/praw-dev/praw/issues/186", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "bboe", "created_utc": 1370501158, "gilded": 0, "name": "t1_cad7hum", "num_comments": null, "score": 1, "selftext": "The '\\n' is not a problem -- that's how you represent a newline in a string. It's the '&gt' which is an issue because reddit returns the results that way. You either have to replace those with their actual values yourself, or enable PRAW's decoding of HTML entities. Edit: `r.config.decode_html_entities = True` should be how you enable the decoding of entities. That should allow you to call `sub.set_stylesheet(sub.get_stylesheet()['stylesheet'])`.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "bboe", "created_utc": 1370500125, "gilded": 0, "name": "t1_cad79x1", "num_comments": null, "score": 1, "selftext": "Is there a , or & in the CSS? If so then those are probably causing the problem as reddit encodes those characters. One of the PRAW settings enables decoding of all html entities (I don't remember what it's called) but enabling that may fix your problem.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "stickytruth", "created_utc": 1370215326, "gilded": 0, "name": "t1_caawjoy", "num_comments": null, "score": 1, "selftext": "Is that something I can do in my application, or would I need to edit PRAW directly?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "stickytruth", "created_utc": 1370228015, "gilded": 0, "name": "t1_cab0lvb", "num_comments": null, "score": 1, "selftext": "I think I've stumbled on a potential cause. Using just requests.get(url) the logging shows: INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): proxy.server But PRAW's connections show: INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.reddit.com I edited the selftext with some more information. How can I have PRAW use PythonAnywhere's proxy? AFAIK, there's no username or password for it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "stickytruth", "created_utc": 1370218531, "gilded": 0, "name": "t1_caaxhh5", "num_comments": null, "score": 1, "selftext": "I'll try reproducing with Requests. The script I'm trying to use PRAW with fails at the first request, though. Every time.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "bboe", "created_utc": 1370210833, "gilded": 0, "name": "t1_caav7io", "num_comments": null, "score": 1, "selftext": "~~Where do you see a suggestion to use `comments_flat`?~~ Nevermind, I see that the [page](https://praw.readthedocs.org/en/latest/pages/comment_parsing.html) is out of date (at least the \"complete\" example at the end). ~~Updating...~~ It's fixed now. Edit: /u/Buffer_Underflow has the correct answer.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fhjbt/praw_error_when_using_flat_comments/"}, {"author": "bboe", "created_utc": 1370045055, "gilded": 0, "name": "t1_ca9tdmg", "num_comments": null, "score": 2, "selftext": "The index is usually a pretty good place to look: https://praw.readthedocs.org/en/latest/genindex.html Interesting that readthedocs doesn't do matching within function names. Edit: And the answer is yes.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fe7pj/does_praw_support_wiki_stuff/"}, {"author": "im14", "created_utc": 1369945762, "gilded": 0, "name": "t1_ca910ic", "num_comments": null, "score": 1, "selftext": "Unfortunately I can't afford the luxury of sleeping that long (my current sleep time is 15 sec) because I have to scrape comments from /r/all in the same loop. If I sleep too long, I won't be able to scrape 'em all - reddit.subreddits.get_comments(limit=99999) seems to be giving me at most ~700-800 comments per call; if I sleep for 30 seconds and meanwhile 1,000 comments get posted, I'll miss ~200-300 of them. One solution would be to run check_inbox() and check_comments() in separate processes, but then I'll be messing with API calls/second limit that's handled by PRAW. So what I can do is have two different bot accounts, one to check inbox, and one to scrape comments, running from two different hosts.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "bboe", "created_utc": 1369945936, "gilded": 0, "name": "t1_ca912vn", "num_comments": null, "score": 2, "selftext": "Also -- you might be interested in using [comment_stream](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.helpers.comment_stream).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "created_utc": 1369945064, "gilded": 0, "name": "t1_ca90qq6", "num_comments": null, "score": 1, "selftext": "At this point I don't check the return value because I rely on PRAW to throw an exception if anything goes wrong (which have worked well up to this point). Since no exception is thrown, I assume Reddit has reported that message has been marked as read successfully. My fix now is to store every incoming message in database and check for duplicates, which solves my problem, but it's still not ideal. The odd thing is that after a few runs, the \"unread\" message will finally become \"read\", so the inconsistency in behavior is unsettling.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "bboe", "created_utc": 1369841187, "gilded": 0, "name": "t1_ca85r60", "num_comments": null, "score": 5, "selftext": "PRAW automatically handles the normal delay required between API requests. The RateLimitExceeded exception occurs when you try to do something too frequently such as post comments, or post submissions to subreddits that you are not an approved submitter. I contemplated automatically handling such errors in PRAW, however, I thought it would be odd to sleep upwards of 4 minutes thus it's up to you to properly handle it. Here's an example you can follow to automatically handle RateLimitExceeded exceptions yourself: https://gist.github.com/bboe/1860715", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/"}, {"author": "shaggorama", "created_utc": 1369860933, "gilded": 0, "name": "t1_ca8d3g1", "num_comments": null, "score": 3, "selftext": "bboe is correct as always, but he's not telling you the whole story: your problem isn't the bot, it's the bot's reddit account. If you were using the bot's account through the website, you'd probably be seeing \"you're doing that too often\" messages. I recommend using the bot account as your main account for a few days to accumulate karma. Once reddit sees that your bot's account is a reasonably well behaved account, you shouldn't see those errors anymore and praw will handle the minimal rate limiting your bot will require.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/"}, {"author": "radd_it", "created_utc": 1369715084, "gilded": 0, "name": "t1_ca79s5i", "num_comments": null, "score": 2, "selftext": "There's a way to do a reddit search by UTC time (seconds since whenever). I'd suggest \"randomly\" picking a time and then trying to find your \"random\" post that way. Roll a random time, grab the top 25 posts, and then see which of those first matches your criteria. No idea how you'd do that in PRAW though.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f61mp/is_it_possible_to_roll_random_threads_with_given/"}, {"author": "bboe", "created_utc": 1369704125, "gilded": 0, "name": "t1_ca765o0", "num_comments": null, "score": 2, "selftext": "https://praw.readthedocs.org/en/latest/pages/configuration_files.html Edit: Just providing the link so other people can reference it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "emilvikstrom", "created_utc": 1369650204, "gilded": 0, "name": "t1_ca6s6r6", "num_comments": null, "score": 1, "selftext": "\"/home/foobar\" is meant to be your own home directory. \"foobar\" is just a placeholder phrase, like \"example\". If your username is \"whodunit28\" you should look in \"/home/whodunit28/.config/\". You can create the praw.ini file yourself. Praw should have a default praw.ini somewhere that you can copy.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "lawenforcerbot", "created_utc": 1369656231, "gilded": 0, "name": "t1_ca6swpg", "num_comments": null, "score": 1, "selftext": "Thanks. Actually I was able to locate praw.ini in \"/usr/local/lib/python2.7/dist-packages/praw\" and added >[reddit.com] >http_proxy:user:password@host:port to the file replacing username, pwd, etc. But it still isn't working. I am getting this error: >e 327, in send > raise ConnectionError(e) >requests.exceptions.ConnectionError: >HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with >url: /r/opensource/.json?limit=5 (Caused by : [Errno 111] >Connection refused) What I am doing wrong? Edit: OP here. Sorry, I posted with my another account.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "aperson", "created_utc": 1369651425, "gilded": 0, "name": "t1_ca6sbse", "num_comments": null, "score": 1, "selftext": "If you created one and lost it (or whatever), use `find`. `find` is your friend, learning it will serve you well. find /home -iname praw.ini", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "Morphiac", "created_utc": 1369464164, "gilded": 0, "name": "t1_ca5n8tr", "num_comments": null, "score": 1, "selftext": "EDIT: I posted it on the wrong account, but this is the error message I get. Traceback (most recent call last): File \"C:\\Python33\\Scripts\\MeeshuBot.py\", line 3, in r = praw.Reddit(user_agent = 'An automated reddit project - Meeshu') File \"C:\\Python33\\lib\\site-packages\\praw-2.1.3-py3.3.egg\\praw\\__init__.py\", line 993, in __init__ super(AuthenticatedReddit, self).__init__(*args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw-2.1.3-py3.3.egg\\praw\\__init__.py\", line 495, in __init__ super(OAuth2Reddit, self).__init__(*args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw-2.1.3-py3.3.egg\\praw\\__init__.py\", line 611, in __init__ super(UnauthenticatedReddit, self).__init__(*args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw-2.1.3-py3.3.egg\\praw\\__init__.py\", line 286, in __init__ update_check(__name__, __version__) File \"C:\\Python33\\lib\\site-packages\\update_checker-0.5-py3.3.egg\\update_checker.py\", line 146, in update_check result = checker.check(package_name, package_version, **extra_data) File \"C:\\Python33\\lib\\site-packages\\update_checker-0.5-py3.3.egg\\update_checker.py\", line 54, in wrapped retval = function(obj, package_name, package_version, **extra_data) File \"C:\\Python33\\lib\\site-packages\\update_checker-0.5-py3.3.egg\\update_checker.py\", line 105, in check headers=headers) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\api.py\", line 99, in put return request('put', url, data=data, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\api.py\", line 44, in request return session.request(method=method, url=url, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\sessions.py\", line 335, in request resp = self.send(prep, **send_kwargs) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\sessions.py\", line 438, in send r = adapter.send(request, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\adapters.py\", line 292, in send timeout=timeout File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\packages\\urllib3\\connectionpool.py\", line 423, in urlopen conn = self._get_conn(timeout=pool_timeout) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\packages\\urllib3\\connectionpool.py\", line 238, in _get_conn return conn or self._new_conn() File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\packages\\urllib3\\connectionpool.py\", line 205, in _new_conn strict=self.strict) File \"C:\\Python33\\lib\\http\\client.py\", line 737, in __init__ DeprecationWarning, 2) File \"C:\\Python33\\lib\\idlelib\\PyShell.py\", line 59, in idle_showwarning file.write(warnings.formatwarning(message, category, filename, AttributeError: 'NoneType' object has no attribute 'write'", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": null, "created_utc": 1369497615, "gilded": 0, "name": "t1_ca5sfwc", "num_comments": null, "score": 1, "selftext": "It looks like you're running your script in IDLE; try running it outside IDLE and if that works my guess is it's doing something PRAW doesn't like. I know IDLE (especially on Windows) has had issues in the past from doing things like setting stderr to None.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": "bboe", "created_utc": 1369357057, "gilded": 0, "name": "t1_ca4u1so", "num_comments": null, "score": 4, "selftext": "`requests.exceptions.HTTPError` is what you want. Here's an example: https://github.com/praw-dev/praw/blob/master/praw/helpers.py#L25", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "im14", "created_utc": 1369709217, "gilded": 0, "name": "t1_ca77vww", "num_comments": null, "score": 2, "selftext": "After some trial and error I think I have got all the cases handled. Here's the code for anyone looking in the future. import logging from requests.exceptions import HTTPError from praw.errors import ExceptionList, APIException, InvalidCaptcha, InvalidUser, RateLimitExceeded from socket import timeout def _reddit_reply(msg, txt): \"\"\" Reply to a comment/message on Reddit Retry if Reddit is down \"\"\" lg = logging.getLogger() while True: try: msg.reply(txt) break except APIException as e: lg.warning(\"_reddit_reply(): failed (%s)\", str(e)) return False except ExceptionList as el: for e in el: lg.warning(\"_reddit_reply(): failed (%s)\", str(e)) return False except (HTTPError, RateLimitExceeded) as e: if str(e) == \"403 Client Error: Forbidden\": lg.warning(\"_reddit_reply(): banned to reply to %s\", msg.permalink) return False lg.warning(\"_reddit_reply(): Reddit is down (%s), sleeping...\", str(e)) time.sleep(30) pass except timeout: lg.warning(\"_reddit_reply(): Reddit is down (timeout), sleeping...\") time.sleep(30) pass except Exception as e: raise", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "bboe", "created_utc": 1369076309, "gilded": 0, "name": "t1_ca2huwz", "num_comments": null, "score": 2, "selftext": "I am at a loss too as the error is occurring in the python standard library. Did you use `pip` or `easy_install` to install PRAW? If not, you may have installed the wrong version of packages. Step one would be to make sure the environment is setup correctly. If that doesn't work, you may want to see if you can replicate the issue using requests directly and if so file a bug there: https://github.com/kennethreitz/requests/issues?state=open", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/"}, {"author": "Prcrstntr", "created_utc": 1369084145, "gilded": 0, "name": "t1_ca2ksqb", "num_comments": null, "score": 1, "selftext": "I am having this same problem http://www.reddit.com/r/learnpython/comments/1epc9y/praw_i_thought_i_installed_it_with_pip_but/", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/"}, {"author": "djimbob", "created_utc": 1368743710, "gilded": 0, "name": "t1_ca07qp0", "num_comments": null, "score": 3, "selftext": "Let's simplify things. First can you get praw to work? Go to the interactive python (python.exe) terminal and try >>> import praw >>> r = praw.Reddit(user_agent = 'praw test - undergroundmonorail') >>> r.login() Username: undergroundmonorail Password for undergroundmonorail: >>> r.send_message('undergroundmonorail', 'hello', 'body') {u'errors': []} If you get that far it works (should send a message to undergroundmonorail with hello body -- you have to check your inbox to see). Otherwise, something is going wrong, likely with PyScripter (your IDE) implementing stdin wrong which interacts poorly with praw. Reading through the traceback, it does not seem to be a reddit, praw, or python issue, but a \"PyScripter\" issue, where pyscripter (replaced `sys.stdin` with a `DebugOutput` instance) that doesn't have an attribute `closed` that is there in the normal [sys.stdin](http://docs.python.org/2/library/sys.html#sys.stdin) (a [file object](http://docs.python.org/2/library/stdtypes.html#file-objects) that takes standard input from the terminal and should have a closed attribute). So when praw checks that the stdin hasn't been closed yet, it raises an exception, as the DebugOutput overwriting the normal sys.stdin implemented by your version of PyScripter doesn't have it. This possibly is due to running an old version of pyscripter; so if you upgrade it could work. I'm a linuxer, so don't know about windows IDEs (personally, I like `ipython` and a standard text editor (run it with `%run file.py`) and pycharm for large projects, but here's a list of windows [IDEs](http://stackoverflow.com/questions/126753/is-there-a-good-free-python-ide-for-windows).)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "bboe", "created_utc": 1368743675, "gilded": 0, "name": "t1_ca07qal", "num_comments": null, "score": 1, "selftext": "Are you doing something funny with sys.stdin? PRAW assumes sys.stdin is a file stream, I don't know where this `DebugOutput` object came from, but I suppose PRAW should make sure sys.stdin is of the expected type. Can you provide me insight on how you are running your code so I can best handle the it? It seems like even without this bug, you'd still get a CaptchaException as I would have to assume you cannot type in captchas.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "undergroundmonorail", "created_utc": 1368743786, "gilded": 0, "name": "t1_ca07rla", "num_comments": null, "score": 1, "selftext": "I'm not doing anything funny anywhere. The one thing I thought of that might cause a problem is that I'm on my Windows machine today (instead of my usual Linux one) and he PRAW install wasn't quite as foolproof. When I get a chance I'll try the same code on there and see if I get a better result.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "ExceedinglyEdible", "created_utc": 1368517708, "gilded": 0, "name": "t1_c9yg5du", "num_comments": null, "score": 2, "selftext": "$ pip install --upgrade praw or else $ pip install praw==", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "noun_exchanger", "created_utc": 1368519240, "gilded": 0, "name": "t1_c9ygchz", "num_comments": null, "score": 1, "selftext": "At what point should I be typing that code into the command prompt? Do i type it directly into the command prompt or with python.exe? Do I have to import PIP and/or PRAW before I type that? I've tried most of those combinations but it never seems to recognize the \"$\".", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "ExceedinglyEdible", "created_utc": 1368520282, "gilded": 0, "name": "t1_c9ygh5l", "num_comments": null, "score": 2, "selftext": "Oh, I skipped-out on the whole Windows part. (that $ symbol is Linux boilerplate stuff, btw) PIP is an executable, which means you'll probably find it in `C:\\Python\\bin\\pip.exe`. Open a command prompt (`cmd.exe` in Start>Run) then type: cd C:\\Python\\bin pip install --upgrade praw For easier access, you can add the Python bin folder to your PATH through Properties of \"My Computer\". Google \"windows PATH variable\" for help. Doing so will let you type \"pip \" from any folder while in the command prompt.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "shaggorama", "created_utc": 1368482718, "gilded": 0, "name": "t1_c9y5gxz", "num_comments": null, "score": 5, "selftext": "My end game here is for this feature to be an attribute on the subreddit object in praw. That way if a bot is scraping comments in /r/all, it could quickly see if the subreddit the comment came from is attributed to a subreddit the bot is welcome in. This doesn't necessarily preclude your idea (I'm guessing you're thinking something along the lines of a page titled \"nobots\" or something). But I imagine this being delivered in reddit's JSON response. The problem with implementing it on the wiki is that this would mean an additional GET request to the subreddit's wiki each time praw builds a new subreddit instance. I really don't want the implementation of something as trivial as this flag to have any potential to slow people (or reddit) down.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "bboe", "created_utc": 1368484741, "gilded": 0, "name": "t1_c9y66jf", "num_comments": null, "score": 3, "selftext": "Yeah to be effective (and efficient) such a flag would need to be added to the json data for every submission and comment (when not part of an existing subtree) as by default PRAW only receives the subreddit name when fetching the listings most bots are interested in.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "bboe", "created_utc": 1367808586, "gilded": 0, "name": "t1_c9t6xa9", "num_comments": null, "score": 3, "selftext": "Unless you want to do something that PRAW doesn't provide yet (e.g., fetching listings by domain) you shouldn't use `get_content` directly. It looks like what you actually want to use is `r.get_submission(url=)`. [This answer](http://stackoverflow.com/a/12738719/176978) I provided on SO may also be useful.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dro2b/problems_with_the_praw_get_content_method/"}, {"author": "_Daimon_", "created_utc": 1367239616, "gilded": 0, "name": "t1_c9ov7oa", "num_comments": null, "score": 1, "selftext": "I would recommend you create your own private test subreddit, this is usually the quickest way of finding out how stuff like this works. There is a setting that doesn't allow link flair, makes it mod only or allow mods + post creators to set it. Only one of the created css templates can be used, but the text can be anything. What link flair a submission has is exposed via the API yes. Try doing the following and see if you can find something interesting in the output. import praw from pprint import pprint r = praw.Reddit('link flair testing by u/_Daimon_') subreddit = r.get_subreddit('askscience') submission = next(subreddit.get_hot()) pprint(vars(submission)) There is AFAIK, no way to search Reddit for all submissions with a specific link_flair.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/"}, {"author": "bboe", "created_utc": 1367010342, "gilded": 0, "name": "t1_c9nc6n5", "num_comments": null, "score": 2, "selftext": "Just added the documentation page: https://praw.readthedocs.org/en/latest/pages/multiprocess.html", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d2nr4/multiprocess_praw_testing_needed/"}, {"author": "killver", "created_utc": 1367237304, "gilded": 0, "name": "t1_c9ousdz", "num_comments": null, "score": 2, "selftext": "If you have the PRAW Comment Object, there is a field \"link_id\" that specifies the corresponding submission.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/"}, {"author": "radd_it", "created_utc": 1366902594, "gilded": 0, "name": "t1_c9mef3l", "num_comments": null, "score": 0, "selftext": "> http://www.reddit.com/by_id/t1_c9mcwag Just kidding, but damn that'd be nice. I've not used PRAW, but how do you have a comment ID without its post ID? Do you not get a link_id with it? What about fries? Do you get fries with that?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/"}, {"author": "ketralnis", "created_utc": 1366758991, "gilded": 0, "name": "t1_c9la8z5", "num_comments": null, "score": 1, "selftext": "Are you hitting reddit too hard? e.g. by disabling PRAW's built-in rate limiting", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "bboe", "created_utc": 1366825449, "gilded": 0, "name": "t1_c9ls7is", "num_comments": null, "score": 1, "selftext": "What version of PRAW are you running? How long does it take to timeout? The setting should be at 45 seconds.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "bboe", "created_utc": 1366654159, "gilded": 0, "name": "t1_c9kdtrp", "num_comments": null, "score": 2, "selftext": "What version of PRAW are you using as I cannot reproduce. Also to remove your own posting you should use `delete` not `remove`.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "bboe", "created_utc": 1366852712, "gilded": 0, "name": "t1_c9m2o1y", "num_comments": null, "score": 2, "selftext": "Either version of python is fine. import praw user='redditnick' passwd='password' reddit.login(user,passwd) That code example doesn't make sense. Where is `reddit` defined there? Assuming you did `reddit = praw.Reddit('user-agent string')` at some point, then either you're neglecting to share some of your code, or your PRAW installation is not as it should be.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "lamerx", "created_utc": 1366935662, "gilded": 0, "name": "t1_c9mqx24", "num_comments": null, "score": 1, "selftext": "sorry, its in a test scrippt with a bunch of commented lines tha ti uncomment for reading stuff, i didnt want to paste that crap in here import praw user='redditnick' passwd='password' reddit = praw.Reddit(user_agent='BotScript 1.0, by u/LamerX') print \"Logging in...\" reddit.login(user,passwd) print \"Logged In\" target = reddit.get_submission(submission_id='1cwepg') target.upvote()", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "_Daimon_", "created_utc": 1366672813, "gilded": 0, "name": "t1_c9kkpxp", "num_comments": null, "score": 1, "selftext": "He meant version of PRAW, not python. Try doing python -c \"import praw; print praw.__version__\" When you're deleting your own posts, you use `delete` not `remove`. `remove` is what moderators use when they delete somebody else's post. Could you post a full program causing the upvote / downvote error like in your first example?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "lamerx", "created_utc": 1366700344, "gilded": 0, "name": "t1_c9ktiak", "num_comments": null, "score": 1, "selftext": "2.0.15 is the praw version ##CODE import praw user='redditnick' passwd='password' reddit.login(user,passwd) print \"Logged In\" target=reddit.get_submission(submission_id='1cwepg') target.upvote() ##OUTPUT Traceback (most recent call last): File \"C:\\Python27\\Scripts\\testB0T2.py\", line 62, in target.upvote() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 447, in upvote return self.vote(direction=1) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 341, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 464, in vote return self.reddit_session.request_json(url, data=data) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 239, in error_checked_function raise error_list[0] NotLoggedIn: `please login to do that` on field `None`", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "frumious", "created_utc": 1366672864, "gilded": 0, "name": "t1_c9kkqkv", "num_comments": null, "score": 1, "selftext": "I guess I'm asking you for a little reading comprehension. I'm asking if anyone has insinuated a persistent store into praw.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "_Daimon_", "created_utc": 1366558918, "gilded": 0, "name": "t1_c9jnm3u", "num_comments": null, "score": 1, "selftext": "The simplest solution is to simply change [cache_timeout in the configuration files](https://praw.readthedocs.org/en/latest/pages/configuration_files.html) to a very large number to prevent items in the cache ever timing out.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "bboe", "created_utc": 1366853194, "gilded": 0, "name": "t1_c9m2uff", "num_comments": null, "score": 1, "selftext": "Just FYI -- in the current development version (soon to be 2.1.0) I've redone how requests are handled by adding a concept of a request handler that must provide two methods, `request` and `evict`. With this addition, it is trivial to provide your own handler that provides a persistent cache: https://github.com/praw-dev/praw/blob/master/praw/handlers.py#L90 In fact, if you can write it in a generic way that works on Windows, OSX, and Linux without additional dependencies I would be happy to add it as one of the \"official\" PRAW handlers. As I imagine, loading from disk should only occur when PRAW is loaded, and writing to disk need only occur when PRAW terminates ([atexit](http://docs.python.org/2/library/atexit.html)). If you need multiprocess support, then something similar should be added to the [multiprocess server](https://github.com/praw-dev/praw/blob/master/praw/multiprocess.py#L17) as it is responsible for the cache in that case.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "ketralnis", "created_utc": 1366586451, "gilded": 0, "name": "t1_c9jwep5", "num_comments": null, "score": 2, "selftext": "It is in-memory only. Your best bet sounds like something like SQLite (which ships with Python) You could replace praw's own cache with SQLite reasonably straight-forwardly", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "bboe", "created_utc": 1366440095, "gilded": 0, "name": "t1_c9iyvjk", "num_comments": null, "score": 6, "selftext": "Catch the 404 exception and continue your crawling. PRAW can raise a number of exceptions that your code should be able to handle.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": null, "created_utc": 1366487677, "gilded": 0, "name": "t1_c9j87d2", "num_comments": null, "score": 1, "selftext": "Where can I find all the different exceptions that PRAW has? I know when posting it sometime requires captcha's and you can also get the \"You're posting too much. Wait a couple minutes.\".", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": "nemec", "created_utc": 1366490803, "gilded": 0, "name": "t1_c9j9222", "num_comments": null, "score": 2, "selftext": "`ClientException`, `OAuthException`, `APIException` are the base classes. https://github.com/praw-dev/praw/blob/master/praw/errors.py You could also inspect the code through reflection and filter out what doesn't belong: >>> import praw >>> dir(praw.errors) ['APIException', 'BadCaptcha', 'ClientException', 'ERROR_MAPPING', 'ExceptionList', 'InvalidUserPass', 'LoginRequired', 'ModeratorRequired', 'NonExistentUser', 'NotLoggedIn', 'RateLimitExceeded', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '_build_error_mapping', 'inspect', 'six', 'sys']", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": "nemec", "created_utc": 1366492195, "gilded": 0, "name": "t1_c9j9fmk", "num_comments": null, "score": 1, "selftext": "Looks like you'll have to monitor `sys.stdout` for a [prompt](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L178). It'll give you a .png for you to display, then send the response to `sys.stdin` terminated by a newline. Anything with the `requires_captcha` decorator in the [init](https://github.com/praw-dev/praw/blob/master/praw/__init__.py) file could potentially prompt you for a captcha.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": "nemec", "created_utc": 1365827891, "gilded": 0, "name": "t1_c9e8puu", "num_comments": null, "score": 3, "selftext": "Have you actually tried inspecting a comment in the shell? >>> import praw >>> r = praw.Reddit(user_agent=\"sdlfkn /u/nemec\") >>> s = r.get_submission(submission_id=\"1c7og2\") >>> c = s.comments[0] >>> dir(c) [..., 'approve', 'approved_by', 'author', 'author_flair_css_class', 'author_flair_text', ...] >>> c.author Redditor(user_name='ProtoKun7')", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "_Daimon_", "created_utc": 1365833585, "gilded": 0, "name": "t1_c9ea2r2", "num_comments": null, "score": 3, "selftext": "I see you've already got your answer, but for any fututre questions like this the [PRAW tutorial about introspecting PRAW and reddit](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) should prove helpful :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "_Daimon_", "created_utc": 1365697245, "gilded": 0, "name": "t1_c9d3l9m", "num_comments": null, "score": 3, "selftext": "import praw r = praw.Reddit('user reply example by u/_Daimon_') user = r.get_redditor('nannal') comment_generator = user.get_comments() first_comment = next(comment_generator) print first_comment.body r.login('BOT USERNAME', 'BOT PASSWORD') r.get_inbox() # Logged in users inbox. Hope this helped :) If you have any more questions, then feel free to ask.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "_Daimon_", "created_utc": 1365358126, "gilded": 0, "name": "t1_c9ac2wq", "num_comments": null, "score": 5, "selftext": "You can use the `place_holder` attribute to only have reddit return return results until it meets that submission. import praw r = praw.Reddit('placeholder example by u/_daimon_') subreddit = r.get_subreddit('redditdev') for submission in subreddit.get_new(limit=100, place_holder='1bu7ak'): print submission.title This will return all results younger than the id **on the assumption that a submission with that id can be found**. This is importment. If it cannot be found, say because the submission was deleted, then this script will return the last 100 submissions. If you're doing something where doing the same thing twice would be bad, say running a script that does statistics on submissions to reddit. Then you need additional security to ensure you don't process the same item twice. Keeping the id of the youngest submission you've handled is a good idea, but one id is not enough. You need more. I would recommend at least the youngest 5 submissions. Hope this helped :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bv06t/stuck_at_processing_each_submission_on_a/"}, {"author": "damontoo", "created_utc": 1365325149, "gilded": 0, "name": "t1_c9a5ksm", "num_comments": null, "score": 1, "selftext": "What functionality do you need? I had the same problem so I just wrote a small class that fetches JSON and handles the request limits like PRAW. It's very, very basic but that's all most of my projects need. Also, are you taking Steve Huffman's Udacity course or is this just coincidence?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/"}, {"author": "naive_babes", "created_utc": 1365341141, "gilded": 0, "name": "t1_c9a7i99", "num_comments": null, "score": 2, "selftext": "Just going to update the post. I did manage to get it working, but only after getting rid of dependencies on update_checker. I unzipped all the egg files of the praw installation and put them in the local directory of my app. There is some trouble with the update_checker dependency, and i got rid of it completely. There's an additional problem with obtaining the platform of google app engine while constructing the user agent string. I got rid of that bit as well. Now i have it working fine. No, im not taking the udacity course you speak of. What is it about?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/"}, {"author": "Jjonahjamesonjunior", "created_utc": 1365373618, "gilded": 0, "name": "t1_c9ah7zp", "num_comments": null, "score": 1, "selftext": "Hey, can you elaborate a little bit more about what you did to get this working? I'm new to both App Engine and PRAW.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/"}, {"author": "AndrewNeo", "created_utc": 1364110541, "gilded": 0, "name": "t1_c91ed25", "num_comments": null, "score": 1, "selftext": "This is technically a python question, not a Reddit question, but. 1. Try `easy_install --user praw` first 1. Can you use [pip](https://pypi.python.org/pypi/pip) instead? (`pip install --user praw`) A lot of people would likely recommend it over easy_install, which is considered outdated. You also can't remove packages with easy_install. 2. Can you use [virtualenv](https://pypi.python.org/pypi/virtualenv)? This creates an environment specifically to run your scripts in with whatever packages you want without affecting the rest of the system.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1awkvu/permissions_issue_when_trying_to_easyinstall_praw/"}, {"author": "notanasshole53", "created_utc": 1364229753, "gilded": 0, "name": "t1_c924fq3", "num_comments": null, "score": 1, "selftext": "The problem is the imgur link you guys are looking up. You're pointing at the full-res image, which wasn't what was submitted. Try this link instead: http://imgur.com/jOG9a That works for me: >>> import praw >>> r = praw.Reddit(user_agent='testing /u/StealsTopComments') >>> info = r.get_info(url='http://imgur.com/jOG9a') >>> pprint.pprint(vars(info[0]) ... ) {'_comment_sort': None, '_comments': None, '_comments_by_id': {}, '_info_url': 'http://www.reddit.com/api/info/', '_orphaned': {}, '_populated': True, '_replaced_more': False, '_underscore_names': None, 'approved_by': None, 'author': Redditor(user_name='CocoaBeans'), 'author_flair_css_class': None, 'author_flair_text': None, 'banned_by': None, 'clicked': False, 'created': 1348899894.0, 'created_utc': 1348896294.0, 'distinguished': None, 'domain': u'imgur.com', 'downs': 10, 'edited': False, 'hidden': False, 'id': u'10no3v', 'is_self': False, 'likes': None, 'link_flair_css_class': None, 'link_flair_text': None, 'media': None, 'media_embed': {}, 'name': u't3_10no3v', 'num_comments': 12, 'num_reports': None, 'over_18': False, 'permalink': u'http://www.reddit.com/r/gaming/comments/10no3v/the_pokemon_holy_grail/', 'reddit_session': , 'saved': False, 'score': 14, 'selftext': u'', 'selftext_html': None, 'subreddit': , 'subreddit_id': u't5_2qh03', 'thumbnail': u'http://e.thumbs.redditmedia.com/UG34OE9OjiDQyXid.jpg', 'title': u'The Pokemon holy grail', 'ups': 24, 'url': u'http://imgur.com/jOG9a'}", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1aw7jc/praw_get_info_not_working/"}, {"author": "bboe", "created_utc": 1363979246, "gilded": 0, "name": "t1_c90kcqh", "num_comments": null, "score": 1, "selftext": "Flask (or another lightweight framework) + PRAW and you can do this very simply.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1atckm/a_seemingly_simple_task_but_i_have_no_clue_any/"}, {"author": "bboe", "created_utc": 1363540625, "gilded": 0, "name": "t1_c8xb4p4", "num_comments": null, "score": 1, "selftext": "Yes, it is kind of strange that re-authorization is required each time for the same application and the same scope. I haven't played with the OAuth stuff much, other than the PRAW implementation, but I don't remember specifically if the same behavior occurs when obtaining permanent grants. Have you tried that?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1agkmm/praw_and_oauth/"}, {"author": "_Daimon_", "created_utc": 1363385511, "gilded": 0, "name": "t1_c8wdywn", "num_comments": null, "score": 3, "selftext": "Also, [PRAW's description on the cheeshop is now beautiful :)](https://pypi.python.org/pypi/praw) It's not a fact that's easy to find, but PyPi cannot parse markdown. If you want it to be pretty, then you need reStructured text.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/"}, {"author": "_Daimon_", "created_utc": 1362669254, "gilded": 0, "name": "t1_c8reg8s", "num_comments": null, "score": 1, "selftext": "I'm not entirely sure what you're asking. If it's how to use PRAWs `search` method to search for posts matching a title keyword and/or other requirements then you can get more information about the method with. import praw r = praw.Reddit('Search post info example by u/_Daimon') help(r.search) If you didn't know that you needed to use the `search` method to search reddit then you can search for \"search\" on [the search page](https://praw.readthedocs.org/en/latest/search.html?q=search) of PRAWs documentation on ReadTheDocs and it will tell you what methods to use. I'll add a link to the search page from index today or so. If I misunderstood you, or you have more questions then feel free to ask.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/"}, {"author": "_Daimon_", "created_utc": 1362691864, "gilded": 0, "name": "t1_c8rm2u1", "num_comments": null, "score": 1, "selftext": "You can't go further back than a 1000 instances, it's an upstream cache limitation. See [PRAW's wiki](https://praw.readthedocs.org/en/latest/pages/faq.html). Maybe you could just run your script once in a while and get the newest results rather than doing one request going really far back? If you want to know more about how to use PRAWs `search` method to search for posts matching a title keyword and/or other requirements then you can get more information about the method with. import praw r = praw.Reddit('Search post info example by u/_Daimon') help(r.search)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/"}, {"author": "mrg3_2013", "created_utc": 1362693256, "gilded": 0, "name": "t1_c8rmkqw", "num_comments": null, "score": 1, "selftext": "OK. Thanks, _Daimon_. BTW it was a breeze to get praw running - so kudos!", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/"}, {"author": "Pathogen-David", "created_utc": 1362576652, "gilded": 0, "name": "t1_c8qqmwq", "num_comments": null, "score": 2, "selftext": "The error basically means you're getting rate-limited. ([See this thread for more info](http://www.reddit.com/r/redditdev/comments/witfv/redmproductions_humble_flairbot_so_many_429s/)). Have you changed the user agent in your bots settings.json? Make sure it is not a browser's UA and it is unique to your bot. I would make it something like \"`Groompbot/1.0 (Operated by /u/SN4T14)`\" Also, if you changed the api_request_delay value in the [praw config file](https://github.com/praw-dev/praw/blob/master/praw/praw.ini) to something less than 2.0, you need to change it back.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "created_utc": 1362581131, "gilded": 0, "name": "t1_c8qrjw5", "num_comments": null, "score": 2, "selftext": "The bots haven't been run for days, I already completely changed the user agent, and I haven't messed with the praw config file at all, although I'll take a look at it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "bboe", "created_utc": 1362591408, "gilded": 0, "name": "t1_c8qul82", "num_comments": null, "score": 1, "selftext": "Is it possible you are running your script multiple times in parallel? PRAW does not ratelimit properly across multiple processes.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "_Daimon_", "created_utc": 1362606545, "gilded": 0, "name": "t1_c8qzsms", "num_comments": null, "score": 1, "selftext": "Do you know if other reddit scripts / apss (praw or otherwise) are running on the dedicated server?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "created_utc": 1362643282, "gilded": 0, "name": "t1_c8rat2g", "num_comments": null, "score": 1, "selftext": "Nope, nothing, the only thing that's running are my game servers, should I try uninstalling praw just to be safe?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "catmoon", "created_utc": 1362616183, "gilded": 0, "name": "t1_c8r304h", "num_comments": null, "score": 2, "selftext": "I just started getting a similar error on /r/NBA's NBA_MOD bot. It hadn't experienced any down time in weeks and that was just for maintenance. I'm still running the older version of PRAW. This makes me think that the problem is probably on Reddit's end since we both started getting the error at the same time. I wonder if the server issues Reddit has had today has anything to do with it. Could you let me know if anyone figures this out?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "catmoon", "created_utc": 1362701322, "gilded": 0, "name": "t1_c8rp86a", "num_comments": null, "score": 1, "selftext": "I updated praw to the latest and greatest and my bot works again. [[LINK]](https://github.com/praw-dev/praw)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "created_utc": 1362764101, "gilded": 0, "name": "t1_c8s3sja", "num_comments": null, "score": 2, "selftext": "Still broken for me, going to try uninstalling all the praw stuff and reinstalling it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "created_utc": 1362764315, "gilded": 0, "name": "t1_c8s3v2w", "num_comments": null, "score": 2, "selftext": "I'll try that, I had old praw 2.0.10 and praw 2.0.11 stuff, so that might be it, too.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "bboe", "created_utc": 1361832394, "gilded": 0, "name": "t1_c8lle9p", "num_comments": null, "score": 3, "selftext": "/u/Deimorz is mostly correct. If you specify any limit via `limit=XXX` PRAW will implicitly use that value to make each request hitting reddit's 100 item maximum. If you omit the limit, then the limit parameter is omitted in each request as well, thus defaulting to 25. Here's the [source](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L311) if you're interested which also explains the possible keyword arguments these functions take. If you want to explicitly override the url parameter `limit` the `url_data` parameter to `get_hot` (and other similar functions) has been renamed to `params` for consistency. Edit: Added source link.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/"}, {"author": "Deimorz", "created_utc": 1361831899, "gilded": 0, "name": "t1_c8ll89j", "num_comments": null, "score": 2, "selftext": "The new version of praw defaults to using a limit of 100, so you shouldn't need that any more anyway.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/"}, {"author": "lamerx", "created_utc": 1361824725, "gilded": 0, "name": "t1_c8likbb", "num_comments": null, "score": 1, "selftext": "praw-1.0.16-py2.7.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "lamerx", "created_utc": 1361915059, "gilded": 0, "name": "t1_c8m6utz", "num_comments": null, "score": 1, "selftext": "I upgraded one of my python installs with praw-2.0.11-py2.7. the same code block is now giving me this ClientException: Unexpected redirect from http://www.reddit.com/reddits/mine/moderator/.json to http://www.reddit.com/reddits/login.json?dest=%2Freddits%2Fmine%2Fmoderator%2F.json%3Flimit%3D1024", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "bboe", "created_utc": 1361915825, "gilded": 0, "name": "t1_c8m75fj", "num_comments": null, "score": 1, "selftext": "If PRAW didn't think you were logged-in in 2.0+ then you _should_ see a LoginRequired exception. This unexpected redirect to the login page seems to indicate that your session is no longer valid. If you actually are running the above code, I really don't see how that's possible without the login failing with an InvalidUserPass exception.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "_Daimon_", "created_utc": 1361534607, "gilded": 0, "name": "t1_c8jolt7", "num_comments": null, "score": 3, "selftext": "You've hit a bug with PRAW's cache. I've sent bboe [a bugfix](https://github.com/praw-dev/praw/pull/184). BBoe is the maintainer and in charge of versions, but there should be a version increase with this commit. So wait until he pulls and upgrade your version of PRAW. ps. sorry for the terse reply. Bit pressed for time atm.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "shaggorama", "created_utc": 1361202788, "gilded": 0, "name": "t1_c8h89yk", "num_comments": null, "score": 4, "selftext": "I'm guessing you're a kind of new (python) programmer. One of the beautiful things about python is that it's set up to document itself. To ask the environment to help you figure out how to use a particular object or what attributes/methods it has attached to it, pass the object name to dir() and/or help(). type() is also useful: submission.selftext is a string, and all strings have a .lower() method. In addition to the wiki \\_\\_Daimon__ pointed you towards, you might find [this praw documentation page](http://python-reddit-api-wrapper.readthedocs.org/en/latest/) useful. I'm not sure if this is completley up-to-date, but the page says 2.0 so it can't be too far behind.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "_Daimon_", "created_utc": 1361203839, "gilded": 0, "name": "t1_c8h8k2d", "num_comments": null, "score": 2, "selftext": "I didn't intend it. But I can tell if someone knows their reddit if they can write my name :) > In addition to the wiki \\_\\_Daimon\\_\\_ pointed you towards, you might find this praw documentation page[1] useful. I'm not sure if this is completley up-to-date, but the page says 2.0 so it can't be too far behind. I'm pretty sure PRAW uses [service hooks](https://read-the-docs.readthedocs.org/en/latest/webhooks.html) to update the documentation on readthedocs. So it's *always* up-to-date. (readthedocs is pretty awesome).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "_Daimon_", "created_utc": 1361177306, "gilded": 0, "name": "t1_c8h4o49", "num_comments": null, "score": 3, "selftext": "We have an article called [writing a bot](https://github.com/praw-dev/praw/wiki/Writing-A-Bot) on the wiki, which talks about using introspection to discover attributes, methods etc of praw using `help`, `dir`, `vars` and the webway via `json` version of pages. There is also a [code overview](https://python-reddit-api-wrapper.readthedocs.org/en/latest/praw.html) page, which you might also find useful. If you have any other questions, then feel free to ask. :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "shaggorama", "created_utc": 1361116463, "gilded": 0, "name": "t1_c8govhm", "num_comments": null, "score": 3, "selftext": "I understand your concerns, but reddit is just a text based site, and you're just going to be snapping up short XML/JSON packets. I have trouble believing you will suck up significant bandwidth on whatever server you use just with a single reddit bot. As far as best practices, PRAW will control the rate limiting for you, so I recommend that whatever scraping you are doing you only run a single praw.Reddit instance from your box: this way, all calls to reddit are governed to no more than 1 GET every 2 seconds. I don't know how cheap your hosting is, but I suspect you'll be fine. You could always try testing your bot locally to see how much it affects your bandwidth, or just go for it on your external server: worst case, you use your quota for the month.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "Rotten194", "created_utc": 1361143117, "gilded": 0, "name": "t1_c8gvwpn", "num_comments": null, "score": 1, "selftext": "What kind of hosting do you have? Is it a full Nix VM? In that case, you should be fine just grabbing all the dependencies (python, PRAW) and setting it up in a cron job, yeah.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "grendelt", "created_utc": 1371585727, "gilded": 0, "name": "t1_calforj", "num_comments": null, "score": 1, "selftext": "I'm with dreamhost and was going to run a bot, but can't get PRAW to install. :( Any ideas?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "bboe", "created_utc": 1360712860, "gilded": 0, "name": "t1_c8e4o16", "num_comments": null, "score": 2, "selftext": "Thanks for posting! > In this case that will work (even though I am not actually trying to get json data)... So, I'd like to ask bboe to consider putting some sort of _request functionality into supported \"user space\" Every request PRAW makes to reddit's API _should_ receive json data in response. Within PRAW there are only two requests that don't expect json as a response, one for getting a random subreddit, and one for uploading images. Aside from those two cases (which PRAW already handles internally) you should always get json back and that's why `request_json` is part of the _public_ interface and `_request` is not. If you find something that actually requires using `_request` through a supported API endpoint, please [file a bug](https://github.com/praw-dev/praw/issues/new).", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/"}, {"author": "shaggorama", "created_utc": 1360737304, "gilded": 0, "name": "t1_c8ecpgt", "num_comments": null, "score": 1, "selftext": "You should consider adding [how to replace praw.Submission.all_comments_flat](http://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/"}, {"author": "shaggorama", "created_utc": 1359924284, "gilded": 0, "name": "t1_c88qddo", "num_comments": null, "score": 2, "selftext": "Going through the recent commit messages I found helpers.flatten_tree, but if I use this repeatedly the number of items returned just grows and grows apparently without limit. from praw.helpers import flatten_tree as f subm = r.get_submission(submission_id = '178ki0' ) subm.num_comments # 34 len(subm.comments) # 20 len(f(subm.comments)) # 30 len(f(f(subm.comments))) # 82 len(f(f(f(subm.comments))) # 275 wtf?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/"}, {"author": "_Daimon_", "created_utc": 1359930727, "gilded": 0, "name": "t1_c88s9kn", "num_comments": null, "score": 2, "selftext": "> Going through the recent commit messages I found helpers.flatten_tree PRAW has a [Changelog](https://github.com/praw-dev/praw/wiki/Changelog) which is usually a lot better place to find api changes than digging through the commit messages. Two excerpts from it * **[CHANGE]** Remove `comments_flat` property of Submission objects. The new `praw.helpers.flatten_tree` can be used to flatten comment trees. * **[CHANGE]** Remove `all_comments` and `all_comments_flat` properties of Submission objects. The now public method `replace_more_comments` must now be explicitly called to replace instances of `MoreComments` within the comment tree. `help(submission.replace_more_commets)` and `help(praw.helpers.flatten_tree)` provide more information about the usage on these functions, alternatively you can look at the PRAW package on [readthedocs.org](https://python-reddit-api-wrapper.readthedocs.org/en/latest/praw.html) which is also useful as a nice reference. I'll try looking at `flatten_tree` returning more and more results. It shouldn't do that. Hope that helped :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/"}, {"author": "perezdev", "created_utc": 1359777844, "gilded": 0, "name": "t1_c87x8s6", "num_comments": null, "score": 2, "selftext": "Have you updated PRAW to the latest version? There was a recent change to the API that caused my libraries and apps to break. But PRAW was just recently updated to comply with the new changes.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "bboe", "created_utc": 1359828814, "gilded": 0, "name": "t1_c885jsd", "num_comments": null, "score": 2, "selftext": "We need more information. Does it fail on the same accounts? Do you get the same request failures using your browser (turn on request logging in praw.ini)? How are you checking to see if an account exists?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "MrFanzyPanz", "created_utc": 1360801851, "gilded": 0, "name": "t1_c8eswbj", "num_comments": null, "score": 1, "selftext": "def accountscrape(): #initialize necessary imports import praw import time r = praw.Reddit(user_agent='MrFanzyPanz Account Scraper :D') #open necessary files. txt format for now, CSV or ORM database once out of testing. name = open('dataset_accountlist.txt', 'r') userdata = open('accountdata.txt', 'a') userdata.write('username, submission type, submission number, subreddit, submission_created_utc , current_utc , score , usercommentK , userlinkK , is_mod , account_created_utc , text , num_comments , is_self , over_18 , ups, downs , url , is_imgur \\n') #pull a username from the namelist document. this must be a loop which #iterates until there are no more names in the file, since we do not know #how many are going to be in the file once data collection is finished. while True: usertemp = name.readline() #", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "_Daimon_", "created_utc": 1359165459, "gilded": 0, "name": "t1_c83pw96", "num_comments": null, "score": 4, "selftext": "There is a server-side cache that store most stuff for 30 secs, in addition the API Rules say you shouldn't request the same resource more than once every 30 secs. PRAW handles this by creating an internal cache, so if you request the same resource within a 30 sec timespan then you'll be served the cached results rather than sending a new request. It is this internal cache timeout you reduced, but obviously this does nothing to the server-side cache. **API Rules** > Most pages are cached for 30 seconds, so you won't get fresh data if you request the same page that often. Don't hit the same page more than once per 30 seconds. So just make a request every 30 seconds and ensure the limit is high enough that you get all the new results. But don't request far more than you need to reduce the load on reddit.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "bboe", "created_utc": 1359167854, "gilded": 0, "name": "t1_c83qjyp", "num_comments": null, "score": 6, "selftext": "> With that said, are you able to think of a reason for new threads not to appear even after a minute or two has passed? I could somehow manage with a 30-second delay, but more than that would be too much. This very likely has to do with the Akamai cache. reddit uses Akamai to help reduce load on their servers, and so when you request a page as a non-authenticated user that request goes through Akamai which is set to cache the page for 1 minute (if I'm not mistaken). If you make a request just before a new item is posted it will take 1 minute before the cache updates and you see the new listing. One option is to login, and you _should_ be able to bypass the Akamai cache. If you do that, I'm going to strongly suggest only making one request every 30 seconds for the same URL. Breaking reddit's API rules (that PRAW enforces) is strongly discouraged. > Would cycling my user-agent between three or four different ones do the trick? Is that an acceptable thing to do if I absolutely need it? That likely won't accomplish anything other than getting your IP banned. Don't mess around with user-agent strings; there's nothing regarding the reddit API that you \"absolutely need\".", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "_Daimon_", "created_utc": 1359194007, "gilded": 0, "name": "t1_c83vtxy", "num_comments": null, "score": 3, "selftext": "It was a keyword argument error. `set_oauth_app_info` doesn't take redirect_url as an argument it takes redirect_ur**i**. This is the same name reddit uses on the [app page](https://ssl.reddit.com/prefs/apps/). But there is no reason to use OAuth if it's for your own account. Login in with the username/password combination and retrieve the things you want. There is a very in-depth tutorial about [OAuth with PRAW](https://github.com/praw-dev/praw/wiki/OAuth) on the wiki.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/"}, {"author": "bboe", "created_utc": 1358895449, "gilded": 0, "name": "t1_c81qlo7", "num_comments": null, "score": 2, "selftext": "/r/redditdev is not the place to request programs; for that try /r/forhire. If you're interested in writing it yourself, I recommend you check out PRAW and visit /r/learnpython or the associated learning community for whatever language you want to use.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1730dr/a_request_for_saved_links/"}, {"author": "_Daimon_", "created_utc": 1358716151, "gilded": 0, "name": "t1_c80fnjn", "num_comments": null, "score": 3, "selftext": "It's usually a good idea to post the traceback with a bug report. That makes debugging far easier. It appears the error is not in PRAW, but in update_checker (a PRAW dependency) see the traceback included at the bottom. Looking at update_checkers source, it's probably because it uses the url \"'http://csil.cs.ucsb.edu:65429/check'\" and that url is blocked by PythonAnywhere. See bboe's response. EDIT: Just confirmed it. This is the origin of the error. What you need to do is simply disable update_checker in your programs. So instead of doing r = praw.Reddit(\"Descriptive and unique useragent here. See API Rules\") You do r = praw.Reddit(\"Descriptive and unique useragent here. See API Rules\", disable_update_check=True) **TRACEBACK:** import praw r = praw.Reddit('Descriptive and unique useragent here. See API Rules') Traceback (most recent call last): File \"\", line 1, in File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/praw/__init__.py\", line 699, in __init__ super(AuthenticatedReddit, self).__init__(*args, **kwargs) File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/praw/__init__.py\", line 393, in __init__ super(OAuth2Reddit, self).__init__(*args, **kwargs) File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/praw/__init__.py\", line 242, in __init__ update_check(__name__, __version__) File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/update_checker.py\", line 94, in update_check result = checker.check(package_name, package_version, **extra_data) File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/update_checker.py\", line 33, in check data = response.json() File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/requests/models.py\", line 604, in json return json.loads(self.text or self.content) File \"/usr/local/lib/python2.7/json/__init__.py\", line 326, in loads return _default_decoder.decode(s) File \"/usr/local/lib/python2.7/json/decoder.py\", line 366, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"/usr/local/lib/python2.7/json/decoder.py\", line 384, in raw_decode raise ValueError(\"No JSON object could be decoded\") ValueError: No JSON object could be decoded", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "_Daimon_", "created_utc": 1358744387, "gilded": 0, "name": "t1_c80oiq2", "num_comments": null, "score": 2, "selftext": "It's not a stupid question. Debugging is really MIA in CS / programming / Python tutorials education. I'll start with a simpler example to discuss reading tracebacks, then how that is relevant to the example. Try saving the following code snippet as 'tracebacktest.py' on your pc and running it. def crash(): a = all_good() b = this_is_bad() def all_good(): return 5 def this_is_bad(): something_something = 20 something_else = 30 5 / 0 crash() This program will crash and create the following traceback when run. Traceback (most recent call last): File \"tracebacktest.py\", line 11, in crash() File \"tracebacktest.py\", line 3, in crash b = this_is_bad() File \"tracebacktest.py\", line 11, in this_is_bad 5 / 0 ZeroDivisionError: integer division or modulo by zero The last line tells us the program crashed because of a ZeroDivisionError and gives us the error message. The last line is the final line of code that was executed `5 / 0`, the line above is the filename, line number and function name where the crashing line is located. The line above is the line that called this function `b = this_is_bad` and the line above is the filename, line number and function number of where that line is located. It continues like this all the way back up to the very first function call we made. Notice that the call to `this_is_good` is not part of the traceback. This is because the traceback contains the direct path your program took from initialization. Detours are not part of it. Back to how this relates to your traceback. If you read the filenames, you'll see that they are part of packages. And that they call each other in the following order PRAW -> update_checker -> requests -> json. So the error is either internal in json, which is extremely unlikely since it's an old built-in library, or one call somewhere in the chain is made with invalid arguments. I see the error message is \"No JSON object could be decoded\", I know this error is raised when you call `json.loads` with a string that cannot be decoded into a json object, such as empty string. So I make an educated guess that this is the cause of the crash and go upwards. A few lines up are these lines File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/update_checker.py\", line 33, in check data = response.json() File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/requests/models.py\", line 604, in json return json.loads(self.text or self.content) So if `self.text or self.content` evaluates to a string that cannot be decoded into a json object .At this point I read bboe's reply which state that certain urls are blocked by PythonAnywhere and return a 403 error. This makes me think that what PythonAnywhere returns instead of the correct result is either an empty string or some HTMl i.e. not a JSON object. I didn't do it at the time, but if you do it then you\u00b4ll get the following. '\\n \\n Access Denied\\n \\n\\nAccess Denied\\n\\nAccess to arbitrary websites is onl y available to Premium users.\\nYou can sign up for a premium account at http://www.pythonanywhere.com/account/\\nAlternativ ely, if you want to suggest something to add to our whitelist \\n(http://www.pythonanywhere.com/whitelist) drop us a line a t \\nsupport@pythonanywhere.com \\n\\n\\n\\n\\n\\nGenerated Mon, 21 Jan 2013 04:51:43 GMT by hansel-liveproxy (squid/2.7.STABLE9)\\n\\n\\n' So I now know that in the function `check` in `update_checker` there is a line that tries to do `json.loads` with an invalid string and this cause a crash. Looking at the code on github and you see the url sounds like something that wouldn't be automatically whitelisted. A manual test later with `requests.get('http://csil.cs.ucsb.edu:65429/check')` confirms it. Then all that's left is disabling it update_checker and see if we still get a crash. We don't, so everything is good :) tl;dr I should write a blog or something....", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "_Daimon_", "created_utc": 1358745968, "gilded": 0, "name": "t1_c80oyc8", "num_comments": null, "score": 1, "selftext": "As for your second error. It appears PythonAnywhere does not support SSL at least not for free accounts. Reddit requires SSL for login, both normally or with OAuth, it's a security thing. So without SSL PRAW cannot login. You'll have to ask PythonAnywhere staff on how to fix this. Just to demonstrate that this is a PythonAnywhere thing, not PRAW or Reddit. This gives the same error requests.get('https://www.google.dk')", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "bboe", "created_utc": 1358747583, "gilded": 0, "name": "t1_c80pdr6", "num_comments": null, "score": 2, "selftext": "reddit does not require ssl to login, however, it's strongly encouraged. If you are okay with sending your username and password in plaintext, then you can overwrite the `reddit` site definition in `praw.ini` such that it does not define `ssl_host` (copy everything but that line when re-defining the `reddit` site). Alternatively you can use a different site-name altogether. The config file [wiki](https://github.com/praw-dev/praw/wiki/The-Configuration-Files) page may be of use. Edit: grammar", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "bboe", "created_utc": 1358442909, "gilded": 0, "name": "t1_c7ymrdv", "num_comments": null, "score": 2, "selftext": "Giving people access to run arbitrary python code that can open network connections is not a great idea. PRAW will run on something like heroku, however.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16r0m0/has_anyone_built_a_web_app_that_works_with_praw/"}, {"author": "_Daimon_", "created_utc": 1358387341, "gilded": 0, "name": "t1_c7ya5km", "num_comments": null, "score": 8, "selftext": "Don't feel bad about asking questions. I've tried writing about discovering more information about methods, functions etc in [writing a bot](https://github.com/praw-dev/praw/wiki/Writing-A-Bot). Stupid title, I know. I'm going to rename it, but I'm going to wait a bit more on that until I'm more sure about the future structure of the documentation. I don't want to rename it twice. I would like to hear any critisism of the documentation and suggestions on how to make it better. Feedback on the documentation is something I never get, which makes it hard to make good documentation because people don't tell me what they want or what the current documentation problems are. Anyway, try doing this help(comment) This will print the following Help on Comment in module praw.objects object: class Comment(Approvable, Deletable, Distinguishable, Editable, Inboxable, Report able, Voteable) | A class for comments. | | Method resolution order: | Comment | Approvable | Deletable | Distinguishable | Editable | Inboxable | Reportable | Voteable | RedditContentObject | __builtin__.object | | Methods defined here: | | __init__(self, reddit_session, json_dict) | | __unicode__(self, *args, **kwargs) | | ---------------------------------------------------------------------- | Data descriptors defined here: | | is_root | Indicate if the comment is a top level comment. | | permalink | Return a permalink to the comment. | | replies | Return a list of the comment replies to this comment. | | score | Return the comment's score. | | submission | Return the submission object this comment belongs to. Looking at that, score sounds like what you want. Alternatively, in situations like this both `vars` and `dir` are quite helpful. This information is specific to this comment. For this example I choose [a recent r/python comment](http://www.reddit.com/r/Python/comments/16p63p/tackling_graphical_programming_in_python_i_had_to/c7y5f1d). Have it open on another tab, while reading the following. from pprint import pprint # Just makes the output more readable pprint(vars(comment)) {'_info_url': 'http://www.reddit.com/button_info/', '_populated': true, '_replies': [], '_submission': , '_underscore_names': ['replies'], 'approved_by': none, 'author': redditor(user_name='phaedrusalt'), 'author_flair_css_class': none, 'author_flair_text': none, 'banned_by': none, 'body': u'i bet they get a little suspicious when you have a copy of *work*~~pla y~~boy on your desk.', 'body_html': u'<div class=\"md\"><p>i bet they get a little suspicious when you have a copy of <em>work</em><del>play</del>boy on your desk.</p>\\n</div>', 'created': 1358402223.0, 'created_utc': 1358373423.0, 'downs': 0, 'edited': false, 'gilded': 0, 'id': u'c7y5f1d', 'likes': none, 'link_id': u't3_16p63p', 'name': u't1_c7y5f1d', 'num_reports': none, 'parent_id': u't3_16p63p', 'reddit_session': , 'subreddit': , 'subreddit_id': u't5_2qh0y', 'ups': 15} Comparing the output with what we can see on the webend, it seems that `ups` is almost identical to the comments upvote. Considering reddits obfuscation of upvotes/downvotes `ups` is very likely to be the comments upvotes. `downs` seem clear to be the downvotes. So from this we could get the score (what you called karma) by doing `karma = comment.ups - comment.downs`. There is also the `dir` builtin function, which gives us a list of all methods and attributes in the object. This is especially interesting as PRAW has multiple property-decorated methods. Eg, methods that pretend to be attributes. They will be listed with `dir`, not with `vars` but be callable just like an attribute. I removed class methods and private methods from the output to keep it a bit shorter. [ 'approve', 'approved_by', 'author', 'author_flair_css_class', 'author_flair_text', 'banned_by', 'body', 'body_html', 'clear_vote', 'content_id', 'created', 'created_utc', 'delete', 'distinguish', 'downs', 'downvote', 'edit', 'edited', 'from_api_response', 'gilded', 'id', 'is_root', 'likes', 'link_id', 'mark_as_read', 'mark_as_unread', 'name', 'num_reports', 'parent_id', 'permalink', 'reddit_session', 'remove', 'replies', 'reply', 'report', 'score', 'submission', 'subreddit', 'subreddit_id', 'undistinguish', 'ups', 'upvote', 'vote'] Looking this list over, most can be easily discarded as having anything to do with a comments karma. The score sounds like a possibility, so we try `s.score` and it returns 15. Which matches the score of the comment we used as an example. Again, if you have any feedback whatsoever on the current documentation then please tell us so we can improve it. This goes for any user of PRAW, not just OP.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16q02d/praw_karma/"}, {"author": "lamerx", "created_utc": 1358388565, "gilded": 0, "name": "t1_c7yak7q", "num_comments": null, "score": 2, "selftext": "Wow , thank you very much. Thats JUST want i needed sir. >Again, if you have any feedback whatsoever on the current documentation then please tell us so we can improve it. This goes for any user of PRAW, not just OP. I will daimon", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16q02d/praw_karma/"}, {"author": "bboe", "created_utc": 1358274241, "gilded": 0, "name": "t1_c7xeppm", "num_comments": null, "score": 2, "selftext": "Absolutely! I encourage anyone to implement missing API functionality and make pull requests. For tracking purposes it would be nice if you file an issue about what you intend to add. Even if you don't have the time to add features, if there is something you wish PRAW supported, please create an issue on github.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/"}, {"author": "mpheus", "created_utc": 1357862049, "gilded": 0, "name": "t1_c7uqryr", "num_comments": null, "score": 3, "selftext": "Thanks! I was going to use the Reddit API but was really amazed to discover PRAW that made everything easier. Really loving Python and the vast number of libraries.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/"}, {"author": "MrFanzyPanz", "created_utc": 1357778032, "gilded": 0, "name": "t1_c7u4rqg", "num_comments": null, "score": 2, "selftext": "Thanks! That solved it. On a related note, what will praw return if there aren't any posts left in a subreddit? Say the subreddit is new and only has 50 posts on it, but I still tell praw to pull the first 500 posts? Does it return [none] like the deleted accounts do?", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/"}, {"author": "Deimorz", "created_utc": 1357226265, "gilded": 0, "name": "t1_c7qbd1b", "num_comments": null, "score": 2, "selftext": "The praw wiki has a number of examples of how to do common tasks with it: https://github.com/praw-dev/praw/wiki", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15w2cs/help_with_submitting_with_praw/"}, {"author": "_Daimon_", "created_utc": 1357253081, "gilded": 0, "name": "t1_c7qjzcm", "num_comments": null, "score": 2, "selftext": "At the bottom of that page are a bunch of example applications/scripts using PRAW. One is [newsfrbot](https://github.com/gardaud/newsfrbot) which *\"parses RSS feeds from some major french publications and posts them to relevant subreddits.\"*. That might be useful to you :) You're more than welcome to add your own application to that list when you are done writing it.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15w2cs/help_with_submitting_with_praw/"}, {"author": "_Daimon_", "created_utc": 1356871561, "gilded": 0, "name": "t1_c7o8fns", "num_comments": null, "score": 1, "selftext": "I can provide you a short example of how to do it :) You need moderator credentials in the subreddit to run this script. Also it will raise an exception if the CSS is invalid. Speaking of documentation, I'm sure you already know of our exisiting documentation on the [Github wiki page](https://github.com/praw-dev/praw/wiki)? import praw r = praw.Reddit(USERAGENT) r.login() subreddit = r.get_subreddit('NAME OF YOUR SUBREDDIT') current_css = subreddit.get_stylesheet()['stylesheet'] subreddit.set_stylesheet(FANCY_NEW_CSS) # Overrides existing CSS.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15nuau/praw_css/"}, {"author": "lamerx", "created_utc": 1356915877, "gilded": 0, "name": "t1_c7oiepc", "num_comments": null, "score": 1, "selftext": "I have been trying all day to get even this to work but im getting no luck at all. I know its something simple but cant for the life of me figgure out what it is import praw r = praw.Reddit(USERAGENT) r.login() subreddit = r.get_subreddit('NAME OF YOUR SUBREDDIT') current_css = subreddit.get_stylesheet()['stylesheet'] subreddit.set_stylesheet(current_css) # Overrides existing CSS. That code when ran against my testing sub gives me the following error APIException: (BAD_CSS) `invalid css` on field `stylesheet_contents` All i did was read the style sheet and try to put it back in.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15nuau/praw_css/"}, {"author": "bboe", "created_utc": 1356689109, "gilded": 0, "name": "t1_c7n8qcr", "num_comments": null, "score": 5, "selftext": "The latest release of PRAW does not yet support reddit's OAuth. I actually have been working today on fully integrating a pull request /u/intortus made a while back. Soon, I will get the last bit working, then I'll push to master as one of the final release candidates for version 1.1 of PRAW. To answer your question: once you have an access token, you need to add the following header to all requests: Authorization: bearer Edit: The latest git version of PRAW now should have all the necessary OAuth2.0 stuff integrated. Specifically: * r.get_authorize_url [[src](https://github.com/praw-dev/praw/blob/89e31cb39cb0a66d4c79f7c8e235b2835b3f0d6a/praw/__init__.py#L392)] * r.get_access_token [[src](https://github.com/praw-dev/praw/blob/89e31cb39cb0a66d4c79f7c8e235b2835b3f0d6a/praw/__init__.py#L374)] * r.refresh_access_token [[src](https://github.com/praw-dev/praw/blob/89e31cb39cb0a66d4c79f7c8e235b2835b3f0d6a/praw/__init__.py#L402)]", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/"}, {"author": "rram", "created_utc": 1356584140, "gilded": 0, "name": "t1_c7mn8hc", "num_comments": null, "score": 1, "selftext": "That is not the ban message. You're accessing ssl.reddit.com without a secure connection (i.e. You're not using https://ssl.reddit.com/). What do you mean by \"I cannot get any information with PRAW\"? A the least PRAW should provide you with the HTTP status code and response body.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "expiredtofu", "created_utc": 1356584242, "gilded": 0, "name": "t1_c7mn9c9", "num_comments": null, "score": -2, "selftext": ">A the least PRAW should provide you with the HTTP status code and response body. How would I get that? Sorry, but I'm very new to using PRAW.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "rram", "created_utc": 1356599271, "gilded": 0, "name": "t1_c7mq392", "num_comments": null, "score": 1, "selftext": "PRAW show throw an exception if you get a page with a bad status code. You can catch that exception and get the data from there: import praw r = praw.Reddit('test by /u/rram') sr = r.get_subreddit('rram') try: for l in sr.get_new(): print l except Exception as e: print \"Code: %s\" % e.code print \"Reason: %s\" % e.reason print \"URL: %s\" % e.url print \"Body: \\n%s\" % e.read() That should print something like: Code: 403 Reason: Forbidden URL: http://www.reddit.com/r/rram/new.json Body: {\"error\": 403} Note: My knowledge of PRAW is limited, so this might not work 100% of the time.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "bboe", "created_utc": 1356584663, "gilded": 0, "name": "t1_c7mncq8", "num_comments": null, "score": 1, "selftext": "> I think I ran my script a bit too fast. Unless you manually changed the rate limit settings, you needn't worry about such things with PRAW.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "_Daimon_", "created_utc": 1355865327, "gilded": 0, "name": "t1_c7ip09z", "num_comments": null, "score": 2, "selftext": "Hey, I see that Bboe has already answered your specik question. If you have any more questions, then you might want to check out the [wiki]( https://github.com/praw-dev/praw/wiki). It has a bunch of examples and some tutorials, that should help you getting started with PRAW. :)", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152d2w/praw_url_parameters/"}, {"author": "bboe", "created_utc": 1355858464, "gilded": 0, "name": "t1_c7imuej", "num_comments": null, "score": 3, "selftext": "Edit: Oops I had it backwards. You have an older version of PRAW where the name is `get_saved_links`. Either update, or use `get_saved_links`. In this particular case, I have not made a release including this change hence the confusion between the documentation and the \"stable\" version. Some of this information is contained in the [changelog](https://github.com/praw-dev/praw/wiki/Changelog). The method was renamed from `get_saved_links` to `get_saved` in a recent version. The wiki documentation will always reflect the latest stable version. Sorry for the confusion. When in doubt, python's `dir` command is your friend. If you ran: from pprint import pprint pprint(dir(r.user)) you will see a list of available functions on the `r.user` (LoggedInRedditor) object.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "LudoA", "created_utc": 1355858902, "gilded": 0, "name": "t1_c7imzig", "num_comments": null, "score": 1, "selftext": "Of course, thanks... I did consider the name could've changed, but looking at https://github.com/praw-dev/praw/blob/master/praw/objects.py#L518 I saw *get_saved = _get_section('saved')* so I thought that wasn't the issue. I'm now looking at LoggedInExtension to understand a bit better how praw works :) Thanks Bryce.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "bboe", "created_utc": 1355859153, "gilded": 0, "name": "t1_c7in2fh", "num_comments": null, "score": 3, "selftext": "PRAW currently has two branches 1.0 and 1.1 which is making things complicated. I'm hopefully going to release 1.1 soon so I no longer have to maintain the separate releases.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "LudoA", "created_utc": 1355863082, "gilded": 0, "name": "t1_c7iobvi", "num_comments": null, "score": 1, "selftext": "I installed PRAW using pip earlier today. I have 1.0.16 installed. However, I have neither get_saved() nor get_saved_links() available: >>> dir(r.user) ['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattr__', '__getattribute__', '__hash__', '__init__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_get_json_dict', '_info_url', '_mod_subs', '_populate', '_populated', '_underscore_names', '_url', 'comment_karma', 'compose_message', 'content_id', 'created', 'created_utc', 'friend', 'from_api_response', 'get_comments', 'get_disliked', 'get_hidden', 'get_inbox', 'get_liked', 'get_modmail', 'get_overview', 'get_sent', 'get_submitted', 'get_unread', 'has_mail', 'has_mod_mail', 'id', 'is_friend', 'is_gold', 'is_mod', 'link_karma', 'mark_as_read', 'modhash', 'my_contributions', 'my_moderation', 'my_reddits', 'name', 'reddit_session', 'refresh', 'send_message', 'unfriend']", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "_Daimon_", "created_utc": 1355865909, "gilded": 0, "name": "t1_c7ip6ot", "num_comments": null, "score": 3, "selftext": "`get_saved_links` is in the `Reddit` object. So you do import praw r = praw.Reddit(user_agent='example') r.login('LudoA', 'foobar') r.get_saved_links() Sorry for the confusion, I'll add a comment to the wiki about the difference between the last stable edition and the last released verision tomorrow.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "LudoA", "created_utc": 1355858267, "gilded": 0, "name": "t1_c7ims5j", "num_comments": null, "score": 1, "selftext": "By the way, other methods, such as *my_contributions()* and *get_liked()* are working correctly: >>> r.user.get_liked() By passing that to list() and passing limits=100, I can see that the number of praw.objects.Submission objects contained by the list changes as I would expect. Which leads me to think that I'm overlooking something obvious...", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "bboe", "created_utc": 1355086506, "gilded": 0, "name": "t1_c7du3u1", "num_comments": null, "score": 2, "selftext": "`user.has_mail` is an attribute that is set upon login and is only updated when the user object is updated. None of the PRAW actions, aside from directly fetching the user object update the user object. You can manually update the user object by running `r.user.refresh()` before checking the `has_mail` attribute. That should solve the problem. However, it's more efficient to simply check for the presence of new messages, than to first check for messages (refresh the user info), and then conditionally retrieve the messages.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "esacteksab", "created_utc": 1355086857, "gilded": 0, "name": "t1_c7du7uf", "num_comments": null, "score": 1, "selftext": "You're recommending something like r.user.get_unread? Or am I missing another way via PRAW to get unread messages? Before, I'd touch /api/me.json and 'has_mail' http://www.reddit.com/dev/api#GET_api_me.json Which I'm guessing is what r.user.has_mail is doing.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "bboe", "created_utc": 1348956885, "gilded": 0, "name": "t1_c6f9zzm", "num_comments": null, "score": 4, "selftext": "/u/Deimorz was looking into doing something similar. If you want to effectively count the requests, add a wrapper to `helpers._request`. With PRAW's caching and automatic handling of listing pages you can't easily tell how many requests are actually being made. However, what I think you really want is to make a PRAW request manager that runs as a single process, and has many associated clients that make PRAW requests through the manager. It'd take a little bit of work to make transparent to a single-process model, but I think it's worth it if you're going to be running multiple bots in separate processes.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/"}, {"author": "bboe", "created_utc": 1341797278, "gilded": 0, "name": "t1_c5ba2h7", "num_comments": null, "score": 1, "selftext": "I'm not certain, nevertheless, there may be an issue if you have session parameters set and are not sending the mod-hash. PRAW [implicitly adds](https://github.com/praw-dev/praw/blob/master/praw/helpers.py#L94) the `uh` parameter to all POST requests associated with a user-session.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "TankorSmash", "created_utc": 1341797648, "gilded": 0, "name": "t1_c5ba5cm", "num_comments": null, "score": 1, "selftext": "It turns out that I wasn't setting the parameters properly; I'm not exactly sure which ones were wrong though, but it certainly seems to be working now. The modhash not the problem though, as far as I can tell, because I could post comments and submit stories just fine. I think it's just the matter of building the URL correctly. How long have you been buidling the PRAW? It's quite a handy tool, from the looks of the source.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "bboe", "created_utc": 1341804375, "gilded": 0, "name": "t1_c5bbpu8", "num_comments": null, "score": 1, "selftext": "I took over as the PRAW package maintainer around last November if I recall correctly.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "bboe", "created_utc": 1341182733, "gilded": 0, "name": "t1_c586jcq", "num_comments": null, "score": 1, "selftext": "Confusion with the name of the python package. I have been using the name PRAW for a few months and I usually have to clarify what I am referring to.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/vv4tg/python_reddit_api_wrapper_package_rename_and/"}, {"author": "RedditSrc4Research", "created_utc": 1337101390, "gilded": 0, "name": "t1_c4oc6e0", "num_comments": null, "score": 2, "selftext": "Actually what happens on the browser is that the login will redirect you to whatever domain is in the ini file (though normal browsing won't). So likely you would have to detect the redirect to avoid the issue in PRAW.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/"}, {"author": "bboe", "created_utc": 1337105412, "gilded": 0, "name": "t1_c4od431", "num_comments": null, "score": 2, "selftext": "That's what I figured regarding the browser behavior. However, there is no redirect sent to PRAW as it is completely push based. The reddit2 server in this case allows you to login for its domain, yet, all the links returned I guess appear for the wrong domain. Since the browser implicitly redirects to the correct domain, I'm going to say the reddit_api.cfg should have the \"correct\" domain as well.", "title": null, "url": "https://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/"}]