[{"author": "DrBaily", "body": "**Scraper worked fine last week, made no changes since then. This morning, run the module to get:** Traceback (most recent call last): File \"scraper.py\", line 75, in submission.replace_more_comments(limit=None, threshold=1) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/objects.py\", line 1288, in replace_more_comments new_comments = item.comments(update=False) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/objects.py\", line 806, in comments response = self.reddit_session.request_json(url, data=data) File \"\", line 2, in request_json File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/__init__.py\", line 445, in _request _raise_response_exceptions(response) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/internal.py\", line 205, in _raise_response_exceptions raise Forbidden(_raw=response) Forbidden: HTTP error **Here is the script:** import praw import csv from datetime import datetime r = praw.Reddit(app_ua) r.set_oauth_app_info(app_id, app_secret, app_uri) r.refresh_access_information(app_refresh) subreddit = r.get_subreddit('') submit = subreddit.get_new(limit=None) subids = set() for sub in submit: subids.add(sub.id) subid = list(subids) with open('comments.csv', 'wb') as fp: a = csv.writer(fp, delimiter=',') i = 0 while i", "created_utc": 1454996621, "gilded": 0, "name": "t3_44v87n", "num_comments": 3, "score": 3, "title": "Forbidden: HTTP error", "url": "https://www.reddit.com/r/redditdev/comments/44v87n/forbidden_http_error/"}, {"author": "boib", "body": ">>> srch=r.search(\"[praw]\") >>> srchs=list(srch) Traceback (most recent call last): File \"\", line 1, in File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 1171, in search **kwargs): File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 564, in get_content root = page_data.get(root_field, page_data) AttributeError: 'list' object has no attribute 'get' Edit: fixed now. Thanks all.", "created_utc": 1454972239, "gilded": 0, "name": "t3_44tixp", "num_comments": 7, "score": 5, "title": "[PRAW] Search suddenly stopped working.", "url": "https://www.reddit.com/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/"}, {"author": "theonefoster", "body": "Using praw on python 3.5.1 I'm trying to get a comment tree of all replies to a given comment. If I say `get_info(id=commentID)`, I get the comment object but the replies attribute is always `[]` even when I know the comment has replies (the rest of the data, eg `comment.body`, checks out). I can get the same comment object by doing `comment.submission.comments` and picking the right top-level comment - doing this gives me the list of replies in `.replies`... ?!?! I tried using the get_content method instead but I couldn't get it to work - I kept getting `AttributeError: 'generator' object has no attribute 'replies'`. I tried using fetch=True and even list() on the generator but couldn't get it to fetch the actual comment. Is there an API rule which stops comment replies showing up if you directly request a comment from its `thing_id`? ---- Example in case I wasn't clear: >> r = login() # I'm logged in with mod authority, not that it should make a difference >> c = r.get_info(thing_id=\"t1_czrxngy\") >> c.body \"huh, I went...\" # the expected body of the comment >> c.replies [] # there should be a tree of replies here >> s = c.submission >> c = s.comments[0] #I know that my comment happens to be s.comments[0] now, but if I didn't I'd have to iterate over s.comments searching for the correct comment.body >> c.body \"huh, I went...\" #the correct body, same as before >> c.replies [, , ] What gives? What did I do wrong? I don't want to implement the \"search for the comment in comment.submission.comments\" method because that's a terrible way to do it, and it also only works on top-level comments which is no good for me. Is it a bug in praw? A bug in the reddit API? Or did I do something wrong?", "created_utc": 1454929856, "gilded": 0, "name": "t3_44qh61", "num_comments": 1, "score": 2, "title": "Unable to see comment replies unless I get them from submission.comments. Help?", "url": "https://www.reddit.com/r/redditdev/comments/44qh61/unable_to_see_comment_replies_unless_i_get_them/"}, {"author": "SirCutRy", "body": "Is it better to use RedditSharp, PRAW or something else? I have some experience with C# but nothing with Python or any kinds of packages. I want to write a comment bot that doesn't submit.", "created_utc": 1454624571, "gilded": 0, "name": "t3_447pp6", "num_comments": 5, "score": 4, "title": "What to use for a bot?", "url": "https://www.reddit.com/r/redditdev/comments/447pp6/what_to_use_for_a_bot/"}, {"author": "allthefoxes", "body": "Using PRAW", "created_utc": 1453874939, "gilded": 0, "name": "t3_42w4l7", "num_comments": 8, "score": 7, "title": "With the OAuth-mageddon approaching, what is a quick and easy way to migrate already existing scripts/bots to use it?", "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "brain_emesis", "body": "I've been trying to make an app that requires that I get the top X posts in a subreddit. I thought the get_top_from_week request would work for me (documented [here](https://praw.readthedocs.org/en/stable/pages/code_overview.html?highlight=get_top_from_week#praw.objects.Subreddit.get_top_from_week)) but for some reason it is only returning posts that have occurred before Jan 21 (when executing it today, Jan 25). This is true even if I pass a limit of None. The documentation for it is very short so I might be misunderstanding how to use it. Does anyone know if this is correct behaviour?", "created_utc": 1453736718, "gilded": 0, "name": "t3_42lzes", "num_comments": 4, "score": 2, "title": "[PRAW] subreddit.get_top_from_week isn't returning anything after Jan 21", "url": "https://www.reddit.com/r/redditdev/comments/42lzes/praw_subredditget_top_from_week_isnt_returning/"}, {"author": "theonefoster", "body": "try: user = r.get_redditor(\"skldjhfgksdjhfg\", fetch=True) except praw.errors.HTTPException as e: if e.response.status_code == 404: print \"404\" elif e.response.status_code == 503: print \"503 else: print \"Other HTTPError\" I've tried the above, as suggested by previous posts, but it doesn't work. I can catch a generat HTTPException fine, but I want to distinguish between a 404 exception and a 503 exception and take different action. However the above line crashes with \"AttributeError: 'NotFound' object has no attribute 'response'\". What should I do?", "created_utc": 1453586885, "gilded": 0, "name": "t3_42d03i", "num_comments": 6, "score": 1, "title": "How can I access the specific HTTP code in an HTTPException in praw?", "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "Joshjayk", "body": "Using praw, if i get a comment from the comment stream, is there a way for me to obtain the parent comment? I want to use this for those \"complete sentence\" replies. For example: Comment: I Reply: LOVE Reply to reply: CAKE. If I see 'CAKE.' on the comment stream, is there a way I could reconstruct the tree back up to the parent?", "created_utc": 1453522440, "gilded": 0, "name": "t3_429bps", "num_comments": 10, "score": 3, "title": "[PRAW] Getting parent comments?", "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "spookyyz", "body": "So, I feel like a complete and utter idiot, but I'm really struggling to set a post I'm making with a bot to be the sticky for that sub with PRAW. I am posting via this line: GAME_THREAD = REDDIT.submit(SUBREDDIT, submission_title, text=submission_text) How do I set that submission to be the sticky for the sub when it is posted? I'm sorry I know this question is stupid as hell, but I'm really struggling with it sadly. PS. REDDIT is a PRAW object.", "created_utc": 1453333839, "gilded": 0, "name": "t3_41x45c", "num_comments": 9, "score": 1, "title": "[PRAW] Stickying a submission posted", "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "bAndkAllDay", "body": "Hi. I'm quite new to PRAW and so far using the Python IDE writing the code line by line and it was working great. I was able to retrieve the top 5 posts for a subreddit. However, when I try and save the code into a .py and execute it, the code works but it doesn't retrieve the subreddit information, and the fields are blank. The code is grabbing the top 5 posts from a subreddit and saving the information to a text file. Anyone else run into this issue? Thanks http://pastebin.com/qkbusiaZ", "created_utc": 1453249677, "gilded": 0, "name": "t3_41roji", "num_comments": 4, "score": 2, "title": "PRAW Python not retrieving info when executed", "url": "https://www.reddit.com/r/redditdev/comments/41roji/praw_python_not_retrieving_info_when_executed/"}, {"author": "elihusmails", "body": "I am interested in getting into writing bots. I have been a full time developer of Java since the 1.1 days. I have messed around with Python, but I am certainly no expert. From what I can tell, the reddit bot community is strongly slanted towards Python. So I have a couple questions: 1. Is the Java Reddit bot community active? 2. Are there API's for Java that are as good as PRAW? If No, then it'll be a good opportunity to learn Python. If that is the case, should I use Python 2.x or 3.x ? Thanks all.", "created_utc": 1453212780, "gilded": 0, "name": "t3_41otyb", "num_comments": 13, "score": 9, "title": "Python or Java for bot development", "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "haigaguy", "body": "I am trying to fetch the most downvoted comments from the top 15 submissions of the day of a particular subreddit. PRAW has get_comments() but I don't see any way to check for votes. Thank you!", "created_utc": 1452916842, "gilded": 0, "name": "t3_416r43", "num_comments": 2, "score": 2, "title": "[PRAW] How to get the most downvoted comments from a submission?", "url": "https://www.reddit.com/r/redditdev/comments/416r43/praw_how_to_get_the_most_downvoted_comments_from/"}, {"author": "PieCrafted", "body": "I have this script that I found on GitHub, and the top few lines state: import praw, time, datetime, re, urllib, urllib2, pickle, pyimgur, os, traceback, wikipedia, string, socket, sys, collections from nsfw import getnsfw from util import success, warn, log, fail, special, bluelog from bs4 import BeautifulSoup from HTMLParser import HTMLParser But, the error log states: Traceback (most recent call last): File \"bot.py\", line 4, in from nsfw import getnsfw ImportError: No module named nsfw The requirements.txt only downloads praw, pyimgur, beautifulsoup4, wikipedia modules. I attempted to manually install via pip install nsfw but it states the package does not exist. Help?", "created_utc": 1452694719, "gilded": 0, "name": "t3_40s7gb", "num_comments": 9, "score": 1, "title": "Help with reddit bot (ImportError: No module named nsfw)", "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "tusing", "body": "[Relevant file on GitHub](https://github.com/tusing/reddit-ffn-bot/blob/master/ffn_bot/reddit/queues.py) I'm running a bot that parses comments for requests to fanfiction stories and replies to the comments with a fully-formatted description of the fanfiction. Unfortunately, our latest update is having a few errors when people call the refresh function for our bot (which will make it delete its old comment and reply again). Basically, what happens is that people call the refresh function, when the bot parses it, that error gets thrown. After the reply is made, the bot stops querying Reddit for new comments as it's supposed to do [in this line](https://github.com/tusing/reddit-ffn-bot/blob/master/ffn_bot/reddit/queues.py#L91). Here is the error ([GitHub Gist in case formatting broke](https://gist.github.com/tusing/834f8ddaa9ebae95ff3d)): ``` Exception in thread Thread-2: Traceback (most recent call last): File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner self.run() File \"/home/ubuntu/reddit-ffn-bot/ffn_bot/reddit/queues.py\", line 93, in run self._fetch(fetcher) File \"/home/ubuntu/reddit-ffn-bot/ffn_bot/reddit/queues.py\", line 79, in _fetch fetcher(self) File \"/home/ubuntu/reddit-ffn-bot/ffn_bot/reddit/queues.py\", line 156, in _run queue.add(*func(limit=self.limit, params=params)) File \"\", line 2, in _sorted File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 247, in wrap assert not obj._use_oauth # pylint: disable=W0212 AssertionError ``` The code was updated by a contributor, so I'm not 100% sure what is going on here, which means I'm not sure how to deal with the error.", "created_utc": 1452645596, "gilded": 0, "name": "t3_40pehq", "num_comments": 10, "score": 2, "title": "Bot freezing due to Oauth-related AssertionError", "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "Joshjayk", "body": "Hello! I need some help with matching a specified word in the helpers.comment_stream. Here is a snippet of what I have thus far: word = \"random\" comments = praw.helpers.comment_stream(wordbot, 'all') for comment in comments: if findWholeWord(word)(str(comment)) is not None: print(\"Word found: %s\" % str(comment)) For findWholeWord(word), I have a simple regex search: def findWholeWord(w): return re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search Basically, it searches for a word in the comments, and if it finds a comment that uses that word, it'll tell me. The problem is, it doesn't seem to be working. In many tests I've tried on random subreddits, I can't get it to properly work. If I change the 'all' to a specific subreddit it'll work, but I'd like for it to work for every subreddit. Thanks!", "created_utc": 1452553024, "gilded": 0, "name": "t3_40jfcf", "num_comments": 5, "score": 0, "title": "[PRAW] Matching a keyword in comment stream.", "url": "https://www.reddit.com/r/redditdev/comments/40jfcf/praw_matching_a_keyword_in_comment_stream/"}, {"author": "kamac496", "body": "Hey. So far I've seen that I cannot get more than 1000 comments when using praw.objects.MoreComments, so I'd like to read the latest comments *in a submission* instead. The only problem I have is that I don't know how, because I can't seem to find that in the documentation. Anyone?", "created_utc": 1452544211, "gilded": 0, "name": "t3_40ipg4", "num_comments": 1, "score": 2, "title": "[PRAW] Get recent comments from a submission", "url": "https://www.reddit.com/r/redditdev/comments/40ipg4/praw_get_recent_comments_from_a_submission/"}, {"author": "SandyRegolith", "body": "What's the URL I should construct to fetch posts between timestamp A and timestamp B? Not using PRAW, just regular HTTP requests.", "created_utc": 1452119393, "gilded": 0, "name": "t3_3zs8un", "num_comments": 2, "score": 1, "title": "[not PRAW] What's the query to get all posts between two timestamps?", "url": "https://www.reddit.com/r/redditdev/comments/3zs8un/not_praw_whats_the_query_to_get_all_posts_between/"}, {"author": "Almenon", "body": "\\_\\_getattr__ in objects.py appears to call itself repeatedly. My code: http://pastebin.com/BudzyX3T Where the error happens: https://github.com/praw-dev/praw/blob/master/praw/objects.py#L81 This is probably a bug in PRAW but I want to make sure I'm not doing something wrong on my end first. EDIT: The error happens when I pass in a subreddit with a question mark at the end of the name.", "created_utc": 1452115389, "gilded": 0, "name": "t3_3zrwso", "num_comments": 12, "score": 1, "title": "Recursion error when using praw to iterate through subreddits. help?", "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "NotCalledBill", "body": "**The Context** Hey redditdev, so I've got some working praw code and now want to add in exception handling to gracefully handle when reddit is down or when a request goes wrong. My problem is that after reading up on the lazy objects documentation I'm still pretty lost as to when my code is actually making requests. I've searched around with google for similar questions but couldn't find any so I'm asking here in the hopes that somebody else can point me in the right direction. Here is the relevant function, it's simply scraping data from the first 100 hot posts in a subreddit: if name: sub = r.get_subreddit(name) else: sub = r.get_random_subreddit() name = sub.display_name submissions = list(sub.get_hot(limit=100)) database.cache([x.title for x in submissions], name, \"r/title\") for submission in submissions: if submission.is_self: database.cache(submission.selftext.split(\"\\n\\n\"), name, \"r/self\") if submission.num_comments >= 20: comments = submission.comments # Don't flatten tree to cut down on amount of overhead database.cache([x.body for x in comments if getattr(x, \"body\", None)], name, \"r/comment\") database.cache([x.author.name for x in comments if getattr(x, \"author\", None)], name, \"r/user\") **** **The Questions** So initially I had a play around with the `.has_fetched` attribute to see if I could figure out when the request was made to get information on the `sub` object. I figured it would be true after the line where I'm fetching the hot posts because surely data has to be retrieved there, but that was an incorrect assumption (presumably it just prepares an empty generator or something along those lines). My thinking now is that it must be when I'm iterating through the posts, however I'm unsure how that works exactly, it should only make a single request right? I'm also unsure if further requests are being made to get information on the comments, or if that is all included in the earlier requests (my thinking is that since I'm not expanding the `MoreComments` objects there aren't any further requests being made after the posts are retrieved). If anybody could clarify these things (or correct me) that'd be great!", "created_utc": 1452114564, "gilded": 0, "name": "t3_3zrucp", "num_comments": 2, "score": 1, "title": "[PRAW] Having trouble figuring out when requests are made", "url": "https://www.reddit.com/r/redditdev/comments/3zrucp/praw_having_trouble_figuring_out_when_requests/"}, {"author": "Developx", "body": "Hi everyone. I'm new to Reddit's API and PRAW. This problem came up in a more complex script I'm writing for private subreddits, but I was able to simplify it down to these calls. I am using Python 3.4.4. I have tried PRAW 3.3.0 and the newest GitHub version. The account I'm using is a moderator of . It has granted all OAuth scopes to the script. I have authenticated manually with `r.refresh_access_information()` ([after following this tutorial](https://www.reddit.com/r/GoldTesting/comments/3cm1p8/how_to_make_your_bot_use_oauth2/) by /u/GoldenSights) and with OAuth2Util but it doesn't change anything. So I don't think it's a problem with my authentication. All normal PRAW methods work as expected (`r.get_subreddit()`, etc.). `request()` and `request_json()` seem like edge cases. ---- `mybot.py` is a setup script, using OAuth2Util for simplicity. # mybot.py import praw import OAuth2Util def login(): r = praw.Reddit(, log_requests=2) o = OAuth2Util.OAuth2Util(r) # o.refresh(force=True) # Makes no difference to this problem return r ---- **1:** Here are some examples from the Python Interpreter (with `log_requests=2`) to show the problem: >>> import mybot >>> r=mybot.login() # Login works fine substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.request(\"https://www.reddit.com\") # Works fine GET: https://www.reddit.com status: 200 >>> r.request(\"https://www.reddit.com/r/AskReddit\") # Public subreddit, works fine GET: https://www.reddit.com/r/AskReddit status: 200 >>> r.request(\"https://www.reddit.com/r/\") # Private subreddit moderated by account, does not work, returns 403 GET: https://www.reddit.com/r/ status: 403 Traceback (most recent call last): File \"\", line 1, in File \"\", line 2, in request File \"/python3.4/site-packages/praw/decorators.py\", line 116, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/python3.4/site-packages/praw/__init__.py\", line 599, in request retry_on_error=retry_on_error, method=method) File \"/python3.4/site-packages/praw/__init__.py\", line 451, in _request _raise_response_exceptions(response) File \"/python3.4/site-packages/praw/internal.py\", line 208, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden: HTTP error >>> r.request(\"https://www.reddit.com/r/\") # Exact same request, this time it works fine substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 >>> r.request(\"https://www.reddit.com/r/\") # Exact same request, still works fine substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 >>> r=mybot.login() # Authenticate again substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.request(\"https://www.reddit.com/r/\") # Exact same request after authenticating again, returns 403 GET: https://www.reddit.com/r/ status: 403 Traceback (most recent call last): File \"\", line 1, in File \"\", line 2, in request File \"/python3.4/site-packages/praw/decorators.py\", line 116, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/python3.4/site-packages/praw/__init__.py\", line 599, in request retry_on_error=retry_on_error, method=method) File \"/python3.4/site-packages/praw/__init__.py\", line 451, in _request _raise_response_exceptions(response) File \"/python3.4/site-packages/praw/internal.py\", line 208, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden: HTTP error >>> r.request(\"https://www.reddit.com/r/\") # Exact same request, working fine again substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 >>> r.request(\"https://www.reddit.com/r/\") # Exact same request, still working fine substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 ---- **2:** Here is another Python Interpreter session showing the same problem (I think), without needing to use a private subreddit: >>> import mybot >>> r=mybot.login() substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.request(\"https://oauth.reddit.com\") GET: https://oauth.reddit.com status: 403 Traceback (most recent call last): File \"\", line 1, in File \"\", line 2, in request File \"/python3.4/site-packages/praw/decorators.py\", line 116, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/python3.4/site-packages/praw/__init__.py\", line 599, in request retry_on_error=retry_on_error, method=method) File \"/python3.4/site-packages/praw/__init__.py\", line 451, in _request _raise_response_exceptions(response) File \"/python3.4/site-packages/praw/internal.py\", line 208, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden: HTTP error >>> r.request(\"https://oauth.reddit.com\") GET: https://oauth.reddit.com status: 200 >>> r.request(\"https://oauth.reddit.com\") GET: https://oauth.reddit.com status: 200 ---- **3:** The first 403 does not need to be on the exact same URL as the subsequent requests. Just the first request needing OAuth of the authenticated session: >>> import mybot >>> r=mybot.login() substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.request(\"https://oauth.reddit.com\") # First request needing OAuth returns 403 GET: https://oauth.reddit.com status: 403 Traceback (most recent call last): File \"\", line 1, in File \"\", line 2, in request File \"/python3.4/site-packages/praw/decorators.py\", line 116, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/python3.4/site-packages/praw/__init__.py\", line 599, in request retry_on_error=retry_on_error, method=method) File \"/python3.4/site-packages/praw/__init__.py\", line 451, in _request _raise_response_exceptions(response) File \"/python3.4/site-packages/praw/internal.py\", line 208, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden: HTTP error >>> r.request(\"https://www.reddit.com/r/\") # Second request needing OAuth works fine, even though it's a different URL substituting https://oauth.reddit.com for https://www.reddit.com in url GET: https://oauth.reddit.com/r/ status: 200 ---- Substitute `request_json()` for `request()` in these examples and it has the same problem. ---- I could use a try/except like this: try: r.request(\"https://www.reddit.com/r/\") # Will always raise 403 except: r.request(\"https://www.reddit.com/r/\") # Will work fine But I don't want to cause a lot of API errors, and I would rather fix the root of the problem. From the above examples, it seems like the problem is: The **first** `request()` or `request_json()` call **that needs to use OAuth** returns a 403 error, all other calls are fine. [PRAW issue #497 on GitHub](https://github.com/praw-dev/praw/issues/497) could possibly be related. Would really appreciate any help with this. Thanks. Edit: Added example 3.", "created_utc": 1451928098, "gilded": 0, "name": "t3_3zfnsf", "num_comments": 15, "score": 7, "title": "403 Forbidden on first PRAW request() or request_json() call that needs to use OAuth. All subsequent calls work fine.", "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "askLubich", "body": "My bot calls the 'get_sticky' function on a subreddit and it has been working flawlessly for a couple of weeks now. However, recently it crashed with the following message: File \"C:\\Users\\...\\site-packages\\praw\\internal.py\", line 184, in _raise_redirect_exceptions if 'reddits/search' in new_url: # Handle non-existent subreddit UnicodeDecodeError: 'ascii' codec can't decode byte 0xc3 in position 78: ordinal not in range(128) It appears that praw encouters some encoding error trying to parse the actual error message. At the time of the crash, a link and not a self-post was sticky. I did a couple of tests and it appears that the 'get_sticky' function works fine as long as a self-post is sticked. However, it crashes if a link is sticked. My questions are: * Is this a known bug? Can someone reproduce it? * Is there any known workaround? Any other way to get the sticked post? Update: As it turned out, unlike my first thought, the problem was likely not caused by a sticked link, but rather by an umlaut (\u00e4\u00f6\u00fc) in the sticked post's title.", "created_utc": 1451744618, "gilded": 0, "name": "t3_3z57b4", "num_comments": 9, "score": 2, "title": "[PRAW] Using 'get_sticky', if a link is sticked causes crash", "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "sufficiency", "body": "Just out of curiosity since Google has seemingly failed me. I am thinking running two different Reddit bots (both via PRAW, in case it matters) from the same machine - they are completely different bots and will be using different accounts. Do these bots have different caps on the number of requests, or do they have to share the bandwidth?", "created_utc": 1451704531, "gilded": 0, "name": "t3_3z3ff3", "num_comments": 5, "score": 10, "title": "Two bots, one IP?", "url": "https://www.reddit.com/r/redditdev/comments/3z3ff3/two_bots_one_ip/"}, {"author": "RemindMeBotWrangler", "body": "Anytime there are special characters in the send message parameters httpException is raised. TEXT: \"Hol\u00e0!\" Traceback (most recent call last): File \"\", line 2, in send_message File \"/usr/local/lib/python2.7/site-packages/praw/decorators.py\", line 268, in wrap return function(*args, **kwargs) File \"\", line 2, in send_message File \"/usr/local/lib/python2.7/site-packages/praw/decorators.py\", line 174, in require_captcha return function(*args, **kwargs) File \"/usr/local/lib/python2.7/site-packages/praw/__init__.py\", line 2492, in send_message retry_on_error=False) File \"\", line 2, in request_json File \"/usr/local/lib/python2.7/site-packages/praw/decorators.py\", line 113, in raise_api_exception s return_value = function(*args, **kwargs) File \"/usr/local/lib/python2.7/site-packages/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/site-packages/praw/__init__.py\", line 445, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python2.7/site-packages/praw/internal.py\", line 212, in _raise_response_excep tions raise HTTPException(_raw=exc.response) Is there something specifically I have to do with the text before it's sent to make sure it stays intact? The text is coming from mysql saved by doing `.encode('utf-8')` The HTTPException is a 500 response.", "created_utc": 1451692213, "gilded": 0, "name": "t3_3z2pgs", "num_comments": 4, "score": 3, "title": "PRAW send_message special characters", "url": "https://www.reddit.com/r/redditdev/comments/3z2pgs/praw_send_message_special_characters/"}, {"author": "Albuyeh", "body": "I have a Reddit Bot made in PRAW that runs every 5m. A typical output is: Searching SUBREDDIT at 2015-12-31 10:25:13 Adding 3yxq7y to DB Adding 3yxq9p to DB ... But recently I have sometimes been getting the following error: > sys:1: ResourceWarning: unclosed I added a try/except block for my Reddit login and had it print exceptions and sometime the exception is shown as: > Error EOF occurred in violation of protocol (_ssl.c:598) What could my issue be? I think it is related to SSL? Is there a better way to debug my code to see exactly what is unclosed?", "created_utc": 1451594475, "gilded": 0, "name": "t3_3yy8ai", "num_comments": 5, "score": 4, "title": "Error in PRAW: ResourceWarning: unclosed", "url": "https://www.reddit.com/r/redditdev/comments/3yy8ai/error_in_praw_resourcewarning_unclosed/"}, {"author": "BearlyBreathing", "body": "I'm trying to write a script that pulls down lots of reddit data (submissions, comments, and user info; basically everything), and I want to do this for entire sub-reddits for their entire history so obviously speed is an important factor. PRAW wants to break things up into multiple API requests because it assumes you don't want that much info, but is there a way to force it to pull more data at once? For example, I can get PRAW to pull all the comments from a submission like so: sub = r.get_submission(some_url) cms = praw.helpers.flatten_tree(s.comments) But if I want to know the author of those posts I have to do something like: users = [i.author.id for i in cms] Suddenly a handful of API calls (depending on the number of comments) becomes dozens, hundreds, or even thousands as PRAW fetches the authors one-by-one. At 1-2 seconds per-call this is obviously way too slow for my purposes. Is there a way to force PRAW to fetch all related data in a more efficient way? Alternatively, searching this sub for the answer has lead me to a few posts recommending r.request_json as a means for pulling a lot more data at once (1000 lines) and then parsing that instead of working within PRAW's object model. However, the documentation on r.request_json appears scant, and I'm not sure how to get it to do anything useful. It appears to simply be spitting out JSON formatted PRAW objects, which is not at all what I need. Ex: reddit.request_json('https://www.reddit.com/r/movies/comments/3ysj7z/leonardo_di_caprio_reveals_he_turned_down_the/') [{'data': {'after': None, 'before': None, 'children': [], 'modhash': ''}, 'kind': 'Listing'}, {'data': {'after': None, 'before': None, 'children': [, , , , , , , , , , , , , , , , , ], 'modhash': ''}, 'kind': 'Listing'}] Can anyone give me a hint or direct me to a good tutorial on using request_json or similar tools? Might it be easier to drop PRAW entirely and just parse /.json pages directly?", "created_utc": 1451514251, "gilded": 0, "name": "t3_3yu6nj", "num_comments": 6, "score": 5, "title": "[PRAW] Need to speed up big data collection. PRAW, r.request_json, or something else?", "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "cutety", "body": "Is there a way to get praw to not write the HTTP requests it makes to the log? Or at least to a different log? I'm not a pro when it comes to python's logging module, so I hope this isn't some stupid noob question. My logging basicConfig: logging.basicConfig(filename='postlimit.log', level=logging.DEBUG, format='%(asctime)s %(message)s')", "created_utc": 1451175217, "gilded": 0, "name": "t3_3ycb4r", "num_comments": 2, "score": 1, "title": "[PRAW] Stop praw from writing requests to .log", "url": "https://www.reddit.com/r/redditdev/comments/3ycb4r/praw_stop_praw_from_writing_requests_to_log/"}, {"author": "Squid__", "body": "I used PRAW to get all the usernames from a secret santa thread and a couple users messaged me saying they didn't show up on the list. The only thing I know is that the two users who asked me about it when clicking 'context' on the comment they made on that thread it says \"There doesn't seem to be anything here\". I just want to know if this is an issue with my script or if this is a Reddit issue. See for yourself: https://www.reddit.com/user/avisbaseball https://www.reddit.com/user/_FinanciallyFucked_ The 'HHH Secret Santa Mixtape Thread' is the one in question. Here's the code I use to grab usernames from every parent level comment. import praw def get_usernames(thread_id): user_agent = '/r/HipHopHeads Secret Santa Mixtape Messages 2k15 by /u/Squid__' r = praw.Reddit(user_agent=user_agent) r.login(disable_warning=True) print('Script is running, depending on the number of comments this could take awhile') submission = r.get_submission(submission_id=thread_id) # replaces \"MoreComments\" with all comments while type(submission.comments[-1]) == praw.objects.MoreComments: submission.replace_more_comments(limit=None, threshold=0) # adds the usernames to a set to avoid repeats usernames = set() for comment in submission.comments: usernames.add(comment.author.name) return list(usernames)", "created_utc": 1450940215, "gilded": 0, "name": "t3_3y24fg", "num_comments": 7, "score": 5, "title": "Issue with getting all usernames from a thread?", "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "washerdreier", "body": "I'm trying to fetch a user's comments or submissions (or if logged in their saves, likes, etc) and have no issue with fetching all or a limit from the most current, but I can't figure out how to get a subset before/after/between timestamps or before/after a placeholder post. I'm thinking of pretty much the same method [clockstalker](https://github.com/ClockStalker/clockstalker/blob/master/responder.py#L182) uses but that was back when the module was reddit verses praw. Is there still a way to get a slice of submissions/comments/etc or is that no longer supported? I've seen some comments about using search to get around the 1000 post limit, but that is usually for a whole subreddit and not focusing on a single user's content. If this isn't supported, should I be querying for the data in another way?", "created_utc": 1450666993, "gilded": 0, "name": "t3_3xnl2o", "num_comments": 6, "score": 2, "title": "[PRAW] How to get submissions, comments, saves, etc with a placeholder or date filter?", "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "godlikesme", "body": "There is this hack that allows you to download all submissions from a given subreddit [by doing multiple search requests using cloudsearch syntax with timestamp:XXXXXXXXXX..YYYYYYYYYY query. I used this hack/feature multiple times, so after my fifth time using it I made [a pull request to PRAW](https://github.com/praw-dev/praw/pull/554). It took a few weeks and 57 comments to merge it, but now it is available in the master branch. Which means that if you'd like to try it out, you can simply do: # pip install git+https://github.com/praw-dev/praw.git and then do: from praw.helpers import submissions_between r = praw.Reddit(\"\") for s in submissions_between(r, 'redditdev'): print s # or your own great code The helper additionally takes lowest_timestamp and highest_timestamp for filtering using, well, timestamps. Also, there is extra_cloudsearch_fields which gives you a way to filter by parameters like \"self\", \"author\", \"title\", etc: [full list of (mostly working) fields](https://www.reddit.com/wiki/search) I figured I could make this announcement because why not and because many people use PRAW and maybe they'll find this helper useful(also because I like bragging about my work). PS: If any reddit developers are reading this, please go and fix several bugs I found while developing this helper: [bug1](https://www.reddit.com/r/bugs/comments/3x9hgd/nsfw1_includes_nonnsfw_results_at_least_when/), [bug2](https://www.reddit.com/r/bugs/comments/3vkzts/cloudsearch_with_timestamp_for_rmod_returns_503/), [bug3](https://www.reddit.com/r/bugs/comments/3uxc09/api_sorting_by_new_is_broken/) (at least you could've changed the flairs to \"confirmed\")", "created_utc": 1450481652, "gilded": 0, "name": "t3_3xem6j", "num_comments": 2, "score": 7, "title": "PSA: I've just added a PRAW helper that allows you to easily download all submissions from a subreddit(or all submissions between two timestamps)", "url": "https://www.reddit.com/r/redditdev/comments/3xem6j/psa_ive_just_added_a_praw_helper_that_allows_you/"}, {"author": "TriggerHappyMoron", "body": "import praw import time import datetime userA = \"WarfRPScanner Vr0.1 /u/TriggerHappyBot\" userN = \"TriggerHappyBot\" passW = \"[password]\" thread = \"test\" upVoteThread = \"freekarma\" r = praw.Reddit(user_agent = userA) def get_date(sub): time = sub.created return datetime.datetime.fromtimestamp(time) def dsh(stringToSearch, stringToFind): if(str(stringToSearch).find(stringToFind)!=-1): return True else: return False def concatListRed(lisT): tempString = \"\" for item in lisT: tempString = tempString + \" /u/\" + item return tempString while True: print(\"--------------------\") print(\"Checking /\"+thread+\"/ for RP subreddits\") r.login(userN,passW,disable_warning=True) subreddit = None subreddit = r.get_subreddit(thread) subredditHot = subreddit.get_hot(limit=5) for submission in subredditHot: if(not dsh(submission, \"[IC]\")): continue redditors = [] print(submission) print(\"Submitted: \" + str(get_date(submission))) submission.replace_more_comments(limit=None, threshold=0) flatComments = praw.helpers.flatten_tree(submission.comments) print(\"Comments: \" + str(len(flatComments))) lateTime = -1 lateComment = None for comment in flatComments: if(comment.author == None): continue if(redditors.count(str(comment.author)) == 0): redditors.append(str(comment.author)) if(lateTime == -1 or comment.created > lateTime): lateTime = comment.created lateComment = comment if(lateComment == None): print(\"Empty thread\") else: print(\"Latest Comment by \" + str(lateComment.author).lower()) print(lateComment.body) redditors = [\"TriggerHappyMoron\", \"TriggerHappyBot\"] if(not dsh(lateComment.body, \"[PING]\")): print(\"Last comment was not a ping, pinging\") lateComment.reply(\"[PING]\" + concatListRed(redditors)) time.sleep(2) print(\"--------------------\") print(\"upvoting posts in /\"+upVoteThread+\"/\") subreddit = r.get_subreddit(upVoteThread) for sub in subreddit.get_hot(limit=5): if(sub.author != userN): print(sub) sub.upvote() time.sleep(5)", "created_utc": 1450273195, "gilded": 0, "name": "t3_3x2sy7", "num_comments": 3, "score": 4, "title": "Keep getting RateLimitExceeded when my bot comments twice, using praw", "url": "https://www.reddit.com/r/redditdev/comments/3x2sy7/keep_getting_ratelimitexceeded_when_my_bot/"}, {"author": "sunbolts", "body": "[The documentation](http://praw.readthedocs.org/en/stable/pages/code_overview.html) lists the methods and classes, but doesn't list any of the attributes, and it would be very helpful to know these and what information they contain. Are there any plans to include the attributes of each class in the docs? Thanks.", "created_utc": 1450058421, "gilded": 0, "name": "t3_3wq68t", "num_comments": 2, "score": 2, "title": "Documentation doesn't list attributes of Redditor, Submission, etc. classes", "url": "https://www.reddit.com/r/redditdev/comments/3wq68t/documentation_doesnt_list_attributes_of_redditor/"}, {"author": "souldeux", "body": "I recently built a bot in response to [this post](https://www.reddit.com/r/RequestABot/comments/3we9u1/a_bot_that_creates_a_topic_and_increases_a/) in /u/requestabot and documented the experience in my blog. I thought maybe someone here might find the tutorial useful. If anyone's interested, here's a quick writeup (one of many, I know) that takes you through building a Reddit bot with Python: http://souldeux.com/blog/build-reddit-bot-with-praw/", "created_utc": 1450050243, "gilded": 0, "name": "t3_3wpn6e", "num_comments": 3, "score": 18, "title": "Quick \"build-a-bot\" tutorial", "url": "https://www.reddit.com/r/redditdev/comments/3wpn6e/quick_buildabot_tutorial/"}, {"author": "BlackFayah", "body": "I am running Ubuntu 14.04 LTS and I'm trying to use PRAW in Python 3. Unfortunately, it keeps giving me this known error: Traceback (most recent call last): File \"timerbot.py\", line 2, in import praw ImportError: No module named 'praw' However, when I changed the default Python version to Python 3, it worked, but then other applications started crashing. How can I solve this problem? ***** **Edit:** I solved it by doing the following: sudo -rm /usr/local/bin/python3 sudo ln -s /usr/bin/python3 /usr/local/bin/python3", "created_utc": 1449743550, "gilded": 0, "name": "t3_3w7h97", "num_comments": 4, "score": 1, "title": "[PRAW] Can't get it to work with Python 3", "url": "https://www.reddit.com/r/redditdev/comments/3w7h97/praw_cant_get_it_to_work_with_python_3/"}, {"author": "chizdippler", "body": "I'm having trouble figuring out what's causing this problem, but at this point I'm 99% sure it's a conflict with OAuth2 and `content_md`. I'm also using prawoauth2, but since I've tested my code using the basic `login(user, pass)` function, there's no way it's causing a problem here. Here's my code with irrelevant parts removed: import praw from prawoauth2 import PrawOAuth2Mini reddit = praw.Reddit(USER_AGENT) oauth = PrawOAuth2Mini(reddit, app_key=CLIENT_ID, app_secret=SECRET, access_token=ACCESS_TOKEN, refresh_token=REFRESH_TOKEN, scopes=SCOPES) oauth.refresh() content = reddit.get_subreddit(SUBREDDIT).get_wiki_page('sidebar_edit').content_md And the console output: File \"test.py\", line 10 content = r.get_subreddit(SUBREDDIT).get_wiki_page('sidebar_edit').content_md File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\praw\\objects.py\", line 81, in __getattr__ self._has_fetched = self._populate(None, True) File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\praw\\objects.py\", line 157, in _populate json_dict = self._get_json_dict() if fetch else {} File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\praw\\objects.py\", line 150, in _get_json_dict self._info_url, params=params, as_objects=False) File \"\", line 2, in request_json File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\praw\\decorators.py\", line 123, in raise_api_exceptions raise exc praw.errors.HTTPException: HTTP error I'm trying to read the wiki and then update the sidebar so my scope is `['readwiki', 'modconfig']`. My bot is a moderator and the subreddit and its wiki are public. I double checked that the sidebar_edit wiki page exists and has content in it. I have also double checked my tokens, secret, and app key. I've done some searching and apparently there's a similar problem to this but specific to private subreddits, which is not the case here. What could be the problem?", "created_utc": 1449726474, "gilded": 0, "name": "t3_3w6n7z", "num_comments": 0, "score": 2, "title": "[PRAW] content_md causes HTTPException when using OAuth2", "url": "https://www.reddit.com/r/redditdev/comments/3w6n7z/praw_content_md_causes_httpexception_when_using/"}, {"author": "Dashing_in_the_90s", "body": "Reddit gold has a feature that lets me sort my saved posts by subreddit. Since my main list is over 1000 posts long the only way to access the older ones is to sort by subreddit. I am trying to make a script with PRAW that will backup my saved posts. Is it possible to return the list of saved posts from a specific subreddit rather than from the main list? edit: Problem Solved Thank you /u/GoldenSights for finding the solution: s=r.user.get_saved(params={'sr':'Askreddit'})", "created_utc": 1449450487, "gilded": 0, "name": "t3_3vqb21", "num_comments": 6, "score": 6, "title": "Using PRAW can I get my saved posts from a specific subreddit rather than from the main list?", "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "reseph", "body": "Does anyone use OAuth2Util? It's giving me this error: Traceback (most recent call last): File \"update_sidebar.py\", line 6, in o = OAuth2Util.OAuth2Util(r) File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 162, in __init__ self.refresh() File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 364, in refresh self._get_new_access_information() File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 254, in _get_new_access_information self._start_webserver(url) File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 229, in _start_webserver self.server = OAuth2UtilServer(server_address, OAuth2UtilRequestHandler, authorize_url) File \"/usr/lib/python2.7/site-packages/OAuth2Util/OAuth2Util.py\", line 58, in __init__ super().__init__(server_adress, handler_class, bind_and_activate) TypeError: super() takes at least 1 argument (0 given) Code: import praw import OAuth2Util user_agent = \"/r/ffxiv sidebar helper by /u/reseph\" r = praw.Reddit(user_agent=user_agent) o = OAuth2Util.OAuth2Util(r) Config file is already set up.", "created_utc": 1449368155, "gilded": 0, "name": "t3_3vm3q6", "num_comments": 24, "score": 4, "title": "[OAuth2Util] TypeError: super() takes at least 1 argument (0 given)", "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "lumbdi", "body": "I'm using PRAW library. E.g. we have this link: [https://www.reddit.com/r/**DotA2**/comments/**3sx4su**/**put_camera_yaw_into_option_menu_allow_us_to**/cx13thd](https://www.reddit.com/r/DotA2/comments/3sx4su/put_camera_yaw_into_option_menu_allow_us_to/cx13thd) I do: SUBREDDIT = \"AnalyzeLast100Games+Dota2+LearnDota2\" subreddit = r.get_subreddit(SUBREDDIT) posts = subreddit.get_comments(limit=100) and my bot finds that comment. (Let's assume the post was recently made.) I have the post id of that comment via: for post in posts: pid = post.id How do I get the \"thread id\"? And how do I get the thread title? How do I get the subreddit name (if relevant)? edit: #solution: thread title and subreddit name is irrelevant. \"thread id\" or link id is retrieved via: post.link_id constructing the link: https://www.reddit.com//comments/// [Some text](/comments///)", "created_utc": 1449072493, "gilded": 0, "name": "t3_3v5muu", "num_comments": 11, "score": 5, "title": "How to generate a link to the comment?", "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "helpmewebbit", "body": "Setting `reddit.config.store_json_result = True` and writing a standalone comment from a flattened tree to a json file works fine. But whenever the comment has replies, I get this error: TypeError: is not JSON serializable I know this happens because the `replies` field cannot be serialised and it would be redundant to serialise it since I'm using a flattened tree anyway. But I'm at a loss to figure out a consistent way to handle this type of error. Can praw be set to simply ignore the replies field (which I don't need)? Or do I have to create an entirely new `dict` object sans \"replies\", which feels cumbersome?", "created_utc": 1449006006, "gilded": 0, "name": "t3_3v1yuv", "num_comments": 7, "score": 5, "title": "How to serialize praw.objects.Comment in a consistent way?", "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "theonefoster", "body": "I paused execution to [check the attributes of a submission object](https://imgur.com/8anwxE2), but nowhere was the full URL provided. If you go to the [submission in question](https://www.reddit.com/r/food/comments/3udre2/honey_roasted/), you'll see that the link has a \".jpg\" on the end, whereas in my object watch window it just has the imgur link. There are no other attributes containing the full link. [Here's the API link to that submission](https://api.reddit.com/r/food/comments/3udre2/honey_roasted/). You can see that there's no extension under \"url\", and a page search for \"jpg\" returns no results other than thumbnail links. How can I get the full link in praw? I need to get the full link, then check whether or not it ends in \".jpg\", \".png\" etc to see whether the submission is a direct image link. --- edit: [this submission seems to have the extension..](https://api.reddit.com/r/WTF/comments/3uchq3/who_needs_a_boat_when_you_got_a_car/) | [as does this one](https://api.reddit.com/r/WTF/comments/3ua3gy/a_rare_condition_called_congenital_arthrogryposis/). What's going on then? Edit2: for future people with the same problem, RES was the issue as per [this comment](https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/cxe19nc). Disabling RES displays the correct link in the browser.", "created_utc": 1448568185, "gilded": 0, "name": "t3_3udusd", "num_comments": 6, "score": 6, "title": "How do I get the FULL submission URL from a praw submission object? submission.url seems to chop the file extension off imgur links", "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "cogsbox", "body": "I have read a few of the posts that have been written about deleted users, but I can't seem to handle the errors well. In my code I look for users in the title of a post then return their subreddits. Once I hit a deleted user my code crashes. This is the error: praw.errors.NotFound: HTTP error Thanks for your help!", "created_utc": 1448528472, "gilded": 0, "name": "t3_3ubstm", "num_comments": 5, "score": 2, "title": "Deleted Users", "url": "https://www.reddit.com/r/redditdev/comments/3ubstm/deleted_users/"}, {"author": "thetoethumb", "body": "Hey all, I'm having a first crack at interfacing with reddit using Python but can't seem to get even a basic example to work. I followed the example from the [github page](https://github.com/praw-dev/praw) pretty much exactly but no dice. Anyone have any advice? --- **MWE** import praw r = praw.Reddit(user_agent=\"/u/thetoethumb test\") submissions = r.get_subreddit('askreddit').get_hot(limit=10) print [str(x) for x in submissions] **Result/error** (full traceback in comment below) TypeError: data must be a byte string **Versions** I'm using Windows 8 64bit if that's any use, but: >>> praw.__version__ 3.3.0 >>> requests.__version__ 2.5.0 >>> sys.version '2.7.9 (default, Dec 10 2014, 12:24:55) [MSC v.1500 32 bit (Intel)]' --- I'm thinking maybe an installation error when I installed PRAW or maybe incompatibility with Python 2.7.9? I installed PRAW with ``easy_install praw`` using CMD and it didn't seem to throw any errors (still have the CMD window open if that's any use). Anyone have any advice? Thanks in advance!", "created_utc": 1448505248, "gilded": 0, "name": "t3_3uapp8", "num_comments": 4, "score": 2, "title": "PRAW error: \"data must be a byte string\"", "url": "https://www.reddit.com/r/redditdev/comments/3uapp8/praw_error_data_must_be_a_byte_string/"}, {"author": "theonefoster", "body": "If I execute the following code, I get the output below. >>comments = r.get_subreddit(\"redditdev\").get_comments(limit=500, fetch=True) >>list(comments) [ , ... etc] >>list(comments) [] Notice the second time I try to list the comments, I get an empty list. This is true for seemingly any operation I perform on comments - the operation works fine, but seems to erase the list when it's done. Can anyone explain what's going on, and how to read 'comments' more than once?", "created_utc": 1448483918, "gilded": 0, "name": "t3_3u9f4p", "num_comments": 2, "score": 1, "title": "Why do comment objects erase themselves when I read them?", "url": "https://www.reddit.com/r/redditdev/comments/3u9f4p/why_do_comment_objects_erase_themselves_when_i/"}, {"author": "dClauzel", "body": "The previous script has been working for several months, but it stops some days ago (error 500). No idea why. Other operations (banning a user, getting the karma from a user, etc) are fine. Does someone have any clue? `test-Europe-banlist.py` #!/usr/bin/env python3 # -*- coding: utf-8 -*- import praw import OAuth2Util import datetime print(\"Connexion\u2026\", end=\" \") r = praw.Reddit(user_agent=\"posix:test-Europe-banlist:v0 (by /u/dClauzel)\", site_name=\"Reddit\") o = OAuth2Util.OAuth2Util(r, print_log=True) o.refresh() print(\"connect\u00e9.\") # liaison sur le sousjlailu de travail; ici /r/Europe sousjlailu = r.get_subreddit(\"Europe\") bannis = [i for i in sousjlailu.get_banned(limit=None,user_only=False)] print( \"Il y a {0} bannis.\".format(len(bannis)) ) for leBanni in bannis: bNom = leBanni['name'] bId = leBanni['id'] bDate = datetime.datetime.fromtimestamp(leBanni['date']) bNote = leBanni['note'] print(\"/u/{0} ({1}) : le {2}; {3}\".format( bNom, bId, bDate, bNote) ) print(\"fin Liste des bannis\") `./test-Europe-banlist.py` Connexion\u2026 connect\u00e9. Traceback (most recent call last): File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 210, in _raise_response_exceptions response.raise_for_status() # These should all be directly mapped File \"/usr/local/lib/python3.4/dist-packages/requests/models.py\", line 837, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://oauth.reddit.com/r/Europe/about/banned/.json?limit=1024&after=rb_cyrenm During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"./test-Europe-banlist.py\", line 19, in bannis = [i for i in sousjlailu.get_banned(limit=None,user_only=False)] File \"./test-Europe-banlist.py\", line 19, in bannis = [i for i in sousjlailu.get_banned(limit=None,user_only=False)] File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 1900, in _get_userlist for data in content: File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 557, in get_content page_data = self.request_json(url, params=params) File \"\", line 2, in request_json File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 445, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 212, in _raise_response_exceptions raise HTTPException(_raw=exc.response) praw.errors.HTTPException: HTTP error sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty", "created_utc": 1448054725, "gilded": 0, "name": "t3_3tm6r6", "num_comments": 1, "score": 3, "title": "[praw] A script accessing the banlist is not working anymore (error 500)", "url": "https://www.reddit.com/r/redditdev/comments/3tm6r6/praw_a_script_accessing_the_banlist_is_not/"}, {"author": "AllHailTheCATS", "body": "Im trying to make a bot that replys to me every time I comment, right now its not doing what its supposed to, if I remove the not from the if statement I might get a reply but I also get a traceback error, if I keep it there nothing happens which makes sense as it checks if the id is in the used ids db. But if I make a new comment nothing happens still? Can someone tell me where im going wrong in my code? import praw import BotAccess import sqlite3 USERAGENT = \"A account that will reply under certain users comments \" USERNAME = BotAccess.username PASS = BotAccess.password ACCOUNT = BotAccess.account SUBREDDIT = \"test\" MAXPOSTS = 100 print(\"Setting up database..\") sql = sqlite3.connect(\"sql.db\") cur = sql.cursor() cur.execute('CREATE TABLE IF NOT EXISTS oldposts(ID TEXT)') sql.commit() print(\"Logging into reddit\") r = praw.Reddit(USERAGENT) r.login(USERNAME, PASS, disable_warning=True) def HypeManBot(): subreddit = r.get_subreddit(SUBREDDIT) comments = subreddit.get_comments(limit=MAXPOSTS) for comment in comments: cur.execute('SELECT * FROM oldposts WHERE ID=?', [comment.id]) if not cur.fetchone: try: myComment = comment.author.name if myComment == ACCOUNT: print(\"Replying..\") comment.reply(\"Yeaaaah!!\") except AttributeError: pass cur.execute('INSERT INTO oldposts VALUES(?)', [comment.id]) sql.commit() while True: HypeManBot()", "created_utc": 1448054491, "gilded": 0, "name": "t3_3tm67g", "num_comments": 8, "score": 0, "title": "My bot wont reply when its ment to.", "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "avinassh", "body": "TLDR; Check the demo! - https://kekday.herokuapp.com --- Reddit gives all the user info in a handy JSON at this URL: `https://www.reddit.com/user//about.json` example: [https://www.reddit.com/user/spez/about.json](https://www.reddit.com/user/spez/about.json) The `created_utc` field in `data` is the date of user's registration aka Cake Day in [unix epoch](https://en.wikipedia.org/wiki/Unix_time) format (in UTC) and we can easily convert that to readable format: >>> import time >>> time.strftime(\"%D\", time.gmtime(1118030400)) '06/06/05' Using [Python Requests](http://python-requests.org), we can turn this into a handy function: import time import requests def get_my_cake_day(username): url = \"https://www.reddit.com/user/{}/about.json\".format(username) r = requests.get(url) created_at = r.json()['data']['created_utc'] return time.strftime(\"%D\", time.gmtime(created_at)) Though above function will work, but soon it will start throwing HTTP 429 error i.e Too Many Requests. Thing is, Reddit doesn't really like when someone tries to fetch the data like this. The requests are made directly on Reddit servers without using the API. Now if you have want to find cake day of hundreds of users, you cannot use this method. Solution? Use [Reddit's API](https://www.reddit.com/dev/api). In Python, we will use [praw](https://github.com/praw-dev/praw) and [prawoauth2](https://github.com/avinassh/prawoauth2). praw is a Python wrapper for Reddit's API and prawoauth2 helps dealing with [OAuth2](https://github.com/reddit/reddit/wiki/OAuth2). Let's start by installing praw: pip install praw Now we can convert the `get_my_cake_day` to praw version and get the user details like this: import time import praw reddit_client = praw.Reddit(user_agent='my amazing cake day bot') def get_my_cake_day(username): redditor = reddit_client.get_redditor(username) return time.strftime(\"%D\", time.gmtime(redditor.created_utc)) Above code pretty much self explanatory. What if the user doesn't exist or shadowbanned? In such cases, praw throws an exception: `praw.errors.NotFound`. Lets modify `get_my_cake_day` to catch this: def get_my_cake_day(username): try: redditor = reddit_client.get_redditor(username) return time.strftime(\"%D\", time.gmtime(redditor.created_utc)) except praw.errors.NotFound: return 'User does not exist or shadowbanned' This is better compared to earlier version and we will stop getting rate limit errors often. Also, praw will handle such cases and makes requests again to fetch the data. But what if we want to increase the limit? The above requests are not authenticated, meaning Reddit does not recognise your app. However, if we register this app in Reddit and let Reddit know, then requests limits will increase. So to authenticate our app over Oauth2, we will use prawoauth2. Lets install it first: pip install prawoauth2 Follow the simple steps [here](https://prawoauth2.readthedocs.org/usage_guide.html) to register your app on Reddit. Once done, you will get `app_token` and `app_secret`. Then you need to get `access_token` and `refresh_token`. You could use this handy [`onetime.py`](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/onetime.py) script. For detailed instructions check the documentation of [prawoauth2](https://prawoauth2.readthedocs.org). You should never make `app_token`, `app_secret`, `access_token` and `refresh_token` public and never commit them to version control. Keep them always secret. Here is the complete script using prawoauth2: import time import praw from secret import (app_key, app_secret, access_token, refresh_token, user_agent, scopes) reddit_client = praw.Reddit(user_agent='my amazing cakeday bot') oauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key, app_secret=app_secret, access_token=access_token, refresh_token=refresh_token, scopes=scopes) def get_my_cake_day(username): try: redditor = reddit_client.get_redditor(username) return time.strftime(\"%D\", time.gmtime(redditor.created_utc)) except praw.errors.NotFound: return 'User does not exists or shadowbanned' Again, pretty much self explanatory. If your tokens are correct and once `PrawOAuth2Mini` is initialized properly, there will be no issues with the app and you will have twice as many requests as compared to unauthenticated version. Want to see above app in action? Check this - [kekday](https://kekday.herokuapp.com). The [app is open source](https://github.com/avinassh/kekday) and released under MIT License.", "created_utc": 1448049203, "gilded": 0, "name": "t3_3tlt8o", "num_comments": 6, "score": 9, "title": "[Tutorial] Find a Redditor's cake day using praw and prawoauth2 over OAuth", "url": "https://www.reddit.com/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/"}, {"author": "avinassh", "body": "I need `created` and/or `created_utc` of praw Redditor object: user = reddit_client.get_redditor('avinassh') print(user.created) However it seems that praw is making two requests. Is it possible to fetch this in a single request? Thanks! EDIT: After /u/13steinj's comment I checked again, looks like I was wrong.", "created_utc": 1447997338, "gilded": 0, "name": "t3_3tj7qw", "num_comments": 2, "score": 3, "title": "[PRAW] How to get `user` info a single request?", "url": "https://www.reddit.com/r/redditdev/comments/3tj7qw/praw_how_to_get_user_info_a_single_request/"}, {"author": "lumbdi", "body": "I'm new to Python PRAW and I'm sure there are a few other bots that already have this functionality. I need some example codes. I recently learned programming a bit in Python and how to use Steam API and wrote a small Python script that [displays some information](http://i.imgur.com/C4fQGXW.png). Now I want it to post to Reddit. I'm already having problems with auth.", "created_utc": 1447943753, "gilded": 0, "name": "t3_3tfqea", "num_comments": 11, "score": 2, "title": "[PRAW] Need example code for bot logging in and replying to a comment when the bot is summoned via /u/botname", "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "ffranglais", "body": "Shreddit: https://github.com/x89/Shreddit No I'm not a Voater, I hate those freeze peach FPH scum. Here's what I did: I was trying to get OAuth2 authorization, so I created a personal use script with the callback URL https://127.0.0.1:65010 as instructed. In Shreddit, I edited praw.ini and cross referenced the oauth_client_id, oauth_client_secret and oauth_client_uri with the script, also as instructed, and was asked to authorize said script to access my user information. So far, so good, right? Well, when I clicked accept, I got a 500 internal server error. **Why is this happening?** This error has been making my hair fall out for days, I can't figure out what the problem is. I don't know much about computer networking. I've heard that the Reddit API is finicky with HTTPS, maybe that's the problem.", "created_utc": 1447907009, "gilded": 0, "name": "t3_3te2i6", "num_comments": 4, "score": 4, "title": "I'm getting a 500 internal server error when I try to run Shreddit, and I don't know why.", "url": "https://www.reddit.com/r/redditdev/comments/3te2i6/im_getting_a_500_internal_server_error_when_i_try/"}, {"author": "alexleavitt", "body": "According to the API, you can query for \"related\" posts given a submission_id. https://www.reddit.com/dev/api#GET_related_{article} But as far as I could tell, you can't do this in PRAW... or am I incorrect?", "created_utc": 1447737422, "gilded": 0, "name": "t3_3t4cax", "num_comments": 1, "score": 7, "title": "Can you query PRAW for 'related' posts?", "url": "https://www.reddit.com/r/redditdev/comments/3t4cax/can_you_query_praw_for_related_posts/"}, {"author": "tobiasvl", "body": "Hey, I get this error when authing with Praw: https://gist.github.com/tobiasvl/71ded8d6940b357c9235 Looks like it might be an issue deeper down in my Python installation than Praw, but if you have an inkling and can nudge me in the right direction, I'd be grateful! Edit: Fixed it myself, posted solution in comment", "created_utc": 1447696234, "gilded": 0, "name": "t3_3t1mum", "num_comments": 3, "score": 1, "title": "Error when logging in with Praw", "url": "https://www.reddit.com/r/redditdev/comments/3t1mum/error_when_logging_in_with_praw/"}, {"author": "Frenchiie", "body": "when i do the authorization part, do i need to specify a scope if i dont actually need any permission granting user information? In non oauth mode i'm currently fetching people's publicly view-able comments with PRAW: > reddit_user = r.get_redditor(user) > for comment in reddit_user.get_comments(limit=100): My reason for switching to oauth is that 2 seconds per request is just too slow. I dont actually need anything that requires oauth except for the benefit of being able to request every second. Also for Step 4 of the [PRAW(OAuth) tutorial](http://praw.readthedocs.org/en/stable/pages/oauth.html) how does reddit know that the \"code\" from the url belongs to 1 specific user? What stops a program from re-using the code to automatically get authorization from any user?", "created_utc": 1447564295, "gilded": 0, "name": "t3_3sv72o", "num_comments": 6, "score": 6, "title": "OAuth(with PRAW) if i don't actually need permission granting information?", "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "IAPark", "body": "I'm of course aware of PRAW, but it would be much more useful to me to have something using something like the template method pattern to provide more high level behavior. For example having listeners for events like: * Submission Added * Comment Added * Comment Edited * Keyword Mentioned Basically, is there something that takes the grunt work out of making a Reddit Bot? If not would there be interest from others to use such a framework if I were to develop it as part of my current project. I'd probably be coding in Python just because it has the best libraries for what I need, but what could other bot makers use?", "created_utc": 1447352818, "gilded": 0, "name": "t3_3sk8sy", "num_comments": 12, "score": 7, "title": "Is There an Existing Framework for Reddit Bots?", "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "CVance1", "body": "Hello, I'm a bit new to the reddit API. Is there a way in PRAW to get the date something was submitted and return it, so you could potentially see when something was last posted?", "created_utc": 1447341810, "gilded": 0, "name": "t3_3sjioe", "num_comments": 8, "score": 2, "title": "[PRAW] Searching posts in a subreddit by timestamp", "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "Albuyeh", "body": "I am using PRAW for Python to create a bot that will detect if a users post was caught by spam filter. What is the best way to detect this and not if a post was removed by a mod.", "created_utc": 1446962341, "gilded": 0, "name": "t3_3rz98x", "num_comments": 7, "score": 3, "title": "Notify User if Post Caught By Spam Filter", "url": "https://www.reddit.com/r/redditdev/comments/3rz98x/notify_user_if_post_caught_by_spam_filter/"}, {"author": "VRCkid", "body": "I've been trying to make a bot that finds a certain domain in the submission or the body of the comment doing something with that information. What I have tried is going for submissions is submissions = r.get_subreddit('all').get_new(limit=None) #iterate through and submissions = praw.helpers.submission_stream(r, \"all\", limit=10) #iterate through I don't know which one these methods would work best for getting all submissions. I've tried looking for a solution and looking at other bots and I haven't found a good answer yet. Does anyone have an idea?", "created_utc": 1446834572, "gilded": 0, "name": "t3_3rsk3k", "num_comments": 0, "score": 6, "title": "[PRAW] What's the best way of checking all Submissions and all Comments?", "url": "https://www.reddit.com/r/redditdev/comments/3rsk3k/praw_whats_the_best_way_of_checking_all/"}, {"author": "hullcrush", "body": "There's at least 20 posts on how to do this, yet I'm unable to complete this basic task. Several days of research later, I'm still stuck. import praw r = praw.Reddit('Comment parser example by u/_Daimon_') subreddit = r.get_subreddit(\"python\") comments = subreddit.get_comments() The subreddit is https://www.reddit.com/r/ifyoulikeblank/ I want the title of the thread and the comments within. That's it. Or get_comments from posts with only 10 comments or above would be preferred. So then what happens? Python does this... >> I'm missing something fundamental here, I apologize, as I'd like the result dumped somewhere, even if it's in hieroglyphics. I've tried print and a convoluted dump to a .txt file but I still get >> Thanks in advance, this highlights my limited code understanding.", "created_utc": 1446410643, "gilded": 0, "name": "t3_3r4fv9", "num_comments": 6, "score": 5, "title": "Using PRAW to scrape first page comments of a subreddit", "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "SecretAg3nt", "body": "So I've been playing around with praw the last two days and decided to write a bot that would message my account when ever someone mentioned a given keyword in their comments. It's worked pretty well so far, except I keep getting this type of error after a seeming random amount of time: TypeError: getresponse() got an unexpected keyword argument 'buffering' Which then stops the bot and I have to start it again. Here is the full code I have written: import praw from time import ctime from time import sleep import obot from time import time commDone = set() NUMCOMMENTS = 0 SETPHRASES = [The keywords I'm interested in] COMMLIMIT = 100 r = obot.login() print(\"Logged in\") for comment in praw.helpers.comment_stream(r, 'all', limit=COMMLIMIT): if comment.id not in commDone: cbody = comment.body.lower() cwords = cbody.split() commDone.add(comment.id) for word in cwords: for key in SETPHRASES: if word == key: NUMCOMMENTS = NUMCOMMENTS + 1 msg = str(NUMCOMMENTS) + \") *\" + ctime() + \"* Someone is talking about \\\"\" + key + \"\\\": \" + comment.author.name + \" \" + comment.permalink print(msg) subject = \"Someone is talking about \\\"\" + key r.send_message('SecretAg3nt', subject, msg) print(\"Message Sent\") and here is the full error I am getting: Traceback (most recent call last): File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 376, in _make_request httplib_response = conn.getresponse(buffering=True) TypeError: getresponse() got an unexpected keyword argument 'buffering' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 378, in _make_request httplib_response = conn.getresponse() File \"/usr/lib/python3.4/http/client.py\", line 1147, in getresponse response.begin() File \"/usr/lib/python3.4/http/client.py\", line 351, in begin version, status, reason = self._read_status() File \"/usr/lib/python3.4/http/client.py\", line 313, in _read_status line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\") File \"/usr/lib/python3.4/socket.py\", line 371, in readinto return self._sock.recv_into(b) File \"/usr/lib/python3.4/ssl.py\", line 746, in recv_into return self.read(nbytes, buffer) File \"/usr/lib/python3.4/ssl.py\", line 618, in read v = self._sslobj.read(len, buffer) socket.timeout: The read operation timed out During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/adapters.py\", line 370, in send timeout=timeout File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 609, in urlopen _stacktrace=sys.exc_info()[2]) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/util/retry.py\", line 245, in increment raise six.reraise(type(error), error, _stacktrace) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/packages/six.py\", line 310, in reraise raise value File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 380, in _make_request self._raise_timeout(err=e, url=url, timeout_value=read_timeout) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 308, in _raise_timeout raise ReadTimeoutError(self, url, \"Read timed out. (read timeout=%s)\" % timeout_value) requests.packages.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='oauth.reddit.com', port=443): Read timed out. (read timeout=45.0) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"redditbotcommentstream.py\", line 15, in for comment in praw.helpers.comment_stream(r, 'all', limit=COMMLIMIT): File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/helpers.py\", line 138, in _stream_generator for i, item in gen: File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/__init__.py\", line 557, in get_content page_data = self.request_json(url, params=params) File \"\", line 2, in request_json File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/__init__.py\", line 444, in _request response = handle_redirect() File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/praw/handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/sessions.py\", line 576, in send r = adapter.send(request, **kwargs) File \"/home/nitrous/code/redditbot/lib/python3.4/site-packages/requests/adapters.py\", line 435, in send raise ReadTimeout(e, request=request) requests.exceptions.ReadTimeout: HTTPSConnectionPool(host='oauth.reddit.com', port=443): Read timed out. (read timeout=45.0) sys:1: ResourceWarning: unclosed /home/nitrous/code/redditbot/lib/python3.4/importlib/_bootstrap.py:2127: ImportWarning: sys.meta_path is empty", "created_utc": 1446159525, "gilded": 0, "name": "t3_3qroun", "num_comments": 3, "score": 1, "title": "Problem keeping praw with OAuth2 running", "url": "https://www.reddit.com/r/redditdev/comments/3qroun/problem_keeping_praw_with_oauth2_running/"}, {"author": "StormZero1", "body": "import praw user_agent = \"rketchupsoup\" r= praw.Reddit(user_agent = user_agent) submissions = r.get_subreddit('worldnews').get_hot(limit=5) print submissions this always prints \"\" but has never seemed to do that before, have I left something out? (edit) managed to get it to be a bit less broken. Using this code: def scrape(): subreddit = r.get_subreddit('4chan') for submission in subreddit.get_hot(limit=10): print submission I've managed to get it to work, well at least in shows in the python interpreter. However, the purpose of this script is to email me the top x posts on a subreddit, and when it arrives in my email inbox, it simply prints as \"none\" rather than showing the posts, even though it has done this in the interpreter. No idea what could be causing this.", "created_utc": 1446071753, "gilded": 0, "name": "t3_3qmign", "num_comments": 6, "score": 2, "title": "[PRAW]Instead of displaying posts, my script is printing \"<generator object get_content at 0x02F44F80>\"", "url": "https://www.reddit.com/r/redditdev/comments/3qmign/prawinstead_of_displaying_posts_my_script_is/"}, {"author": "theonefoster", "body": "subreddit = r.get_subreddit('all', fetch=True) comments = subreddit.get_comments(limit=100) That's the code I'm using. The first line is throwing a HTTP error , and I've no idea why. If I replace the first line with **either** of the following, it works fine: subreddit = r.get_subreddit('test', fetch=True) subreddit = r.get_subreddit('all') The trouble is that PRAW caches the comments for 30 seconds, so when I query the new comments every 5 seconds I just get the same list back, even if there are new comments. My bot then recognises the comment IDs and refuses to process them (intended). So I can only access the /r/all comments every 30 seconds, but I can access any individual subreddit's as often as I like. Does anyone know how to fix the HTTP error? I think what it's doing is passing the fetch=True parameter to Reddit instead of cutting it off internally. The full traceback is below, although I don't know how useful it is. Traceback (most recent call last): File \"D:\\Stuff\\Visual Studio\\SlashBotFinal\\SlashBot.py\", line 69, in checkComments() File \"D:\\Stuff\\Visual Studio\\SlashBotFinal\\SlashBot.py\", line 28, in checkComments subreddit = r.get_subreddit('all', fetch=True) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 1078, in get_subreddit return objects.Subreddit(self, subreddit_name, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 1528, in __init__ info_url, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 71, in __init__ self._has_fetched = self._populate(json_dict, fetch) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 157, in _populate json_dict = self._get_json_dict() if fetch else {} File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 150, in _get_json_dict self._info_url, params=params, as_objects=False) File \"\", line 2, in request_json File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 445, in _request_raise_response_exceptions(response) File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 207, in _raise_response_exceptions raise NotFound(_raw=response) praw.errors.NotFound: HTTP error", "created_utc": 1445801626, "gilded": 0, "name": "t3_3q6mhj", "num_comments": 4, "score": 2, "title": "I'm getting a HTTP Error on \"r.get_subreddit('all', Fetch=True)\" but I'm sure the code is correct. Can anyone help?", "url": "https://www.reddit.com/r/redditdev/comments/3q6mhj/im_getting_a_http_error_on_rget_subredditall/"}, {"author": "habnpam", "body": "Sort of similar to how this in PRAW: `subreddit.get_top_from_day(limit=10)` I'm able to create a Subreddit object, but I don't know how to fetch submissions(like above) from it. --- These are links to the subreddit and redditclient objects. [subreddit.java](https://github.com/thatJavaNerd/JRAW/blob/master/src/main/java/net/dean/jraw/models/Subreddit.java) [redditclient.java](https://github.com/thatJavaNerd/JRAW/blob/master/src/main/java/net/dean/jraw/RedditClient.java) From looking at the files, it doesn't seem possible atm? --- edit. found solution. An example would be this: RedditClient r = new RedditClient(new UserAgent(\"...\")); SubredditPaginator sp = new SubredditPaginator(r); sp.setLimit(100); sp.setSorting(Sorting.TOP); sp.setTimePeriod(TimePeriod.DAY); sp.setSubreddit(\"pics\"); sp.next(true); Listing list = sp.getCurrentListing(); System.out.println(list.get(0).getAuthor());", "created_utc": 1445738518, "gilded": 0, "name": "t3_3q3q5i", "num_comments": 3, "score": 3, "title": "[JRAW] Get top submissions from subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/3q3q5i/jraw_get_top_submissions_from_subreddit/"}, {"author": "EndeRDIT", "body": "I'm currently working on a project where I'm setting up multiple submission_streams to monitor subreddits. Each stream is running in its own thread, but sharing a reddit_session, which is authenticated using OAuth. My code right now works fine with a single thread. However, whenever I try to up the thread count >= 2, PRAW throws an AssertionError in the decorators.py file [here](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L247). Each thread uses the following as its target: def submission_loop(self, subreddit): stream = praw.helpers.submission_stream(self.reddit_session, subreddit, limit = 15) try: for submission in stream: self.logger.debug('{}: SUBMISSION: {}'.format(subreddit, submission.title)) self.act_submission(submission) except Exception as e: self.logger.error('something went wrong') exit(-1) Am I missing something here with regards to PRAW? Can anyone point me in the right direction? *EDIT: Do I need to have a single reddit_session per thread in addition to the MultiprocessHandler (already in use)?*", "created_utc": 1445546725, "gilded": 0, "name": "t3_3ptjiy", "num_comments": 5, "score": 2, "title": "[PRAW] submission_stream AssertionError", "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "Naurgul", "body": "I just tried the following with PRAW: for s in r.get_subreddit(\"all\").get_top_from_day(limit=None): words = s.title.split() if getDates(words): print s.score, s.title and I get way less results if I change it to limit=1000 Did anyone else notice anything similar?", "created_utc": 1445033595, "gilded": 0, "name": "t3_3p1nu1", "num_comments": 2, "score": 7, "title": "Was the 1000 limit removed?", "url": "https://www.reddit.com/r/redditdev/comments/3p1nu1/was_the_1000_limit_removed/"}, {"author": "DanielGibbs", "body": "I have a script which runs weekly to create some weekly discussion threads and then update the sidebar and submission text with links to them, yet after upgrading to use PRAW 3.3.0 and OAuth, it doesn't seem to be able to update the sidebar anymore (although the submission text is updated). # Prior to this point, weekly_thread has been successfully submitted. sidebar_text = my_subreddit.description sidebar_text = re.sub(\"(?", "created_utc": 1444556893, "gilded": 0, "name": "t3_3obe5g", "num_comments": 2, "score": 2, "title": "Are there restrictions on changing a subreddit's sidebar?", "url": "https://www.reddit.com/r/redditdev/comments/3obe5g/are_there_restrictions_on_changing_a_subreddits/"}, {"author": "haiguise1", "body": "I have been getting the following error when trying to authenticate reddit. I then tried to get new access details but this gave the same error. The error started when I updated praw to 3.3.0 with pip. I tried to downgrade back to 3.2.1 but the error persists. I'm using python 2.7. Now, whenever I try to do something which gets data from reddit the error occurs. The following code caused the error: import praw reddit = praw.Reddit(\"testing praw, by haiguise1\") a = reddit.get_subreddit(\"askscience\") b = a.get_top_from_day() c = [x for x in b] # Traceback (most recent call last): File \"test.py\", line 6, in c = [x for x in b] File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 557, in get_content page_data = self.request_json(url, params=params) File \"\", line 2, in request_json File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 444, in _request response = handle_redirect() File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 579, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 369, in send timeout=timeout File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\", line 353, in _make_request conn.request(method, url, **httplib_request_kw) File \"/usr/lib/python2.7/httplib.py\", line 1001, in request self._send_request(method, url, body, headers) File \"/usr/lib/python2.7/httplib.py\", line 1035, in _send_request self.endheaders(body) File \"/usr/lib/python2.7/httplib.py\", line 997, in endheaders self._send_output(message_body) File \"/usr/lib/python2.7/httplib.py\", line 850, in _send_output self.send(msg) File \"/usr/lib/python2.7/httplib.py\", line 826, in send self.sock.sendall(data) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 216, in sendall data = memoryview(data) TypeError: cannot make memory view because object does not have the buffer interface", "created_utc": 1444515434, "gilded": 0, "name": "t3_3o9mt2", "num_comments": 8, "score": 2, "title": "Praw crashes when trying to do anything.", "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "phenomist", "body": "I'm writing a bot that fetches comments on a ~20 second delay using PRAW. However, upon looking at the timestamps, it appears that it sometimes misses comments until about 2 minutes later. Is this an inherent limitation with PRAW / the reddit API or am I doing something wrong?", "created_utc": 1444457953, "gilded": 0, "name": "t3_3o6y4x", "num_comments": 7, "score": 1, "title": "Is there a delay in getting comments/posts?", "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "schmodd", "body": "So I have something like this code below. Is there any better solution to filter the user submitted output for a specific subreddit? Does get_submitted() feature some subreddit filter option? I am using Python 3.5. ... users = ('somenames', 'evenmorenames') handler = MultiprocessHandler() r = praw.Reddit(user_agent='fubar', handler=handler) for user in users: current_user = r.get_redditor(user) submissions = current_user.get_submitted() for submission in submissions: if submission.subreddit.display_name != targetSubreddit: ...", "created_utc": 1444400423, "gilded": 0, "name": "t3_3o3qtc", "num_comments": 11, "score": 1, "title": "how to get user submitted posts filtered by specific subreddit", "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "cogsbox", "body": "I have been trying to get author names of posts in this sub, but all I get returned it 'Loanbot' for the author. Is their a way around this? Here is my code import praw user_agent = \"test authorName 1.0 by /u/user\" r = praw.Reddit(user_agent=user_agent) submission = r.get_submission( \"https://www.reddit.com/r/borrow/comments/3o05n0/late_ujgohmart87_200_interest_107/\" ) comment = submission.comments[0] author = comment.author print(author.item)", "created_utc": 1444349426, "gilded": 0, "name": "t3_3o1fu2", "num_comments": 4, "score": 1, "title": "Return author name on r/borrow", "url": "https://www.reddit.com/r/redditdev/comments/3o1fu2/return_author_name_on_rborrow/"}, {"author": "TeroTheTerror", "body": "Using the helpers function of PRAW, specifically the `comment_stream` function to gather all comments posted in a sub. Basic code looks like: import praw from time import sleep r = praw.Reddit(userAgent) h = praw.helpers Bot is logged in via OAuth. for comment in h.comment_stream(r,subreddit): try: things occur here....(including a check for OAuth key refreshes every 45 min) sleep(10) except Exception as e: print e sleep(180) The error that keeps occurring is: ('Connection aborted.', error(10054, 'An existing connection was forcibly closed by the remote host')) At first I thought the sleep time between comments wasn't enough, but now it's still doing that while sleeping 10 seconds in between each new comment. What am I missing?", "created_utc": 1444347223, "gilded": 0, "name": "t3_3o1bae", "num_comments": 0, "score": 0, "title": "Connection keeps being forcibly closed with comment-scanning PRAW bot", "url": "https://www.reddit.com/r/redditdev/comments/3o1bae/connection_keeps_being_forcibly_closed_with/"}, {"author": "Fireislander", "body": "Hi all, I am trying to create a self post and then edit the info in that post a few mins later all using praw. I am submitting the post using the following code: submission = r.submit(whichSub, myThreadTitle, text=threadText) A few mins later I need the script to update the text in this thread. My script was initially set up to take in a URL so extracting the URL from this would be the easiest solution for me, but if there is a more efficient way to update the text in a thread I would love to hear it. Thank you for all of your help in advance", "created_utc": 1444277845, "gilded": 0, "name": "t3_3nxjnq", "num_comments": 5, "score": 3, "title": "Getting the URL of a post I just submitted", "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "rbevans", "body": "I'm new to bot writing and could use some help to understand why i'm getting the errors below. Traceback (most recent call last): File \"C:\\scripts\\RecipesTopTen.py\", line 55, in r.login(USERNAME, PASSWORD) File \"\", line 2, in login File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\decorators.py\", line 75, in wrap return function(*args, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 1418, in login self.request_json(self.config['login'], data=data) File \"\", line 2, in request_json File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 444, in _request response = handle_redirect() File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\requests\\sessions.py\", line 579, in send r = adapter.send(request, **kwargs) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\requests\\adapters.py\", line 369, in send timeout=timeout File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 353, in _make_request conn.request(method, url, **httplib_request_kw) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\http\\client.py\", line 1083, in request self._send_request(method, url, body, headers) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\http\\client.py\", line 1123, in _send_request self.putheader(hdr, value) File \"C:\\Users\\\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\http\\client.py\", line 1060, in putheader raise ValueError('Invalid header value %r' % (values[i],)) ValueError: Invalid header value b\"Weekly Top-Ten bot for /r/recipes. Makes a weekly sticky thread with the top ten posts for the week\\n PRAW/3.3.0 Python/3.5.0 b'Windows-10.0.10240'\"", "created_utc": 1444245192, "gilded": 0, "name": "t3_3nvjri", "num_comments": 14, "score": 0, "title": "I could use some help to understand why I'm getting the following errors when running my bot.", "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "sWeeX2", "body": "Hey guys, So I'm trying to scrape a list of subreddits for a project I'm working on. In my head the best way to do this was using the reddit.com/reddits.json because from what I can see with PRAW there's no clear way of getting a list of subreddits. My problem is that I keep running into an { \"error\": 429} which is given back when I'm making too many requests I believe. Has anyone come up with a good way of getting a list of subreddits already? Any help is greatly appreciated.", "created_utc": 1443439439, "gilded": 0, "name": "t3_3morip", "num_comments": 4, "score": 1, "title": "Trying to scrape a list of subreddits", "url": "https://www.reddit.com/r/redditdev/comments/3morip/trying_to_scrape_a_list_of_subreddits/"}, {"author": "13steinj", "body": "I'm fixing a small issue regarding the `refresh()` method on PRAW that affects specially deleted and removed comments (these have to be removed / deleted with no replies to be \"special\"), but I don't know if DMCA'd comments are affected too, so I wanted to check. How would I DMCA a comment on a local install to test what I need to check?", "created_utc": 1443396196, "gilded": 0, "name": "t3_3mmt0t", "num_comments": 5, "score": 3, "title": "How would I DMCA a comment on a local install?", "url": "https://www.reddit.com/r/redditdev/comments/3mmt0t/how_would_i_dmca_a_comment_on_a_local_install/"}, {"author": "boib", "body": "Traceback (most recent call last): File \"\", line 3, in File \"/usr/local/lib/python3.4/dist-packages/praw/objects.py\", line 377, in unhide return self.hide(_unhide=True) File \"\", line 2, in hide File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 268, in wrap return function(*args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/objects.py\", line 369, in hide return self.reddit_session.hide(self.fullname, _unhide=_unhide) File \"\", line 2, in hide File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 247, in wrap assert not obj._use_oauth # pylint: disable=W0212 AssertionError", "created_utc": 1443128502, "gilded": 0, "name": "t3_3m8ya2", "num_comments": 2, "score": 3, "title": "PRAW v3.2.1 crash on .unhide() using oauth2", "url": "https://www.reddit.com/r/redditdev/comments/3m8ya2/praw_v321_crash_on_unhide_using_oauth2/"}, {"author": "13steinj", "body": "I've been working on a bit of a personal project, and I found adding moderators with specific permissions is needed. This is currently unavailable in PRAW, (I'd link the issue number but github sucks on mobile), so I was going to add it in myself in the process of getting to the 50% marker of completion. However, I've encountered a bit of a problem. When looking at [/api](/api), if you ctrl-f for \"permissions\", the various endpoints that allow the use of choice permissions don't have info as to what those permissions are supposed to be (string, list, etc). I'd love to play around, but I'm afraid of fucking something up.", "created_utc": 1443062240, "gilded": 0, "name": "t3_3m5fvi", "num_comments": 0, "score": 1, "title": "What are valid values for \"permissions\" in the api?", "url": "https://www.reddit.com/r/redditdev/comments/3m5fvi/what_are_valid_values_for_permissions_in_the_api/"}, {"author": "pyskell", "body": "So I'm writing a reddit bot that responds to username mentions with PRAW. I can do something like: r = praw.Reddit(\"example\") mentions = r.get_mentions() mentions = list(mentions) mention = mentions[0] mention.mark_as_read() However if I call r.get_mentions() again I will get that old mention back. I do plan to track mentions on my end, however, it seems odd (and wasteful) to get back all my old mentions whenever I make a new request. Is there a way in the reddit API to only get mentions marked as unread? I can't seem to find anything. Alternatively is there a way to stop retrieving mentions made before a certain date, or before a certain comment id?", "created_utc": 1442847318, "gilded": 0, "name": "t3_3lt7vv", "num_comments": 7, "score": 2, "title": "Ignoring \"read\" mentions", "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "MaxwellSalmon", "body": "I successfully installed pip on my raspberry pi, but I can't install PRAW. I read that I should write \"pip-3.2 install \" But I get this message: \"pip-3.2 command not found\" - any more ideas? Thank you!", "created_utc": 1442773958, "gilded": 0, "name": "t3_3lpbsc", "num_comments": 3, "score": 1, "title": "Trying to install PRAW on Raspberry Pi", "url": "https://www.reddit.com/r/redditdev/comments/3lpbsc/trying_to_install_praw_on_raspberry_pi/"}, {"author": "avinassh", "body": "Hey folks, I am the author of `prawoauth2`, a library which makes writing Reddit bots/apps using OAuth2 super easy and simple. Lately I have been receiving private messages, seeking help in migration, so I thought I would write a tutorial and redirect them here next time. Most of the text below is copied from the documentation. - Link to [Github](https://github.com/avinassh/prawoauth2) - Link to [Documentation](http://prawoauth2.readthedocs.org/) --- TLDR version: Remove every references of `username`, `password` and `praw.login` in your code. [Register](https://www.reddit.com/prefs/apps/) your bot/app in Reddit. Create an instance of `PrawOAuth2Mini` with valid params. So you have to remove one line and add another in your main code. That's all ;) --- Installation: pip install prawoauth2 --- 1. Stop using `praw.login`. Your current code probably uses Reddit account (your's or your bot's) username and password with `praw.login` and you have to remove that. With OAuth2, there should be NO references to Reddit username and password in your code: reddit_client = praw.Reddit(user_agent=user_agent) # you gotta remove the following line reddit_client.login(reddit_username, reddit_password) 2. Figure out what all `scopes` you need. Scopes specify what all permissions your app (or bot script) needs from user's Reddit account(or your bot account), like read private messages, spend gold credits etc. You can read about different scopes on praw's [official documentation](https://praw.readthedocs.org/en/stable/pages/oauth.html#oauth-scopes). For example, if your bot replies to comments and also responds to private messages, then it will need atleast these scopes: scopes = ['identity', 'read', 'submit', 'privatemessages'] 3. You need to register your bot/app on Reddit. The praw documentation already has a nice overview about [how](https://praw.readthedocs.org/en/stable/pages/oauth.html#a-step-by-step-oauth-guide). Go [here](https://www.reddit.com/prefs/apps/) and here's what I recommend for a bot: [registering](http://i.imgur.com/qiBMIl1.png) Just make sure you are setting `redirect uri` to `http://127.0.0.1:65010/authorize_callback`. Rest doesn't matter. Once you have created the app, you will get `app_key` and `app_secret`: [tokens](http://i.imgur.com/qxEKyOe.png) 4. `prawoauth2` comes with two components, `PrawOAuth2Mini` and `PrawOAuth2Server`. `PrawOAuth2Server` authorizes your app/script with the Reddit account and gives you access token. This is one time only operation. Let's call this script as `onetime.py`. `PrawOAuth2Mini` uses these tokens for all next transactions with Reddit. Remember, for a bot, you only need a valid `refresh_token` so it can *refresh* the expired `access_token`. 5. So lets first build `onetime.py`. As name suggests, you need to run this script only once for the first time. You should run this script locally, on your computer since it requires browser access. Import the required modules and create a `praw` instance: import praw from prawoauth2 import PrawOAuth2Server user_agent = 'some string that uniquely identifies my bot' reddit_client = praw.Reddit(user_agent=user_agent) 6. Pass the `app_key` and `app_secret` of your app, along with the praw instance to the `PrawOAuth2Server` oauthserver = PrawOAuth2Server(reddit_client, app_key, app_secret, state=user_agent, scopes=scopes) 7. Now, you need to start the oauth client server, which runs internally. oauthserver.start() The moment you start it, it opens the default web browser. If you are not logged in, log in with your bot account credentials and authorize the script (i.e. clicking on `accept`). 8. Once it is successful, you can get the tokens by calling `get_access_codes`. tokens = oauthserver.get_access_codes() The `tokens` is a `dict` type: >>> tokens {'access_token': '2...U', 'scope': set(['identity', 'read', 'submit']), 'refresh_token': u'2...s'} 9. Now in your main script, create an instance of `PrawOAuth2Mini` with all the required parameters: reddit_client = praw.Reddit(user_agent=user_agent) oauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key, app_secret=app_secret, access_token=access_token, refresh_token=refresh_token, scopes=scopes) That's all! Now rest of your code would require no changes and it will work as usual. --- - Here's a working example of `onetime.py` - [link](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/onetime.py) - And here's an example of bot which uses PRAW OAuth2 - [link](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/bot.py) - Detailed user guide of `prawoauth2` - [link](http://prawoauth2.readthedocs.org/usage_guide.html) - When to use refresh - [link](http://prawoauth2.readthedocs.org/tips_and_more.html#when-to-use-refresh).", "created_utc": 1442766661, "gilded": 0, "name": "t3_3lotxj", "num_comments": 17, "score": 16, "title": "[Tutorial] How to migrate your existing bots from HTTP to OAuth2 (Python-PRAW)", "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "lapropriu", "body": "I'd like to get all submissions in a certain subreddit over a period of a few months. This goes over the 1,000 limit, so I'm trying to limit the query using timestamps and iterate, as per the Reddit wiki ([cloudsearch syntax](https://www.reddit.com/wiki/search#wiki_cloudsearch_syntax)). But I can't seem to get this to work with PRAW. My code: r = praw.Reddit(user_agent) posts = r.search('',subreddit='europe',sort='new',limit=None,syntax='cloudsearch',params={'timestamp':'1420027200..1420070400'}) I'm expecting this to return submissions between 1420027200 (Wed, 31 Dec 2014 12:00:00 GMT) and 1420070400 (Thu, 01 Jan 2015 00:00:00 GMT), so a 12hr interval. But what I get is submissions between 1442041040 (Sat, 12 Sep 2015 06:57:20 GMT) and 1442777039 (Sun, 20 Sep 2015 19:23:59 GMT). I've also tried using get_new(), with similar results. r = praw.Reddit(user_agent) sub = r.get_subreddit('europe') posts = sub.get_new(limit=None,syntax='cloudsearch',params={'timestamp':'1420027200..1420070400'}) Super grateful for any help!", "created_utc": 1442752225, "gilded": 0, "name": "t3_3lo4gn", "num_comments": 2, "score": 1, "title": "PRAW and timestamp searches", "url": "https://www.reddit.com/r/redditdev/comments/3lo4gn/praw_and_timestamp_searches/"}, {"author": "EchoLogic", "body": "I want to build the world's simplest application. When someone on my website pastes in a permalink to a comment, I want my site to retrieve that comment and return it. The person doing the pasting does not even need to log in. **I just want to retrieve a comment.** Yet, it seems like I'm being forced to jump through 1000 hoops of fire to get everything to work. Why is Reddit's API so complicated to use compared to Twitter, AWS, or any other API? Based off what I read, it seemed like I needed an API wrapper, so I went and downloaded https://github.com/jcleblanc/reddit-php-sdk. And so, the problems began. 1. Oh, my project uses composer. This API wrapper doesn't. I tried for hours to try and get class autoloading set up. It didn't work. I then had to fork it, reorganize the project, add PSR-0 support, and then add it to packagist. That took up all my time last night. 2. Cool, now I'm using an semi-abandoned API wrapper to try and interact with a poorly documented API. 3. Now I can't get OAuth2 to work correctly. Why do I need to even use OAuth2 for this?! 4. Spend some more hours wasting my time looking at other API wrappers, PRAW, Guzzle, OAuth-2 repositories. I've spend the last 2 days of my time doing this, with no results to show for it. The simplest task ever, and there's roadblocks everywhere. Sorry for the rant, but I feel like I'm doing something fundamentally wrong here. Is it really this difficult to retrieve a comment from Reddit? I feel like I'm not even using the correct architecture here, should I just be using a bot? How does that work and how does it differ from an \"app\"?", "created_utc": 1442630676, "gilded": 0, "name": "t3_3ligwq", "num_comments": 21, "score": 6, "title": "How does a reddit bot differ from an application? Are they the same thing? When do I need OAuth2?", "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "sWeeX2", "body": "Hey guys, So I'm working on a project right now that involves using PRAW. I'm quite new to it and I'm running into a bit of a brick wall. For my project I need to get lots of comments for lots of threads. When I get the comments off a thread however it'll never quite be a complete list, it'll always be off by a bit, say there might be 500 comments and I might get 480 for example. It is also really slow to retrieve those comments. If anyone could help me out suggesting the most optimal way to do this it would be greatly appreciated.", "created_utc": 1442428151, "gilded": 0, "name": "t3_3l7as5", "num_comments": 7, "score": 3, "title": "[PRAW] Downloading Comments From Threads", "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "Flewloon", "body": "I have a couple questions about PRAW and using it to post stuff to a subreddit I control. Is there a way to setup the settings so that my bot doesn't have to be approved to have its post shown? I set the bot up as a moderator to the subreddit but still have to approve its posts. Also is there a way to not have to do CAPTCHA when posting to an owned subreddit. I assume no on this, but felt it couldn't hurt to ask.", "created_utc": 1442318011, "gilded": 0, "name": "t3_3l15u0", "num_comments": 3, "score": 4, "title": "PRAW no CAPTCHA posting and approving posts on owned subreddit", "url": "https://www.reddit.com/r/redditdev/comments/3l15u0/praw_no_captcha_posting_and_approving_posts_on/"}, {"author": "-SuicideThrowaway-", "body": "If I remember right I should be allowed 1 request per second, right? And I mean I shouldn't request a page too often and praw limits that but if I'm just getting my unread messages I don't think it should be a big deal, right? Is it ok to poll for unread messages every second?", "created_utc": 1442258745, "gilded": 0, "name": "t3_3ky4l4", "num_comments": 2, "score": 2, "title": "How often can I request my inbox?", "url": "https://www.reddit.com/r/redditdev/comments/3ky4l4/how_often_can_i_request_my_inbox/"}, {"author": "atlusio", "body": "Hey guys, I am making a simple script to get the content a certain Redditor has upvoted. import praw redd = praw.Reddit(user_agent='upvoted post counter') redd.login('account', 'pwd') upvoted_threads = redd.user.get_upvoted print(upvoted_threads) which gives me: bound method LoggedInRedditor.get_upvoted of Redditor(user_name='account') When I do get subreddits, however, I get: >>> new_threads = redd.get_subreddit('subreddit').get_new(limit=10) >>> print(new_threads) I can iterate through the new_threads, but I cannot iterate through the upvoted threads... clearly because you cannot iterate on a method like you can a generator object. Any help with what I may be doing wrong would be lovely.", "created_utc": 1442254076, "gilded": 0, "name": "t3_3kxsbb", "num_comments": 5, "score": 1, "title": "PRAW get_upvoted returning a bound method rather than a get_content generator?", "url": "https://www.reddit.com/r/redditdev/comments/3kxsbb/praw_get_upvoted_returning_a_bound_method_rather/"}, {"author": "NotoriousHakk0r4chan", "body": "I get this mess of errors: Traceback (most recent call last): File \"/home/zibot/BeetusBot/subscription.py\", line 2, in from beetusbot import bot File \"/home/zibot/BeetusBot/beetusbot/bot.py\", line 13, in reddit.login(config.USERNAME.encode('utf-8'), config.PASSWORD.encode('utf-8')) File \"\", line 2, in login File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/decorators.py\", line 75, in wrap return function(*args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/__init__.py\", line 1416, in login self.request_json(self.config['login'], data=data) File \"\", line 2, in request_json File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/__init__.py\", line 444, in _request response = handle_redirect() File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw-3.2.1-py2.7.egg/praw/handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"/usr/lib/python2.7/dist-packages/requests/sessions.py\", line 569, in send r = adapter.send(request, **kwargs) File \"/usr/lib/python2.7/dist-packages/requests/adapters.py\", line 362, in send timeout=timeout File \"/usr/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 516, in urlopen body=body, headers=headers) File \"/usr/lib/python2.7/dist-packages/urllib3/connectionpool.py\", line 308, in _make_request conn.request(method, url, **httplib_request_kw) File \"/usr/lib/python2.7/httplib.py\", line 1001, in request self._send_request(method, url, body, headers) File \"/usr/lib/python2.7/httplib.py\", line 1035, in _send_request self.endheaders(body) File \"/usr/lib/python2.7/httplib.py\", line 997, in endheaders self._send_output(message_body) File \"/usr/lib/python2.7/httplib.py\", line 850, in _send_output self.send(msg) File \"/usr/lib/python2.7/httplib.py\", line 826, in send self.sock.sendall(data) File \"/usr/lib/python2.7/dist-packages/urllib3/contrib/pyopenssl.py\", line 208, in sendall return self.connection.sendall(data) File \"/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py\", line 969, in sendall raise TypeError(\"buf must be a byte string\") TypeError: buf must be a byte string -------------- Whats going on? Someone else had the problem [here](https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/), but never listed any kind of fix for it.", "created_utc": 1442189886, "gilded": 0, "name": "t3_3kuj2j", "num_comments": 7, "score": 1, "title": "TypeError: buf must be a byte string in PRAW", "url": "https://www.reddit.com/r/redditdev/comments/3kuj2j/typeerror_buf_must_be_a_byte_string_in_praw/"}, {"author": "Frenchiie", "body": "I know you can get a list of a users comments but what about what subreddits all of those comments came from? Does the API let you do something like that? If it makes a difference I am looking at PRAW.", "created_utc": 1442015110, "gilded": 0, "name": "t3_3klvrj", "num_comments": 3, "score": 2, "title": "Can the API get a list of subreddits where a users posts come from?", "url": "https://www.reddit.com/r/redditdev/comments/3klvrj/can_the_api_get_a_list_of_subreddits_where_a/"}, {"author": "DarkMio", "body": "There is little to no documentation on the handlers, but how can I keep up the rate-limit with the DefaultHandler instead of praw-multiprocess when I run a multithreaded bot in Python3+? I can't seem to get that worked out, using a new Handler or giving all Reddit Sessions the same handler don't seem to limit the rate: Something like this: import praw handler = praw.handler.DefaultHandler() r_1 = Reddit('somethingsomething', handler=handler) r_2 = Reddit('somethingdifferent', handler=handler) def c_stream(): for comments in praw.helpers.comment_stream(r_1, 'all') #something cool pass def s_stream(): for submission in praw.helpers.comment_stream(r_2, 'all') # something more cool pass and this is set into two threads. Looking at the request-log (a patched DefaultHandler found here: https://github.com/DarkMio/RedditRover/blob/master/core/PRAWHandler.py#L12) reveals multiple requests per second - here's a snippet: 07:10:17 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=88&after=t3_3k7i20&limit=1000&before=t3_3k7i1x 07:10:23 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=89&limit=1000&before=t3_3k7i2b 07:10:23 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=6&after=t1_cuvdbzb&limit=1024&before=t1_cuvdbza 07:10:25 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=7&limit=1024&before=t1_cuvdc29 07:10:26 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=89&after=t3_3k7i2f&limit=1000&before=t3_3k7i2b 07:10:26 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=90&limit=1000&before=t3_3k7i2o 07:10:26 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=7&after=t1_cuvdc2a&limit=1024&before=t1_cuvdc29 07:10:28 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=8&limit=1024&before=t1_cuvdc53 07:10:29 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=90&after=t3_3k7i2q&limit=1000&before=t3_3k7i2o 07:10:29 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/comments/.json?count=8&after=t1_cuvdc54&limit=1024&before=t1_cuvdc53 07:10:32 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://api.reddit.com/r/all/new.json?count=91&limit=1000&before=t3_3k7i2w 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json 07:10:34 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- POST https://api.reddit.com/api/v1/access_token/ 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/api/v1/me.json 07:10:35 [DEBUG] -- [hndl:PRAWHandler/request]-- GET https://oauth.reddit.com/message/unread/.json", "created_utc": 1441856815, "gilded": 0, "name": "t3_3kci9s", "num_comments": 4, "score": 1, "title": "How does tze DefaultHandler work in praw?", "url": "https://www.reddit.com/r/redditdev/comments/3kci9s/how_does_tze_defaulthandler_work_in_praw/"}, {"author": "metaranha", "body": "I'm a beginner with PRAW and i'm trying to write a simple script to pull the hot list in IAMA, but whenever I print the variable holding the results, I end up getting this output: Why do I keep getting this output? I can't seem to find an explanation I understand for why I'm getting this rather than the actual output. Here's my code: #!/usr/bin/python import praw import time r = praw.Reddit('AMA request compiler /u/metaranha') r.login() count = 0 while(count", "created_utc": 1441466485, "gilded": 0, "name": "t3_3jr2qc", "num_comments": 17, "score": 1, "title": "printing search results: <generator object search at (mem location)>", "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "green_flash", "body": "If you have a script based on PRAW and get an error like this when logging in: ERROR: Unexpected redirect from http://www.reddit.com/user//about/.json to https://www.reddit.com/user//about/.json that is likely because you're running a very old version of PRAW that doesn't support https. The switch seems to have been made today.", "created_utc": 1441320750, "gilded": 0, "name": "t3_3jjqlq", "num_comments": 6, "score": 11, "title": "Reddit login is https only now it seems, be sure to use an up-to-date PRAW version", "url": "https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/"}, {"author": "codsane", "body": "I was looking to use PRAW get all comments of a submission, but found that that was not feasible. for comment in praw.helpers.comment_stream(r, 'subreddit_containing_thread'): if comment.link_id == \"thread_i_want_to_monitor\": # Save comment The above code simply grabs a comment stream of the entire subreddit, but only saves the comments that are within the thread I would like to monitor. Is there any more efficient way to get this done, perhaps it is possible to get a comment stream of a single submission?", "created_utc": 1440913405, "gilded": 0, "name": "t3_3ixgq2", "num_comments": 5, "score": 2, "title": "Comment stream of submission?", "url": "https://www.reddit.com/r/redditdev/comments/3ixgq2/comment_stream_of_submission/"}, {"author": "natos20", "body": "Noob here. I'm learning how to write reddit bots in python, and I always get that error. If it's any more useful, here's the whole traceback: Traceback (most recent call last): File \"C:/Users/User/Desktop/myprogram.py\", line 4, in r = praw.reddit(user_agent = \"blahblahblah\") AttributeError: 'module' object has no attribute 'reddit' Thanks in advance for any help.", "created_utc": 1440887744, "gilded": 0, "name": "t3_3iw7ij", "num_comments": 2, "score": 2, "title": "'module' object has no attribute 'reddit'?", "url": "https://www.reddit.com/r/redditdev/comments/3iw7ij/module_object_has_no_attribute_reddit/"}, {"author": "SeriousBug", "body": "I have an [open-source application](https://github.com/SeriousBug/redditcurl) that depends on the Reddit API. There is one thing I don't understand about the switch to OAuth2, looking at the [documentation for PRAW](https://praw.readthedocs.org/en/v3.1.0/pages/oauth.html#step-2-setting-up-praw), I see that I need to keep `client_secret` as a secret. However, how can I do that when the application is open source? Am I supposed to ask all the users to register their application? Edit: Reading the [reddit's documentation](https://github.com/reddit/reddit/wiki/OAuth2) > Installed app: Runs on devices you don't control, such as the user's mobile phone. Cannot keep a secret, and therefore, does not receive one. The correct solution is to pick an installed app.", "created_utc": 1440756150, "gilded": 0, "name": "t3_3ipjs6", "num_comments": 1, "score": 7, "title": "OAuth2 with open source apps", "url": "https://www.reddit.com/r/redditdev/comments/3ipjs6/oauth2_with_open_source_apps/"}, {"author": "boib", "body": "I don't see that in the API but it must be possible, right? /r/sub/about/flair gives that list, right? Is that possible in PRAW?", "created_utc": 1440555172, "gilded": 0, "name": "t3_3iey8r", "num_comments": 2, "score": 1, "title": "PRAW: How do I get a list of all the users in my that have set user flair?", "url": "https://www.reddit.com/r/redditdev/comments/3iey8r/praw_how_do_i_get_a_list_of_all_the_users_in_my/"}, {"author": "PoodleWorkout", "body": "Hi all! I'm new to PRAW, and I'm trying to get a list of all saved links from a logged-in user. However, it seems that I'm only getting the most recent 25 links for my own account. I'm pasting a code snippet below, and any insight would be greatly appreciated! saved_links = reddit_user.get_saved(sort=\"new\", time='all') for value in saved_links: print (value.title + \" \" + value.short_link)", "created_utc": 1440470294, "gilded": 0, "name": "t3_3ia8m4", "num_comments": 3, "score": 1, "title": "Getting ALL saved links?", "url": "https://www.reddit.com/r/redditdev/comments/3ia8m4/getting_all_saved_links/"}, {"author": "TomSparkLabs", "body": "It seems very complex for a simple script, that only needs to access my account. However, if I use import praw r = praw.Reddit('just a test - /u/TomSparkLabs') r.login(username='TomSparkLabs', password='notreallymypass') I get this error thrown at me: reddit intends to disable password-based authentication of API clients sometime in the near future. As a result this method will be removed in a future major version of PRAW. For more information please see: * Original reddit deprecation notice: https://www.reddit.com/comments/2ujhkr/ * Updated delayed deprecation notice: https://www.reddit.com/comments/37e2mv/ Pass ``disable_warning=True`` to ``login`` to disable this warning. What do I do now? If I need to upgrade, how? The guide is very confusing.", "created_utc": 1440425991, "gilded": 0, "name": "t3_3i7dcc", "num_comments": 4, "score": 4, "title": "How do I upgrade to OAuth2 authentication with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/3i7dcc/how_do_i_upgrade_to_oauth2_authentication_with/"}, {"author": "D0cR3d", "body": "Is there a way to get the ID of a object in the search list from a praw search? So far all I'm seeing is the number of votes, and the title. Thanks.", "created_utc": 1440262326, "gilded": 0, "name": "t3_3hzg8r", "num_comments": 1, "score": 1, "title": "Get Submission ID from the results of a praw search", "url": "https://www.reddit.com/r/redditdev/comments/3hzg8r/get_submission_id_from_the_results_of_a_praw/"}, {"author": "cartoonii", "body": "So, this is what happens when I go to my Python33\\Scripts file and type in \"pip install --allow-external praw-oauth2util\" and then I get this error: Could not find a version that satisfies the requirement install (from versions : ) Some externally hosted files were ignored as access to them may be unreliable (use --allow-external install to allow). No matching distribution found for install Any help? Solution: * Changed PATH System Variable to the version I was using..", "created_utc": 1440180870, "gilded": 0, "name": "t3_3hvli0", "num_comments": 11, "score": 2, "title": "Installing PRAW OAuth2Util", "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "D0cR3d", "body": "For instance, if you visit [/r/mod/about/edited](https://www.reddit.com/r/mod/about/edited/) you see any comments or submissions edited. Does that exist in PRAW itself?", "created_utc": 1440001213, "gilded": 0, "name": "t3_3hlmtc", "num_comments": 7, "score": 2, "title": "Is there a Get_Edited() function in PRAW to be able to get any things that have been edited?", "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "wlhlm", "body": "I'm working on a script that modifies the sidebar. For this purpose, PRAW has `set_settings()`, which allows to update the subreddit settings including the sidebar. Here's the [documentation](https://praw.readthedocs.org/en/v3.1.0/pages/code_overview.html#praw.__init__.ModConfigMixin.set_settings) for it: > **set_settings(**subreddit, **title**, **, \\*\\*kwargs**)** > Set the settings for the given subreddit. > **Parameters**: *subreddit* \u2013 Must be a subreddit object. > **Returns:** The json response from the server. > Requires the modconfig oauth scope oruser/password authentication as a mod of the subreddit. What struck me as odd is, that `title` is a mandatory argument. Why do I have to set it every time I change settings? It looks to me like any other subreddit setting and I don't think it can be used as identifier.", "created_utc": 1439980332, "gilded": 0, "name": "t3_3hkgou", "num_comments": 5, "score": 1, "title": "[PRAW] Why is 'title' a required argument for set_settings()?", "url": "https://www.reddit.com/r/redditdev/comments/3hkgou/praw_why_is_title_a_required_argument_for_set/"}, {"author": "Bromskloss", "body": "It was [declared](https://www.reddit.com/r/videos/comments/3hgons/how_oldschool_graphics_worked/cu7jise) that anyone who replies to a particular comment will get a notification when the next part of a certain interesting video becomes available. I'd like to automate this. So far, I can extract the list of users that are to receive the notification: #!/usr/bin/env python3 # coding=utf-8 import praw r = praw.Reddit(user_agent='generic:one-off-reminder-thing:v0 (by /u/Bromskloss)) c = r.get_submission(url='https://www.reddit.com/r/videos/comments/3hgons/how_oldschool_graphics_worked/cu7jise') c.replace_more_comments() authors = {ch.author.name for ch in c.comments[0]._replies} print(\"\\n/u/\".join(authors)) What remains is to send a message to each user. I count on getting stuck in a spam filter if I go ahead and just try to do it. What is the solution? How do other bots that sends many PMs do it? (This is the first time I'm doing anything with the API. Suggestions for improvements are welcome.)", "created_utc": 1439947399, "gilded": 0, "name": "t3_3hiwod", "num_comments": 10, "score": 0, "title": "How to send a message to a few hundred users without being stopped by a spam filter?", "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "solaceinsleep", "body": "Here is post on stackoverflow: [http://stackoverflow.com/questions/32083842/how-to-add-reddit-overload-support-to-my-script-that-is-using-the-praw-module](http://stackoverflow.com/questions/32083842/how-to-add-reddit-overload-support-to-my-script-that-is-using-the-praw-module). Thank you.", "created_utc": 1439942261, "gilded": 0, "name": "t3_3hil7j", "num_comments": 4, "score": 2, "title": "How to add reddit overload support to my script that is using the praw module?", "url": "https://www.reddit.com/r/redditdev/comments/3hil7j/how_to_add_reddit_overload_support_to_my_script/"}, {"author": "EliteMasterEric", "body": "Since I updated to PRAW 3.2.0, when I run this code: r.get_flair_choices(subreddit) I only receive the first 20 possible flairs for the subreddit. Previously, running this command would retrieve all possible flairs. Does anyone have a fix for this?", "created_utc": 1439864422, "gilded": 0, "name": "t3_3he8ev", "num_comments": 8, "score": 1, "title": "[PRAW] Not getting all flair choices after PRAW 3.2.0", "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "KidKrule", "body": "I'm making a bot that will take a tweet, reformat it a little and then post it to a dedicated sub. I'm using PRAW and login in through OAuth. My useragent string follows the guidelines. I'm posting my tests in /r/reddit_api_test and after only 4 links posted, I'm asked to wait one hour. Before that, I couldn't post 2 links in a row, I had to wait something like 10 minutes (using praw.errors.RateLimitExceeded to get the time remaining before I can post again). Am I doing anything wrong ? I find those limits a little bit too hard. Especially when testing... **[UPDATE]** I was actually running into the \"new account limit\", and not the API's limit. How I got over it ? I got someone to make the bot moderator of a test subreddit, and it can post there freely (as long as it follows the API's guidelines of course).", "created_utc": 1439766821, "gilded": 0, "name": "t3_3h91fe", "num_comments": 10, "score": 4, "title": "Trying to make a bot but running into drastic API limits", "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "dClauzel", "body": "Using `get_banned()` from praw v3.2.0 (*wink wink \ud83d\ude09*), I can get the following data about bans: * 'name': the banned account; * 'id': the banned account\u2019s id; * 'date': timestamp of when the ban was set; * 'note': the private (moderators only) comment associated to the ban. What is missing is: - the time remaining for a ban to expire; - the name of the moderator who placed the ban. Is there a way to get those informations?", "created_utc": 1439658068, "gilded": 0, "name": "t3_3h40nx", "num_comments": 4, "score": 2, "title": "[PRAW] Getting the expiration time of a ban and the person who placed it?", "url": "https://www.reddit.com/r/redditdev/comments/3h40nx/praw_getting_the_expiration_time_of_a_ban_and_the/"}, {"author": "spawnofyanni", "body": "Hi all. I've had a bot running for a little over a year, and recently moved it over to OAuth what with the reddit HTTPS switch. A big part of the bot is the ability to reply to private messages. The core functionality has been working fine both before the switch and for a few weeks after it; recently, though, the bot has been entirely unable to reply to PMs, coming up with the following error: Traceback (most recent call last): File \"<pyshell#41>\", line 1, in <module> msg.reply('test') File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 407, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 2524, in _add_comment retval = decorator(add_comment_helper)(self, thing_id, text) File \"<string>\", line 2, in add_comment_helper File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 255, in wrap return function(*args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 2517, in add_comment_helper retry_on_error=False) File \"<string>\", line 2, in request_json File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 604, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 437, in _request _raise_response_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 200, in _raise_response_exceptions raise Forbidden(_raw=response) Forbidden If I switch to the bot account just using a browser I can send and reply to private messages fine. If I manually login in a python shell using the OAuth details I can recreate the error above - I can start a new PM thread but I can't reply to any messages. I'm not sure if it's a scope thing (I found [this recent thread](https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/) which seems sort of relevant, but I'm not sure) or something else entirely, though I'm certain the OAuth scope includes 'privatemessages' (and 'submit'). Again, this is a pretty recent thing, private messaging was working fine before I started using OAuth and for a while after I switched to it, too. All other functionality - submitting threads, editing, deleting, etc - also still works as expected. This was happening with both PRAW 3.1 and 3.2 after I upgraded this morning. I'm neither a python nor an OAuth savant, but I've been able to get by. I'm completely lost here, though, so I was hoping someone could help out.", "created_utc": 1439653834, "gilded": 0, "name": "t3_3h3s34", "num_comments": 10, "score": 3, "title": "Bot gets \"Forbidden\" error when trying to reply to private messages", "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "soymilkbot", "body": "Hello everyone, I'm new to making bots and python in general and I have this problem that's been bugging me for a while. I can't figure out how to catch a specific exception, in particular a 503 HTTPError. Here is the relevant piece of code: try: # some code except KeyboardInterrupt: print(\"Shutting down.\") break except requests.exceptions.HTTPError as e: #second block print(\"Some thing bad happened! HTTPError\", str(e.response.status_code)) if e.response.status_code == 503: print(\"Let's wait til reddit comes back! Sleeping 60 seconds.\") time.sleep(60) except Exception as e: #third block print(\"Some thing bad happened!\", e) traceback.print_exc() The thing is that last night while the bot was running a 503 occurred but instead of the second except block running, the third one did. Here is the traceback: Some thing bad happened! Traceback (most recent call last): File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 201, in _raise_response_exceptions response.raise_for_status() # These should all be directly mapped File \"C:\\Python34\\lib\\site-packages\\requests\\models.py\", line 851, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 503 Server Error: Service Unavailable During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"soymilk.py\", line 19, in for comment in test_comments: File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 524, in get_content page_data = self.request_json(url, params=params) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 424, in _request _raise_response_exceptions(response) File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 203, in _raise_response_exceptions raise HTTPException(_raw=exc.response) praw.errors.HTTPException As you can see even though the traceback says it's a requests.exceptions.HTTPError, my try except still went to the third block. Why? I've googled the whole day for an answer and I still can't figure it out. Also, since it went into the third block I was expecting it to print a descriptive string of the error next to \"Something bad happened!\" because my print statement is print(\"Some thing bad happened!\", e) But somehow it doesn't print anything for the e. So summarizing I have two questions: 1. Why does my try except block not detect that the exception is a HTTPError? 2. Why is nothing printed when I print the exception as e? Thank you for the help in advance!", "created_utc": 1439647799, "gilded": 0, "name": "t3_3h3hmm", "num_comments": 4, "score": 4, "title": "How do you catch a specific exception?", "url": "https://www.reddit.com/r/redditdev/comments/3h3hmm/how_do_you_catch_a_specific_exception/"}, {"author": "Stuck_In_the_Matrix", "body": "I've been working overtime to bring you the latest and greatest. The new SSE stream can now deliver all publicly available submissions and comments straight to your doorstep with a plethora of useful options. Let's dive into the complete feature set and sprinkle some examples as we go along. First, the SSE stream is almost out of beta, so you can start depending on it by early next week. I've had a lot of testers helping out and I've gotten a ton of great feedback -- so I appreciate everyone who is using it. **The endpoint:** http://stream.pushshift.io ------------------------------------------------------------------- Connecting directly to that will feed all publicly available comments and submissions to you. For those who are unfamiliar with what an SSE stream is, [please take a look at this great introduction](http://www.html5rocks.com/en/tutorials/eventsource/basics/). Each \"event\" contains three fields: id, event and data. The id field for my SSE stream is an internal value I use to keep track of all new submissions and comments as they come in. The event field can be one of two values -- \"t1\" for comments or \"t3\" for submissions. When connecting to the stream without using any parameters, you will receive all events (both t1 and t3 events). **But I don't care about submissions (t3 events), I just want a comment stream** Very well! I have a solution for you. Use the \"event\" parameter when making your request to specify what type of event you want delivered. In this case, you would make the following API call: **http://stream.pushshift.io?event=t1** **That's better, but I don't want *all* reddit comments, I just want comments from askreddit** Fair enough, I've got you covered. To restrict your new comment stream to just askreddit comments, use the following call: http://stream.pushshift.io?event=t1&subreddit=askreddit Now you'll just get comments from that subreddit. **That's all well and good, but you should give people the ability to limit to more than just one subreddit** You've got it! If you want to get comments only from specific subreddits, you can add more by using a comma to separate them. For instance, if you want to get comments for the subreddits science, askscience, pics and funny, you would make the following API call: http://stream.pushshift.io?event=t1&subreddit=askscience,science,pics,funny (make sure you use subreddit as the parameter and not subreddits!) **Wow, so you've got a lot of functionality here. I know what will stump you -- what if I want to get all comments but EXCLUDE the previous subreddits?** Demanding, aren't you! I like that -- no sweat, you would make a call very similar to the previous call but instead of subreddit=askscience,science,pics,funny -- you would make this call: http://stream.pushshift.io?event=t1&subreddit!=askscience,science,pics,funny (!= instead of =) **What other parameters can I use?** **author** will allow you to limit to a particular author. You can follow multiple authors as well by using the same format as the subreddit call -- author=john,tony,ryan Do you want to get rid of comments from automoderator? Just use this: http://stream.pushshift.io?event=t1&author!=automoderator **match** is a particularly powerful parameter. Let's say you want to only receive comments with the words reddit, voat and jupiter --- you would use this: http://stream.pushshift.io?event=t3&match=reddit,voat,jupiter **previous** is a great parameter if you want to get a lot of comments and/or submissions that were made before you connect to the stream. For instance, if you want to get 100,000 items when you connect and then have it go into real-time mode, just set previous=100000. **start_id** allows you to start the stream at a specific location. This is really handy in your application logic if you happen to disconnect from the stream and want to reconnect and pick up where you left off. Also, this SSE stream is fully compliant with the last-event-id header that browsers automatically send in case you are developing a web app for your users. **Can I mix and match all of these? Let's say I wanted to get all comments and submissions made to nfl, but also follow certain users -- is that possible?** Absolutely! You would make a call like this http://stream.pushshift.io?subreddit=nfl&author=tony,billy,alexis,pao&match=cowboys,football,draft,pick&mode=or That call would deliver all submissions and comments made to the subreddit nfl, any comments or submissions authored by tony, billy, alexis or pao and include any comments with the words cowboys,football,draft or pick. **Wait -- what if I only want those keywords within the nfl subreddit?** Did you see that new parameter at the end? The mode parameter will allow you to make boolean choices. Set the mode to \"and\" instead of \"or\" to limit it only to the subreddit nfl and only comments within that subreddit with those keyworks. To demonstrate, you would make this call: http://stream.pushshift.io?event=t1&subreddit=nfl&match=cowboys,football,draft,pick&mode=and That would only pull matching comments with those keywords exclusively from within the nfl subreddit. **What if I want to get only safe for work submissions? How would I do that?** That brings us to the \"over_18\" parameter. If you wanted to keep nsfw submissions out of your stream, you would use over_18=false (or over_18=0) If, after a long day at work, you wanted to kick back and deliver only nsfw content, you would make the following call: http://stream.pushshift.io?event=t3&over_18=true **Questions, Comments, Criticisms, etc.** --------------------------------------------------------------- As always, if you have any questions or comments, please let me know. I've spend a lot of time developing this, so if you find it useful for your application, I would appreciate a shout-out or a small donation. If you notice any bugs with this stream, please feel free to post them in this thread. All the best and happy developing! **Cheat Sheet** --------------------------------------------------------------- **Parameter List:** ---------------------------------------- *Bold value is the default value* | Parameter | Values | Description | |:-----------|------------:|:-------------| | previous | 0", "created_utc": 1439534148, "gilded": 0, "name": "t3_3gy4zd", "num_comments": 11, "score": 15, "title": "Supercharging the SSE stream -- now supports a ton of new options. Save your precious API calls for other things!", "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "dClauzel", "body": "# solved Problem solved with this: /r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/cu1b1nc ---- I am a moderator of /r/Europe, and I am trying to get a list of the banned accounts on this subreddit. I am using PRAW and praw-OAuth2Util with python3.4 on Debian, from pip: # pip3 search praw praw-oauth2util - OAuth2 wrapper for PRAW INSTALLED: 0.2.2 (latest) prawtools - A collection of utilities that utilize the reddit API. INSTALLED: 0.19 (latest) prawoauth2 - Library to make your life easier using OAuth2 for PRAW praw - PRAW, an acronym for `Python Reddit API Wrapper`, is a python package that allows for simple access to reddit's API. INSTALLED: 3.1.0 (latest) For Reddit\u2019s oauth, the app is declared as a script, and all scopes are given to it. I fail to see where my mistake is. Help? ---- demo.py #!/usr/bin/env /usr/bin/python3.4 # -*- coding: utf-8 -*- import praw import OAuth2Util ### # connexion print(\"connexion\u2026\", end=\" \") r = praw.Reddit(user_agent=\"posix:eu.clauzel.couteau-suisse:v0 (by /u/dClauzel)\", log_requests=1, api_request_delay=4.0, timeout=300.0, site_name=\"dClauzel\") o = OAuth2Util.OAuth2Util(r, print_log=True) o.refresh() print(\"connect\u00e9.\") ### # informations utilisateur print(\"Je suis {0} et j\u2019ai un karma de {1} pour mes commentaires.\".format(r.get_me().name, r.get_me().comment_karma) ) ### # top des 5 soumissions populaires print(\"Top 5\") sousjlailu = r.get_subreddit(\"Europe\", fetch=True) for soumission in sousjlailu.get_hot(limit=5): print(\" - {0} \u2014 {1} \u2014 {2}\".format( soumission.title, soumission.author, soumission.url) ) ### # bannissements print(\"Liste des bannis\") bannis = sousjlailu.get_banned() bannis = [x for x in bannis] print(bannis) ### # nettoyage print(\"fin\") ---- oauth.txt # Config scope=identity,account,edit,flair,history,livemanage,modconfig,modflair,modlog,modothers,modposts,modself,modwiki,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote,wikiedit,wikiread refreshable=True # Appinfo app_key=*redacted* app_secret=*redacted* # Token token=*redacted* refresh_token=*redacted* ---- praw.ini # -*- coding: utf-8 -*- [dClauzel] check_for_updates: True ---- Running the program: $ ./demo.py substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json connexion\u2026 connect\u00e9. substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json Je suis dClauzel et j\u2019ai un karma de 15936 pour mes commentaires. Top 5 substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/Europe/about/.json substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/europe/.json - Immigration Megathread - Part VI \u2014 ModeratorsOfEurope \u2014 http://www.reddit.com/r/europe/comments/3frno2/immigration_megathread_part_vi/ - Sweden boosts security for asylum seekers after IKEA knife attack; two Eritrean suspects detained \u2014 Chunkeeguy \u2014 http://www.abc.net.au/news/2015-08-12/sweden-boosts-security-for-asylum-seekers-after-ikea-attack/6690180 - A reminder that there is an active war going on in Eastern Europe. Pro-Russian separatists film one of their unsuccessful attacks on Ukrainian positions. \u2014 RabbitOfCaerbanog \u2014 https://www.youtube.com/watch?v=0N0rplcH32g&t=241 - A creeping occupation in action: Russian forces again move the border with Georgia, this time a further 800 meters into the Georgian territory. \u2014 RabbitOfCaerbanog \u2014 https://www.youtube.com/watch?v=l1HUk2LEJxU - The European Union wastes about 22 million tonnes of food a year and Britain wastes the most, according to a study by European Commission-backed researchers. \u2014 Libertatea \u2014 http://www.reuters.com/article/2015/08/11/europe-food-waste-idINKCN0QG2DB20150811?feedType=RSS&feedName=worldNews Liste des bannis GET: https://api.reddit.com/r/europe/about/banned/.json Traceback (most recent call last): File \"./demo.py\", line 43, in bannis = [x for x in bannis] File \"./demo.py\", line 43, in bannis = [x for x in bannis] File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 1811, in _get_userlist for data in content: File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 524, in get_content page_data = self.request_json(url, params=params) File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 424, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 196, in _raise_response_exceptions raise Forbidden(_raw=response) praw.errors.Forbidden sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty sys:1: ResourceWarning: unclosed ----", "created_utc": 1439370514, "gilded": 0, "name": "t3_3gpbiu", "num_comments": 21, "score": 0, "title": "[PRAW][OAuth2Util] Problem using get_banned() : raise Forbidden(_raw=response)", "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "sufficiency", "body": "Hi, I am currently hosting a bot which, in a nutshell, posts stuff from another website as a comment on Reddit. This in itself works fine, but occasionally the comment to be made is more than 10k characters long, making it impossible to be posted on Reddit. I believe the best way to overcome this problem is to make a \"comment chain\". Basically, if the bot wants to post a 15k characters essay, it will make an initial comment with around 10k characters, then it will reply to the first comment with the remaining 5k. My question is how can I achieve this in a way that's reliable. I COULD do the following: 1. Make initial comment 2. Load the bot's own comments, newest first. Presumably the initial comment will be on top 3. Reply to the initial comment. But the problem here is that the Reddit API (as far as I can tell) has some lags. Additionally, the initial comment could potentially get deleted due to automod, etc. So I am hoping to see if there are better approaches. EDIT: I am using PRAW", "created_utc": 1439260818, "gilded": 0, "name": "t3_3gjm58", "num_comments": 4, "score": 1, "title": "Making a comment chain to overcome Reddit comment size limit", "url": "https://www.reddit.com/r/redditdev/comments/3gjm58/making_a_comment_chain_to_overcome_reddit_comment/"}, {"author": "Aton_Five", "body": "Using OAuth2Util for OAuth2 handling with the following permissions (and several variations for testing): scope=identity,account,read,report,wikiedit,wikiread,modposts,modcontributors The issue is with: sub.add_ban(comment2.author.name) Error: Traceback (most recent call last): File \"F:\\XXXX\\Bots\\test2.py\", line 42, in sub.add_ban(comment2.author.name) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 346, in wrapped raise errors.LoginRequired(function.__name__) praw.errors.LoginRequired: `do_relationship` requires a logged in session` Using the old form of logging in r.login('username', 'password') causes the code to work and the user in question to be banned as expected. I'm not sure what to try next to get this to work, I am pretty sure the permissions are right.", "created_utc": 1438767035, "gilded": 0, "name": "t3_3fuv52", "num_comments": 8, "score": 2, "title": "PRAW: Issues banning with local script using OAuth2", "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "EmperorSofa", "body": "So I thought it would be fun to create a bot to gather some meta data for me. My idea is I give my bot a time frame to work from say from now to five days ago, I give it a few keywords to filter titles based on, and I specify if I want self posts or liink posts or both. Then it will go off on its merry way and return all the posts that have been made between those times that fit my criteria. The issue is I want to have the option for my bot to return information on all the posts made since the subreddit's creation. Using subreddit.get_new(limit=100) might not cut it. My question is does the API have any kind of function that would allow me to sort a subreddit's posts based on time of creation. Would that function allow me to get all the posts ever made on a particular subreddit? I understand the API has built in limits and PRAW enforces them for me. If a subreddit has only existed for a few months and only has a few hundred posts that's not that many API calls but if I pointed this bot at a default subreddit that's existed for the better part of a decade I might be in for a long wait. Speed isn't my concern so much as automating an otherwise mind numbing task.", "created_utc": 1438757957, "gilded": 0, "name": "t3_3fujpg", "num_comments": 9, "score": 2, "title": "[PRAW] Iterating through subreddit posts based on time of creation.", "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "RemindMeBotWrangler", "body": "#!/usr/bin/env python import praw import OAuth2Util import traceback r = praw.Reddit(\"Testing 123\") o = OAuth2Util.OAuth2Util(r, print_log=True) try: print str(r.get_submission(\"http://www.reddit.com/r/videos/comments/38ae9m/construct/crtswre\").comments[0]) except Exception as err: print traceback.print_exc() print err._raw When I run the above with oauth2, I get 403s with that URL. But other URLs work and there are other URLs that don't. There is no ambiguity like this when using the old username/password login. The traceback: Traceback (most recent call last): File \"testing.py\", line 12, in print str(r.get_submission(\"http://www.reddit.com/r/videos/comments/38ae9m/construct/crtswre\").comments[0]) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 985, in get_submission params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 348, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 1034, in from_url response = reddit_session.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 424, in _request _raise_response_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 196, in _raise_response_exceptions raise Forbidden(_raw=response) Forbidden None ---- Does the same happen for others? The scope I'm using for oAuth2Util: scope=identity,account,edit,flair,history,livemanage,modconfig,modflair,modlog,modothers,modposts,modself,modwiki,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote,wikiedit,wikiread", "created_utc": 1438664122, "gilded": 0, "name": "t3_3fpmx8", "num_comments": 5, "score": 1, "title": "PRAW - get_submission() works only \"sometimes\" with oAuth2 - 403s", "url": "https://www.reddit.com/r/redditdev/comments/3fpmx8/praw_get_submission_works_only_sometimes_with/"}, {"author": "NotSinceYesterday", "body": "Hi all, I have a bot that I'm updating to use Oauth, and it appears that the scopes listed [here](http://praw.readthedocs.org/en/latest/pages/oauth.html#oauth-scopes) aren't fully correct. Replying to a private message using .reply() appears to need the 'Submit' scope instead of the 'privatemessages' scope. File \"C:\\Users\\x\\Dropbox\\Pokemontrades\\Oauth Scripts\\Porygon2-Bot.py\", line 96, in messagesuccess message.reply(\"Flair updated.\") File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 376, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 342, in wrapped raise errors.LoginOrScopeRequired(function.__name__, scope) LoginOrScopeRequired: ``_add_comment` requires a logged in session or the OAuth2 scope `submit`` requires a logged in session", "created_utc": 1438596159, "gilded": 0, "name": "t3_3flqqh", "num_comments": 6, "score": 3, "title": "PRAW - Incorrect Scopes?", "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "Phteven_j", "body": "EDIT: RESOLVED via: $ pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip --- So I got my OAuth working for the most part, but for the life of me I cannot get banning working. Every other mod-type thing works for me: removing, distinguishing, flairing, etc. I am using the modcontributors scope to enable banning and if I print my _authorization variable, I get: set([u'wikiedit', u'save', u'wikiread', u'subscribe', u'edit', u'modcontributors', u'mysubreddits', u'privatemessages', u'modconfig', u'read', u'modlog', u'modposts', u'modflair', u'vote', u'modwiki', u'submit', u'identity', u'flair']) And here is the error I get when I attempt a ban (yes I'm a mod of the sub): File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py\", line 346, in wrapped raise errors.LoginRequired(function.__name__) praw.errors.LoginRequired: `do_relationship` requires a logged in session I appreciate any help and let me know if I can provide more info.", "created_utc": 1438476283, "gilded": 0, "name": "t3_3fgo6t", "num_comments": 8, "score": 5, "title": "[PRAW] Getting \"LoginRequired\" error when attempting to ban", "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "Phteven_j", "body": "Hey guys, I have a series of simple python reddit bots that need to be logged in to perform their tasks. I've been using r.login() but I'm converting over to OAuth like I should. The problem is that when I stop and restart a script, the authorization needs to be redone and I don't know a good way to do it. I followed the guide here https://praw.readthedocs.org/en/v3.1.0/pages/oauth.html. Here's the relevant code: r.set_oauth_app_info(client_id,client_secret,redirect_uri) url = r.get_authorize_url('hcebot', 'identity', True) import webbrowser webbrowser.open(url) access_information=r.get_access_information(\"\") r.set_access_credentials(**access_information) authenticated_user=r.get_me() So I run the bot, grab the code, put it into my script, and run the bot. It runs fine, but if I need to restart it or it crashes/restarts, I need to update the code. Is there a way to do this automatically? This is the error: praw.errors.OAuthInvalidGrant: invalid_grant on url https://api.reddit.com/api/v1/access_token/ Thanks for your help and let me know if I left anything out.", "created_utc": 1438370238, "gilded": 0, "name": "t3_3fbqfw", "num_comments": 20, "score": 2, "title": "OAuth help for simple python bots: getting invalid_grant on subsequent runs", "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "DontKillTheMedic", "body": "I know that very soon, PRAW will be transitioning to another authentication protocol for logging into reddit accounts. Does anybody know exactly when this change will occur? And I assume the simple fix is to switch to the OAuth protocol mentioned in the documents. Am I correct in doing so? Thanks!", "created_utc": 1438264279, "gilded": 0, "name": "t3_3f5rzi", "num_comments": 28, "score": 11, "title": "[PRAW] Reddit.login(), when is it leaving?", "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "jian142857", "body": "I'd like to grab some latest users for research purpose. One method might be possible is increasing the user id and query if it exists or not. but I found that PRAW doesn't provide one API to get user by id but only by the user name. Is there any method I could get the latest user or get the user by id?", "created_utc": 1438222242, "gilded": 0, "name": "t3_3f3ydz", "num_comments": 6, "score": 2, "title": "How to get the latest registered users?", "url": "https://www.reddit.com/r/redditdev/comments/3f3ydz/how_to_get_the_latest_registered_users/"}, {"author": "OtakuSRL", "body": "I updated PRAW accordingly as well (via pip) with no change. Am I still on an outdated build somehow? I am looking to use the following functions which was added in a recent build/release of PRAW. `newpost.sticky(bottom=True)` returns an error that it doesn't recognize the \"bottom\" part, and the error obviously terminates the program. The rest of the program works fine and posts the thread, it just lacks any sort of sticky effect as the code terminates before it could finish properly. I am thinking this is a version issue but I am not sure. Is pip not the best route to take when upgrading PRAW? I believe this was pushed to the release or whatever. Sorry for the lack of a screenshot of the error but hopefully this is simple enough to understand.", "created_utc": 1438122781, "gilded": 0, "name": "t3_3eyfz9", "num_comments": 3, "score": 2, "title": "[PRAW] Sticky bottom=True function not being recognized, even after updating PRAW", "url": "https://www.reddit.com/r/redditdev/comments/3eyfz9/praw_sticky_bottomtrue_function_not_being/"}, {"author": "iceph03nix", "body": "So I'm reading through the [PRAW guide](https://praw.readthedocs.org/en/v3.1.0/pages/comment_parsing.html) and I'm trying to just grab the top comments from a specific submission. I found the option to load all comments... >>> submission.replace_more_comments(limit=None, threshold=0) >>> all_comments = submission.comments ...but due to the number of comments, it's taking quite a while, and I have a feeling I'm pulling a lot of extraneous info. Is there a good way to grab all the 'root' comments without pulling everything?", "created_utc": 1438043150, "gilded": 0, "name": "t3_3eu705", "num_comments": 8, "score": 2, "title": "Loading all top level comments from a specific submission", "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "Triplanetary", "body": "Two questions: 1. Does PRAW use the 1 call per second rate limit when connected via OAuth or does it still wait two seconds between requests in this situation? 2. Does PRAW multiprocess take into account that the OAuth rate limit is per-user? AND/OR could I safely ignore multiprocess and run multiple scripts connected to different OAuth'd users in separate processes? (If the answer to the latter question is a resounding no, that's cool, just let me know.) Thanks.", "created_utc": 1437952666, "gilded": 0, "name": "t3_3epjgl", "num_comments": 1, "score": 2, "title": "PRAW and OAuth rate limit", "url": "https://www.reddit.com/r/redditdev/comments/3epjgl/praw_and_oauth_rate_limit/"}, {"author": "catcint0s", "body": ">>> from praw import Reddit >>> r = Reddit(\"hey\") >>> s = r.get_submission(\"https://www.reddit.com/r/worldnews/comments/3ek0h4/ne w_horizons_finds_nitrogen_glaciers_and_hazy_air/\") >>> s.title \"New Horizons Finds Nitrogen Glaciers and Hazy Air on Pluto: Astronomers astoun ded by the dwarf planet's active geology and atmosphere\" >>> s.subreddit Subreddit(subreddit_name='worldnews') >>> s.subreddit.name 't5_2qh13' >>> s.subreddit.fullname 't5_2qh13' Tried this way but it gives the id", "created_utc": 1437856459, "gilded": 0, "name": "t3_3eld2o", "num_comments": 3, "score": 1, "title": "[PRAW] How to get a Subreddit object's name", "url": "https://www.reddit.com/r/redditdev/comments/3eld2o/praw_how_to_get_a_subreddit_objects_name/"}, {"author": "codsane", "body": "Hello. Was messing around with some bot ideas, and it seems like I am having trouble logging into Reddit. I have tried running some of my old bots, and they authenticate completely fine. Here is the little test script I am trying to run: >import praw > >redditPraw = praw.Reddit(user_agent = \"User Agent\") >redditPraw.login('Username', 'Password') >redditPraw.get_subreddit('test') After running, I am getting praw.errors.Forbidden on the login line. Any ideas as to why this is happening? All my other bots run fine. I am stuck here. Thanks! :)", "created_utc": 1437800577, "gilded": 0, "name": "t3_3ej576", "num_comments": 2, "score": 0, "title": "Getting forbidden error, not sure why.", "url": "https://www.reddit.com/r/redditdev/comments/3ej576/getting_forbidden_error_not_sure_why/"}, {"author": "MycTyson", "body": "Hello - today I fired up Python, Installed PRAW and got down with my first PRAW shenanigans. Now that I know what I am doing in terms of the basics, I wanted to ask before I set out on a quest to do it myself, if there exists something somewhere to fulfill my need. I run a small sub, and enforce the [TAG] of every post. No tag, no post. Recently, I added CSS Flair to make sorting the sub easier. I have to manually go through and assign all the flair, or count on users to do it for their posts. Relying on users to put in effort is less than ideal. Enter Python and PRAW to do the automation for me in terms of applying flair to existing posts, and enforcing it with new posts. I have both a Windows VPS and a CentOS VPS at my disposal to let this sucker run on once I get it hashed out locally, but as with most things there is probably someone who has wanted to do this before and I don't want to waste any time if I don't have to. Again, today is literally day one however I am a big fan of not reinventing the wheel if possible. TL;DR: See title.", "created_utc": 1437776096, "gilded": 0, "name": "t3_3ehx0t", "num_comments": 3, "score": 3, "title": "[PRAW] Automaticallly assign flair to posts in sub, based on content of [Brackets] in the title?", "url": "https://www.reddit.com/r/redditdev/comments/3ehx0t/praw_automaticallly_assign_flair_to_posts_in_sub/"}, {"author": "MrRogersbot", "body": "[Automoderator can do it]( https://www.reddit.com/r/modnews/comments/36fcrd/moderators_automoderator_updates_filter_action/) **Ability to display a reason for acting in the moderation log** This is a much-requested feature that I've finally been able to add today - you can now set action_reason on any rule that has an action, and the reason will be displayed in the moderation log for approvals/removals, or used as a report reason if it's a report rule. So for example, you could define this rule: title: [\"red\", \"yellow\", \"blue\"] action: remove action_reason: primary color in title And if AutoModerator removes a post because of that rule, the entry in the moderation log would read something like: AutoModerator removed link \"DAE think red is overrated?\" by Deimorz (primary color in title) This should help with one of the biggest difficulties with AutoModerator - not being able to tell exactly why it approved or removed something (unless you used comments/modmail/flair, which all have their own issues). Note that action_reason completely replaces report_reason, but report_reason is still supported (and just acts as an alias of action_reason) so that all the existing rules using report_reason are still functional. Can this be added to praw?", "created_utc": 1437586943, "gilded": 0, "name": "t3_3e80jx", "num_comments": 3, "score": 0, "title": "[PRAW] Setting action reason for removal in mod log?", "url": "https://www.reddit.com/r/redditdev/comments/3e80jx/praw_setting_action_reason_for_removal_in_mod_log/"}, {"author": "AchillesDev", "body": "I more or less understand how to use OAuth for a bot (I am working on a quick implementation of it to make sure I get it), however I am working on a utility that will commit specific actions on two given (and existing) user accounts. I know this is done successfully by other apps, but I'm not sure if I am thinking about structuring my utility the right way. How would the app get the correct information for a given user and commit those actions on their behalf using OAuth? I am more familiar with using PRAW's login method and config files than I am with using OAuth, so maybe I am thinking about the logging in concept the wrong way? As I understand it, the OAuth information is tied to a bot's account for a bot implementation, but the utility I am using will be using existing accounts that others own. Currently, I'm monkeying around with OAuth2Util.", "created_utc": 1437352766, "gilded": 0, "name": "t3_3dw2y8", "num_comments": 8, "score": 2, "title": "[PRAW/OAuth] Using OAuth to automate user actions on existing accounts", "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "Ouiski", "body": "I've developed a python crawler (not using praw) that collects comments using an existing list of submissions that I've already collected. It uses OAuth. When I try to run it for extended periods of time after about 1800 submissions worth of comments, the call doesn't return the data correctly. I am using a timer to prevent over calling, (I'm definitely doing less than 60 a minute) , but does anyone know of any other rule that prevents calling after a certain amount of calls in a row? Or has anyone had any similar problems? Thanks!", "created_utc": 1436965538, "gilded": 0, "name": "t3_3ddlcv", "num_comments": 2, "score": 2, "title": "Crawling issue", "url": "https://www.reddit.com/r/redditdev/comments/3ddlcv/crawling_issue/"}, {"author": "pcjonathan", "body": "I'm running a script to collect content from a few subreddits I moderate (which then gets emailed and dumped). Every now and then, it comes across a post containing characters that aren't normal. It's a very rare occurrence (I've just added a new sub to it hence why the example link is very old). The problem occurs with praw-multiprocess so there's not much I can do. This is the line that crashes: if r.get_submission(url=sub.perma).approved_by != None: An example sub.perma is: https://www.reddit.com/r/gallifreyan/comments/1uerl8/i_tried_doing_a_name_with_accents_b%C5%91g%C3%A9r_eszter/ (sub is a peewee database object) It gets me the following output and error when I run my script: Lost connection with multiprocess server during read. Trying again. Lost connection with multiprocess server during read. Trying again. Lost connection with multiprocess server during read. Trying again. Traceback (most recent call last): File \"/home/pcj/py3/lib/python3.4/site-packages/praw/handlers.py\", line 209, in _relay retval = cPickle.load(sock_fp) EOFError: Ran out of input and this: praw.errors.ClientException: Successive failures reading from the multiprocess server. The praw-multiprocess output is as follows: GET https://www.reddit.com/r/gallifreyan/comments/1uerl8/i_tried_doing_a_name_with_accents_b%C5%91g%C3%A9r_eszter/.json Exception in thread Thread-69877: Traceback (most recent call last): File \"/usr/lib/python3.4/threading.py\", line 920, in _bootstrap_inner self.run() File \"/usr/lib/python3.4/threading.py\", line 868, in run self._target(*self._args, **self._kwargs) File \"/usr/lib/python3.4/socketserver.py\", line 612, in process_request_thread self.handle_error(request, client_address) File \"/usr/lib/python3.4/socketserver.py\", line 609, in process_request_thread self.finish_request(request, client_address) File \"/usr/lib/python3.4/socketserver.py\", line 344, in finish_request self.RequestHandlerClass(request, client_address, self) File \"/usr/lib/python3.4/socketserver.py\", line 665, in __init__ self.handle() File \"/usr/local/lib/python3.4/dist-packages/praw/multiprocess.py\", line 77, in handle cPickle.dump(retval, self.wfile, # pylint: disable=E1101 UnboundLocalError: local variable 'retval' referenced before assignment Does anyone have any advice or a possible fix for PRAW? Sidenote: Is there anyway to increase PRAW's caching for everything but override that increase for certain pages (or visa versa)? (e.g. 5 minutes on submissions, 30 seconds on /r/sub/new and /r/sub/comments).", "created_utc": 1436910346, "gilded": 0, "name": "t3_3db07x", "num_comments": 3, "score": 3, "title": "[PRAW] Crashing on Non-Standard Characters", "url": "https://www.reddit.com/r/redditdev/comments/3db07x/praw_crashing_on_nonstandard_characters/"}, {"author": "zzpza", "body": "Now we have two stickies, roughly how long before this functionality will be available in PRAW? I have a sticky rotation script I need to update... (In case you missed the announcement: https://www.reddit.com/r/modnews/comments/3d7i0q/moderators_you_can_now_have_two_stickies_in_your/) Also, where should I be watching for new release announcements for PRAW? I tried looking at /r/praw but that is private. TIA. :D", "created_utc": 1436862377, "gilded": 0, "name": "t3_3d8csx", "num_comments": 7, "score": 1, "title": "PRAW support for two stickies?", "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "austin_18", "body": "**Problem Solved** So I have a bot that I've recently coded in Python 3.4 and ran on and off for the past week. Throughout the week I've noticed an error or warning that keeps interrupting my bot. Traceback (most recent call last): * File \"C:\\Users\\Austin\\Dropbox\\DongerBot-Counter.py\", line 99, in comment.reply(\"\u30fd\u0f3c\u0e88\u0644\u035c\u0e88\u0f3d\uff89 \\n\\n ^Now ^With ^**Donger** ^**Facts!**: \\n\\n ^Dongers ^Supplied: ^**%s** \\n\\n ^^That ^^Is ^^**%s** ^^Upvote(s) ^^Per ^^Donger!\" %(x, avgKarma)) * File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 399, in reply response = self.reddit_session._add_comment(self.fullname, text) * File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 348, in wrapped return function(cls, *args, **kwargs) * File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 2407, in _add_comment retry_on_error=False) * File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) * File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 579, in request_json retry_on_error=retry_on_error) * File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 424, in _request _raise_response_exceptions(response) * File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 196, in _raise_response_exceptions raise Forbidden(_raw=response) * praw.errors.Forbidden I am completely lost at why this is being displayed. I am using comment_stream, if that is any help. I can post my code if needed. Thanks! Edit: formatting", "created_utc": 1436829123, "gilded": 0, "name": "t3_3d6p0y", "num_comments": 5, "score": 0, "title": "My bot code is returning a rather lengthy error/warning/Traceback, and I'm not sure why. Any help is appreciated!", "url": "https://www.reddit.com/r/redditdev/comments/3d6p0y/my_bot_code_is_returning_a_rather_lengthy/"}, {"author": "teaearlgraycold", "body": "praw.errors.RedirectException: Unexpected redirect from https://api.reddit.com/api/wiki/edit/.json to https://www.reddit.com/r/explainlikeimfive/login.json?dest=https%3A%2F%2Fapi.reddit.com%2Fapi%2Fwiki%2Fedit%2F.json The bot can still use the API (make modmails etc.) but throws this when it tries to edit a wiki page. I gave it both `wikiedit` and `edit` OAuth scopes. **Edit:** Title should have been \"strange error\" :Pb", "created_utc": 1436581240, "gilded": 0, "name": "t3_3cvdof", "num_comments": 13, "score": 4, "title": "String error after implementing OAuth", "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "asdfusernameasdf", "body": "^(Sorry for probably being the 50th person to post this but I couldn't find any useful answers.) I'm asking because 1) there are a metric ton of comments per second and I have no idea how to keep up and 2) I haven't found an API function that gives me /r/all/comments. I know PRAW can do it but I'd prefer C# over Python because everything else I need is already written in it.", "created_utc": 1436545345, "gilded": 0, "name": "t3_3ct9m8", "num_comments": 7, "score": 12, "title": "How do Reddit bots read every single comment?", "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "Bitani", "body": "I'm currently trying to create a program that retrieves as many comments as it can from a user's page and then searches the first-child replies to those comments for certain keywords. Problem is, I can't find an easy way to load the replies to each comment. It seems like the only viable method I can find is to load the entire submission each comment is a child of. Whether through PRAW or the raw API, what would be the best way to load the replies to a comment just having its ID?", "created_utc": 1436385325, "gilded": 0, "name": "t3_3cl8oq", "num_comments": 3, "score": 1, "title": "Load replies to a comment fetched from user page?", "url": "https://www.reddit.com/r/redditdev/comments/3cl8oq/load_replies_to_a_comment_fetched_from_user_page/"}, {"author": "avinassh", "body": "**TLDR;** [Github](https://github.com/avinassh/prawoauth2) | [Example](https://github.com/avinassh/prawoauth2/tree/master/examples/halflife3-bot) | [pypi](https://pypi.python.org/pypi/prawoauth2/) --- **What it does?** - It makes your life super easy for handling OAuth with Praw - It can get you new `access_token` and `refresh_token` - It can 'refresh' your praw instance to use new tokens - You don't really need to worry about token expiry - Best used for bots, which run on Heroku, AWS etc. It will help you run your bot forever! - Comes with nice documentation and a working example you can play with! - It's open source and released under MIT License! (: **Installation:** pip install prawoauth2 **Info:** `prawoauth2` comes with two components, `PrawOAuth2Mini` and `PrawOAuth2Server`. `PrawOAuth2Server` authorizes your app/script with the Reddit account and gives you access token. `PrawOAuth2Mini` uses these tokens for all next transactions with Reddit. Remember, for a bot, you only need `access_token`. Why it is written like that? If you are writing a bot and running it on a headless server, something like Amazon AWS or Openshift, you cannot authorize your script with the Reddit account, as it tries to open the browser for authorization. This is one time only operation(if you pass the parameter `permanent`). So, I decided to break this into two parts. Run the `PrawOAuth2Server` locally on your computer, get the `access_token`, `refresh_token` and save them somewhere. Later, `PrawOAuth2Mini` can make use of these tokens for further transactions. And it does not require browser at all, so that it can run in a headless server without any hiccups. **TLDR;** `PrawOAuth2Server` meant to be run only once locally on your main machine to fetch the first `access_token`, `refresh_token` and `PrawOAuth2Mini` later and for everything.", "created_utc": 1436377770, "gilded": 0, "name": "t3_3ckq5a", "num_comments": 27, "score": 28, "title": "Hi, I released a helper module for Praw, which makes writing OAuth bots super easy and fun", "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "Nouv", "body": "Hey, Is there any way to use SnooCore with a comment stream, PRAW style? Checked docs but may have been missing something.", "created_utc": 1436334368, "gilded": 0, "name": "t3_3ciqeq", "num_comments": 0, "score": 1, "title": "Comment stream using SnooCore?", "url": "https://www.reddit.com/r/redditdev/comments/3ciqeq/comment_stream_using_snoocore/"}, {"author": "umop_aplsdn", "body": "Hi, I'm trying to mark my messages as read and unread. Here is my code: messages = [message for message in r.get_unread(limit=None) if message.subject == ''] messages.sort(key=lambda m: m.created) for message in messages: print type(message) message.mark_as_read() Here is the error: Traceback (most recent call last): File \"/Users/tyler/Documents/code/ModInviterBot/main.py\", line 49, in message.mark_as_unread() File \"/usr/local/lib/python2.7/site-packages/praw/objects.py\", line 367, in mark_as_unread return self.reddit_session.user.mark_as_read(self, unread=True) AttributeError: 'NoneType' object has no attribute 'mark_as_read' It seems like self.reddit_session.user.mark_as_read isn't being initialized. My Python version is 2.7.9 on Mac OSX, installed using Homebrew.", "created_utc": 1436313600, "gilded": 0, "name": "t3_3chntm", "num_comments": 4, "score": 4, "title": "[PRAW] Message.mark_as_read() and mark_as_unread() broken?", "url": "https://www.reddit.com/r/redditdev/comments/3chntm/praw_messagemark_as_read_and_mark_as_unread_broken/"}, {"author": "swim1929", "body": "Hi. I just want to make sure I'm not exceeding the request limit or anything, since I don't completely understand how comment_stream works yet. If anybody could explain, I'd really appreciate it. Anyways, here's the relevant code: word_to_match = ['example'] cache = [] def corrector_bot(): comments = praw.helpers.comment_stream(r, 'all', limit=None, verbosity=0) for comment in comments: comment_text = comment.body.lower() isMatch = any(string in comment_text for string in word_to_match) if comment.id not in cache and isMatch: #stuff here while True: try: corrector_bot() time.sleep(2) except: traceback.print_exc() print('Resuming in 1 minute...') time.sleep(60)", "created_utc": 1436236566, "gilded": 0, "name": "t3_3cdo5a", "num_comments": 2, "score": 3, "title": "[PRAW] Am I breaking any API rules with this?", "url": "https://www.reddit.com/r/redditdev/comments/3cdo5a/praw_am_i_breaking_any_api_rules_with_this/"}, {"author": "gschizas", "body": "My goal was to work with OAuth2, and to make the end code as simple as possible. State is kept in a config file, with a very simple structure. The end script should look something like this: from bot import reddit_agent r = reddit_agent(user_agent='Some (optional) user agent', ini_section='optional_section_defaults_to_DEFAULT') print(r.user) The ini file looks something like this: [DEFAULT] client = client_id_from_/prefs/apps secret = secret_key_from_/prefs/apps The bot.py (the bulk of the work) is the following: # coding: utf-8 import urllib.parse import praw import configparser import datetime import http.server import webbrowser from dateutil.parser import parse as dateparser def start_web_server(port): \"\"\" :rtype : string \"\"\" server = http.server.HTTPServer(('', port), ScriptCallbackWebServer) print('Started httpserver on port:', port) server.now_serving = True server.callback_code = None # Wait for incoming http requests # until a proper result is found while server.now_serving: server.handle_request() server.server_close() return server.callback_code class ScriptCallbackWebServer(http.server.BaseHTTPRequestHandler): def do_GET(self): url = urllib.parse.urlparse(self.path) query = urllib.parse.parse_qs(url.query) if url.path != '/authorize_callback' or 'code' not in query: self.send_response(404) return self.send_response(200) self.send_header(\"Content-type\", \"text/html\") self.end_headers() self.wfile.write(\"Simple Bot Helper\".encode('utf-8')) self.wfile.write(\"This is the authorise callback page.\".encode('utf-8')) self.wfile.write(\"You accessed path: {}\".format(self.path).encode('utf-8')) self.wfile.write(\"You can close your browser\".encode('utf-8')) self.wfile.write(\"\".encode('utf-8')) self.server.callback_code = query['code'][0] self.server.now_serving = False def reddit_agent(user_agent=None, ini_section='DEFAULT'): if user_agent is None: user_agent = 'Reddit Temporary Script by /u/gschizas version ' + datetime.date.today().isoformat() scope = {'identity', 'flair', 'read', 'modflair', 'modlog', 'modposts', 'mysubreddits'} result = praw.Reddit(user_agent=user_agent) result.config.decode_html_entities = True # result.config.log_requests = 1 cfg = configparser.ConfigParser() with open('bot.ini') as f: cfg.read_file(f) client = cfg[ini_section]['client'] secret = cfg[ini_section]['secret'] access_token = cfg[ini_section].get('access_token', '') refresh_token = cfg[ini_section].get('refresh_token', '') redirect_url = 'http://localhost:65281/authorize_callback' result.set_oauth_app_info(client, secret, redirect_url) # first time running if access_token == '' or refresh_token == '': url = result.get_authorize_url('reddit_scratch', scope, True) webbrowser.open(url) callback_code = start_web_server(65281) access_information = result.get_access_information(callback_code) access_token = access_information['access_token'] refresh_token = access_information['refresh_token'] save_state(cfg, ini_section, access_token, refresh_token) last_refresh = dateparser(cfg[ini_section]['last_refresh']) minutes = (datetime.datetime.now() - last_refresh).total_seconds() / 60 print(minutes) if minutes", "created_utc": 1436017701, "gilded": 0, "name": "t3_3c3phr", "num_comments": 10, "score": 4, "title": "[praw] The simplest bot I could make (I hope someone can make it simpler)", "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "swim1929", "body": "Hi, I'm having trouble with the following code. The bot runs fine when I define a word such as \"fish\" as the words_to_match string. However, as soon as I put spaces (such as defining the string as 'I could care less'), the bot can no longer find the comment even if it's actually there. I tried removing the brackets around the string (I just put 'I could care less') and it started replying to every comment. Thanks! import praw import time r = praw.Reddit(user_agent = \"TestBot by Ali /u/BlessYouBot_\") words_to_match = ['I could care less'] cache = [] def run_bot(): subreddit = r.get_subreddit(\"test\") comments = subreddit.get_comments(limit=25) for comment in comments: comment_text = comment.body.lower() isMatch = any(string in comment_text for string in words_to_match) if comment.id not in cache and isMatch: comment.reply(\"\"\"I think you meant to say, \"I **couldn't** care less\".\\n\\n*____________^I ^am ^a ^bot, ^and ^this ^action ^was ^performed ^automatically. ^Please ^message ^/u/swim1929 ^with ^any ^feedback ^and/or ^hate.*\"\"\") cache.append(comment.id) print(\"Comments loop finished, time to sleep v.v\") while True: run_bot() time.sleep(10)", "created_utc": 1435982561, "gilded": 0, "name": "t3_3c2jeu", "num_comments": 6, "score": 3, "title": "[Simple Noob Question] Bot not finding comments containing string.", "url": "https://www.reddit.com/r/redditdev/comments/3c2jeu/simple_noob_question_bot_not_finding_comments/"}, {"author": "armandg", "body": "I'm taking the risk of asking a possibly stupid question, but here goes: [The PRAW code](https://github.com/praw-dev/praw/blob/c64e3f71841e8f0c996d42eb5dc9a91fc0c25dcb/praw/__init__.py#L2416-L2420) states that the \"send_replies\"-function is a Gold Only-feature. So, naturally nothing happens when I try to deactivate the *\"send_replies\"*-feature. Here's my code: r.submit(subreddit, title, output, captcha=None, send_replies=None, resubmit=None) I am, however, able to deactivate this myself manually when submitting and even after submission has been done. Is only the automation of this feature a \"Gold Only Feature\" or am I missing something?", "created_utc": 1435835105, "gilded": 0, "name": "t3_3buzu2", "num_comments": 2, "score": 1, "title": "PRAW: send_replies a \"Gold Only Feature\"?", "url": "https://www.reddit.com/r/redditdev/comments/3buzu2/praw_send_replies_a_gold_only_feature/"}, {"author": "thomasbomb45", "body": "I am trying to write a program that, given a URL, will find a comment containing the URL and search the replies to *that* comment for other URLs. If my understanding of PRAW and the API is correct, there is no way for me to do this, other than to scrape and cache the entire website. Is there a better way to do this, or a public copy of the database I can download?", "created_utc": 1435628933, "gilded": 0, "name": "t3_3bky0m", "num_comments": 2, "score": 1, "title": "Search for links in comments?", "url": "https://www.reddit.com/r/redditdev/comments/3bky0m/search_for_links_in_comments/"}, {"author": "EliteMasterEric", "body": "**NOTE:** I just realized I wrote the title wrong, I know how to write a bot, just not how to integrate it with OAuth. I've read the tutorial on [the PRAW website](https://praw.readthedocs.org/en/v3.0.0/pages/oauth.html), but it only raises more questions for me. In particular, it says it requires the user to grant authorization by accessing a URL in their browser. This is, of course, impossible on a Python script running on a schedule through cron on an Amazon EC2 server. I don't think there even IS a web growser on that computer! Regardless, when I run my PRAW script, I get a message in the console reading: DeprecationWarning: Password-based authentication will stop working on 2015/08/03 and as a result will be removed from PRAW4. For more information please see: https://www.reddit.com/comments/2ujhkr/ Does this change make it impossible to create automated bots, or can I somehow give a program \"permanent\" permission to access my bot?", "created_utc": 1435593738, "gilded": 0, "name": "t3_3bit3y", "num_comments": 18, "score": 3, "title": "[PRAW][OAuth] How do I make an automated bot?", "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "Liudvikam", "body": "So because cookie authentication is being deprecated, OAuth will have to be used for authentication. But since I'm quite inexperienced when it comes to the Reddit API, PRAW, and coding in general, I just *can't* fully understand the [guide on how to use OAuth with PRAW on PRAW's website](https://praw.readthedocs.org/en/v3.0.0/pages/oauth.html). So could someone ELI5 how OAuth works and how to use it with PRAW?", "created_utc": 1435592308, "gilded": 0, "name": "t3_3bipzt", "num_comments": 10, "score": 1, "title": "Help with OAuth", "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "TerminalCase", "body": "Using PRAW, is there a way to search all subreddits in a given multireddit? E.g., if I want to search for every post made within a given time period on all subs in the [nationalphotosubs multi](http://www.reddit.com/user/I_AM_STILL_A_IDIOT/m/nationalphotosubs), is there a way to do it?", "created_utc": 1435545230, "gilded": 0, "name": "t3_3bgtlp", "num_comments": 6, "score": 1, "title": "How can I search a multi with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "dado3212", "body": "I'm trying to extract some data from my subreddit /r/mathriddles, but I don't know if it's possible. The way the flairing is set up, problems can be flaired with a difficulty setting, and then can be changed to solved, so you can easily see what problems are still open. I was curious if there was a way to see when the flair was changed, or a list of the flairs (and dates) that it's had. Any ideas if this is possible in general with Reddit, or more specifically with PRAW?", "created_utc": 1435512286, "gilded": 0, "name": "t3_3bf2qz", "num_comments": 2, "score": 1, "title": "Getting History of Link Flair", "url": "https://www.reddit.com/r/redditdev/comments/3bf2qz/getting_history_of_link_flair/"}, {"author": "_thelichking_", "body": "Python. So I have the following code: submissions = r.get_subreddit(targetSubreddit).get_top_from_all(limit=10) for submission in submissions: if contname in ???: here contname is a keyword which the user gives as an input, and I would like to check for that word in the name of the submission,is there any object( which i can put in place of ???) in PRAW which does this? Like submission.url checks the url of the submission,so something like that? I already checked the docs but couldn't find it.", "created_utc": 1435507243, "gilded": 0, "name": "t3_3betip", "num_comments": 6, "score": 1, "title": "[PRAW] [PYTHON] Reading keywords in submission name using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/3betip/praw_python_reading_keywords_in_submission_name/"}, {"author": "avinassh", "body": "I am using [Praw Oauth wrapper](https://www.reddit.com/r/botwatch/comments/38k30h/oauth2util_a_wrapper_around_praws_oauth2/). Now problem is, the script asks access to my bot account everytime I run it or whenever `access token` is expired (I guess) So, how do I make it permanent setting? The app appears in https://www.reddit.com/prefs/apps/ but why it does asks me to authorize everytime?", "created_utc": 1435419943, "gilded": 0, "name": "t3_3bb8tw", "num_comments": 18, "score": 1, "title": "How do I give access of my bot account to my script, permanently (OAuth)?", "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "codsane", "body": "I was wondering if this possible? I am posting news articles to my own subreddit and sometimes users post before me, I would like to check and see if the link has already been submitted. I tried adding an exception for the praw already submitted error but it does not accomplish what I am looking for. Thanks!", "created_utc": 1435210819, "gilded": 0, "name": "t3_3b1jry", "num_comments": 2, "score": 3, "title": "Check if link already submitted to a subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/3b1jry/check_if_link_already_submitted_to_a_subreddit/"}, {"author": "BananaGranola", "body": "I'm getting this. The same code has worked fine until I updated praw yesterday. Does anyone know what I'm doing wrong? Does it have to do with the recent HTTPS requirement? Thanks! > Traceback (most recent call last): File \"./linkrav_bot.py\", line 128, in main() File \"./linkrav_bot.py\", line 83, in main reddit.login(reddit_username, reddit_password) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 88, in wrapped return function(self, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 1321, in login self.request_json(self.config['login'], data=data) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 170, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 569, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 413, in _request response = handle_redirect() File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 383, in handle_redirect timeout=timeout, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 147, in wrapped result = function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 102, in request allow_redirects=False) File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 573, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 370, in send timeout=timeout File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/connectionpool.py\", line 349, in _make_request conn.request(method, url, **httplib_request_kw) File \"/usr/lib/python2.7/httplib.py\", line 1001, in request self._send_request(method, url, body, headers) File \"/usr/lib/python2.7/httplib.py\", line 1035, in _send_request self.endheaders(body) File \"/usr/lib/python2.7/httplib.py\", line 997, in endheaders self._send_output(message_body) File \"/usr/lib/python2.7/httplib.py\", line 850, in _send_output self.send(msg) File \"/usr/lib/python2.7/httplib.py\", line 826, in send self.sock.sendall(data) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 208, in sendall sent = self._send_until_done(data) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py\", line 198, in _send_until_done return self.connection.send(data) File \"/usr/lib/python2.7/dist-packages/OpenSSL/SSL.py\", line 947, in send raise TypeError(\"data must be a byte string\") TypeError: data must be a byte string", "created_utc": 1435154760, "gilded": 0, "name": "t3_3aybk8", "num_comments": 9, "score": 3, "title": "Login error", "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "Spedwards", "body": "So I'm trying to get my scripts completely converted to a OAuth login however I have some questions. Currently, I have set the `client_id`, `client_secret`, and `redirect_url` and then in IDLE, used that to get the access key which is requested from the script. I'm looking at the docs and they have the following as an example config file: [bboe] domain: www.reddit.com ssl_domain: ssl.reddit.com user: bboe pswd: this_isn't_my_password [reddit_dev] domain: www.reddit.com ssl_domain: ssl.reddit.com user: someuser pswd: somepass [local_dev1] domain: reddit.local:8000 user: someuser pswd: somepass [local_wacky_dev] domain: reddit.local:8000 user: someuser pswd: somepass api_request_delay: 5.0 default_content_limit: 2 So I looked in mine and found their test oauth login which I'll use in this as well: [reddit_oauth_test] oauth_client_id: stJlUSUbPQe5lQ oauth_client_secret: iU-LsOzyJH7BDVoq-qOWNEq2zuI oauth_redirect_uri: https://127.0.0.1:65010/authorize_callback So I copied that below and changed the information for my script but I have no idea how to actually use it. Also, what does it change exactly? I noticed that a `oauth_refresh_token` field was added in the latest version but I don't even know how to get that! My current script: # prompted for ACCESS_KEY above r = praw.Reddit('My UserAgent') r.set_oauth_app_info(CLIENT_ID,CLIENT_SECRET,REDIRECT) access_information = r.get_access_information(ACCESS_KEY) r.set_access_credentials(**access_information) import time while True: try: r.refresh_access_information(access_information['refresh_token']) # do stuff time.sleep(300) except KeyboardInterrupt as e: print('Exiting...') sys.exit(-1) Is it possible to modify it so I don't have to get the access token every time I reboot the script? This whole thing is giving me the shits.", "created_utc": 1435148809, "gilded": 0, "name": "t3_3ay1c9", "num_comments": 4, "score": 3, "title": "[PRAW] OAuth Login", "url": "https://www.reddit.com/r/redditdev/comments/3ay1c9/praw_oauth_login/"}, {"author": "BnMcGi", "body": "I've followed the tutorial at the Praw docs (http://praw.readthedocs.org/en/latest/pages/oauth.html) to get an initial access token and refresh token. I've then taken this refresh token and added it to my script, adding the praw configuration values for oauth_client_id, oauth_client_secret and oauth_redirect_uri. I'm lead to believe I should just be able to call \"self.reddit.refresh_access_information(self.oauth_refresh_token)\" to get a new access token to use, but this is raising a \"requests.exceptions.HTTPError: 400 Client Error: Bad Request\" exception... Any ideas what I'm doing wrong?", "created_utc": 1434917754, "gilded": 0, "name": "t3_3amxbp", "num_comments": 2, "score": 0, "title": "PRAW and refresh tokens", "url": "https://www.reddit.com/r/redditdev/comments/3amxbp/praw_and_refresh_tokens/"}, {"author": "TeroTheTerror", "body": "Using praw, uploading 80K+ flairs and it errors out most times, it seems to work fine in smaller batches and I was wondering if anyone knows the upper limit? I tried checking readthedocs for praw and didn't see any number mentioned.", "created_utc": 1434744914, "gilded": 0, "name": "t3_3afuui", "num_comments": 4, "score": 1, "title": "Limit on r.set_flair_csv?", "url": "https://www.reddit.com/r/redditdev/comments/3afuui/limit_on_rset_flair_csv/"}, {"author": "GoldenSights", "body": "In my attempts to determine the character limit for wiki pages, I've found that they do not have a hard ceiling like submissions, comments, messages, etc. My subreddit is /r/GoldTesting. I've found that on the wiki page called [75000](/r/goldtesting/wiki/75000), I can submit a maximum of 511,873 characters. On the page [7500](/r/goldtesting/wiki/7500), I can submit a maximum of 511,874 characters. For page [7](/r/goldtesting/wiki/7), that limit goes up to 511,877... but that's only when I'm in the browser. When I use PRAW, the limits are 511,891, 511,892, and 511,895 respectively. For every byte that comes off of the page name, I can include one more in the text. This tells me that the limit is based on the **total size of the HTTP request**, because Chrome obviously sends more headers. This means that different pages on different subreddits, edited using different clients will experience a different character cap. I understand that Rule #1 of reddit's API is that it's inconsistent, but this is a little extreme. I tried to follow the source code to see if there's a number anywhere, but the closest I got was [here](https://github.com/reddit/reddit/blob/c6f959504466333c0d7d51c131240473aaf78b04/r2/r2/models/wiki.py#L375). The special_length_restriction_bytes dict doesn't have a number for this, so it falls back on [pylons.g.wiki_max_page_length_bytes](https://github.com/reddit/reddit/blob/c6f959504466333c0d7d51c131240473aaf78b04/r2/r2/models/wiki.py#L47) but I don't know what pylons is or where it comes from. Can wiki pages please receive a standard character limit? And perhaps an error message when editing over the limit in the browser (Currently the saving wheel just spins around, then stops)? It's not common for wiki pages to become this large, but there is definitely some information missing for those who need it.", "created_utc": 1434685736, "gilded": 0, "name": "t3_3ad6j0", "num_comments": 10, "score": 7, "title": "Why do wiki pages have an arbitrary character ilmit?", "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "ErrorX", "body": "So, I have been using PRAW and the r.Login() function for my bot, which I see is being depreciated. Trying to figure out how to use OAuth to do the same thing. I got the token and I can get the comments and parse the data I need, but I'm confused at how to reply to the comments. I see that you need to make a call to get submit scope, but I'm not sure how to go about that. Once you get the scope, how do you actually reply?", "created_utc": 1434548965, "gilded": 0, "name": "t3_3a5v11", "num_comments": 2, "score": 5, "title": "OAuth2 Authorization for Python", "url": "https://www.reddit.com/r/redditdev/comments/3a5v11/oauth2_authorization_for_python/"}, {"author": "norbornyl", "body": "The school gives us some space on their Apache servers. I'm using mine to host my website and a Python script that processes some text, and I'd like to be able to feed this script some reddit comments. I'm just unsure how to install custom Python modules. I tried pasting the praw directory into my folder but obviously that didn't work because of the various other modules praw imported. Thanks for reading.", "created_utc": 1434349849, "gilded": 0, "name": "t3_39vzmt", "num_comments": 2, "score": 5, "title": "How to install PRAW on my personal webspace on my University's servers?", "url": "https://www.reddit.com/r/redditdev/comments/39vzmt/how_to_install_praw_on_my_personal_webspace_on_my/"}, {"author": "MrJohz", "body": "Are there any instructions on how to use the script-base OAuth system? Is PRAW set up to allow that, or do I still need to use the older login API?", "created_utc": 1434207581, "gilded": 0, "name": "t3_39pmbl", "num_comments": 13, "score": 2, "title": "[PRAW] Guide to Script-base OAuth", "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "godlikesme", "body": "Hi, I'm using praw to automatically approve submissions of certain shadowbanned users(those submissions are great and very relevant to the subreddit). The code stopped working a few weeks ago. IIRC my code stayed the same. Was there any changes in api regarding approving submissions? Relevant issue on praw github: https://github.com/praw-dev/praw/issues/422", "created_utc": 1433880189, "gilded": 0, "name": "t3_397dng", "num_comments": 1, "score": 1, "title": "Approving submissions of shadowbanned users doesn't work using praw/API", "url": "https://www.reddit.com/r/redditdev/comments/397dng/approving_submissions_of_shadowbanned_users/"}, {"author": "raymestalez", "body": "Hi! I want to make some sort of interface for /r/writingprompts, so that users of my website could write a story(on my website) and it would automatically get submitted as a comment(from the user's account) to a thread on /r/writingprompts. Is it possible to do that with praw? And is it okay with reddit's TOS? Edit: This is what I was looking for - http://praw.readthedocs.org/en/latest/pages/oauth.html", "created_utc": 1433784256, "gilded": 0, "name": "t3_391vlc", "num_comments": 6, "score": 4, "title": "Can I automatically submit comments to reddit using praw?", "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "Xeran_", "body": "Is there a way to get the sidebar contents of a subreddit to which you don't have moderator access to it using praw? I know one can retrieve the sidebar contents (if you're a mod of the specific sub) using: r.get_settings(SUBREDDITNAME)['description'] I suppose the problem can be worked around it by scraping the http response of the subreddits about page. However, this adds at least one unnecessary dependency for a python module as well as there is most likely a better way of doing it. If it is not possible to retrieve the sidebar as a non-moderator in a normal way using the praw reddit api, then my question would be: why not? Is there some reason behind this?", "created_utc": 1433767559, "gilded": 0, "name": "t3_390uf4", "num_comments": 2, "score": 2, "title": "PRAW retrieve sidebar contents as non-moderator", "url": "https://www.reddit.com/r/redditdev/comments/390uf4/praw_retrieve_sidebar_contents_as_nonmoderator/"}, {"author": "haiguise1", "body": "I've been having some trouble getting OAuth to work without using a browser to get the code (step 3 in the [tutorial](http://praw.readthedocs.org/en/latest/pages/oauth.html)). What I want to do get the url code (for r.get_access_information(url code)) which will then allow me to get an access and refresh token for my script through praw so that it uses OAuth instead of user/password. I have been trying to write some code using the requests library and I can get an access token, but I can't seem to get praw to work with it, and I can't get the url code with which I can get a refresh token and so hopefully let praw use the refresh token to set itself up correctly but I haven't gotten that far yet. So in short, I want to either get a url code or a refresh token without having to open the Allow page in a browser.", "created_utc": 1433465402, "gilded": 0, "name": "t3_38lo61", "num_comments": 5, "score": 6, "title": "PRAW and OAuth", "url": "https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/"}, {"author": "picflute", "body": "Context: I'm looking to archive as much modmail as I can get. Currently with PRAW I am able to get 6,668 ModMail Links using modmail = r.get_mod_mail('leagueoflegends ,params = None,limit = None) Problem is that it's not enough. There's information I wish to access farther back but do not know how to get there. Any advice on how to do it with PRAW or maybe even a basic demo of how to do it in the reddit API?", "created_utc": 1433398237, "gilded": 0, "name": "t3_38hgnu", "num_comments": 9, "score": 3, "title": "How to deep dive into ModMail Archives?", "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "i-am-you", "body": "I want to submit a self post with an empty \"\" body, but praw seems to complain. Here is the specific error: if bool(text) == bool(url): raise TypeError('One (and only one) of text or url is required!')", "created_utc": 1433384271, "gilded": 0, "name": "t3_38gokh", "num_comments": 8, "score": 3, "title": "[PRAW] How to submit empty self post?", "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "obamabot9000", "body": "I have created the ObamaBot (See my post in /r/botwatch), and he is made with Python in PRAW. He is set to search all subreddits for \"Thanks obama,\" (and respond with \"You're welcome!\")but he doesn't respond to all comments. Sometimes he will reply to one, but other times, he won't. He responded to one comment in a random thread, but I never saw him respond to anything else. If I tell him to search /r/botwatch or any other independent subreddit, he works fine. Why is this, and how can I fix it? I am relatively noonish, so bear with me. The full code is available upon request.", "created_utc": 1433091124, "gilded": 0, "name": "t3_37z2ql", "num_comments": 5, "score": 0, "title": "ObamaBot not responding to all posts", "url": "https://www.reddit.com/r/redditdev/comments/37z2ql/obamabot_not_responding_to_all_posts/"}, {"author": "habnpam", "body": "I am able to edit the wiki page just fine if I use the old login `r.login(\"name\", \"pass\")`. But when I try to edit the Wiki page using OAuth, I get this error: `requests.exceptions.HTTPError: 403 Client Error: Forbidden`. I allowed the full scope [from the OAuth tutorial page](http://praw.readthedocs.org/en/latest/pages/oauth.html), and also [from this post](http://www.reddit.com/r/redditdev/comments/1wxkbb/oauth2_wiki_scopes/). When I go to `/prefs/apps/`, it does show that I gave the script permission to edit the wiki. The subreddit I am trying to edit is set to private, but anyone with access should be able to edit the wiki. Here is some of the code(edit: I put in the log-in stuff): r = praw.Reddit(user_agent=\"************* /u/habnpam\") r.set_oauth_app_info(client_id='[client_id]', client_secret='[client_secret]', redirect_uri='http://127.0.0.1:65010/authorize_callback') # the function will return a new 'access_token' refreshed = r.refresh_access_information('[refresh_token]') #\"log in\" with the credentials. r.set_access_credentials(access_token=refreshed['access_token'], refresh_token=refreshed['refresh_token'], scope=refreshed['scope']) wiki = r.get_wiki_page(\"habnpam\", 'index')#This causes the error. new_content = wiki.content_md.encode('utf-8') #this changes the unicode to string...I think. print(new_content) #check point. new_content = new_content + \"\\n\\nOauth initialized GOOD TO GO\" #updating the content. json = r.edit_wiki_page(\"habnpam\", page='', content=new_content, reason='') #making the edit. print(\"Oauth Successful.\")", "created_utc": 1432941079, "gilded": 0, "name": "t3_37s4or", "num_comments": 8, "score": 2, "title": "How can I edit wiki while using OAuth?", "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "gimunu", "body": "Hi guys, I am a teaching assistant in Maths with a good interest in data science. I have been thinking about an illustration for my students and for that I would need data from reddit, basically sampling in a non-biased way the comment karma distribution. I coded a small script yesterday, not using PRAW but primitively reading the comment karma score on the user page and then jumping to a new user taken from a queue. The queue is itself fed on the user page as I extract every possible user name from there. I have a doubt about biases in this sampling, but I can already tell that it is quite inefficient: in a day I only got 30 000 user inputs. Could there be a smarter way of getting access to such information? ps: sorry if this is not the right place to ask this", "created_utc": 1432931292, "gilded": 0, "name": "t3_37riu0", "num_comments": 4, "score": 5, "title": "Scraping comment karma data in an efficient way", "url": "https://www.reddit.com/r/redditdev/comments/37riu0/scraping_comment_karma_data_in_an_efficient_way/"}, {"author": "Day_Old_Pizza", "body": "I get the unexpected redirect from http to https error when I try to login to reddit through my python praw script. I tried to run the following from [this post from /u/bboe](https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/): pip install git+git://github.com/praw-dev/praw.git@master but it gave me an error too: Error [WinError 2] The system cannot find the file specified while executing command git clone -q git://github.com/pra w-dev/praw.git C:\\Users\\<username>\\AppData\\Local\\Temp\\pip-uxo4pebj-build Cannot find command 'git' Please help. Thanks.", "created_utc": 1432905222, "gilded": 0, "name": "t3_37psu6", "num_comments": 2, "score": 1, "title": "New to using praw and pip", "url": "https://www.reddit.com/r/redditdev/comments/37psu6/new_to_using_praw_and_pip/"}, {"author": "1ama", "body": "Hello, I want to get submissions from the domain - *https://www.reddit.com/domain/imgur.com* with Python using PRAW. When I want to get subreddit I use this code: submissions = r.get_subreddit('ampoll').get_hot(limit=5) Is there method to get domain?", "created_utc": 1432880388, "gilded": 0, "name": "t3_37ow7j", "num_comments": 4, "score": 1, "title": "How to get domain instead of get subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/37ow7j/how_to_get_domain_instead_of_get_subreddit/"}, {"author": "bboe", "body": "It would appear that maybe the field could be useful for using a more appealing title than what isn't available in a URL. However, at the moment it appears there is no way through reddit's website to actually make these differ, and when doing it manually through the API there appears to be no visual use of `display_name`? Example where `name` and `display_name` differ: https://api.reddit.com/api/multi/user/PyAPITestUser2/m/praw_32dm76iiu0/", "created_utc": 1432713690, "gilded": 0, "name": "t3_37foul", "num_comments": 1, "score": 7, "title": "Why do multireddit's have a separate `display_name` field?", "url": "https://www.reddit.com/r/redditdev/comments/37foul/why_do_multireddits_have_a_separate_display_name/"}, {"author": "thomasbomb45", "body": "Currently, when you initialize a comment or submission stream (using praw.helpers), it must load up to the limit set or the earliest item is reached before any results are yielded. This is a problem if you don't set a limit and use multiple subreddits in one stream, because it loads 1000 per subreddit. Is there a setting that allows me to access them as they are loaded? I'm guessing this results from those helper functions returning oldest first.", "created_utc": 1432696823, "gilded": 0, "name": "t3_37eyjb", "num_comments": 2, "score": 2, "title": "Praw comment/submission streams", "url": "https://www.reddit.com/r/redditdev/comments/37eyjb/praw_commentsubmission_streams/"}, {"author": "jbw976", "body": "Using praw and inspecting comment objects from the comment_stream function, I see a subreddit_id field for comments such as: 'subreddit_id': u't5_2r8lo' how can i convert that to a subreddit name, such as /r/dragonage? I could not figure this out after a few searches of this subreddit and the internet at large. thanks!", "created_utc": 1432614878, "gilded": 0, "name": "t3_37ah9n", "num_comments": 7, "score": 2, "title": "convert subreddit_id to subreddit name?", "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "AnneBancroftsGhost", "body": "I'm not new to coding, but I am new to PRAW/python/php. Please bear with me. :) I've been wanting to learn how to code/run a reddit bot for fun and for experience. I've been spending this afternoon in that rabbit hole trying to figure it out. So I've been following the instructions here (https://www.reddit.com/r/redditdev/comments/2bc04n/how_to_build_a_reddit_bot_noob_friendly/) to try and make the bot. The basic premise is that I would like someone to be able to call the bot (using the karmalytics service to search and report the call statement to the bot) and then send the standard reply to the parent comment. And also send a second reply to the keyworded comment, letting them know a reply has been sent to the parent comment (though I'm pretty sure I have a handle on this part). I went looking for a bank of keywords so that I could figure it out myself. But I'm stuck. Maybe I'm just asking the wrong questions. Anyway, how would you go about it? Here's the sample code that will reply to the keyworded comment. \"; if(!isset($_POST['redditAlertJson'])){ die('Missing redditAlertJson'); } $redditAlertJson = $_POST['redditAlertJson']; $redditAlert = json_decode($redditAlertJson); if($redditAlert->webhookKey!=$webhookKey){ die('Wrong webhookKey'); } require 'reddit.php'; $reddit = new reddit($redditUsername,$redditPassword); $response = $reddit->addComment($redditAlert->reddit->name,'\u30fd\u0f3c\u0e88\u0644\u035c\u0e88\u0f3d\uff89'); var_dump($response); exit;", "created_utc": 1432588747, "gilded": 0, "name": "t3_378zz9", "num_comments": 4, "score": 3, "title": "Reply to parent comment.", "url": "https://www.reddit.com/r/redditdev/comments/378zz9/reply_to_parent_comment/"}, {"author": "BananaGranola", "body": "My bot suddenly stopped seeing child comments. I check the authors of the child comments to see whether my bot has already responded to a given comment. Given a valid **comment** object, **comment.replies** used to return a list of child comments. As of this morning, > prettyprint(vars(comment)) says, among other things, > '_replies': [] when I know there are child comments. A check of the [json](http://api.reddit.com/r/knitting/comments/36qke2/looking_for_suggestions_for_a_quickish_a_50s/crg8gda) reveals that the replies are showing up in the json returns. They're just not showing up in praw. Did something change? Am I doing something wrong? Code is [here](https://github.com/bananagranola/LinkRav_Bot/blob/master/linkrav_bot.py).", "created_utc": 1432357758, "gilded": 0, "name": "t3_36yk6c", "num_comments": 3, "score": 1, "title": "comment.replies returns empty list", "url": "https://www.reddit.com/r/redditdev/comments/36yk6c/commentreplies_returns_empty_list/"}, {"author": "Fi8_CoC", "body": "I have a script which I run weekly with cron. It makes a post to a subreddit every week, and I would like that post distinguished (yes the bot does have mod permissions). The post is made successfully each time, but I get an error with the distinguish line below. I have tried my best to look at the docs as well as other examples. #!/usr/bin/env python import praw r = praw.Reddit(user_agent=\"My user agent\") r.login(\"USER\", \"PASS\") sub = r.get_subreddit(\"SUBREDDIT\") sub.submit(\"Title of post\", text='''Long winded body of the post''') ## Here is what I thought should make the post distinguished sub.distinguish(as_made_by=u'mod') Any help would be greatly appreciated. Thanks.", "created_utc": 1432335536, "gilded": 0, "name": "t3_36xioo", "num_comments": 3, "score": 2, "title": "[PRAW] How can my weekly autopost script distinguish the post?", "url": "https://www.reddit.com/r/redditdev/comments/36xioo/praw_how_can_my_weekly_autopost_script/"}, {"author": "DarkMio", "body": "Currently I am developing a nice plugin-based bot framework, which I wanted to feed some test-data. Naive as I am, I thought: Great, lets simply pickle some stuff from praw, without any success. I am hitting the recursion limit (setting it higher doesn't resolve the problem ^(or crashes python)). This is what I have done and tried so far: \"\"\"Looks like PRAW doesn't want to get pickle'd. Both single submissions and generators are indeed broken.\"\"\" from praw import Reddit import pickle from sys import setrecursionlimit from time import time from os import listdir def get_session(): return Reddit(user_agent=\"Data Pickler for data training.\") def get_submissions(r, subreddit): return r.get_subreddit(subreddit).get_hot(limit=1) def get_single_submission(generator): for sub in generator: return sub def write_reddit(obj, path): with open(path, \"wb\") as f: pickle.dump(obj, f) def load_reddit(path): with open(path, \"rb\") as f: return pickle.load(f) def single_submission(r): return r.get_submission(url='http://www.reddit.com/r/thebutton/comments/36salf/') if __name__ == \"__main__\": r = get_session() # Get hot: submissions = get_submissions(r, \"dota2\") # Return first of hot: submission = get_single_submission(submissions) # Get a singular submission via URL: single_submission = single_submission(r) carelist = [submissions, submission, single_submission] for pls_pickle in carelist: try: write_reddit(pls_pickle, \"../t_data/{}.pi\".format(int(time()))) except Exception as e: print(\"Writing error: {}\".format(e)) setrecursionlimit(10000) for file in listdir(\"../t_data/\"): try: print(load_reddit(\"../t_data/{}\".format(file))) except Exception as e: print(\"Loading error: {}\".format(e)) yields: Writing error: Can't pickle : attribute lookup generator on builtins failed Loading error: maximum recursion depth exceeded Loading error: maximum recursion depth exceeded This is in Python 3.4.3, I'll edit all errors once it's done executing again (as trying pickling a single loaded submission takes a ton of time. Edit: Just to be clear: I actually mostly care about having some training data to develop offline, while having no internet whatsoever. If there is an alternative way (storing some json and saving it - then reading it with praw, or importing a single stored api-request would work for me too.", "created_utc": 1432278565, "gilded": 0, "name": "t3_36unvz", "num_comments": 2, "score": 1, "title": "Unable to pickle single submissions / comment with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/36unvz/unable_to_pickle_single_submissions_comment_with/"}, {"author": "Agent_HK-47", "body": "In the praw docs there is a section [here](https://praw.readthedocs.org/en/v2.1.21/pages/configuration_files.html#the-sites) on configuring praw. At the end it has this code saying that they can define multiple \"sites\" in the config. However there is no explanation of how the proper \"site\" is selected. Does anyone know more about this topic? How is [bboe] chosen over [reddit_dev]. [bboe] domain: www.reddit.com ssl_domain: ssl.reddit.com user: bboe pswd: this_isn't_my_password [reddit_dev] domain: www.reddit.com ssl_domain: ssl.reddit.com user: someuser pswd: somepass [local_dev1] domain: reddit.local:8000 user: someuser pswd: somepass [local_wacky_dev] domain: reddit.local:8000 user: someuser pswd: somepass api_request_delay: 5.0 default_content_limit: 2", "created_utc": 1432173585, "gilded": 0, "name": "t3_36pfcm", "num_comments": 2, "score": 2, "title": "[PRAW] How is the correct site selected in praw.ini", "url": "https://www.reddit.com/r/redditdev/comments/36pfcm/praw_how_is_the_correct_site_selected_in_prawini/"}, {"author": "Agent_HK-47", "body": "I've looked through the docs and I even grepped through the open source code and I couldn't find the `praw.Reddit()` method. Could anyone tell me where the documentation for this method is?", "created_utc": 1432092392, "gilded": 0, "name": "t3_36kwbq", "num_comments": 2, "score": 1, "title": "[PRAW] Where is the praw.Reddit() method defined?", "url": "https://www.reddit.com/r/redditdev/comments/36kwbq/praw_where_is_the_prawreddit_method_defined/"}, {"author": "ibbignerd", "body": "Hey guys. Very basically I'm trying to check if my bot already replied to a comment. I thought the easiest way to do this would be to check the authors of it's children. Is there a better way to do this? If not, how would I easily get the children authors. I'm using Praw on Python 2.7!", "created_utc": 1432011470, "gilded": 0, "name": "t3_36ggaz", "num_comments": 3, "score": 2, "title": "Getting child comments from parent comment object", "url": "https://www.reddit.com/r/redditdev/comments/36ggaz/getting_child_comments_from_parent_comment_object/"}, {"author": "lizardsrock4", "body": "Is there any way to add a ban reason and length in praw using add_ban(), I only one param for the username Thanks!", "created_utc": 1431823713, "gilded": 0, "name": "t3_367w2j", "num_comments": 2, "score": 1, "title": "[PRAW] Ban reason and length?", "url": "https://www.reddit.com/r/redditdev/comments/367w2j/praw_ban_reason_and_length/"}, {"author": "Arnoyo12", "body": "Hi guys, Sorry to bother you with such a basic issue but I'm a beginner. I'm basically trying to retrieve the max nr (200) of comments per threads I choose. From what I understand you can simply do so by specifying your submission details and associating it with the actual post. However, when I try to print the ouput after iterating over the comments, I get the following: for every comment. this is the code I used: import praw import pprint user_agent = (\"Comment breakdown 1.0 by /u/Arnoyo12\") r = praw.Reddit(user_agent=user_agent) submission = r.get_submission(submission_id='34pp8j') thing_limit = 100 user_name = \"Arnoyo12\" comment = {} for thing in submission.comments: pprint.pprint(thing) Thanks in advance!", "created_utc": 1431514020, "gilded": 0, "name": "t3_35tfe9", "num_comments": 5, "score": 2, "title": "[PRAW] Noob issue while retrieving comments from a thread", "url": "https://www.reddit.com/r/redditdev/comments/35tfe9/praw_noob_issue_while_retrieving_comments_from_a/"}, {"author": "treacherous_tim", "body": "So this is for a command. If someone types the command, I need to take the text of the comment above that command. Any ideas how to do this? I'm pretty new to PRAW. Thanks!", "created_utc": 1431453550, "gilded": 0, "name": "t3_35qi47", "num_comments": 1, "score": 1, "title": "[PRAW] How to reference a comment above", "url": "https://www.reddit.com/r/redditdev/comments/35qi47/praw_how_to_reference_a_comment_above/"}, {"author": "Toofifty", "body": "I'm not too experienced with PRAW, but I know there's a few ways to go about this. I just need to get new comments in one particular thread as fast as possible (within a few seconds). I could use `comment_stream`, `get_comments`, or `submission.comments` but I'm unsure which is the best for my case. Thanks :)", "created_utc": 1431410798, "gilded": 0, "name": "t3_35okwm", "num_comments": 2, "score": 2, "title": "[PRAW] What's the best way to get comments as fast as possible in a single thread/submission?", "url": "https://www.reddit.com/r/redditdev/comments/35okwm/praw_whats_the_best_way_to_get_comments_as_fast/"}, {"author": "opsoyo", "body": "I'm trying my hardest to capture the replies for a comment, process the given replies, then request more replies for that comment as the MoreComments object appears, but I can't for the life of me figure out how to do it. **What I have:** submission = r.get_submission(submission_id=XXXXX) ('XXXXX' is defined elsewhere in file) for c in submission.comments: # Iterate comments within specified post if isinstance(c, praw.objects.MoreComments): # Bypass for prototype continue if comment.body == X: # Iterate replies for specified comment ('X' is defined elsewhere in file) replies = comment.replies # Gives list, which doesn't respond to 'replace_more_comments' replies.replace_more_comments(limit=None, threshold=0) # Returns error of no attribute **What I'm looking to accomplish:** submission = r.get_submission(submission_id='XXXXX') for c in submission.comments: # Iterate comments within specified post if isinstance(c, praw.objects.MoreComments): # Bypass for prototype continue if comment.body == X: # Iterate replies for specified comment # for reply in replies # try: # Mess with first reply # Mess with second reply # Mess with hundredth reply # except AttributeError: # Manipulate MoreComments when done with already given replies And, I'm really trying to not use 'replace_more_comments' where I don't *have* to. I'd like to keep it isolated to running it on that particular comment to get its replies when it comes time. Possible? Am I asking too much? Am I flat wrong? Please, any help is appreciated! :)", "created_utc": 1431231984, "gilded": 0, "name": "t3_35got2", "num_comments": 8, "score": 1, "title": "How to use replace_more_comments with replies in PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "hizinfiz", "body": "Hello, I have python, pip, praw, and six installed on my Macbook but when I try to `import praw` in Python 3.4.3, I get Traceback (most recent call last): File \"\", line 1, in ImportError: No module named 'praw' Interestingly enough, I'm able to import it just fine in Python 2.7.7 (OS X default install of Python) Software: * OS X 10.10.3 * Python 2.7.7 * Python 3.4.3 * pip 6.1.1 * praw 2.1.21 * six 1.9.0", "created_utc": 1431052945, "gilded": 0, "name": "t3_358zpt", "num_comments": 4, "score": 3, "title": "Having trouble getting PRAW to work with Python 3", "url": "https://www.reddit.com/r/redditdev/comments/358zpt/having_trouble_getting_praw_to_work_with_python_3/"}, {"author": "SpotiList", "body": "I probably have a skewed view of oauth, as I'm pretty new to all this, but is there a reason I shouldn't put my refresh token in praw.ini? I have a bot that isn't on 24/7 and only ever uses one reddit account. I wanted it to be able to start up and just do: import praw r = praw.Reddit('UA-string') r.refresh_access_information() and go about my business. It was just a couple of lines changed in \\_\\_init__.py to make this work, but I'm wondering if there's a reason the functionality isn't in there to begin with. Thanks.", "created_utc": 1431047037, "gilded": 0, "name": "t3_358osw", "num_comments": 3, "score": 3, "title": "refresh_token and praw.ini", "url": "https://www.reddit.com/r/redditdev/comments/358osw/refresh_token_and_prawini/"}, {"author": "redalastor", "body": "I'd like to know which praw or requests exception I can get when reddit is busy or down so I can have those requests tried again a few seconds later.", "created_utc": 1431045517, "gilded": 0, "name": "t3_358lu6", "num_comments": 3, "score": 1, "title": "What exception may I expect when reddit is \"busy\"", "url": "https://www.reddit.com/r/redditdev/comments/358lu6/what_exception_may_i_expect_when_reddit_is_busy/"}, {"author": "WheelsOnPavement", "body": "Not sure why this has suddenly started happening... I'm fairly new to bot creation, and am, quite frankly, stumped. It was working totally fine, but now every time I try to post a reply it gives me a 403. I don't have the word \"bot\" inside of the user_agent, and I've been grabbing posts at a very low rate. The code is a mishmash of some tutorials and (mostly) my own writing. I am 90% positive it's a 403- all other bots that I've tried making return the same result as of now. Similarly, there was no difference when attempting to run the script on another machine. I'm hoping somebody can shed some light on the situation... I'd hate to think I was put on some sort of a spam blacklist. Maybe I made a mistake and am missing something? Here's the bot that --was-- working: import praw import time import os import sqlite3 sql = sqlite3.connect('sql.db') cur = sql.cursor() print (\"Logging in...\") r = praw.Reddit(user_agent=\"BB by /u/WheelsOnPavement\") r.login(\"-----\", \"-----\") print (\"Sucessfully logged in!\") time.sleep(2) print(\"Writing SQL...\") cur.execute('CREATE TABLE IF NOT EXISTS oldposts(id TEXT)') sql.commit() print(\"SQL written successfully!\") time.sleep(2) os.system('cls') words_to_match = ['bagels', 'bagel', 'cream cheese', 'cream-cheese'] postLimit = 50 def run_bot(): subreddit = r.get_subreddit(\"-----\", \"bottest3000\") print(\"Fetching comments...\") comments = subreddit.get_comments(limit=postLimit) print(\"Successfully fetched comments!\") print(\"\") for comment in comments: comment_text = comment.body.lower() cur.execute('SELECT * FROM oldposts WHERE ID=?', [comment.id]) try: comment.id = comment.author.name except AttributeError: print('Comment has been previously deleted') continue if cur.fetchone(): print(\"Comment already in database\") continue elif comment.author.name == '-----': print(\"Not replying to posts already written by -------\") continue elif any(string in comment_text for string in words_to_match): try: print('Replying to %s by %s' % (comment.id, comment.author)) comment.reply('You have summoned the bagel gods! Bask in their glory!') cur.execute('INSERT INTO oldposts VALUES(?)', [comment.id]) except praw.requests.exceptions.HTTPError as e: if e.response.status_code == 403: print('403 FORBIDDEN - is the bot banned from %s?' % comment.subreddit.display_name) continue elif e.response.status_code == 503: print('503 Service Unavailable') continue else: print('Unknown service error %s' % e.response.status.code) continue while True: run_bot() time.sleep(10)", "created_utc": 1431039640, "gilded": 0, "name": "t3_358amz", "num_comments": 7, "score": 2, "title": "Suddenly getting a 403 when attempting to use PRAW to reply to posts?", "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "GoldenSights", "body": "According to the api docs, the [get unread](http://www.reddit.com/dev/api#GET_message_unread) url has a parameter called \"mark\" that should clear my orangereds, on [this line](https://github.com/reddit/reddit/blob/98ddd8cd065290e6448f1ae74c7092e28644f613/r2/r2/controllers/listingcontroller.py#L1164) I think. However, the PRAW parameter in get_unread that's supposed to trigger this doesn't seem to work. I tried it using the request client Postman and [it didn't work either](http://i.imgur.com/ZuXR4mP.png) (I also tried using GET with url parameters instead of post data). I toggled the \"mark messages as read when I open my inbox\" preference multiple times, but the unreads never go away until I visit them in my browser. Am I missing something, or is this a bug? I'm probably missing something. Thanks!", "created_utc": 1430870319, "gilded": 0, "name": "t3_3502mg", "num_comments": 2, "score": 3, "title": "/message/unread/ does not mark my mail as read even with the \"mark\" parameter.", "url": "https://www.reddit.com/r/redditdev/comments/3502mg/messageunread_does_not_mark_my_mail_as_read_even/"}, {"author": "jordanzzz", "body": "I'm working with PRAW right now in Python trying to find comments that match a search term so what I'm looking for is any comment that has @Search: searchterm So right now its looking for any comment with @Search: in it, however I want to strip the comment so it removes anything before the @Search: and anything after searchterm. Any clues to help me here? Perhaps this is more of a python issue, but maybe there is soemthing within PRAW that can help. def findSummoner(): print(\"Fetching subreddit\") subreddit = r.get_subreddit(SUBREDDIT) print(\"Fetching comments\") comments = subreddit.get_comments(limit=MAXPOSTS) for comment in comments: try: cbody = comment.body.lower() if any(key.lower() in cbody for key in SETPHRASES): print(\"replying to someone\") comment.reply(cbody) else: print(\"nothing found...\") except AttributeError: pass findSummoner() This is working as expected as far as replying, but I need to only get the first word after @Search appears in the comment. =/", "created_utc": 1430813040, "gilded": 0, "name": "t3_34x4ri", "num_comments": 3, "score": 1, "title": "Parsing comments?", "url": "https://www.reddit.com/r/redditdev/comments/34x4ri/parsing_comments/"}, {"author": "demonlordghirahim", "body": "I got pip via \"sudo easy_install pip\" and now I'm trying to install praw but get thrown an exception and it doesn't work. This is the error I'm getting > Exception: Traceback (most recent call last): File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/basecommand.py\", line 246, in main status = self.run(options, args) File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/commands/install.py\", line 352, in run root=options.root_path, File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/req/req_set.py\", line 693, in install **kwargs File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/req/req_install.py\", line 817, in install self.move_wheel_files(self.source_dir, root=root) File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/req/req_install.py\", line 1018, in move_wheel_files isolated=self.isolated, File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/wheel.py\", line 237, in move_wheel_files clobber(source, lib_dir, True) File \"/Library/Python/2.7/site-packages/pip-6.1.1-py2.7.egg/pip/wheel.py\", line 208, in clobber os.makedirs(destdir) File \"/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/os.py\", line 157, in makedirs mkdir(name, mode) OSError: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/requests' I think it's because I'm running python 3.4 and pip doesn't work with that I guess? Is there a version of pip that runs with python 3.4 or is there another problem?' edit: this is the last thing written to the terminal before it errors out > Requirement already satisfied (use --upgrade to upgrade): six>=1.4 in /System/Library/Frameworks/Python.framework/Versions/2.7/Extras/lib/python (from praw) Installing collected packages: requests, update-checker, praw ----------------- NEVERMIND. apparently uninstalling it and using the file get-pip.py on [this page](https://pip.pypa.io/en/latest/installing.html) worked better. Although I'm now a littler nervous I fucked up my computer somehow using sudo since I know that's kind of a bad thing to do am I okay?", "created_utc": 1430794789, "gilded": 0, "name": "t3_34wfsa", "num_comments": 18, "score": 1, "title": "Trouble installing PRAW via pip", "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "BullpenInc", "body": "Is it possible to create an account through the API? Searching I found that PRAW seems to support create_redditor but I couldn't find anything in the API documentation (pretty important to be able to do that to streamline the experience for non-experienced new users of reddit)", "created_utc": 1430729632, "gilded": 0, "name": "t3_34t0df", "num_comments": 9, "score": 1, "title": "Creating account through API?", "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "hargikas", "body": "I am trying to write a small program with PRAW, and I am searching and posting to reddit greek content. I have a function: def find_if_posted(usr, title, url): for content in usr.search(title): return True for content in usr.search(url): return True return False which tries to search if there is a post with the same title or url. When it starts to search with the url I am getting the error: Traceback (most recent call last): File \"C:\\Users\\hargikas\\Dropbox\\Private\\src\\reddit\\auto_poster\\leftist.py\", line 133, in while (key is None) or find_if_posted(usr, key, content[key]): File \"C:\\Users\\hargikas\\Dropbox\\Private\\src\\reddit\\auto_poster\\leftist.py\", line 86, in find_if_posted for content in usr.search(url): File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 1108, in search **kwargs): File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 515, in get_content root = page_data.get(root_field, page_data) AttributeError: 'str' object has no attribute 'get' Also when I am trying to post something (using the submit function) I get this error: Traceback (most recent call last): File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 372, in _make_request httplib_response = conn.getresponse(buffering=True) TypeError: getresponse() got an unexpected keyword argument 'buffering' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 374, in _make_request httplib_response = conn.getresponse() File \"C:\\Python34\\lib\\http\\client.py\", line 1162, in getresponse raise ResponseNotReady(self.__state) http.client.ResponseNotReady: Request-sent During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"C:\\Python34\\lib\\site-packages\\requests\\adapters.py\", line 370, in send timeout=timeout File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 597, in urlopen _stacktrace=sys.exc_info()[2]) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\", line 245, in increment raise six.reraise(type(error), error, _stacktrace) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\packages\\six.py\", line 309, in reraise raise value.with_traceback(tb) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"C:\\Python34\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 374, in _make_request httplib_response = conn.getresponse() File \"C:\\Python34\\lib\\http\\client.py\", line 1162, in getresponse raise ResponseNotReady(self.__state) requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',)) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"C:\\Users\\hargikas\\Dropbox\\Private\\src\\reddit\\auto_poster\\leftist.py\", line 140, in usr.submit('greekreddit' , key, url=content[key]) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 338, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 237, in wrapped return function(obj, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 2218, in submit retry_on_error=False) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 561, in request_json retry_on_error=retry_on_error) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 402, in _request response = handle_redirect() File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 375, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\handlers.py\", line 144, in wrapped result = function(cls, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\handlers.py\", line 99, in request allow_redirects=False) File \"C:\\Python34\\lib\\site-packages\\requests\\sessions.py\", line 573, in send r = adapter.send(request, **kwargs) File \"C:\\Python34\\lib\\site-packages\\requests\\adapters.py\", line 415, in send raise ConnectionError(err, request=request) requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',)) Do you have an idea what I am doing wrong? I am using Python 3.4.3 (v3.4.3:9b73f1c3e601, Feb 24 2015, 22:43:06) [MSC v.1600 32 bit (Intel)] on win32 and PRAW Version: 2.1.21 Thank you in advance! Harry", "created_utc": 1430636733, "gilded": 0, "name": "t3_34p5i0", "num_comments": 0, "score": 3, "title": "Problems using PRAW (searching and posting)", "url": "https://www.reddit.com/r/redditdev/comments/34p5i0/problems_using_praw_searching_and_posting/"}, {"author": "manyQuestsSendHelp", "body": "I saw in the PRAW documentation that it's temporary (ref: http://praw.readthedocs.org/en/latest/pages/exceptions.html), but it's been happening all day. Anybody else getting 502 errors? Or should I worry?", "created_utc": 1430428598, "gilded": 0, "name": "t3_34g369", "num_comments": 0, "score": 1, "title": "Anyone else getting 502 errors with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/34g369/anyone_else_getting_502_errors_with_praw/"}, {"author": "Mackan90095", "body": "Hi! I'm messing around with PRAW trying to get a random submission from a NSFW subreddit. The method get_random_submission(subreddit=\"aww\") works. But if I put the subreddit as \"NSFW\" it doesn't work, it just errors like so; Traceback (most recent call last): File \"redd.py\", line 4, in link = r.get_random_submission(subreddit=sys.argv[1]) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 940, in get_random_submission raise errors.ClientException('Expected exception not raised.') praw.errors.ClientException: Expected exception not raised. Why is this? Thanks \\- Mackan", "created_utc": 1430300261, "gilded": 0, "name": "t3_349c0d", "num_comments": 2, "score": 4, "title": "[PRAW] Can't get NSFW submission", "url": "https://www.reddit.com/r/redditdev/comments/349c0d/praw_cant_get_nsfw_submission/"}, {"author": "slack101", "body": "Hi, I've been following [praw's API documentation](https://praw.readthedocs.org/en/PRAW-1.0.9/praw.html), where it mentions that the subreddit's get_comments function should be able to take a \"sort\" argument. Whenever I try the following code, I get an exception: Code: user_agent = \"Foo bot 1.0 by /u/slack101\" r = praw.Reddit(user_agent=user_agent) subredditObj = r.get_subreddit(subreddit) comments = subredditObj.get_comments(sort='top',limit=None) Exception: TypeError: get_content() got an unexpected keyword argument 'sort' Any idea what could be going wrong?", "created_utc": 1430235605, "gilded": 0, "name": "t3_345x51", "num_comments": 1, "score": 1, "title": "praw: Subreddit.get_comments does not take \"sort\" keyword argument", "url": "https://www.reddit.com/r/redditdev/comments/345x51/praw_subredditget_comments_does_not_take_sort/"}, {"author": "Taph", "body": "I've written some (very) rough code for a bot that will take a reddit URL from my clipboard, find the post on reddit, and then make a post on my own subreddit that consists of the post title and link. Basically a cross-post bot. It works, but I'm getting a *ton* of errors when I run it. The last time I ran the bot the results were this: Logging in ... Bot successfully loged in. Captcha URL: http://www.reddit.com/captcha/Q16JkZEiFATLjgNvmwFucHatPqBTxN77.png Captcha: VOGKTR Traceback (most recent call last): File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 372, in _make_request httplib_response = conn.getresponse(buffering=True) TypeError: getresponse() got an unexpected keyword argument 'buffering' During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 374, in _make_request httplib_response = conn.getresponse() File \"/usr/lib/python3.4/http/client.py\", line 1139, in getresponse raise ResponseNotReady(self.__state) http.client.ResponseNotReady: Request-sent During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/adapters.py\", line 370, in send timeout=timeout File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 597, in urlopen _stacktrace=sys.exc_info()[2]) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/util/retry.py\", line 245, in increment raise six.reraise(type(error), error, _stacktrace) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/packages/six.py\", line 309, in reraise raise value.with_traceback(tb) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 544, in urlopen body=body, headers=headers) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/packages/urllib3/connectionpool.py\", line 374, in _make_request httplib_response = conn.getresponse() File \"/usr/lib/python3.4/http/client.py\", line 1139, in getresponse raise ResponseNotReady(self.__state) requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', ResponseNotReady('Request-sent',)) During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"archive_it.py\", line 72, in main() File \"archive_it.py\", line 65, in main make_post() File \"archive_it.py\", line 59, in make_post r.submit('taphs_archive', title, url=link) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/decorators.py\", line 338, in wrapped return function(cls, *args, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/decorators.py\", line 237, in wrapped return function(obj, *args, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/__init__.py\", line 2218, in submit retry_on_error=False) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/__init__.py\", line 561, in request_json retry_on_error=retry_on_error) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/__init__.py\", line 402, in _request response = handle_redirect() File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/__init__.py\", line 375, in handle_redirect timeout=timeout, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/handlers.py\", line 144, in wrapped result = function(cls, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/praw/handlers.py\", line 99, in request allow_redirects=False) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/sessions.py\", line 573, in send r = adapter.send(request, **kwargs) File \"/home/taph/.virtualenvs/taphs_archiver/lib/python3.4/site-packages/requests/adapters.py\", line 415, in send raise ConnectionError(err, request=request) requests.exceptions.ConnectionError: ('Connection aborted.', ResponseNotReady('Request-sent',)) Google hasn't turned up anything specific that I may be doing wrong to get these errors, and since the bot posts as it should (aside from not printing \"Post successful\" as it should after the post is made) I'm not really sure why I'm getting the errors. Here's the bot code: import praw, pyperclip r = praw.Reddit(user_agent = \"Reddit Archiving Manager by /u/Taph\") def reddit_login(): with open(\"credentials.txt\", \"r\") as credentials: text = credentials.read() text = text.split() username, password = text r.login(username, password) if r.is_logged_in: print(\"Bot successfully logged in.\\n\") def post_data(): post_data = [] permalink = pyperclip.paste() post = r.get_submission(permalink) title = post.title link = post.permalink post_data = title, link return post_data def make_post(): title, link = post_data() r.submit('taphs_archive', title, url=link) def main(): print(\"\\nLogging in ...\") reddit_login() make_post() print(\"Post successful.\") if __name__ == \"__main__\": main() I'm sure this is something simple, but I don't have a clue.", "created_utc": 1430072378, "gilded": 0, "name": "t3_33xwym", "num_comments": 5, "score": 2, "title": "PRAW post submit problem", "url": "https://www.reddit.com/r/redditdev/comments/33xwym/praw_post_submit_problem/"}, {"author": "sunbolts", "body": "I'm using PRAW to make a bot which I run from command prompt, and I want to make sure the rate limit usage isn't going over the maximum allowed amount. It says at [this link](https://github.com/reddit/reddit/wiki/API): Make no more than thirty requests per minute. This allows some burstiness to your requests, but keep it sane. On average, we should see no more than one request every two seconds from you. Monitor the following response headers to ensure that you're not exceeding the limits: - X-Ratelimit-Used: Approximate number of requests used in this period - X-Ratelimit-Remaining: Approximate number of requests left to use - X-Ratelimit-Reset: Approximate number of seconds to end of period How can I view these Ratelimit response headers?", "created_utc": 1430017396, "gilded": 0, "name": "t3_33vuok", "num_comments": 9, "score": 5, "title": "How do you check the RateLimit usage?", "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "JustLTU", "body": "I'm a very new coder, both with praw and python in general. I tried to make a very simple bot, just for a learning experience. The bot simply scans through the comment stream, and if it finds a keyword it sends a certain reply. The problem is that it started replying to itself, is there any way to make it ignore certain users (like itself)?", "created_utc": 1429961299, "gilded": 0, "name": "t3_33t71g", "num_comments": 4, "score": 3, "title": "[PRAW] How to make a bot ignore certain user's comments?", "url": "https://www.reddit.com/r/redditdev/comments/33t71g/praw_how_to_make_a_bot_ignore_certain_users/"}, {"author": "GoldenSights", "body": "I'm working on a pull request for PRAW that adds multireddits, or at least the basics. However, all of my DELETE requests are giving me 403 \"please sign in\" errors. http://i.imgur.com/pp4tg9B.png http://i.imgur.com/bGPrgrb.png http://www.reddit.com/dev/api/oauth#DELETE_api_multi_{multipath}_r_{srname} Clearly, post and put both work just fine, but delete fails even though it has the same modhash as the others. I tried watching what Chrome does when deleting a subreddit, but it doesn't seem to have any special headers. Overall, I've found the Multireddit api to be kinda inconsistent and disjointed from everything else. Multireddits and /api/v1/me/friends/ are the only things to use a Delete method, everything else uses a Post to a specific deletion url. Is there any reason for this, and is it related to my problem? Any help is appreciated. Edit: [Pull request Part 1](https://github.com/praw-dev/praw/pull/404).", "created_utc": 1429925483, "gilded": 0, "name": "t3_33rz9o", "num_comments": 2, "score": 7, "title": "All DELETE requests are returning 403 \"Please sign in to do that\"", "url": "https://www.reddit.com/r/redditdev/comments/33rz9o/all_delete_requests_are_returning_403_please_sign/"}, {"author": "heyitzaustin", "body": "I'm trying to make a script that parses through reddit comments in a subreddit. This requires me to get a comment tree for each submissions, which contains instances of both comment and more_comment objects. I want to be able to flatten the tree so I can just get the comments (i don't care if their replies or not), but when I use (submission.replace_more_comments), it slows the script down like crazy(probably since according to the documentation each replacement requires one API request.) Is there away to avoid using this API call for what I want to do? It could be the difference for my script running for 15 minutes vs it running for 40 minutes. Here's the part of my code: for x in submissions: submission = r.get_submission(submission_id=x.id) submission.replace_more_comments(limit=16, threshold=10) flat_comments = praw.helpers.flatten_tree(submission.comments) for comments in flat_comments: #blah blah blah", "created_utc": 1429830457, "gilded": 0, "name": "t3_33nfe0", "num_comments": 2, "score": 1, "title": "[PRAW]replace_more_comments is bottlenecking my script!", "url": "https://www.reddit.com/r/redditdev/comments/33nfe0/prawreplace_more_comments_is_bottlenecking_my/"}, {"author": "seriouslulz", "body": "I keep getting 429 errors even though I'm using praw. I tried to set the delay to 10s but no luck.", "created_utc": 1429812839, "gilded": 0, "name": "t3_33mcom", "num_comments": 0, "score": 1, "title": "How exactly does rate limiting work for GAE apps?", "url": "https://www.reddit.com/r/redditdev/comments/33mcom/how_exactly_does_rate_limiting_work_for_gae_apps/"}, {"author": "Lark_vi_Britannia", "body": "I'm running this script: https://github.com/SwedishBotMafia/RScanBot.Gen/ And it won't allow me to log-on. I've tried over scripts as well and they all return the same thing: a JSON error. Here is the error I get for this bot: http://i.imgur.com/Fr886Um.png I've also tried this bot as well: http://www.reddit.com/r/botrequests/comments/1qqbmu/a_bot_that_says_if_you_know_what_i_mean_every/cdwf6z5 And this is the error I get: http://i.imgur.com/LIbj6Vm.png I've spent the last 3-4 hours googling trying to figure out how to solve this, but no solutions have worked for me so far. I'm using Python2.7 and all of my pip stuff is updated to the latest versions. I'm running the script inside of the Kivy extracted folder. When running out of the Python2.7 Scripts folder, the second script gives me the same error. I can't figure out how to get Kivy installed in the original Python2.7 folder, so I can't run the first one. Anyone have any ideas on how to fix this? TL;DR: Basically, it won't allow me to login to reddit via PRAW, any requests are returned with \"No JSON object could be decoded\"", "created_utc": 1429791412, "gilded": 0, "name": "t3_33l4jt", "num_comments": 30, "score": 5, "title": "Using PRAW and getting a JSON error: \"No JSON Object could be decoded\"", "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "pandoraparadox", "body": "For an analysis project I want to analyse the comments of posts that contain specific keyword(s). In specific I would need the comment: text, time, up down (ratio). And of the post I would need the comment count, and up & down vote ratio. Is this possible with Python Praw? I know there is a cache limit of 1000. Would this imply that topics over 1000 comments need to be split up? Thank You Very Much!", "created_utc": 1429652539, "gilded": 0, "name": "t3_33eill", "num_comments": 4, "score": 1, "title": "PRAW need comments' text, time, upvotes for topics of a certain keyword", "url": "https://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/"}, {"author": "TheEnigmaBlade", "body": "I'm trying to access a mod-only wiki page (bot config) from a praw-based bot using oauth, but the API always returns a 403. The bot's account has proper permissions on reddit. To summarize everything I've tried: * Can access page from the web client with the bot's account * Can access page using praw and old auth method * Can access moderated subreddits * Can access public wiki pages + page list * Can't access public wiki pages in private subreddit (to which it has approved submitter access) * Can't access any mod-only wiki pages (403) * Using every single oauth scope doesn't work I'm out of ideas.", "created_utc": 1429600500, "gilded": 0, "name": "t3_33bx3j", "num_comments": 0, "score": 5, "title": "Access mod-only wiki pages through oauth", "url": "https://www.reddit.com/r/redditdev/comments/33bx3j/access_modonly_wiki_pages_through_oauth/"}, {"author": "Amablue", "body": "I'm trying to scan over all the comments in a thread, and according to various threads and documentation I'm just supposed to need to call `replace_more_comments()` on the submission I'm scanning, but that appears to do nothing at all. Here's a test script to demonstrate: r = praw.Reddit('Test script by /u/amablue') permalink = 'http://www.reddit.com/comments/333jjs/' def print_comments(comment): print comment.body for child in comment.replies: print_comments(child) post = r.get_submission(permalink) post.replace_more_comments(limit=None, threshold=0) for comments in post.comments: print_comments(comments) The output of that is the following: one two three four five six seven eight nine ten If you look at the [test thread](http://www.reddit.com/r/amablue/comments/333jjs/test_post_please_ignore/) though I counted all the way up to thirteen, not ten. Why isn't it printing the whole thing?", "created_utc": 1429418306, "gilded": 0, "name": "t3_333k7r", "num_comments": 6, "score": 1, "title": "replace_more_comments doesn't seem to do anything", "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "habnpam", "body": "So this is the code I ran: r = praw.Reddit(\"/u/habnpam sflkajsfowifjsdlkfj test test test\") for c in praw.helpers.comment_stream(reddit_session=r, subreddit=\"helpmefind\", limit=500, verbosity=1): print(c.author) --- From what I understand, comment_stream() gets the most recent comments. So if we specify the limit to be 100, it will initially get the 100 newest comment, and then constantly update to get new comments. It seems to works appropriately for every subreddit except /r/helpmefind. For /r/helpmefind, it fetches around 30 comments, regardless of the limit.", "created_utc": 1429263348, "gilded": 0, "name": "t3_32wnhw", "num_comments": 10, "score": 2, "title": "[PRAW] comment_stream() messes up when getting comments from a certain subreddit.", "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "scarface1993", "body": "I just want to know how to get all the comments to a post using PRAW. I looked at the PRAW documentation, but it just showed how to do it with the comment id number. Also is it possible to get the comment id number, the time stamp and the author for the comments? Thank you", "created_utc": 1429150742, "gilded": 0, "name": "t3_32rc86", "num_comments": 3, "score": 3, "title": "[PRAW] How do I get the comments to a particular post?", "url": "https://www.reddit.com/r/redditdev/comments/32rc86/praw_how_do_i_get_the_comments_to_a_particular/"}, {"author": "gschizas", "body": "I'm converting a semi-personal mod helper site to use OAuth (mostly to stop it from being semi-personal and make it semi-public), and I seem to be unable to use the modqueue function with OAuth2 (it was definitely working with username and password login). I've been using the praw3 branch (praw-3.0a1), because I was using the username and password login system, and my account is SSL-only, so I was getting an \"unexpected redirect\" on login, but this happens in 2.1.21 (the version on PyPI) as well. I think that the required scope ('read', according to [reddit's API documentation](https://www.reddit.com/dev/api/oauth#scope_read)) is missing somewhere (in PRAW), but I have no idea where.", "created_utc": 1428584696, "gilded": 0, "name": "t3_31zs3a", "num_comments": 3, "score": 1, "title": "PRAW: get_mod_queue doesn't work with OAuth2?", "url": "https://www.reddit.com/r/redditdev/comments/31zs3a/praw_get_mod_queue_doesnt_work_with_oauth2/"}, {"author": "zzpza", "body": "To set expectations from the beginning, I'm a n00b at python, PRAW, and have never used OAuth before. Is the migration to OAuth a necessity for me? All my scripts are mod assistance scripts that all run from the same account. I have no web apps that use other user logins. I've read the guide here: http://praw.readthedocs.org/en/latest/pages/oauth.html My PRAW environment is on a linux server on the net somewhere. It's a text only environment. I could setup an X server for the browser section, but it looks like you just need part of the URL. Can't I just `print url` instead of opening it in a browser and copying the necessary parts? With regards to scopes, my scripts cover several scopes within each script. Is there a super user scope that does everything? Or can I request multiple scopes at the same time? Or do I have to switch scopes for each command? If so, how? Would changing scope be the same as changing user? TIA! :)", "created_utc": 1428569385, "gilded": 0, "name": "t3_31z8g6", "num_comments": 4, "score": 4, "title": "Questions regarding converting from user/pass to OAuth with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/31z8g6/questions_regarding_converting_from_userpass_to/"}, {"author": "MrFanzyPanz", "body": "Hey everybody! I'm part of a research team studying Reddit. We've been collecting and analyzing data for the past 2 years using PRAW. We have a server that is constantly collecting data from 2013, and it's been hitting server errors for the past week or so. The error is requests.exceptions.HTTPError: 500 Server Error: Server Error We've tried using a VPN to see if this was caused by our IP, but it seems that it's not IP related. I tried using PRAW in the console to manually pull just one submission to see if it's a problem with our script, and even that one pull didn't work. The code I used was: import praw r = praw.Reddit(user_agent='MrFanzyPanz miscellanious data collector for research') r.get_info(thing_id='t3_1km4e2') I have the latest version of PRAW and am operating in Python 2.7. Can anybody help me figure this out? It seems like this is an internal problem with PRAW.", "created_utc": 1428520893, "gilded": 0, "name": "t3_31wpmp", "num_comments": 5, "score": 7, "title": "PRAW suddenly returning endless HTTP errors", "url": "https://www.reddit.com/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/"}, {"author": "TomSparkLabs", "body": "Hi! I'm making a comment reply bot, (for my own subreddit) but I don't want to have it get downvotes, so if a comment is incorrect, that the bot's karma won't be affected a lot. Here's what I have so far: from time import sleep import praw r = praw.Reddit(user_agent='Downvoted comments remover 0.1 by /u/TomSparkLabs') r.login(username='Bot', password=' ') user = r.get_redditor('Bot') while True: for comment in user.get_comments(limit=20): if comment.score", "created_utc": 1428450084, "gilded": 0, "name": "t3_31t6py", "num_comments": 5, "score": 1, "title": "Auto-delete downvoted bot's comments", "url": "https://www.reddit.com/r/redditdev/comments/31t6py/autodelete_downvoted_bots_comments/"}, {"author": "gswhoops22", "body": "Hey everyone this is my first time using PRAW and I was looking for a little bit of help. I'm doing some basic comment scraping using: submission = r.get_submission(submission_id=2ul5vi, comment_limit=100 ,comment_sort='new') and then accessing comments through submission.comments my only issue is that while printing these comments all of the longer comments just get cut off and get replaced by \"...\" for example one of the comments printed as : \"Kidd using similar schemes from last year's playoffs - going small with Dudle...\" is there any parameter I can set that refers to the character length of comments? Thanks for any help and sorry if my formatting is crappy lol", "created_utc": 1428386549, "gilded": 0, "name": "t3_31pxl5", "num_comments": 2, "score": 1, "title": "PRAW: Get comments without character limit", "url": "https://www.reddit.com/r/redditdev/comments/31pxl5/praw_get_comments_without_character_limit/"}, {"author": "jar_jar_binks", "body": "I'm trying to return all top-level comments in order: import praw r = praw.Reddit('my subreddit comment listing') r.login('xxxxxxxx', 'xxxxxxxxx') submission = r.get_submission(submission_id='xxxxxxx/yaddayadda/?sort=new&limit=1500') submission.replace_more_comments(limit=None, threshold=0) forest_comments = submission.comments i=0 for comment in forest_comments: print(str(i) + '|' + str(comment.id) + '|' + str(comment.author)) i += 1 This seems to grab all top-level comments, but the order is all messed up after the first few paginations. The final (2300 or so) printed comment is not the oldest comment. Any thoughts?", "created_utc": 1428378594, "gilded": 0, "name": "t3_31pkst", "num_comments": 0, "score": 1, "title": "PRAW: return all comments in order?", "url": "https://www.reddit.com/r/redditdev/comments/31pkst/praw_return_all_comments_in_order/"}, {"author": "Hook3d", "body": "Hi everyone, here is the most recent version of a script/bot I have written to check a common grammatical error: BOT_CONSTANTS.py: __author__ = '/u/Hook3d' #login credentials LOGIN_NAME = \"GrammarBotv1\" LOGIN_PW = \"redacted\" #API rule constants THING_LIMIT = 25 #(sub)reddit constants CURR_SUBREDDIT = \"funny\" #Posting constants MESSAGE_TO_POST = \"One common English error is the incorrect usage of the preposition \\'of\\' in the place of the verb \\'have\\'. For instance:\\n\\n \" \\ \"\\'I could of\\' should be \\'I could have\\', which contracts to \\'I could've.\\'\\n\\n\" \\ \"\\'I should of\\' should be \\'I should have\\', which contracts to \\'I should've.\\'\\n\\n\" \\ \"\\'I would of\\' should be \\'I would have\\', which contracts to \\'I would've.\\'\\n\\n\" \\ \"bot by /u/Hook3d\" SUBS_TO_CRAWL = [\"nba\", \"sports\", \"soccer\", \"videos\", \"gaming\", \"askreddit\", \"news\", \"test\", \"all\", \"funny\", \"pics\", \"gaming\", \"ProgrammerHumor\", \"aww\"] GrammarBot.py: __author__ = '/u/Hook3d' import praw, time from BOT_CONSTANTS import * from pprint import pprint user_agent = \"Simple grammar script v1 by /u/Hook3d\" common_errors = [\"would of \", \"could of \", \"should of \"] completed_submissions = [] reddit = praw.Reddit(user_agent = user_agent) #create reddit object with praw call reddit.login(LOGIN_NAME, LOGIN_PW) #login with bot credentials def has_error(errors, comment): for error in errors: if comment.find(error) != -1: return True return False #main loop while True: for curr_sub in SUBS_TO_CRAWL: subreddit = reddit.get_subreddit(curr_sub) #grab the sub i = 1 for submission in subreddit.get_hot(): #loop over the submissions if submission in completed_submissions: continue completed_submissions.append(submission) print(\"on the \" + str(i) + \" submission\") i = i + 1 #submission.replace_more_comments() flat_comments = praw.helpers.flatten_tree(submission.comments) #grab the comments for comment in flat_comments: if isinstance(comment, praw.objects.Comment): comment_author = comment.author comment_text = comment.body.lower() #grab submission text print(comment_text) if has_error(common_errors, comment_text) and comment_author.name != LOGIN_NAME: #don't reply to own comments comment.reply(MESSAGE_TO_POST) time.sleep(60) Does anyone have any suggestions for improving this script/bot? Edit: Edited GrammarBot.py to use a binary tree. The linear search was getting a bit strong. __author__ = '/u/Hook3d' import praw, time from bintrees import BinaryTree from BOT_CONSTANTS import * from pprint import pprint user_agent = \"Simple grammar script v1 by /u/Hook3d\" common_errors = [\"would of \", \"could of \", \"should of \"] completed_submissions = BinaryTree() reddit = praw.Reddit(user_agent = user_agent) #create reddit object with praw call reddit.login(LOGIN_NAME, LOGIN_PW) #login with bot credentials def has_error(errors, comment): for error in errors: if comment.find(error) != -1: return True return False #main loop while True: for curr_sub in SUBS_TO_CRAWL: subreddit = reddit.get_subreddit(curr_sub) #grab the sub i = 1 for submission in subreddit.get_hot(): #loop over the submissions #pprint(dir(submission)) if completed_submissions.__contains__(submission.id): continue completed_submissions.__setitem__(submission.id, submission) print(\"on the \" + str(i) + \" submission\") i = i + 1 #submission.replace_more_comments() flat_comments = praw.helpers.flatten_tree(submission.comments) #grab the comments for comment in flat_comments: if isinstance(comment, praw.objects.Comment): comment_author = comment.author #pprint(dir(comment)) comment_text = comment.body.lower() #grab submission text print(comment_text) if has_error(common_errors, comment_text) and comment_author.name != LOGIN_NAME: #don't reply to own comments comment.reply(MESSAGE_TO_POST) time.sleep(60)", "created_utc": 1427862689, "gilded": 0, "name": "t3_310mji", "num_comments": 2, "score": 2, "title": "Suggestions for grammar bot?", "url": "https://www.reddit.com/r/redditdev/comments/310mji/suggestions_for_grammar_bot/"}, {"author": "IAMA_YOU_AMA", "body": "Version 2.1.21 of praw now gives me a warning if my user_agent string contains the word 'bot' Is there something going on with reddit that I should know about? Are they cracking down on bots?", "created_utc": 1427767647, "gilded": 0, "name": "t3_30vp0e", "num_comments": 6, "score": 10, "title": "Warning in praw about using the keyword 'bot'", "url": "https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/"}, {"author": "howbigis1gb", "body": "I've been trying my hand at a project to analyse some data. And the data set is moderately large, so I want to download the thread/post data and perform some analytics with it. The problem I'm facing is that sometimes there's an error and the comment or post (as the case may be) isn't downloaded. This is an issue which I'm not sure how to handle, and is imperative that I do as I would rather not run multiple passes to get the data because it often takes many hours to do that. Here is the code I'm using for the same: # Post # creation_utc, prettytime, edited_time, pretty_edited_time, upvotes, upvote_ratio, banned_by, report_reasons, body, permalink, author, gold, time_to_get, error, depth, parent_id, controversial, distinguished, title, num_comments # Comment # creation_utc, prettytime, edited_time, pretty_edited_time, upvotes, upvote_ratio, banned_by, report_reasons, body, permalink, author, gold, time_to_get , error, depth, parent_id, controversial, distinguished, title, num_comments import praw import csv import datetime import time import fileinput # global starttime # starttime = time.time() # global subtime # subtime = starttime def checkComments(bigfile, smallfile, comments, depth): for comment in comments: try: # time2 = time.time() - time - starttime created_time = comment.created_utc pretty_created_time = datetime.datetime.fromtimestamp(created_time).strftime('%Y-%m-%d %H:%M:%S') edited_time = comment.edited if edited_time==\"FALSE\": pretty_edited_time = datetime.datetime.fromtimestamp(edited_time).strftime('%Y-%m-%d %H:%M:%S') else: pretty_edited_time = \"FALSE\" author = comment.author controversial = comment.controversiality distinguished = comment.distinguished report_reasons = comment.report_reasons upvotes = comment.ups # upvote_ratio = comment.upvote_ratio permalink = comment.permalink parent_id = comment.parent_id banned_by = comment.banned_by comment_body = comment.body.encode('utf-8') gold = comment.gilded # time2 = time.time() - time bigfile.writerow([created_time, pretty_created_time, edited_time, pretty_edited_time, upvotes, \"NA\", banned_by, report_reasons, comment_body, permalink, author, gold, \"not implemented\", \"success\", depth, parent_id, controversial, distinguished, \"NA\", \"NA\"]) smallfile.writerow([created_time, pretty_created_time, edited_time, pretty_edited_time, upvotes, \"NA\", banned_by, report_reasons, comment_body, permalink, author, gold, \"not implemented\", \"success\", depth, parent_id, controversial, distinguished, \"NA\", \"NA\"]) # subtime = subtime - time.time() checkComments(bigfile, smallfile,comment.replies, depth+1) except Exception as e: print e print \"recurse\" # time2 = time.time()-time bigfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", e, \"\", \"\", \"\", \"\", \"\", \"\"]) smallfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", e, \"\", \"\", \"\", \"\", \"\", \"\"]) def processSub(sub, bigfile, smallfile): sub.replace_more_comments(limit=None, threshold=0) checkComments(bigfile, smallfile, sub.comments, 0) #replace Comment Scraping Bot with your own user agent ID - be unique r = praw.Reddit('') r.login('', '') bigfile = csv.writer(open(\"bigfile.csv\", \"w\",0), dialect='excel') bigfile.writerow([\"creation_utc\", \"prettytime\", \"edited_time\", \"pretty_edited_time\", \"upvotes\", \"upvote_ratio\", \"banned_by\", \"report_reasons\", \"body\", \"permalink\", \"author\", \"gold\", \"time_to_get\", \"error\", \"depth\", \"parent_id\", \"controversial\", \"distinguished\", \"title\", \"num_comments\"]) for sub_id in fileinput.input([\"1-2500.txt\"]): # list of file names as strings try: starttime = time.time() submission = r.get_submission(submission_id = sub_id) created_time = submission.created_utc pretty_created_time = datetime.datetime.fromtimestamp(created_time).strftime('%Y-%m-%d %H:%M:%S') prettytime_file = datetime.datetime.fromtimestamp(created_time).strftime('%Y-%m-%d %Hh%Mm%Ss') smallfile = csv.writer(open(prettytime_file+\" \"+sub_id.rstrip()+\".csv\", \"w\",0), dialect='excel') smallfile.writerow([\"creation_utc\", \"prettytime\", \"edited_time\", \"pretty_edited_time\", \"upvotes\", \"upvote_ratio\", \"banned_by\", \"report_reasons\", \"body\", \"permalink\", \"author\", \"gold\", \"time_to_get\", \"error\", \"depth\", \"parent_id\", \"controversial\", \"distinguished\", \"title\", \"num_comments\"]) edited_time = submission.edited if edited_time==\"FALSE\": pretty_edited_time = datetime.datetime.fromtimestamp(edited_time).strftime('%Y-%m-%d %H:%M:%S') else: pretty_edited_time = \"FALSE\" upvotes = submission.ups upvote_ratio = submission.upvote_ratio banned_by = submission.banned_by report_reasons = submission.report_reasons num_comments = submission.num_comments author = submission.author title = submission.title.encode('utf-8') body = submission.selftext.encode('utf-8') user_reports = submission.user_reports ups = submission.ups permalink = submission.permalink.encode('utf-8') gold = submission.gilded distinguished = submission.distinguished bigfile.writerow([created_time, pretty_created_time, edited_time, pretty_edited_time, upvotes, upvote_ratio, banned_by, report_reasons, body, permalink, author, gold, \"not implemented\", \"success\", \"0\", \"NA\", \"NA\", distinguished, title, num_comments]) smallfile.writerow([created_time, pretty_created_time, edited_time, pretty_edited_time, upvotes, upvote_ratio, banned_by, report_reasons, body, permalink, author, gold, \"not implemented\", \"success\", \"0\", \"NA\", \"NA\", distinguished, title, num_comments]) processSub(submission, bigfile, smallfile) #terminator line print \"Finished processing id:\"+ sub_id.rstrip() + \" with \"+ str(num_comments) +\" comments\" endtime = time.time()-starttime bigfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", endtime, \"success\", \"\", \"\", \"\", \"\", \"\", \"\"]) smallfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", endtime, \"success\", \"\", \"\", \"\", \"\", \"\", \"\"]) except Exception as e: print \"sub\" print e endtime = time.time()-starttime bigfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", endtime, e, \"\", \"\", \"\", \"\", \"\"]) smallfile.writerow([\"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", endtime, e, \"\", \"\", \"\", \"\", \"\"]) I recognise that this is probably not the best way to do things, and I should probably look into storing things in a heirarchical structure, but my question was pertaining more to error handling Any inputs would be greatly appreciated. Thanks", "created_utc": 1427622033, "gilded": 0, "name": "t3_30oqky", "num_comments": 7, "score": 2, "title": "What is the best way to handle errors and failures, so I can retry the same post/comment?", "url": "https://www.reddit.com/r/redditdev/comments/30oqky/what_is_the_best_way_to_handle_errors_and/"}, {"author": "Phteven_j", "body": "I run multiple bots and a small website on a Digital Ocean server. For some reason, one particular bot (pretty simple python script) keeps throwing the HTTP 429 when I execute it server-side (locally is fine) any time it attempts to login. I think this is deeper than just the particular code, but here it is anyway: import praw user=\"\" pass=\"\" r=praw.Reddit(\"description of this particular bot here\") r.login(user,pass) etc. And the error: Traceback (most recent call last): File \"edcbot.py\", line 8, in r.login(username,password) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 1333, in login self.user = self.get_redditor(user) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 948, in get_redditor return objects.Redditor(self, user_name, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 741, in __init__ fetch, info_url) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 73, in __init__ self.has_fetched = self._populate(json_dict, fetch) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 133, in _populate json_dict = self._get_json_dict() if fetch else {} File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 126, in _get_json_dict as_objects=False) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 557, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 399, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python2.7/dist-packages/praw/internal.py\", line 178, in _raise_response_exceptions response.raise_for_status() File \"/usr/local/lib/python2.7/dist-packages/requests/models.py\", line 831, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 429 Client Error: Too Many Requests Let me know what other info I can provide to help troubleshoot this. Thanks so much!", "created_utc": 1427220491, "gilded": 0, "name": "t3_305o0j", "num_comments": 7, "score": 1, "title": "[PRAW] Getting HTTPError 429: Too many client requests when attempting r.login()", "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "5loon", "body": "I know it sounds like I'm basically asking you guys to write me a script, but I have a reasonably good knowledge of PRAW and Python and I'm just having trouble making a script that could do this efficiently. Where should I begin? It would theoretically grab a post, delete its flair, and move to the next one (chronologically back in time). Thanks in advance!", "created_utc": 1426999264, "gilded": 0, "name": "t3_2zvl1x", "num_comments": 13, "score": 5, "title": "How could I go about removing every flair on every post in an entire subreddit's history?", "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "Isolder", "body": "I'm using Perl instead of PRAW. I want to monitor /r/all/comments.json and basically do something with every new comment posted to reddit. Surely /r/all/comments.json isn't going to expose every comment as there's probably thousands of comments a minute. Just want to know the cleanest way to both call it and get an updated listing. Thanks", "created_utc": 1426642373, "gilded": 0, "name": "t3_2zf5b1", "num_comments": 8, "score": 4, "title": "What's the proper way to call /r/all/comments.json ? What interval? It doesn't look like it's always updating", "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "wonrah", "body": "For example, I want to try this: subreddit = r.get_subreddit(\"my_sub\") submissions = subreddit.get_new(limit=30, period='today', sort='new') --- Now [the documentations](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Subreddit) have a few key word arguments. Are those all the key words arguments that every function in that object can take in?", "created_utc": 1426470965, "gilded": 0, "name": "t3_2z6rzn", "num_comments": 1, "score": 1, "title": "[PRAW] Where do we find the list of **kwargs for a specific object ?", "url": "https://www.reddit.com/r/redditdev/comments/2z6rzn/praw_where_do_we_find_the_list_of_kwargs_for_a/"}, {"author": "Kaphox", "body": "See screenshot: http://i.gyazo.com/248a3ec5a6946d79087d28933e797b86.png I want a PRAW reddit bot to be able to change wiki page permissions to \"only mods may edit and view\" - I've looked absolutely everywhere and not found a response - is this possible to do? Thanks.", "created_utc": 1426289685, "gilded": 0, "name": "t3_2yys8k", "num_comments": 1, "score": 1, "title": "PRAW: making a bot change the \"who can edit this page?\" settings on wiki pages in PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/2yys8k/praw_making_a_bot_change_the_who_can_edit_this/"}, {"author": "ibbignerd", "body": "Whenever I try to run my bot, I get the following traceback: Traceback (most recent call last): File \"/home/jailbreakflairbot/RoundUp/RoundUp2.7.py\", line 2, in import praw File \"/home/jailbreakflairbot/.local/lib/python2.7/site-packages/praw/__init__.py\", line 43, in from six.moves.urllib.parse import parse_qs, urlparse, urlunparse ImportError: No module named urllib.parse | Package | Version | |---|---| |six|1.8.0| |praw|2.1.20| |python|2.7| I can't figure out what is causing this issue.", "created_utc": 1426263821, "gilded": 0, "name": "t3_2yx982", "num_comments": 4, "score": 1, "title": "Praw not working anymore", "url": "https://www.reddit.com/r/redditdev/comments/2yx982/praw_not_working_anymore/"}, {"author": "zathegfx", "body": "I am trying to find the link flair on a submission. for submission in praw.helpers.submission_stream(reddit, 'technology', limit=1, verbosity=1): print \"---------------------------------------\" print submission.get_flair_choices() This returns (in my opinion) a very strange JSON object: { u'current':{ u'flair_css_class':u'netneutrality', u'flair_template_id':u'59948c02-705a-11e4-aee5-12313b0e81b6', u'flair_position':u'left', u'flair_text':u'Net Neutrality' }, u'choices':[ { u'flair_css_class':u'politics', u'flair_text_editable':True, u'flair_template_id':u'ce7632c4-cef6-11e3-a9a4-12313b0a74a7', u'flair_text':u'Politics', u'flair_position':u'left' }, { u'flair_css_class':u'business', u'flair_text_editable':True, u'flair_template_id':u'd9e9442a-cef6-11e3-8dbf-12313d18e464', u'flair_text':u'Business', u'flair_position':u'left' }, { u'flair_css_class':u'puretech', u'flair_text_editable':True, u'flair_template_id':u'e60ffeea-e2fd-11e3-abc6-12313d18e5cd', u'flair_text':u'Pure Tech', u'flair_position':u'left' }, { u'flair_css_class':u'netneutrality', u'flair_text_editable':True, u'flair_template_id':u'59948c02-705a-11e4-aee5-12313b0e81b6', u'flair_text':u'Net Neutrality', u'flair_position':u'left' }, { u'flair_css_class':u'comcast', u'flair_text_editable':True, u'flair_template_id':u'31e0da86-70fe-11e4-bf7d-12313b0e92a7', u'flair_text':u'Comcast', u'flair_position':u'left' } ] } I am looking to grab `flair_css_class` from `current`, however I dont understand how to pass the object through `json.load` ... this is my botched version I tried: k = submission.get_flair_choices str(k); j = json.loads(k) print j[\"u'current'\"] Am I doing something wrong? Thanks -Z", "created_utc": 1426263295, "gilded": 0, "name": "t3_2yx828", "num_comments": 1, "score": 3, "title": "PRAW: find the current flair for a specific link / submission", "url": "https://www.reddit.com/r/redditdev/comments/2yx828/praw_find_the_current_flair_for_a_specific_link/"}, {"author": "NewArithmetic", "body": "I'm sure that there is a way to do this, however I have yet to find it. In my limited experience with praw, I could get the comments, but I think there would be a limit set at 200. I'm pretty sure I could do this with beautifulsoup, however I would prefer to do this the praw way. Any advice?", "created_utc": 1426178715, "gilded": 0, "name": "t3_2yt6lb", "num_comments": 3, "score": 0, "title": "Can praw get the last 2000 comments in a sub?", "url": "https://www.reddit.com/r/redditdev/comments/2yt6lb/can_praw_get_the_last_2000_comments_in_a_sub/"}, {"author": "tooproudtopose", "body": "I'm trying to make a scraper that will download all of the top comments from a particular subreddit and put the result into a csv (I used /r/Portland as an example). The problem I'm getting is that if I set limit=4 or above in my code, then I get the error `AttributeError: '' has no attribute 'body'`, but I don't get any errors at all when limit=3 (or less). What's going wrong? Here's my code: import praw import csv target_subreddit = 'portland' r = praw.Reddit(user_agent=\"u/tooproudtopose reddit-scraper\") sub = r.get_subreddit(target_subreddit) post_generator = sub.get_top(limit=4) fname = '%s.comments.csv' %target_subreddit f = open(fname, \"w\") writer = csv.writer(f, lineterminator = \"\\n\") for submission in post_generator: comments = praw.helpers.flatten_tree(submission.comments) for comment in comments: writer.writerow((comment.body, comment.author))", "created_utc": 1426043556, "gilded": 0, "name": "t3_2yn0dd", "num_comments": 10, "score": 0, "title": "Comment scraper stops working once the limiting number of posts gets too high", "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "cyanoge", "body": "E.g., if I use PRAW to fetch a user's inbox, is that data passed over the wire encrypted or is it in plaintext?", "created_utc": 1425936136, "gilded": 0, "name": "t3_2yhhm3", "num_comments": 3, "score": 5, "title": "[PRAW][python]Are requests encrypted?", "url": "https://www.reddit.com/r/redditdev/comments/2yhhm3/prawpythonare_requests_encrypted/"}, {"author": "JBHUTT09", "body": "I'm currently trying to rewrite my bot to use OAuth2 because of the upcoming change. I've spent the past couple of hours stuck on this one problem. How do I get a refresh token? I've found [this piece of documentation](https://github.com/reddit/reddit/wiki/OAuth2#authorization) but I can't for the life of me figure out how to work it into my code. This is what I have: class FlairBot( object ): def __init__( self, client_info_file ): self.token_expiration = 0 self.client_info = { } with open( client_info_file ) as file: for line in file: ( key, value ) = line.split( ':' ) self.client_info[ key ] = value.strip() print( self.client_info ) self.reddit = praw.Reddit( user_agent = self.client_info[ 'user_agent' ] ) self.client_auth = requests.auth.HTTPBasicAuth( self.client_info[ 'client_id' ], self.client_info[ 'client_secret' ] ) self.get_new_token( ) def get_new_token( self ): headers = { 'user-agent': self.client_info[ 'user_agent' ] } post_data = { \"grant_type\": \"password\", \"username\": self.client_info[ \"username\" ], \"password\": self.client_info[ \"password\" ] } response = requests.post( \"https://www.reddit.com/api/v1/access_token\", auth = self.client_auth, data = post_data, headers = headers ) token_data = response.json( ) self.token_expiration = time.time( ) + token_data[ 'expires_in' ] #self.reddit.set_access_credentials( token_data[ 'scope' ], token_data[ 'access_token' ] ) #headers = { \"Authorization\": token_data[ 'token_type' ] + ' ' + token_data[ 'access_token' ], \"User-Agent\": self.client_info[ 'user_agent' ] } print( token_data ) How do I work in that duration parameter? Also, am I doing anything glaringly wrong? I'm kind of worried that the OAuth2 stuff won't work with PRAW in the way I'm doing things. Thanks.", "created_utc": 1425870745, "gilded": 0, "name": "t3_2yekdx", "num_comments": 5, "score": 1, "title": "How do I get an OAuth2 refresh token for a python script?", "url": "https://www.reddit.com/r/redditdev/comments/2yekdx/how_do_i_get_an_oauth2_refresh_token_for_a_python/"}, {"author": "iforgotmylegs", "body": "I have been using Notepad++ but I have recently switched to Spyder since it came with another module and I just liked it better. Spyder doesn't recognize praw and I found out that I have to load it in using the PYTHONPATH manager. I can't find where to actually point this thing, I found a folder in Python34/Lib/site-packages called \"praw\" but there's nothing to point it to. Can somebody help me out?", "created_utc": 1425752120, "gilded": 0, "name": "t3_2y9cs3", "num_comments": 7, "score": 2, "title": "Where is the praw module stored?", "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "lizardsrock4", "body": "Is there anyway with praw to have it only check edited comments using the subbreddit/about/edited url? Thanks for the help!", "created_utc": 1425597040, "gilded": 0, "name": "t3_2y2rvj", "num_comments": 3, "score": 0, "title": "[PRAW] Comment stream search subreddit/about/edited?", "url": "https://www.reddit.com/r/redditdev/comments/2y2rvj/praw_comment_stream_search_subredditaboutedited/"}, {"author": "Mekhami", "body": "I can't find the 'likes' attribute anywhere in the PRAW docs. Was this removed from PRAW or never implemented? This is still available in the API, right?", "created_utc": 1425587750, "gilded": 0, "name": "t3_2y26ty", "num_comments": 9, "score": 1, "title": "[PRAW] [Python] What happened to the 'likes' attribute?", "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "RuleIV", "body": "I've written a script that gets the mod log and find users that have more than X post removals in Y days. It then messages the mod mail with that information for scrutiny and discussion, and sends a message to each user warning them. For example: \\#.Removals|\\#.Still.Up|User|Posts|B|SB :-:|:-:|:-|:-|:-:|:-: 31|47|/u/Mutt1223|[1](http://redd.it/2wzjn2), [2](http://redd.it/2x3yjv), [3](http://redd.it/2xcshx), [4](http://redd.it/2wzjq6), [5](http://redd.it/2wva8p), [6](http://redd.it/2wjs8p), [7](http://redd.it/2wjigk), [8](http://redd.it/2whgxo), [9](http://redd.it/2whh3n), [10](http://redd.it/2whf0d), [11](http://redd.it/2wh55n), [12](http://redd.it/2wh49k), [13](http://redd.it/2w3fou), [14](http://redd.it/2vno1n), [15](http://redd.it/2vnq1f), [16](http://redd.it/2vnoex), [17](http://redd.it/2vjkl3), [18](http://redd.it/2vlk25), [19](http://redd.it/2veypu), [20](http://redd.it/2vf62e), [21](http://redd.it/2vf3sa), [22](http://redd.it/2vb1tc), [23](http://redd.it/2v6z9m), [24](http://redd.it/2v70le), [25](http://redd.it/2v0hsw), [26](http://redd.it/2v18e7), [27](http://redd.it/2v0e7y), [28](http://redd.it/2uxiur), [29](http://redd.it/2upelo), [30](http://redd.it/2upduy)|No|No I want to have columns indicating if they are banned (B) and if they are shadowed banned (SB) and also not message the user if they are already banned from the subreddit or shadow banned from Reddit. Using PRAW, how can I tell if as user is shadow banned, and also how can I tell if a specific user is banned from a subreddit? I could pull the entire ban list and look for their name, but that wouldn't work if there were too many bans. Thank you. The subreddit I'm writing this for has approximately 500 bans so I can use `subreddit_bans = r.get_banned(subreddit_name, user_only=True, limit=None)` but I'd still like to know if it's possible to check a single name in case I want to use this on a subreddit with more than 1,000 bans.", "created_utc": 1425444451, "gilded": 0, "name": "t3_2xvhp4", "num_comments": 8, "score": 3, "title": "[PRAW] How to tell if a user shadow banned, or is a user banned from a subreddit.", "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "PixelDJ", "body": "Hello! I'm scratching my head because I seem to be having issues iterating through my bot's unread messages. I'm probably missing something simple here. I searched, but didn't quite come up with anything. Here's the relevant code I'm using: import praw def length(iterable): try: return len(iterable) except: i = 0 for x in iterable: i += 1 return i def scanMessages(): ''' Check for new messages and handle them appropriately ''' unread_messages = 0 messages = r.get_unread(limit = None) unread_messages = length(messages) if unread_messages > 0: print \"You have %d unread message(s).\" % unread_messages for message in messages: print \"Message: \", print message.body r = praw.Reddit(user_agent=\"hardyharharrr\") r.login(bot_user, bot_pass) scanMessages() The code enters the IF statement fine, but once it gets to the FOR, it just finishes without printing anything. What am I missing here?", "created_utc": 1425155759, "gilded": 0, "name": "t3_2xhrkc", "num_comments": 4, "score": 2, "title": "Iterating through unread messages isn't working as expected.", "url": "https://www.reddit.com/r/redditdev/comments/2xhrkc/iterating_through_unread_messages_isnt_working_as/"}, {"author": "PixelOrange", "body": "Normally, I use PRAW for everything bot related, but PRAW has not updated their wiki functions to allow for setting a wiki page as unlisted yet (or that I've found, anyway). We have [one of the largest wikis](http://www.reddit.com/r/changemyview/wiki/pages) on reddit and a great deal of that is redundant from previous versions of code that I desperately want to get rid of but it's not feasible without running a script. I've spent all day figuring out how to make direct API calls in Python and I've gotten to the point where I can oauth and get the wiki pages or even get the wiki page settings but when I try to POST I either get a 404, 403, or 500. output = requests.get(\"https://oauth.reddit.com/r/changemyview/wiki/settings/aahdin/.json\",headers=headers) Gives me an appropriate output. hide_req = requests.post(\"https://oauth.reddit.com/r/changemyview/wiki/settings/aahdin/.json\",headers=headers) Obviously doesn't do anything... because I don't know how to pass listed=False. I've tried throwing simply \"listed=False\" before headers=headers but that just gives me an unexpected argument. I know it's listed in json but I don't know how to tell it to accept my new json. Both the API documentation and the PRAW documentation are listed in a way that shows you what the options are but after about an hour of searching I couldn't find any reliable examples of HOW you use those options. Please help... or bboe, if you read this, please update PRAW with the listed page option.", "created_utc": 1424651762, "gilded": 0, "name": "t3_2wtg1n", "num_comments": 2, "score": 1, "title": "[API] How do you pass variables with an API POST?", "url": "https://www.reddit.com/r/redditdev/comments/2wtg1n/api_how_do_you_pass_variables_with_an_api_post/"}, {"author": "iforgotmylegs", "body": "I notice in [the docs](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html) that there are two ways to call get_comments, one is as a method of Redditor and another is as a method of Subreddit. I also notice that the Redditor version has a parameter to indicate the time period from which the retrieve the comments, but I don't understand how it is formatted? Googling it only turns up [this](http://stackoverflow.com/questions/27768132/how-to-get-comments-from-last-week-using-praw) (rather unhelpful) Stackoverflow thread. Is the time parameter passed a string consisting of only \"minute\", \"hour\", etc., and further to that, is it possible to call it within a timeframe, say from Feb. 19th at 00:00:00 GMT to Feb. 19th at 17:00:00 GMT? And even further to that, can this parameter also be passed using Subbreddit's get_comments method? Or only with Redditor's?", "created_utc": 1424564222, "gilded": 0, "name": "t3_2wpkzd", "num_comments": 6, "score": 1, "title": "How does time work for get_comments, and can it be done when called by a Subreddit instead of a Redditor?", "url": "https://www.reddit.com/r/redditdev/comments/2wpkzd/how_does_time_work_for_get_comments_and_can_it_be/"}, {"author": "iforgotmylegs", "body": "I am following the guide [here](https://praw.readthedocs.org/en/v2.1.20/index.html#main-page) and it says to install pip. Fine, I found a link here with easy setup links for pip and distribute, which I ran. Now apparently I have to type in \"pip install praw\"? Where do I type that in? Into cmd? Into My Python interpreter? Don't I have to actually download the praw module first? I'm using Python 2.7.", "created_utc": 1424548355, "gilded": 0, "name": "t3_2woo12", "num_comments": 9, "score": 5, "title": "I don't understand how you are supposed to install praw", "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "SeaCowVengeance", "body": "I'm trying to get a users saved submissions via the [history API](https://www.reddit.com/dev/api/oauth#GET_user_{username}_saved). However, I'm not just trying to get _all_ of their saved submissions, but just the saved submissions for a given category. I see in PRAW there's the method [get_saved](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html?highlight=saved#praw.objects.LoggedInRedditor.get_saved), but that returns all saved submissions for a user, not by category, and there's no parameter to specify category. It looks like if you use `get_content` and specify the category as a subpath it does what I want: r.get_content( \"https://oauth.reddit.com/user//saved/\", params=None, limit=4, place_holder=None, root_field='data', thing_field='children', after_field='after', use_oauth=True, object_filter=None)` Note the category in the path. That will return all the saved posts for a user in a given category. Is this there a better way to do this? If not, maybe `get_saved` should have a category parameter added to it?", "created_utc": 1424544283, "gilded": 0, "name": "t3_2wofnb", "num_comments": 0, "score": 2, "title": "[PRAW] Built in method for getting a user's saved submission sorted by category", "url": "https://www.reddit.com/r/redditdev/comments/2wofnb/praw_built_in_method_for_getting_a_users_saved/"}, {"author": "RudyH246", "body": "Hey /r/redditdev. I downloaded Python and started using PRAW less than an hour ago, so I'm pretty new at this. I'm trying to build a simple auto-responder that only responds to a certain user when I have an unread message from them in my inbox. Here's what I currently have: import praw r = praw.Reddit(user_agent='RudyH246 User Agent Auto Responder Thing') r.login('my_username', 'my_password') for comment in r.get_unread(): if comment.author == r.get_redditor('username') comment.reply('Reply.') comment.mark_as_read() I feel like this should accomplish this simple goal, but I figured I'd run it by here to see if I made any glaring errors in the logic. I've never written in Python before, so I'm also wary of syntactical errors, but I think I got the gist of it.", "created_utc": 1424500880, "gilded": 0, "name": "t3_2wmvah", "num_comments": 8, "score": 1, "title": "[PRAW] A Simple Auto-Responder Based on User Name.", "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "Seventytvvo", "body": "Hey guys, I'm just learning about APIs and PRAW with python, and after doing a few tutorials, I started wondering if it would be possible to create some kind of upvote-circle sniffer bot. The ultimate goal would be to sniff out pairs or groups of usernames who upvote each other so that a human can go take a closer look. Ultimately, I think it would be cool to try to use this as an vote manipulation / astroturfing-protection tool. Given PRAW's limitations, I came up with a way this might work: It will grab get_liked data for a seed user, UserA. It will then find the user, UserB, whom UserA has upvoted the most number of times. The script will move on to UserB and repeat the process, crawling through reddit. The script will keep track of what users it has examined and will look for instances where it starts to repeat a pattern of names. The idea is that if two usernames are upvoting one another, the program will get stuck alternating between these two. The idea of seeing repeating sets of usernames will hopefully allow this to scale up to larger groups of upvote circles. For example, if 4 users all upvote each other, the program may start to cycle through these 4 names. There would probably be some filtering needed. For most users, it's likely they've only voted for other users a low number of times, so filtering out vote tallies of 1 or 2 probably makes sense. If UserA has upvoted other users once or twice, but upvoted UserB 27 times,, that's what we'd be looking for. We would then go look at UserB's voting history... This is probably a crappy way to do this, and I've already thought of a few problems: 1) Not sure if it is possible to access random users' vote history even w/ OAuth 2) Will be bad at detecting a \"funnel\"-type upvote group, where many accounts upvote one user, but that user deoes not reciprocate 3) Will be very slow, crawling through one user at a time. Perhaps it's best to use this on suspected upvote rings rather than to sniff them out? 4) Since upvote counts are changing dynamically, it's possible the program might miss something if a user happened to upvote someone not in the upvote circle at the right time **My questions for you all -** 1) General thoughts on the idea? Does it seem useful? 2) Is this even possible to do within PRAW? You guys know the barriers far better than I do at this point... 3) Any suggestions on a better approach?", "created_utc": 1423491786, "gilded": 0, "name": "t3_2vaun5", "num_comments": 4, "score": 1, "title": "Wanted to run an idea by you guys - I'm new to praw/APIs", "url": "https://www.reddit.com/r/redditdev/comments/2vaun5/wanted_to_run_an_idea_by_you_guys_im_new_to/"}, {"author": "Fireislander", "body": "I am trying to update praw on my mac and I keep getting the following error http://pastebin.com/3uwvCmgc Any help would be appreciated", "created_utc": 1423432540, "gilded": 0, "name": "t3_2v8dic", "num_comments": 2, "score": 1, "title": "Unable to update PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2v8dic/unable_to_update_praw/"}, {"author": "chrisarr", "body": "Hey r/RedditDev community, I have been working on a project as part of an Interaction Design Graduate Thesis project utilizing Reddit AMA. The project, called AskUsAnything, is meant to be an annotated and analytic tool for reading AMA threads. I am interested in bringing a new way of viewing and understanding, even comparing and contrasting AMAs, with the intention of making the wealth of individual information and knowledge in each thread viewable in a new and unique way. The prototype is online at [http://askusanything.cc](http://askusanything.cc) I wanted to bring the prototype version to the subreddit here so that some people who have spent a lot of time working on the platform and with the API could see the project. The project is emphatically \"beta\" at this stage, and I am fast at work on the next version. I have built this on Python/Flask, using the Reddit API and PRAW, utilizing NLTK for language analysis, BokehJS for visualization, and deployed on a DigitalOcean Ubuntu droplet. If you have any feedback on how you might use something like this (as potentially a Reddit/AMA user), how I might do something differently, or if this work reminds of you anything you have seen someone doing with Reddit data, I would love to hear about it. Cheers!", "created_utc": 1423367338, "gilded": 0, "name": "t3_2v5s0c", "num_comments": 6, "score": 7, "title": "Building a tool for Reddit AMA \u2013 Take a peek!", "url": "https://www.reddit.com/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/"}, {"author": "Midgetapple5", "body": "hi, I'm new to praw and python in general, I was wondering what the best way is to make a bot that makes a post in a subreddit once a week. I've looked up guides, but all they tell me is how to make bots that reply to comments.", "created_utc": 1423361266, "gilded": 0, "name": "t3_2v5ho4", "num_comments": 3, "score": 2, "title": "Help making a bot that posts once a week", "url": "https://www.reddit.com/r/redditdev/comments/2v5ho4/help_making_a_bot_that_posts_once_a_week/"}, {"author": "Marorin", "body": "Hey there guys. First timer here as the topic indicates. I like to think I'm a bit of a programmer, recently trying to get into Python for a class. I wanted to utilize the Reddit API for a presentation on it's use for web programming. I was thinking to use it to show like the top 10 hottest reddits for a subreddit. I was wondering how feasible that would be and whether it be better to use the PRAW library instead. I do apologize for being a tad sparse, classes started pretty recently so I'm pretty much willing to take in suggestions and advice for more experienced developers.", "created_utc": 1423237417, "gilded": 0, "name": "t3_2uzqy7", "num_comments": 5, "score": 1, "title": "First Timer here looking for guidance", "url": "https://www.reddit.com/r/redditdev/comments/2uzqy7/first_timer_here_looking_for_guidance/"}, {"author": "Kim___Jong___Un", "body": "I am trying to figure out how to keep my bot from replying to a specific sub. I know that in order to add specific subs, I would do SUBREDDIT = \"FirstSubName+SecondSubName\" but I tried doing a \"-\" instead of \"+\" and it didn't work. Does anyone know how to exclude a specific sub without having a moderator ban my bot? I looked at the PRAW docs and did not find what I seek. Cheers for any insights.", "created_utc": 1422404080, "gilded": 0, "name": "t3_2twjw9", "num_comments": 4, "score": 2, "title": "Excluding specific subs from bot replies (PRAW)", "url": "https://www.reddit.com/r/redditdev/comments/2twjw9/excluding_specific_subs_from_bot_replies_praw/"}, {"author": "SurviAvi", "body": "My link bot for /r/archlinux wasn't able to run for the past few days, since I kept on getting this error: Traceback (most recent call last): File \"bot/allb.py\", line 218, in for item in praw.helpers.comment_stream(r, SUBR, limit = None, verbosity = 0): File \"/usr/local/lib/python3.2/dist-packages/praw/helpers.py\", line 138, in _stream_generator for i, item in gen: File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 504, in get_content page_data = self.request_json(url, params=params) File \"/usr/local/lib/python3.2/dist-packages/praw/decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 562, in request_json delattr(self, '_request_url') AttributeError: _request_url Has anyone experienced this error? I could't find anything similar. Source: https://gist.github.com/phikal/271374cf5eee9c659c7f", "created_utc": 1422225264, "gilded": 0, "name": "t3_2tnplt", "num_comments": 3, "score": 4, "title": "'comment_stream' error", "url": "https://www.reddit.com/r/redditdev/comments/2tnplt/comment_stream_error/"}, {"author": "KnightOfDark", "body": "I'm using a praw-based subreddit scraper to gather corpora for a linguistics project. Before the release of 2.1.20 the scraper worked with no issue - I have already gathered about 3.000 threads worth of comments. Unfortunately, the following code now produces assertion errors: agent = praw.Reddit(user_agent='Subreddit scraper for corpus gathering') sub = agent.get_subreddit(subreddit) content_generator = sub.get_new(limit=thread_limit) for thread in content_generator: thread.replace_more_comments(limit=None, threshold=0) I have narrowed the fault down to the replace_more_comments methods, as scraping threads with no MoreComment objects does not result in errors. Two different assertion errors are produced: Traceback (most recent call last): File \"Scraper.py\", line 194, in SubredditToCSV(sub_name, limit=thread_limit, separator='/') File \"Scraper.py\", line 175, in SubredditToCSV threads, users, comments = ScrapeSubreddit(subreddit, limit) File \"Scraper.py\", line 62, in ScrapeSubreddit thread.replace_more_comments(limit=None, threshold=0) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 1122, in replace_mo re_comments new_comments = item.comments(update=False) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 700, in comments return self._continue_comments(update) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 683, in _continue_c omments assert len(self.children) == 1 and self.id == self.children[0] AssertionError And: Traceback (most recent call last): File \"Scraper.py\", line 194, in SubredditToCSV(sub_name, limit=thread_limit, separator='/') File \"Scraper.py\", line 175, in SubredditToCSV threads, users, comments = ScrapeSubreddit(subreddit, limit) File \"Scraper.py\", line 62, in ScrapeSubreddit thread.replace_more_comments(limit=None, threshold=0) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 1122, in replace_mo re_comments new_comments = item.comments(update=False) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 700, in comments return self._continue_comments(update) File \"C:\\Anaconda\\lib\\site-packages\\praw\\objects.py\", line 686, in _continue_c omments assert len(self._comments) == 1 AssertionError Is this a bug in the newest release, or do I need to change something in my program?", "created_utc": 1422191356, "gilded": 0, "name": "t3_2tlzxl", "num_comments": 8, "score": 1, "title": "[PRAW] Assertion errors on replace_more_comments after 2.1.20 release", "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "Isagoge", "body": "Hello guys, I'm writing a bot on a separate account to help me get relevant posts from the subreddits i'm subscribed to on my main account. What i did was basically grab a list of my subscribed subreddit and write them in a text file, i use the script to refresh the list when i subscribe to new subreddits. After i use another script to try and subscribe to each of the subreddits that are in the text file. The script run without hassle, but when i check on the account, it didn't manage to subscribe to the subreddits. I wanted to know if you guys could help me with this. Here is the code i used to subscribe to the subreddit. The text file is basically only the name of the subreddits with one on each line. r = praw.Reddit(USERAGENT) r.login(REDDIT_USERNAME, REDDIT_PASS) print(\"Logging into Reddit\") with open(\"Subreddits.txt\", \"r\") as p: liste = p.read().split(\"\\n\") for i in range(len(liste) - 1): subreddit = r.get_subreddit(liste[i]) print(\"Obtention du subreddit: \", liste[i]) subreddit.subscribe() print(\"Suscribing to: \", liste[i])", "created_utc": 1422028646, "gilded": 0, "name": "t3_2tevir", "num_comments": 3, "score": 1, "title": "Problems subscribing to a list of subreddits from a .txt file", "url": "https://www.reddit.com/r/redditdev/comments/2tevir/problems_subscribing_to_a_list_of_subreddits_from/"}, {"author": "letgoandflow", "body": "I posted a thread about this yesterday, but I think I've zeroed in on the actual problem (hopefully). I'm creating an authorization URL using PRAW: oauth_link = r.get_authorize_url( session['oauth_token'], ['identity', 'submit'], True ) This part is working just fine. PRAW creates the URL properly and the user is asked to allow permissions for both scopes. When the user returns after giving my app permission, the scope is returned in this format: set([u'identity submit']) This looks like PRAW is interpreting multiple scopes as a single string ('identity submit'), which doesn't match any of the actual scopes. So I have an authorized user with no permissions. Note that if I only ask for a single scope, it works just fine. I'm fairly confident that something has changed with the reddit API because I had two separate apps start showing this problem at the same time and no changes have been made to either of them.", "created_utc": 1421942160, "gilded": 0, "name": "t3_2taq2w", "num_comments": 7, "score": 1, "title": "[PRAW] Not able to authorize a user with multiple OAuth2 scopes", "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "themetallurgist", "body": "When I use the code import praw, time r = praw.Reddit(user_agent=\"Bot experiment\") r.login('redacted', 'redacted') I get the traceback: Traceback (most recent call last): File \"redacted\", line 5, in r.login('redacted', 'redacted') File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/__init__.py\", line 1263, in login self.request_json(self.config['login'], data=data) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/__init__.py\", line 519, in request_json response = self._request(url, params, data) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/__init__.py\", line 383, in _request _raise_response_exceptions(response) File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/internal.py\", line 172, in _raise_response_exceptions response.raise_for_status() File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/requests/models.py\", line 831, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 403 Client Error: Forbidden I've tried everything I can think of. Mac and Windows, python 3.4 & 2.7, different accounts, I've run it from IDLE and the terminal. I tried not putting in my username and password and then trying to login when prompted. I just have no idea. Does anyone have any clues?", "created_utc": 1421893489, "gilded": 0, "name": "t3_2t8t9v", "num_comments": 7, "score": 3, "title": "Can't login with praw", "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "zzpza", "body": "If I run the following script, there is a post missing from the results. ---- zzpza@linux:~/reddit/scripts/testing$ cat search.py #!/usr/bin/env python # from datetime import date, timedelta, datetime from time import strptime import calendar import praw import csv import unicodedata currentWeek = int(format(datetime.now().strftime('%V'))) print \"Current Week: %d\" % currentWeek r = praw.Reddit(user_agent=\"/r/analog sticky post rotation script v0.1 by /u/zzpza\") r.login(\"zzpza\", \"[redacted]\") #subreddit = r.get_subreddit('analog') with open('analog.csv', 'w') as fp: writer = csv.writer(fp) for submission in r.search('flair=Community',subreddit='analog',sort='new',period='week'): id = submission.id print \"ID: %s\" % id title = submission.title title_ascii = unicodedata.normalize('NFKD', title).encode('ascii','ignore') print \"Title: %s\" % title_ascii url = submission.short_link author = submission.author a=(id, title_ascii, url, author) writer.writerow(a) ---- When I run the script, I get two results: zzpza@linux:~/reddit/scripts/testing$ ./search.py Current Week: 4 ID: 2svvrx Title: Weekly 'Gear Photos & Discussion' - Week 04 ID: 2svvr0 Title: Weekly 'Ask Anything About Analog Photography' - Week 04 zzpza@linux:~/reddit/scripts/testing$ ---- If I run the search via a browser (http://www.reddit.com/r/analog/search?q=flair%3Acommunity&sort=new&restrict_sr=on&t=week) I get three results. My initial theory was that it could be that the period='week' is absolute and not relative. The missing post from the search results was posted in a different week if you go by number, but less than a week ago in time. I changed period='week' to period='month' and it still won't return the [OTW] post. How come I don't see the same from the PRAW results as I do in a browser? How do I get the missing result to show up in my PRAW script? Thanks! :) (Also, sorry for the sloppy code - I'm a beginner).", "created_utc": 1421703400, "gilded": 0, "name": "t3_2sz98i", "num_comments": 4, "score": 1, "title": "PRAW search not returning posts expected", "url": "https://www.reddit.com/r/redditdev/comments/2sz98i/praw_search_not_returning_posts_expected/"}, {"author": "letgoandflow", "body": "[Here is the warning](https://praw.readthedocs.org/en/v2.1.19/pages/oauth.html#step-2-setting-up-praw): > This example, like most of the PRAW examples, binds an instance of PRAW to the r varaible. While we\u2019ve made no distinction before, r (or any instance of PRAW) should not be bound to a global variable due to the fact that a single instance of PRAW cannot concurrently manage multiple distinct user-sessions. >If you want to persist instances of PRAW across multiple requests in a web application, we recommend that you create an new instance per distinct authentication. Furthermore, if your web application spawns multiple processes, it is highly recommended that you utilize PRAW\u2019s multiprocess functionality. Currently, my app is setting up the PRAW instance using the r variable when the app is initialized. Based on the warning, I'm pretty sure I shouldn't be doing this anymore, but I'm not sure how I should be doing it differently.", "created_utc": 1421698333, "gilded": 0, "name": "t3_2syxjd", "num_comments": 4, "score": 0, "title": "Not sure what to do about warning in the PRAW docs regarding binding PRAW instance to global variables.", "url": "https://www.reddit.com/r/redditdev/comments/2syxjd/not_sure_what_to_do_about_warning_in_the_praw/"}, {"author": "Hirayu", "body": "Hello all! I recently have started to learn python and I wanted to try to build a bot using praw but I am currently stuck in this situation. I have tried following the praw tutorial and have managed to login and send messages with my bot(yay!). I want my bot to be able to pull from this website http://www.hearthpwn.com/decks/158447-fakenicks-legend-1-eu-season-10-pala and just post the deck list as a comment. Unfortuantely I am unsure on how to begin. I think I may need to use a web scraper of some sort? If someone could point me in the right direction, I would be most grateful. I know Java and C++ so I am not a complete programming noobie but any help is appreciated. Thanks!", "created_utc": 1421561022, "gilded": 0, "name": "t3_2st4us", "num_comments": 2, "score": 3, "title": "Trying to pull data from a website?", "url": "https://www.reddit.com/r/redditdev/comments/2st4us/trying_to_pull_data_from_a_website/"}, {"author": "darkfire613", "body": "I'm learning to write reddit bots, and my first project using praw simply scans submissions to a subreddit's new and records the usernames of posters. I'm using place_holder to prevent getting the name from the same posts twice. However, place_holder is stopping at the provided ID inclusively. That is, I get the list of all the new posts' usernames, including the username of the post whose ID is the placeholder. This means that username gets doubled up on my list. I think I'm probably using place_holder incorrectly. If anyone has tips on a better way to get just new posts since my last request, let me know. Thank you! firstloop = True for submission in subreddit.get_new(limit=25, place_holder=placeholder): username = submission.author f.write(str(username) + '\\n') if firstloop: placeholder = submission.id firstloop = False And sample output when I let the loop run four times on /r/redditdev with a limit of 5 posts: darkfire613 Oxilic avapoet wscottsanders kemitche darkfire613 darkfire613 darkfire613 As you can see, my name gets duplicated because I have the post whose ID is being used as the placeholder.", "created_utc": 1421471001, "gilded": 0, "name": "t3_2sphkf", "num_comments": 0, "score": 1, "title": "using Praw, subreddit.get_new(place_holder) returning info for placeholder ID twice", "url": "https://www.reddit.com/r/redditdev/comments/2sphkf/using_praw_subredditget_newplace_holder_returning/"}, {"author": "wscottsanders", "body": "I'm teaching a class this semester that is using reddit as a forum for class discussion, Q&A, and chatting with guests. I am writing a bot that will self post an update for the students about the karma they earn in the subreddit. Currently, my biggest difficulty is bypassing the captcha. I know this must be possible based on some of the silly bots I've seen on reddit. I've [read the praw documentation](https://praw.readthedocs.org/en/v2.1.19/pages/faq.html#how-can-i-handle-captchas-myself) and I can't seem to figure out how this is done. Any hints would be greatly appreciated? This is the function I have for posting to reddit: def post_to_reddit(message): captcha ={'iden': 'EBG2KudNny2pB6o5x2VTOIeyjT9n92SH', 'captcha': 'ILBZVT'} r.submit('poitest', 'Captcha Works!', text=message, raise_captcha_exception=True, captcha=captcha) print \"Done.\"", "created_utc": 1421360857, "gilded": 0, "name": "t3_2skb9u", "num_comments": 5, "score": 1, "title": "Bypassing Captcha to Post Updates to a Reddit?", "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "Brandon23z", "body": "In reddit, you can hyperlink [](), italics *_*, bold **_**, that kind of stuff. How can I do that in PRAW? Right now my bot just uses plain text, no formatting. I'd like to include hyperlinks. Here is my reply code. SETRESPONSE1 = \"What ain't no country I ever heard of.\" comment.reply(SETRESPONSE1) How can I reply with a hyperlink? Do I just use reddit formatting when replying to the comment?", "created_utc": 1421291800, "gilded": 0, "name": "t3_2sh6sp", "num_comments": 5, "score": 1, "title": "How can I use reddit formatting with a bot? Using PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/2sh6sp/how_can_i_use_reddit_formatting_with_a_bot_using/"}, {"author": "nsfw_only_bot", "body": "For example, [this post](http://www.reddit.com/r/funny/comments/2sav16/sometimes_its_the_person_you_least_suspect/) does not have a thumbnail (at least for me) even though it's just an imgur post. If you look at the source you'll see", "created_utc": 1421211616, "gilded": 0, "name": "t3_2sdb6w", "num_comments": 1, "score": 2, "title": "Why do some thumbnails return \"default\"?", "url": "https://www.reddit.com/r/redditdev/comments/2sdb6w/why_do_some_thumbnails_return_default/"}, {"author": "hassanchug", "body": "How would I go about using PRAW with Google App Engine? I've set up most of the stuff, and have the praw folder in the root directory of my app-engine folder, but when I try to deploy my project, I get the following error: `ImportError: No module named requests` If I then copy over the \"requests\" folder from the \"site-packages\" directory into the root of the app-engine folder and try to re-deploy, I get another list of errors, such as \"No module named _ssl\" and so on. Anybody have any experience with using PRAW on GAE (or generally importing any third-party libraries into GAE)?", "created_utc": 1421100163, "gilded": 0, "name": "t3_2s7mcc", "num_comments": 5, "score": 2, "title": "Using PRAW with Google App Engine?", "url": "https://www.reddit.com/r/redditdev/comments/2s7mcc/using_praw_with_google_app_engine/"}, {"author": "jakethespectre", "body": "I have been trying to make a script that will automatically update the stylesheet on my subreddit /r/jakethespectre. I'm a noob with python and I didn't even understand what an API was 2 days ago, so forgive silly questions. Here is my code: http://pastebin.com/SgbT2mAS (I wrote a bit on the error messages I get next to the def calls at the bottom) Questions: **1.** Why isn't https://ssl.reddit.com/api/v1/access_token in any documentation? It seems pretty important so if there's some doc I'm missing, please point me to it. (also for oauth.reddit.com?) **2.** Why the hell am I getting so many 429 errors? Is there a way to prevent that? I read it was from doing an invalid login too many times but my login info has been correct the whole time. On the documentation it says that I could get that if it's sending requests more than 30 times in 2 seconds, but even if I ran the program with all the functions uncommented that would be 5 requests and I get that even after I've only ran the program once. **3.** I also sometimes get 401, 403, and 404 errors with login() and getModhash() as well. I dont know what they mean and why I'm only getting them SOMETIMES. **4.** Even after login has returned a 200, getModhash() will still give me this: {\"json\": {\"errors\": [[\"USER_REQUIRED\", \"please sign in to do that\", null]]}} **5.** Why isn't getModhash() working? I can get the modhash by typing that address into my browser, and every time I refresh I can get a new code. This function also sometimes gives me {}. Thanks a lot, me.json :/ **6.** Am I making this much more complicated than it needs to be? Note: I'm not using praw because [apparently](https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/) something was updated and it isn't working. I think that's all the questions I had, although I'll probably remember another one approximately two seconds after I click submit on this post. **Ninja_edit:** formatting", "created_utc": 1420536997, "gilded": 0, "name": "t3_2ri0r7", "num_comments": 11, "score": 3, "title": "A lot of questions about the reddit API", "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "sallurocks", "body": "Has somebody tried to run a bot in qpython which uses praw? It does support pip so I installed praw but gives errno 21 when I try to import praw, here's the log Traceback (most recent call last): File \"test.py\", line 1, in import praw File \"/data/data/com.hipipal.qpyplus/files/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\", line 56, in platform_info = platform.platform(True) File \"/QPython/core/build/python-install/lib/python2.7/platform.py\", line 1575, in platform File \"/QPython/core/build/python-install/lib/python2.7/platform.py\", line 163, in libc_ver IOError: [Errno 21] Is a directory: '/storage/emulated/0/com.hipipal.qpyplus/scripts'", "created_utc": 1420444309, "gilded": 0, "name": "t3_2rdtf1", "num_comments": 2, "score": 2, "title": "Praw in qpython?", "url": "https://www.reddit.com/r/redditdev/comments/2rdtf1/praw_in_qpython/"}, {"author": "jakethespectre", "body": "I haven't even started to write the actual code, I'm a n00b with python and praw. I'm very confused. this is my code: r = praw.Reddit('Currently, it is trying to login to my account' 'I plant to have it auto-submit my stylesheet' 'Made by /u/jakethespectre, v1.0') r.login('username','password'); And this is the error: Traceback (most recent call last): File \"C:\\Users\\Steam\\Desktop\\sendStylesheet.py\", line 6, in r.login('username','password'); File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 1266, in login self.user = self.get_redditor(user) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 890, in get_redditor return objects.Redditor(self, user_name, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 663, in __init__ fetch, info_url) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 72, in __init__ self.has_fetched = self._populate(json_dict, fetch) File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 127, in _populate json_dict = self._get_json_dict() if fetch else {} File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 120, in _get_json_dict as_objects=False) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 526, in request_json data = json.loads(response, object_hook=hook) File \"C:\\Python34\\lib\\json\\__init__.py\", line 318, in loads return _default_decoder.decode(s) File \"C:\\Python34\\lib\\json\\decoder.py\", line 343, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"C:\\Python34\\lib\\json\\decoder.py\", line 361, in raw_decode raise ValueError(errmsg(\"Expecting value\", s, err.value)) from None ValueError: Expecting value: line 1 column 1 (char 0) Edit from nath_schwarz: Check what you are copy/pasting!", "created_utc": 1420430066, "gilded": 0, "name": "t3_2rd7n3", "num_comments": 9, "score": 3, "title": "Can't login with praw?", "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "armandg", "body": "Hey guys, I'm fiddling with a bot for /r/Tippeligaen that pushes the newest submissions to the twitter-bot. I use praw for the fetching and I use this script (a simple version): done_subm = [] for submission in subreddit.get_new(limit=5): if submission.id not in done_subm: title = submission.title url = submission.short_link text = title + ': ' + url [code for publishing on twitter] done_subm.append(submission.id) The problem is that the script starts, and haven't published anything yet, it publishes the tweets in the wrong direction when compared with the subreddit, in the way that the oldest submission is the newest tweet. When the script is running, it works perfect, because it always fetches the newest submission and publishes it. Any tips on how to fix this?", "created_utc": 1420314772, "gilded": 0, "name": "t3_2r8974", "num_comments": 2, "score": 2, "title": "Pushing submissions to twitter, but not correct sort?", "url": "https://www.reddit.com/r/redditdev/comments/2r8974/pushing_submissions_to_twitter_but_not_correct/"}, {"author": "zooparoo_skidoo", "body": "I am trying to get a list of replies to a comment. I have been able to load a comment object, where I know there are replies, but when polled, I see no reply objects r = praw.Reddit(user_agent='zooparoo') comment_object = r.get_info(url=None,thing_id='t1_cnbign0',limit=None) At that point, I should have a praw.object.Comment object. I know there is a reply, but if I invoke: replies_to_comment_object = comment_object.replies I get an empty variable when I should be seeing info on the child object, t1_cnbigut. Should I be accessing comments a different way?", "created_utc": 1420149608, "gilded": 0, "name": "t3_2r1g5a", "num_comments": 2, "score": 1, "title": "Proper way to access comment replies in PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2r1g5a/proper_way_to_access_comment_replies_in_praw/"}, {"author": "GAPINGCUNT", "body": "On one of the subreddits I moderate, 99.9% of the posts that are caught by the modqueue are not spam. I want to have my reddit bot automatically approve these posts, but leave anything else (reported comments, etc) alone for me to sift through when I go into the queue. import praw user_agent = ('Automoderator for GAPINGCUNT') r = praw.Reddit(user_agent=user_agent) r.login(username,password) queue = r.get_mod_queue(subreddit='TestSubreddit') for item in queue: ### Need to check to see if it is a post, this line of code is not real ### if item == post: item.approve() Is this possible, or is my bot not going to discriminate against anything in the mod queue?", "created_utc": 1420004551, "gilded": 0, "name": "t3_2qw7gu", "num_comments": 4, "score": 2, "title": "Check if item in Modqueue is a flagged post or a reported comment?", "url": "https://www.reddit.com/r/redditdev/comments/2qw7gu/check_if_item_in_modqueue_is_a_flagged_post_or_a/"}, {"author": "haiguise1", "body": "I've been using PRAW to get comments from Reddit and a couple days ago I started getting 403 Client error: Forbidden errors. This program has been running for close to a month without spitting up any errors. I think it has something to do with reddit's 30 requests per minute rule, but I was timing the requests in my program and they all waited 2 seconds between requests. However, after 30 requests had been made the error pops up. Am I being locked out for some reason? ps. I'm using PRAW 2.1.19 edit: I figured out the problem, HistoricalWhatIf had been set to private, and it was number 30 in my list of subreddits.", "created_utc": 1419962411, "gilded": 0, "name": "t3_2qu0tu", "num_comments": 6, "score": 1, "title": "403 error using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": "rhiever", "body": "It's been ages since I had to log in to scrape reddit data, but I'm trying to access some mod-only data this time. Whenever I run the code: r = praw.Reddit(user_agent=\"/u/rhiever mod log scraping\") r.login(\"username\", \"password\") (obviously with my login information) I get the following error: >HTTPError: 403 Client Error: Forbidden which seems to be happening at: >1263 self.request_json(self.config['login'], data=data) Anyone else having this issue?", "created_utc": 1419882605, "gilded": 0, "name": "t3_2qqm9j", "num_comments": 4, "score": 2, "title": "Can't log in with PRAW any more?", "url": "https://www.reddit.com/r/redditdev/comments/2qqm9j/cant_log_in_with_praw_any_more/"}, {"author": "RuleIV", "body": "###Resolved. Thank you /u/largenocream I discovered today that get_new() will not collect low karma submissions in the same way that they are hidden from users by default. Is it possible to get a full list of submissions? Relevant section of code I'm using to get submissions. user_agent = (\"redacted\") r = praw.Reddit(user_agent = user_agent) subreddit = r.get_subreddit(SUB_NAME) r.login(CRED_USER, CRED_PASS) new_submissions = subreddit.get_new(limit = SCAN_LIMIT) Thanks.", "created_utc": 1419773624, "gilded": 0, "name": "t3_2qm6ml", "num_comments": 2, "score": 2, "title": "[PRAW] Prevent get_new() hiding low karma submissions.", "url": "https://www.reddit.com/r/redditdev/comments/2qm6ml/praw_prevent_get_new_hiding_low_karma_submissions/"}, {"author": "minlite", "body": "Hi all. As you might know at /r/millionairemakers we are trying to run a drawing using the comments retrieved from reddit API. Our picking method is described in detail [here](http://www.reddit.com/r/millionairemakers/comments/2ournt/explanation_of_our_new_drawing_system_inspired_by/). I have wrote a python script to grab the comments and generate the winner using PRAW, but the issue is that apparently 110K comments is too much for the reddit API to handle. The code is available here: https://github.com/millionairemakers/millionairemakers More specifically, the issue rises in [this](https://github.com/millionairemakers/millionairemakers/blob/master/webserver.py) file at line 152, where the system tries to replace the \"more\" comments. submission.replace_more_comments(limit=None, threshold=0) I get the following error: Exception in thread Thread-1: Traceback (most recent call last): File \"/usr/lib64/python2.7/threading.py\", line 811, in __bootstrap_inner self.run() File \"webserver.py\", line 150, in run submission.replace_more_comments(limit=None, threshold=0) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/objects.py\", line 1029, in replace_more_comments new_comments = item.comments(update=False) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/objects.py\", line 638, in comments response = self.reddit_session.request_json(url, data=data) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\", line 519, in request_json response = self._request(url, params, data) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/__init__.py\", line 383, in _request _raise_response_exceptions(response) File \"/usr/lib/python2.7/site-packages/praw-2.1.19-py2.7.egg/praw/internal.py\", line 172, in _raise_response_exceptions response.raise_for_status() File \"/usr/lib/python2.7/site-packages/requests-2.5.0-py2.7.egg/requests/models.py\", line 829, in raise_for_status raise HTTPError(http_error_msg, response=self) HTTPError: 413 Client Error: Too Big I know this is coming from reddit and not from the client, but is there anyway to fix this? Thanks.", "created_utc": 1419333213, "gilded": 0, "name": "t3_2q5zly", "num_comments": 25, "score": 7, "title": "Error 413 : Too Big with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "picflute", "body": "Been a while since I've been to redditdev. Picking up developing bots again and I'm getting this [traceback](http://pastebin.com/raw.php?i=1aueQRLz) and I don't know what it means. My Code: import praw r = praw.Reddit(\"Test Bot\") r.login('user','pass') I have SSL enabled on this account as well.", "created_utc": 1419147757, "gilded": 0, "name": "t3_2pyi07", "num_comments": 2, "score": 3, "title": "Value Error when trying to login?", "url": "https://www.reddit.com/r/redditdev/comments/2pyi07/value_error_when_trying_to_login/"}, {"author": "chaz6", "body": "How can I get comments for a submission ordered by highest rated first? This is my sample code which gets the comments in no particular order:- import praw r = praw.Reddit('PRAW') submissions = r.get_subreddit('AskReddit').get_top_from_month(limit=20) for submission in submissions: submission.replace_more_comments(limit=64, threshold=10) flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: print(comment)", "created_utc": 1419098451, "gilded": 0, "name": "t3_2pwgc9", "num_comments": 6, "score": 1, "title": "[PRAW] Get comments for a submission ordered by votes", "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "brucemo", "body": "I know that the idea of removing a comment in mod mail is weird, but it happens from time to time, and I'd like to know if it is possible to detect this when writing a mod mail scanner with PRAW. I don't see any fields that pertain, and when I looked through the JSON dict there didn't seem to be anything there, as well. Thank you.", "created_utc": 1418434303, "gilded": 0, "name": "t3_2p4un7", "num_comments": 0, "score": 4, "title": "Detecting removed mod mail", "url": "https://www.reddit.com/r/redditdev/comments/2p4un7/detecting_removed_mod_mail/"}, {"author": "Sleepydragn1", "body": "I'm pretty new to PRAW, and I'm wondering if it's possible to grab all the submissions to a particular subreddit within a particular time frame. Subreddit.get_new() allows for a limit, but I cannot seem to find a parameter/arg that's based on date. Any ideas?", "created_utc": 1418421321, "gilded": 0, "name": "t3_2p48qb", "num_comments": 4, "score": 3, "title": "[PRAW] Getting all posts to a subreddit within a certain time frame?", "url": "https://www.reddit.com/r/redditdev/comments/2p48qb/praw_getting_all_posts_to_a_subreddit_within_a/"}, {"author": "coolstorym8", "body": "I'm trying to cover the case when my internet goes down. Assume I've logged in and when im just about to call get_subreddit, my internet cuts. Currently you can see that there is a silent timeout that occurs and no requests.ConnectionError is raised. I want it to appear so I can then drop my function that will wait for a reconnection. Here is my test code.. (not actual, Just for demonstration. I have covered error cases for login) USERAGENT = \"blah blah blah\" USERNAME = \"username\" PASSWORD = \"password\" r = praw.Reddit(USERAGENT) r.login(USERNAME, PASSWORD) print \"Disconnect Now\" time.sleep(5) submissions = r.get_subreddit('videos').get_hot(limit=30) You run this code and disconnect the internet when it prints to do so. Watch what happens when it calls get_subreddit. I want it to force the error instead of remaining silent.", "created_utc": 1418137863, "gilded": 0, "name": "t3_2ordbo", "num_comments": 7, "score": 1, "title": "How do I force an exception error instead of getting a silent timeout when using get_subreddit with no internet connection?", "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "hooked_dev", "body": "I'm trying to debug my code that rips entire threads of reddit. Everything seems to be working, but it seems like there is hard cap on the depth of a conversation in a reddit thread (specifically a depth of ten). It looks like there is a problem with the \"continue this thread\" links. For example, consider: http://www.reddit.com/r/test/comments/2onit4/test_depth_0/ How can I get PRAW to pull the entire thread?", "created_utc": 1418054023, "gilded": 0, "name": "t3_2onkd8", "num_comments": 8, "score": 2, "title": "[PRAW] Maximum depth of subtrees?", "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "SexySatan", "body": "My goal: Retrieve the post titles from the first page of a subreddit using Java. What I've done: Most suggestions I've seen online suggest using jReddit for Java. Their example code seems to do what I need and can be found here. https://github.com/karan/jReddit/blob/master/examples/GetPaginatedTopics.java I've downloaded this entire project and created a JAR using Maven, and added the JAR as an external JAR to the project in eclipse. This method only resolved one of the imports. import com.github.jreddit.utils.restclient.RestClient; The other 4 are still invalid. I've also tried using a premade JAR found here: http://mvnrepository.com/artifact/com.github.jreddit/jreddit/1.0.0 This will resolve the first two imports but not the others. It seems as though different versions of jReddit will allow different parts of the example code to work, but I can't find a version that will make the entire thing function. I'm beginning to think that this example code is just nonfunctional so I look online for different examples and found this reddit thread: http://www.reddit.com/r/redditdev/comments/2col0f/problems_with_jreddit/ The poster who solved the problem for the OP is using a JAR with an identical name to mine, however, I appear to have some missing functions. My User class doesn't have the .about method so I can't properly initialize UserInfo. Just to test I scrapped most of the code except for RestClient restClient = new HttpRestClient(); User user = new User(restClient, \"username\", \"password\"); user.connect(); which will compile just fine, but results in a run time error. At this point I'm frustrated because I've made much more complex things via PRAW with little effort, but I need to use Java because I'm trying to make a simple android app. Does anyone have any insight on how to achieve this in Java? What I need to do is simple enough that jReddit may not even be required, but I'd like to get it working so that I can have the option of adding more advanced features.", "created_utc": 1417461439, "gilded": 0, "name": "t3_2nysxy", "num_comments": 0, "score": 2, "title": "Java: Get top post titles from subreddit", "url": "https://www.reddit.com/r/redditdev/comments/2nysxy/java_get_top_post_titles_from_subreddit/"}, {"author": "beardgoggles", "body": "I'm building my first Reddit API bot and trying to figure out how to call api/me.json. I've been doing the PRAW tutorial here: http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html But I'm not really clear on how to adapt the other parts of the Reddit API to PRAW. Does anyone have an example of how to use me.json with PRAW?", "created_utc": 1416960783, "gilded": 0, "name": "t3_2nfe4j", "num_comments": 4, "score": 2, "title": "Voting by API, can't find examples of me.json with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2nfe4j/voting_by_api_cant_find_examples_of_mejson_with/"}, {"author": "minlite", "body": "Hi. I'm a mod over at /r/millionairemakers and we just had a drawing last night. If you check out the drawing thread you will see that there are over 8000 comments. What I want to accomplish is to retrieve a list of usernames from the top level comments so we can do the drawing as fairly as possible. For that, I wrote a PRAW script that retrieves the comments and then replaces the \"more\" kind with their respective comments: print \"Getting comments...\" submission = r.get_submission(submission_id='') print \"Replacing all more comments...\" submission.replace_more_comments(limit=None, threshold=0) print \"Flatting comments\" flat_comments = praw.helpers.flatten_tree(submission.comments) f = open('usernames', 'w') for comment in flat_comments: if type(comment) is praw.objects.Comment: if type(comment.author) is praw.objects.Redditor: f.write(comment.author.name + \"\\n\") This works fine, but the issue is that it takes a lot of time because PRAW descends to the children of the top level comments as well to replace the \"more\" kind, making hundreds of unnecessary requests. Of course, I can check `is_root` when writing to my file, but I want the script to run as fast as possible and do not make useless requests. Is there anyway to limit the `replace_more_comments` method to only replace the top level comments? In my calculations, if I have 8000 comments and reddit returns 200 comments per request, I would need to make 40 requests. Taking the 30 req/minute limit into account, it shouldn't take more than 2 minutes for this script to run, but currently it takes 25 minutes which is really unacceptable.", "created_utc": 1416949628, "gilded": 0, "name": "t3_2nesds", "num_comments": 5, "score": 3, "title": "Make PRAW only replace top level comments (not the children)", "url": "https://www.reddit.com/r/redditdev/comments/2nesds/make_praw_only_replace_top_level_comments_not_the/"}, {"author": "danielvd", "body": "Hi I'm currently developing an Android application for my mobile computing class and had a question concerning OAuth. Background - * Reddit application to allow users to post to reddit in an attempt to earn more karma * Allow users to verify via the application and receive an access token Where I'm currently at - * I've managed to authorize the user via client-side using AndrOAuth and have received an access token. * On top of that I have a REST API which interfaces over PRAW, and was planning to make a POST endpoint so that users can reply to a submission/comment. Problem - I'm struggling to figure out how to allow the user to actually post with PRAW and an access token. Some APIs allow you to just pass the access token and make a post on the user's behalf, however I'm seeing that PRAW doesn't allow this. I intend to keep my API stateless since it's REST, however I don't know what I can do to fix this problem. I've looked up PRAW's OAuth solution online, however it requires sessions on the backend, which would result in a stateful API. Does anyone have any suggestions on what I could do? I'm extremely stuck and anything would be helpful! Suggested solutions after discussion with my group - * Keep a session on the client-side (some sort of boolean) and store the username/password on the Android device. This is safe from what I've heard since it exists on the device only and doesn't pose too many security risks. * After storing the session and storing the credentials, pass the username/password into API calls and call r.login() every time. However I'm not too well-informed on the security risks of this.", "created_utc": 1416877238, "gilded": 0, "name": "t3_2nblsd", "num_comments": 1, "score": 2, "title": "[PRAW] OAuth and replying to comments", "url": "https://www.reddit.com/r/redditdev/comments/2nblsd/praw_oauth_and_replying_to_comments/"}, {"author": "boibtest", "body": "I have a few accounts and only have this problem with the account \"boibtest\". And I can log in and out of boibtest through the browser. But when using PRAW, I get this: r.login(\"boibtest\", \"password\") Traceback (most recent call last): File \"C:\\Python33\\lib\\json\\decoder.py\", line 367, in raw_decode obj, end = self.scan_once(s, idx) StopIteration During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"\", line 1, in File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 1266, in login self.user = self.get_redditor(user) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 890, in get_redditor return objects.Redditor(self, user_name, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 663, in __init__ fetch, info_url) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 72, in __init__ self.has_fetched = self._populate(json_dict, fetch) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 127, in _populate json_dict = self._get_json_dict() if fetch else {} File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 120, in _get_json_dict as_objects=False) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 526, in request_json data = json.loads(response, object_hook=hook) File \"C:\\Python33\\lib\\json\\__init__.py\", line 316, in loads return _default_decoder.decode(s) File \"C:\\Python33\\lib\\json\\decoder.py\", line 351, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"C:\\Python33\\lib\\json\\decoder.py\", line 369, in raw_decode raise ValueError(\"No JSON object could be decoded\") ValueError: No JSON object could be decoded * I don't get this error when logging in other accounts with PRAW * I don't get an error logging in as boibtest in a web browser. What am I doing wrong?", "created_utc": 1416420568, "gilded": 0, "name": "t3_2msi7y", "num_comments": 7, "score": 3, "title": "[PRAW] - \"No JSON object could be decoded\" error from r.login(u,p)", "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "MoarMag", "body": "I can use submission.ups but I can't use downs to work out the % myself. Can anyone point me to all a list of all the attributes I can get out of '' has no attribute. I also posted in this in requestabot before i knew redditdev existed btw.", "created_utc": 1416391570, "gilded": 0, "name": "t3_2mrcb8", "num_comments": 7, "score": 2, "title": "[Help] How do I get the % upvoted value of a submission using python?", "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "letgoandflow", "body": "I want to pull in some submissions from a specific subreddit and I'm having a hard time figuring out how to do so. The PRAW docs are very helpful for a few specific tasks, but once you step outside of those tasks it is hard to find the specific functions/methods that you need. There is the [Code Overview](https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html), but it's a little confusing. For example, I want to use the [get_top method](https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html#praw.objects.Subreddit.get_top) on the Subreddit class, but it doesn't really tell you how to use any parameters that would modify the request (e.g. t or limit). Am I missing something?", "created_utc": 1416366064, "gilded": 0, "name": "t3_2mqf5z", "num_comments": 7, "score": 3, "title": "Having trouble figuring out how to use PRAW", "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "hooked_dev", "body": "I'm still new to PRAW and I'm trying to figure out if I'm doing something wrong. Here is my comment scraper code: import praw R = praw.Reddit('comment_scraper 0.1 by u/hooked_dev') submission = R.get_submission(submission_id=submission_id) submission.replace_more_comments(limit=None, threshold=0) comments = praw.helpers.flatten_tree(submission.comments) With a `submission_id=\"2e4cvj\"` this finishes in 17 seconds and there are 193 comments with a speed of 11/comments per second. Pulling a **large** submission, `submission_id=\"2e4cvj\"` which is the Bill Gates AMA (19625 comments) takes 6400 seconds giving me a speed of 3 comments per second. Is there any way to speed up the download of a large submission? It seems unreasonable to take over an hour and a half for a *single* submission.", "created_utc": 1416346920, "gilded": 0, "name": "t3_2mpeqt", "num_comments": 4, "score": 1, "title": "[PRAW] Why does it take an hour to download a large submission?", "url": "https://www.reddit.com/r/redditdev/comments/2mpeqt/praw_why_does_it_take_an_hour_to_download_a_large/"}, {"author": "letgoandflow", "body": "There is some great info about handling captchas in the PRAW docs ([see here](https://praw.readthedocs.org/en/v2.1.19/pages/faq.html#how-can-i-handle-captchas-myself)). It tells you how to raise the InvalidCaptcha exception and get the captcha_id, but for some reason it skips over how to actually get the URL of the corresponding captcha image. I see there is a [/captcha/ API method](http://www.reddit.com/dev/api#GET_captcha_{iden}), but I'm not sure how to use it with PRAW. **Edit:** So apparently you can just build the URL using the following format - http://www.reddit.com/captcha/insert_captcha_id_here.png. Easy enough! Hope this helps someone else that is a n00b like me.", "created_utc": 1416273243, "gilded": 0, "name": "t3_2mm9sd", "num_comments": 2, "score": 3, "title": "[PRAW] How to get a captcha image URL after you have the captcha ID", "url": "https://www.reddit.com/r/redditdev/comments/2mm9sd/praw_how_to_get_a_captcha_image_url_after_you/"}, {"author": "heerensharma", "body": "I tried pip as well as easy_install. Yet there is something which is not going right. whenever I am trying to import, I am getting this error message: \"Traceback (most recent call last): File \"\", line 1, in ImportError: No module named praw\"", "created_utc": 1416170937, "gilded": 0, "name": "t3_2mht2a", "num_comments": 4, "score": 1, "title": "How to install PRAW in anaconda in mac?", "url": "https://www.reddit.com/r/redditdev/comments/2mht2a/how_to_install_praw_in_anaconda_in_mac/"}, {"author": "PasDeDeux", "body": "Hi everyone, I'm having a problem with PRAW where the .get_comments() function is only returning ~1000 comments and then dying. Theoretically, get_comments(limit = None) should return all of the comments in a given subreddit, 100 comments at a time. When I do /all I get ~700 comments, /libertarian gives ~990. def downloader(subname): subreddit = r.get_subreddit(subname) corpus = [] comlist = subreddit.get_comments(limit = None) for comment in comlist: commenttext = comment.body.lower() cid = comment.id corpus.append([cid, commenttext]) print(len(corpus)) So just for clarity, comlist is actually a generator object, so it's not like I can just pass that. That wouldn't be economic either, given I'm trying to generate very large amounts of data to save later.", "created_utc": 1416085927, "gilded": 0, "name": "t3_2merm5", "num_comments": 5, "score": 1, "title": "[PRAW] Hitting a limit with .get_comments(limit = None)", "url": "https://www.reddit.com/r/redditdev/comments/2merm5/praw_hitting_a_limit_with_get_commentslimit_none/"}, {"author": "archon_rising", "body": "Hey guys, I'm trying to scrape all comments in the top 5 posts from a subreddit. After getting these comments, I want to print them on screen. What I see are of the form [] etc. How do I print these comments? I'm slightly new to PRAW. Code is here: https://gist.github.com/anonymous/9b8865b33025e2b697db Thanks!", "created_utc": 1416019103, "gilded": 0, "name": "t3_2mcgcm", "num_comments": 7, "score": 1, "title": "[PRAW] Trying to use PRAW to scrape all comments", "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "hooked_dev", "body": "I'm quite new to PRAW, so I'm not sure if it is hidden in the documentation somewhere, but I'm looking for all the top rated posts from a subreddit from the previous week.", "created_utc": 1415822414, "gilded": 0, "name": "t3_2m3qxk", "num_comments": 1, "score": 3, "title": "[PRAW] How to pull all top rated posts from \"last\" week.", "url": "https://www.reddit.com/r/redditdev/comments/2m3qxk/praw_how_to_pull_all_top_rated_posts_from_last/"}, {"author": "BeezInTheTrap", "body": "Newbie to Python and PRAW here. I'm trying to search for links in the comments of a submission. Is there any way to do so? Such as a function that checks whether a comment has links, or a string I can search for that might indicate a link? Thanks", "created_utc": 1415516401, "gilded": 0, "name": "t3_2lqtpq", "num_comments": 2, "score": 1, "title": "PRAW: Any way to tell if a comment contains a link?", "url": "https://www.reddit.com/r/redditdev/comments/2lqtpq/praw_any_way_to_tell_if_a_comment_contains_a_link/"}, {"author": "mjgcfb", "body": "I'm very new to programming and I can't figure out why my code below is so slow going through the loop. This is not the complete code but its the bulk of it. comments = praw.helpers.comment_stream(r, subreddit, limit=500) for comment in comments: # Key ID's comment_id = comment.id link_id = comment.link_id link_id = r.get_info(thing_id=link_id) author = comment.author defaults_post = { 'link_title' : comment.link_title, 'link_url' : comment.link_url, 'domain' : link_id.domain, 'num_comments' : link_id.num_comments, 'permalink' : link_id.permalink, 'url' : link_id.url } try: cleaned_flair = h.unescape(comment.author_flair_text).split(' / ')[0] except TypeError: try: cleaned_flair = comment.author_flair_text.split(' / ')[0] except AttributeError: cleaned_flair = 'None' # Pack non ID Fields into Defaults for Author Model defaults_author = { 'author_flair_css_class' : comment.author_flair_css_class, 'comment_karma' : author.comment_karma, 'link_karma' : author.link_karma } # Pack non ID Fields into Defaults for Comment Model defaults_comments = { # Comment Attributes 'body' : h.unescape(comment.body), 'body_html' : h.unescape(comment.body_html), 'edited' : comment.edited, 'gilded' : comment.gilded, 'parent_id' : comment.parent_id, 'subreddit_id' : comment.subreddit_id, 'subreddit' : comment.subreddit, 'distinguished' : comment.distinguished, #Votable Attributes 'ups' : comment.ups, 'downs' : comment.downs, 'comment_permalink' : comment.permalink } parent_id = comment.parent_id if 't1' in parent_id: parent_id = r.get_info(thing_id=parent_id) defaults_comments['parent_author'] = parent_id.author defaults_comments['parent_author_flair_css_class'] = parent_id.author_flair_css_class defaults_comments['parent_edited'] = parent_id.edited defaults_comments['parent_gilded'] = parent_id.gilded defaults_comments['parent_ups'] = parent_id.ups defaults_comments['parent_downs'] = parent_id.downs defaults_comments['parent_permalink'] = parent_id.permalink", "created_utc": 1415038242, "gilded": 0, "name": "t3_2l6e6b", "num_comments": 7, "score": 1, "title": "Why is my for loop take around 5+ seconds for each loop using praw?", "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "powderblock", "body": "Writing a simple Reddit bot using PRAW that crawls through /r/all on a timer and checks if they are valid imgur posts, if they are, then the image gets piped to other functions for further processing. Problem is, it isn't possible to tell whether or not a .jpg on Imgur is animated or not. From Imgur FAQ \"The maximum animated file size (both GIF and PNG) is 2MB.\" (Section: *Is there a maximum file size I can upload?*) If a gif has a file size greater than 2MB, it gets compressed or cut down and consequently Imgur displays the URL as .jpg. This means when I am checking if a URL is of type .jpg, it will reply true regardless if it's animated or not. How can I check if a compressed .jpg is actually an animated gif file? Is there an API call for this? I found [this](https://api.imgur.com/models/image) on the subject but I don't understand how to implement it within the API. Here is an example of an animated .jpg (Not actually a .jpg, I know, but it's what the browser and PRAW see from /r/all) http://i.imgur.com/9sYwSwz.jpg Any help is appreciated", "created_utc": 1414626295, "gilded": 0, "name": "t3_2kqai0", "num_comments": 4, "score": 2, "title": "Checking if .jpg Imgur URL is Animated Gif File", "url": "https://www.reddit.com/r/redditdev/comments/2kqai0/checking_if_jpg_imgur_url_is_animated_gif_file/"}, {"author": "icedvariables", "body": "I've been writing a bot that searches for the word 'pug' in the title of posts in /r/all (or whatever sub you specify). It collects all the titles of these posts and sends a private message to me. The problem is that when the bot sends the message this error is thrown: Traceback (most recent call last): File \"pugs.py\", line 23, in r.user.send_message(\"icedvariables\", \"Pug posts\", pugPosts) File \"/Library/Python/2.7/site-packages/praw/decorators.py\", line 58, in wrapped return function(self.reddit_session, self, *args, **kwargs) File \"/Library/Python/2.7/site-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/Library/Python/2.7/site-packages/praw/decorators.py\", line 225, in wrapped return function(obj, *args, **kwargs) File \"/Library/Python/2.7/site-packages/praw/__init__.py\", line 2021, in send_message data.update(captcha) ValueError: dictionary update sequence element #0 has length 1; 2 is required Here is my code: import sys, time, praw r = praw.Reddit(\"pug_bot: Searches for posts contain the word 'pug'\") try: subreddit = r.get_subreddit(sys.argv[1]) except IndexError: subreddit = r.get_subreddit(\"all\") r.login(\"pug_bot\", xxx) while(True): pugPosts = \"\" for submission in subreddit.get_new(limit=50): title = submission.title if(\"pug\" in title): print \"PUG POST:\", title pugPosts += \"Pug post:\"+title+\"\\n\\n\" r.user.send_message(\"icedvariables\", \"Pug posts\", pugPosts) time.sleep(1800)", "created_utc": 1414437108, "gilded": 0, "name": "t3_2kho9h", "num_comments": 3, "score": 1, "title": "Praw error when sending PM: ValueError: dictionary update sequence element #0 has length 1; 2 is required", "url": "https://www.reddit.com/r/redditdev/comments/2kho9h/praw_error_when_sending_pm_valueerror_dictionary/"}, {"author": "teaearlgraycold", "body": "So I've put a bit of work into a reddit bot for ELI5, and it seems like it's not working consistently. It's coded in Python with the help of PRAW. For a while it would just look through the ELI5 new queue, keep a running list of all posts, and then remind users who hadn't flaired their post explained after 6 hours to flair it if applicable. Then I began a secondary portion of the bot, a portion that reads through the ELI5 modmail for certain commands. I currently have commands that provide user summaries and shadowbans. But it seems that very often when I move the bot into ELI5 after testing it in a private subreddit it has problems performing. Sometimes it'll log tons of errors claiming it can not send PMs (for the flair checking), or that [the connection to reddit has been dropped](http://i.imgur.com/ZjWG9Uz.png). I've tried to keep the number of API calls per minute down below 30, but still the problems persist. Here's my code (Yes, it's a mess. This is my first project in Python): http://pastebin.com/znWuvsRq I currently have the flair checking commented out and the sleep timer set to 40 (it used to be at 20 seconds back when the flair section was enabled). Is it possible my bot is flagged/banned/something else on ELI5 because it broke the API rules? **Edit:** I talked with an admin and they seem to think I'm having an issue with connection pooling/keep alive and not API limitations. Has anyone else had issues with that? I also took a look at the PRAW GitHub and it seems like there's a setting in a praw.ini file that automatically makes sure that API calls only happen every two seconds. Is this something that is automatically done? Should I remove my sleep() timers and let PRAW time things for me?", "created_utc": 1414379630, "gilded": 0, "name": "t3_2kfiou", "num_comments": 14, "score": 5, "title": "What's wrong with my bot?", "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "GoldenSights", "body": "http://www.reddit.com/r/redditdev/comments/2eur0l/api_update_apiinfo_supports_lists_of_fullnames/ /u/bsimpson says that the info endpoint allows for comma-separated lists of fullnames to fetch. I've been collecting subreddit info, and I only just remembered that this was even possible, but PRAW won't let me pass lists, and strings are only fetching the first item. I really don't understand how to use raw GET and POST http requests. Has PRAW wrapped this yet? If not, would anyone mind giving me the line to do this through urllib? Thank you!", "created_utc": 1414366911, "gilded": 0, "name": "t3_2kex5m", "num_comments": 2, "score": 2, "title": "Does PRAW support multiple things for get_info?", "url": "https://www.reddit.com/r/redditdev/comments/2kex5m/does_praw_support_multiple_things_for_get_info/"}, {"author": "Zapurdead", "body": "Like the title says, I'm using the Reddit praw API, and 504 Errors have been tripping me up. I wrapped all the API calls in a try-catch statement, so it can just sleep when there's an error: from requests.exceptions import HTTPError .... while True: try: print(\"Fetching comments...\") print(\"=====\\n\") with open('completed.json', 'r') as comment_history_file: comment_history = json.load(comment_history_file) for comment in comments_by_keyword(r, 'rip mobile users', subreddit='all', print_comments=True): if is_valid(comment, comment_history): reply_with_image(r, comment, comment_history) # Reddit caches recent comments every 30 seconds, so fetch comments in intervals of a little over 30 seconds print(\"Last Successful Query (System Time): \" + strftime(\"%Y-%m-%d %I:%M:%S\\n\")) except HTTPError as e: msg = \"HTTPError(\" + str(e.errno) + \"): \" + str(e.strerror) r.send_message('Zapurdead', \"[MOBILE-WIZARD] HTTPError\", msg) pass sleep(30) However, the exception still stops the program from running. Any idea what I'm doing wrong? I've searched quite a number of threads on this subreddit to no avail.", "created_utc": 1414127495, "gilded": 0, "name": "t3_2k6668", "num_comments": 2, "score": 1, "title": "PRAW 504 HTTPError, Exception not being caught", "url": "https://www.reddit.com/r/redditdev/comments/2k6668/praw_504_httperror_exception_not_being_caught/"}, {"author": "redditdev1", "body": "At first everything seemed OK. When I manually check out the reddit.com/r/subreddit/.json and reddit.local/r/.json, I get nearly identical responses. The only suspicious value is the \"modhash.\" For my clone, the modhash is just the username, or blank. On reddit it looks like its a real hash. The problem is when I go to PRAW (with the domain changed to the appropriate value), the generator get_subreddit returns 'dicts' instead of praw.objects.Submission. Does anyone have any experience with resolving this conflict?", "created_utc": 1414119278, "gilded": 0, "name": "t3_2k5ui0", "num_comments": 2, "score": 1, "title": "Figuring out the API on a reddit clone", "url": "https://www.reddit.com/r/redditdev/comments/2k5ui0/figuring_out_the_api_on_a_reddit_clone/"}, {"author": "doob10163", "body": "I have ran get-pip.py When executing `pip install praw` in powershell, I get `Import Error: No module named site` I've installed praw on another machine before and I had it run. I have no idea why this is not working on mine though.", "created_utc": 1413854152, "gilded": 0, "name": "t3_2juell", "num_comments": 17, "score": 1, "title": "Can't install praw on this other machine?", "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "catzhoek", "body": "I'm having trouble logging in using the easiest of all approaches. I don't get it. import praw r = praw.Reddit(user_agent='User-Agent: dhvpostbot/1.0 by u/catzhoek') r.login() [traceback](http://pastebin.com/n3mVWnU1) I can successfully run stuff like r.get_redditor('catzhoek') etc. without any problems. Any wrong password cases report back with proper exceptions. Does anyone have a clue what the culprit could be? Thanks. Edit: **Solved**, it's caused by requiring HTTPS on the account you try to login with.", "created_utc": 1413614688, "gilded": 0, "name": "t3_2jl8ks", "num_comments": 6, "score": 3, "title": "Can't get praw (2.1.18) to login", "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "chrisarr", "body": "I am wondering how best to replicate the AMA categorization implemented on the Official AMA app. Is there a categorization that is derivable from the API/PRAW, or would some form of NLP need to be used to manually process and categorize each post? Cheers", "created_utc": 1413487109, "gilded": 0, "name": "t3_2jg19x", "num_comments": 3, "score": 5, "title": "How is the categorization on the Official AMA App achieved?", "url": "https://www.reddit.com/r/redditdev/comments/2jg19x/how_is_the_categorization_on_the_official_ama_app/"}, {"author": "huegue", "body": "I'm trying to put my python script (that uses PRAW) on the Google App Engine so that it will run periodically without my computer. I initially ran into problems running locally (with dev_appserver.py) because of missing libraries, so I copied the necessary libraries over to my application's lib folder (which initially only contained flask). Now I'm getting a problem with this line, but only on the live version (not when I test locally). r = praw.Reddit(user_agent=\"some_agent\") Here's the traceback: Traceback (most recent call last): File \"/base/data/home/runtimes/python27/python27_lib/versions/1/google/appengine/runtime/wsgi.py\", line 266, in Handle result = handler(dict(self._environ), self._StartResponse) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1836, in __call__ return self.wsgi_app(environ, start_response) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1820, in wsgi_app response = self.make_response(self.handle_exception(e)) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1403, in handle_exception reraise(exc_type, exc_value, tb) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1817, in wsgi_app response = self.full_dispatch_request() File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1477, in full_dispatch_request rv = self.handle_user_exception(e) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1381, in handle_user_exception reraise(exc_type, exc_value, tb) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1475, in full_dispatch_request rv = self.dispatch_request() File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/flask/app.py\", line 1461, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/main.py\", line 32, in update_pics r = praw.Reddit(user_agent=\"fresh_pics\") File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/praw/__init__.py\", line 1067, in __init__ super(AuthenticatedReddit, self).__init__(*args, **kwargs) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/praw/__init__.py\", line 536, in __init__ super(OAuth2Reddit, self).__init__(*args, **kwargs) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/praw/__init__.py\", line 648, in __init__ super(UnauthenticatedReddit, self).__init__(*args, **kwargs) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/praw/__init__.py\", line 316, in __init__ update_check(__name__, __version__) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/update_checker.py\", line 170, in update_check result = checker.check(package_name, package_version, **extra_data) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/update_checker.py\", line 67, in wrapped retval = function(obj, package_name, package_version, **extra_data) File \"/base/data/home/apps/s~river-acrobat-728/1.379380117832801085/lib/update_checker.py\", line 121, in check data['platform'] = platform.platform(True) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/platform.py\", line 1603, in platform libcname,libcversion = libc_ver(sys.executable) File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/platform.py\", line 163, in libc_ver f = open(executable,'rb') IOError: [Errno 2] No such file or directory: '/base/data/home/runtimes/python27/python27_dist/python'", "created_utc": 1413306882, "gilded": 0, "name": "t3_2j8dsv", "num_comments": 4, "score": 5, "title": "Using PRAW with the Google App Engine: no such file or directory '/base/data/home/runtimes/python27/python27_dist/python'", "url": "https://www.reddit.com/r/redditdev/comments/2j8dsv/using_praw_with_the_google_app_engine_no_such/"}, {"author": "alexleavitt", "body": "Related to this post \u2013 http://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/ \u2013 I'm wondering if it's possible to get the number of downvotes anymore for comments...? \"upvote_ratio\" works for posts but doesn't seem to work for comments. Also, \"ups\" seems to only report the total score of the comment now.", "created_utc": 1413173615, "gilded": 0, "name": "t3_2j364y", "num_comments": 5, "score": 5, "title": "PRAW - Getting downvotes for comments?", "url": "https://www.reddit.com/r/redditdev/comments/2j364y/praw_getting_downvotes_for_comments/"}, {"author": "PieMan2201", "body": "I'm using comment_stream to get comments from /r/all, but after a while it effectively stops, and shows something like this: `Items: 558 (0.00 ips) ` Sometimes it will go for hours, other times for a few minutes. I've tried it on multiple systems with the same result. Any idea what's going on? Edit: I've also tried re-installing PRAW.", "created_utc": 1413151613, "gilded": 0, "name": "t3_2j28r0", "num_comments": 0, "score": 1, "title": "[PRAW] comment_stream not working?", "url": "https://www.reddit.com/r/redditdev/comments/2j28r0/praw_comment_stream_not_working/"}, {"author": "swissmcnoodle", "body": "r = praw.Reddit(user_agent=USERAGENT) submission = r.get_submission(url='http://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/') print(vars(submission)) Very simple code, outputs the api information on a reddit url. I am new to python and am finding it quite confusing. When I print submission there are a number of fields including:'comments','created','created_utc','delete','distinguish','domain' How do I print for example, 'domain' from submission object. Thanks in advance EDIT: Actually a better question might be to ask this. The comment data you retrieve from a post submission is outputted like this in console: > I am assuming the praw.objects.Comment will contain fields containing username, post string, upvotes etc. How would I access something like that?", "created_utc": 1412999098, "gilded": 0, "name": "t3_2ix3aj", "num_comments": 3, "score": 2, "title": "How do I read the fields of PRAW submission objects/dictionaries?", "url": "https://www.reddit.com/r/redditdev/comments/2ix3aj/how_do_i_read_the_fields_of_praw_submission/"}, {"author": "PixelOrange", "body": "I'm trying to determine if wiki pages are hidden and then hide or unhide based on the results. slyf added the ability to hide wiki pages a couple of months ago but I can't find any reference to that API in the PRAW documents. Alternatively, if this isn't possible currently, could someone walk me through how to do this via API? I've always accessed reddit through PRAW and never directly through an API call.", "created_utc": 1412929356, "gilded": 0, "name": "t3_2iu8de", "num_comments": 0, "score": 1, "title": "Is it possible to hide/unhide wiki pages through PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/2iu8de/is_it_possible_to_hideunhide_wiki_pages_through/"}, {"author": "NosajReddit", "body": "First, I'm extremely new to Python, so I appreciate any help you can offer. I'm trying to scrape reddit posts with PRAW while logged in with oauth. The [reddit API rules](https://github.com/reddit/reddit/wiki/API#rules) say you can make 60 requests a minute if you're logged in with oauth, and that you can monitor various headers to make sure you don't go over. PRAW naturally waits 2 seconds between calls to make sure it doesn't exceed the rate limit. I'd like to make calls every 1 second. I believe the only way to do this is to (manually?) edit the [api_request_delay variable in PRAW.ini](https://github.com/praw-dev/praw/blob/master/praw/praw.ini) from 2.0 to 1.0. I want to make sure I'm following the rules, so I have the following questions before I start sending requests: 1. Is this the best way to use PRAW to exceed the non-oauth rate limit? The only way? 1. Is 'read' the right scope, or do any of them work for the higher rate limit? 1. Is there a way to use PRAW to access the response headers? If not, what's the best way to do this manually? Thanks for your help!", "created_utc": 1412889705, "gilded": 0, "name": "t3_2isomp", "num_comments": 2, "score": 1, "title": "[PRAW] Using OAuth with reddit API ratelimits", "url": "https://www.reddit.com/r/redditdev/comments/2isomp/praw_using_oauth_with_reddit_api_ratelimits/"}, {"author": "sneaky_dragon", "body": "****SOLVED. SEE COMMENT BELOW**** --- I last ran my script maybe around Mar 2014 to set flairs for users in /r/rabbits as a mod. It worked then, but I tried it today with an updated PRAW package (v2.1.18), and I keep getting a RedirectException error. The mod permissions I have in the subreddit are \"access, config, flair, mail, posts, wiki.\" Could anyone help me debug this? Code: import praw r = praw.Reddit(user_agent='/r/rabbits flair script by sneaky_dragon v0.1') r.login('user', 'pwd') ... r.set_flair_csv('rabbits', flairmap); The errors I got: r.set_flair_csv('rabbits', flairmap); File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 313, in wrapped if mod and not is_mod_of_all(obj.user, subreddit): File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 275, in is_mod_of_all mod_subs = user.get_cached_moderated_reddits() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 771, in get_cached_moderated_reddits for sub in self.reddit_session.get_my_moderation(limit=None): File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 472, in get_content page_data = self.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 510, in request_json response = self._request(url, params, data) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 374, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 348, in handle_redirect url = _raise_redirect_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 158, in _raise_redirect_exceptions raise RedirectException(response.url, new_url) praw.errors.RedirectException: Unexpected redirect from http://www.reddit.com/subreddits/mine/moderator/.json?limit=1024 to http://www.reddit.com/subreddits/login.json?dest=http%3A%2F%2Fwww.reddit.com%2Fsubreddits%2Fmine%2Fmoderator%2F.json%3Flimit%3D1024", "created_utc": 1412830504, "gilded": 0, "name": "t3_2iqen5", "num_comments": 2, "score": 1, "title": "Keep getting RedirectException using set_flair_csv with PRAW...", "url": "https://www.reddit.com/r/redditdev/comments/2iqen5/keep_getting_redirectexception_using_set_flair/"}, {"author": "doob10163", "body": "I'm trying to develop a bot similar to /u/MTGCardFetcher for /r/hearthstone. I need to be able to iterate over a given subreddits comments and search for instances of \"[[\" and \"]]\" and then respond with the card's information that is put in between the brackets. Here's what I have so far, it's almost exactly the same as the information at praw.readthedocs.org import praw r = praw.Reddit(\"HS Card Fetcher by u/doob10163\") r.login('doob_test', 'thepassword') submission = r.get_submission(submission_id=\"2ioe5e\") #subreddit = r.get_subreddit('test') #subreddit_comments = subreddit.get_comments() flat_comments = praw.helpers.flatten_tree(submission.comments) already_done = set() for comment in flat_comments: if \"[[\" in comment.body and \"]]\" in comment.body and comment.id not in \\ already_done: comment.reply(\"\"\"This is a test response.\"\"\") already_done.add(comment.id) A few things worth mentioning that I feel like need to be addressed in some way when I develop this: My comment rate is severely limited on this bot, how long do I wait in between comments? How do I limit the bot so it only pulls the requests for the given limit (what is the limit anyways)? Should I use time.sleep() ? How does the bot know how many comments to pull from the subreddit? How do I ensure that I will call only the most recent posts and not do the same thing over again when I cancel the bot running and have it run again fresh?", "created_utc": 1412806528, "gilded": 0, "name": "t3_2ipcik", "num_comments": 4, "score": 1, "title": "Completely new to reddit API and PRAW with python. I'm trying to check all comments in a subreddit for instances of a string", "url": "https://www.reddit.com/r/redditdev/comments/2ipcik/completely_new_to_reddit_api_and_praw_with_python/"}, {"author": "cylindrical418", "body": "I'm getting this error using `praw` after restarting a bot and making it reply to a comment Traceback (most recent call last): File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 378, in bot.fastboot() File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 91, in fastboot self.work() File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 261, in work self.scanComments(comments, submission) File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 295, in scanComments self.parseCommand(comment) File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 305, in parseCommand self.doCommand(command, comment) File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 343, in doCommand self.doFetchUnitCommand(command, comment) File \"F:\\Dev\\Python\\bfbot\\src\\Bot.py\", line 357, in doFetchUnitCommand comment.reply(r) File \"F:\\Dev\\Python\\Python\\Python27\\lib\\site-packages\\praw\\objects.py\", line 348, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"F:\\Dev\\Python\\Python\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"F:\\Dev\\Python\\Python\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 2047, in _add_comment retry_on_error=False) File \"F:\\Dev\\Python\\Python\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 177, in wrapped raise error_list[0] APIException: (NO_TEXT) `we need something here` on field `text` What's weird is that it was working fine before. [This](http://www.reddit.com/r/test/comments/2ictuj/bfbottest/cl3gndo) is the comment that causes this error. I have no idea what this error means. Can anyone help me?", "created_utc": 1412739786, "gilded": 0, "name": "t3_2immwi", "num_comments": 4, "score": 1, "title": "[praw] APIException: (NO_TEXT)", "url": "https://www.reddit.com/r/redditdev/comments/2immwi/praw_apiexception_no_text/"}, {"author": "F3AR3DLEGEND", "body": "How can I gild users and comments, preferably with PRAW? I can't find any method for it. I know how to get the fullname of an object, but I'm still not sure how to give gold. Any help is greatly appreciated.", "created_utc": 1412602453, "gilded": 0, "name": "t3_2ig2ci", "num_comments": 10, "score": 3, "title": "Gild Comments/Users", "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": "phil_s_stein", "body": "I'm trying to write a bot that responds to mentions. I'm using the simple loop below to test `get_mention()`. I start the loop, login as someone who is not UID, then mention them in a thread like so: `Hello /u/UID how are you?` (where UID is the reddit username). Then I login as UID and see the PM from the mention (meaning the mention worked I'm assuming), but the code below never tells me that it sees the mention. What am I doing wrong? The UID account has gold and has mentions enabled. The simple loop I'm testing with: import praw from time import sleep from BotConfig import UID, PASSWD if __name__ == '__main__': r = praw.Reddit('/u/phil_s_stein commandline praw testing: fetching username mentions with praw') r.login(username=UID, password=PASSWD) while True: for m in r.get_mentions(): print('{} mentioned: {}'.format(UID, m)) sleep(5) Does anyone have any bot code that responds to mentions that I could take a look at? Thanks. Edit: updated user agent as suggested.", "created_utc": 1412451639, "gilded": 0, "name": "t3_2iaspn", "num_comments": 7, "score": 1, "title": "How to use PRAW and get_mentions()?", "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "stats94", "body": "So I'm trying to develop a Bot that posts Game Day threads to the /r/KontinentalHL subreddit, but whenever I try to login I get the following error: Traceback (most recent call last): File \"startBot.py\", line 12, in r.login(Username,Password) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 1230, in login self.request_json(self.config['login'], data=data) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 510, in request_json response = self._request(url, params, data) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 375, in _request _raise_response_exceptions(response) File \"C:\\Python33\\lib\\site-packages\\praw\\internal.py\", line 172, in _raise_response_exceptions response.raise_for_status() File \"C:\\Python33\\lib\\site-packages\\requests\\models.py\", line 808, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 403 Client Error: Forbidden I'm certain my login details are correct, I'm confused! I'm probably being an idiot, but any help? Also when I try r.login() and input my username the program freezes and never gets to the password stage...", "created_utc": 1412334956, "gilded": 0, "name": "t3_2i6gcs", "num_comments": 3, "score": 1, "title": "[PRAW] HTTPError when trying to login?", "url": "https://www.reddit.com/r/redditdev/comments/2i6gcs/praw_httperror_when_trying_to_login/"}, {"author": "paulshapiro", "body": "Right now, I'm playing with PRAW but am wondering if there is an easier or already existing implementation for something like this?", "created_utc": 1412009714, "gilded": 0, "name": "t3_2hsvhj", "num_comments": 1, "score": 2, "title": "Easiest way to delete all posts for a particular sub?", "url": "https://www.reddit.com/r/redditdev/comments/2hsvhj/easiest_way_to_delete_all_posts_for_a_particular/"}, {"author": "F3AR3DLEGEND", "body": "Hey guys, I'm trying to give gold via the Reddit API. I want it to utilize the creddits already in the account. I saw the `api/v1/gold/give` method in the API docs but couldn't figure out how to use it. I can get the current user's `modhash` by logging in with PRAW. I tried this: import requests import praw reddit = praw.Reddit(user_agent = \"Gold Testing Bot\") reddit.login(my_user, my_pass) modhash = reddit.modhash client = requests.session() client.headers = {\"user-agent\": \"Gold Testing Bot\"} response = client.post(\"https://www.reddit.com/api/v1/gold/give\", {\"months\": 1, \"username\": some_user}) print response.text I am receiving a bunch of HTML, which appears to be the default Reddit home page. Is there any way to do this? I'm not sure how to utilize the modhash, so I think that's part of the issue. Thanks!", "created_utc": 1411834364, "gilded": 0, "name": "t3_2hmh42", "num_comments": 5, "score": 8, "title": "How to Give Gold with the API", "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "88rarely", "body": "It's supposed to breakdown karma from each subreddit. I got it from [here](https://praw.readthedocs.org/en/v2.1.16/pages/getting_started.html) /u/_Daimon_ didn't implement a print so I don't know where it goes to actually make the program be useful. I've been trying to get it work for more than two hours with no luck. import praw user_agent = (\"Karma breakdown 1.0 by /u/_Daimon_ \" \"github.com/Damgaard/Reddit-Bots/\") r = praw.Reddit(user_agent=user_agent) thing_limit = 10 user_name = \"_Daimon_\" user = r.get_redditor(user_name) gen = user.get_submitted(limit=thing_limit) karma_by_subreddit = {} for thing in gen: subreddit = thing.subreddit.display_name karma_by_subreddit[subreddit] = (karma_by_subreddit.get(subreddit, 0) + thing.score)", "created_utc": 1411362533, "gilded": 0, "name": "t3_2h3n2u", "num_comments": 4, "score": 1, "title": "How would I make this Karma Breakdown program print like it's supposed to?", "url": "https://www.reddit.com/r/redditdev/comments/2h3n2u/how_would_i_make_this_karma_breakdown_program/"}, {"author": "ReBurnInator", "body": "Hello everyone, I apologize in advance if this question has been asked before. I've been digging through this sub looking for an answer and I haven't had any luck. I'm using Python with PRAW, and today is the first time I've used PRAW or the reddit API. Hopefully this has a simple answer. I'm working on a script that reads the new submissions from a subreddit. The purpose of this subreddit is aggregate the most helpful submissions and comments from a group of themed subreddits. The purpose of the bot is to reply to each of the linked submissions/comments with a note informing the community that it has already been submitted. Think something like totes_meta_bot, but it only looks at the submissions in one subreddit and only replies in a handful of related subreddits instead of spamming all of reddit. The bot works by looping through the submissions in the meta subreddit, replying to each submission, and tracking what has been replied to so that it doesn't reply again. What I'm trying to figure out, without much success, is how to determine whether the URL in the submission I'm processing is a link to a comment or if it is a link to a submission. It could be either. I've been searching, and I know that you can use the get_submission function to some degree to get information when you know for sure whether your submission is a submission or a comment: import praw r = praw.Reddit('') submission = r.get_submission('http://www.reddit.com/r/redditdev/comments/10msc8/how_to_calculate_more_comments_count_not_just/') comment = r.get_submission('http://www.reddit.com/r/redditdev/comments/10msc8/how_to_calculate_more_comments_count_not_just/c6euu6b').comments[0] The problem I'm having is that I don't know for sure whether the URL is to a comment or submission without inspecting the URL. I could parse the URL and pull the thing id's out, and if there is a second thing id I would know for sure if the URL pointed to a comment. But that just seems hacky to me. I've been digging through the API docs, but I don't see anything. Is there something in PRAW that can inspect the URL and tell me whether it links to a comment or submission? Or do I need to write code to figure this out for myself? Thanks!", "created_utc": 1411350725, "gilded": 0, "name": "t3_2h35s9", "num_comments": 2, "score": 2, "title": "PRAW: Is there a way to tell whether a URL links to a reddit comment versus a reddit submission?", "url": "https://www.reddit.com/r/redditdev/comments/2h35s9/praw_is_there_a_way_to_tell_whether_a_url_links/"}, {"author": "b0wmz", "body": "I'm using search to figure out how many posts a user has previously submitted to a sub. I know that I'm not able to get more than 100 search results, but I can only retrieve 25 max. How can I raise this limit using praw? Also, is there a better alternative to retrieving the amount of the users's previous posts on a certain sub?", "created_utc": 1411139199, "gilded": 0, "name": "t3_2gv6zy", "num_comments": 9, "score": 1, "title": "Praw: Retrieve more than 25 search results", "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "bboe", "body": "PRAW client developers, I have made a PRAW branch to test using only HTTPS over the API. This change requires some testers to see if there any issues that did not come up from our set of unit tests. This is the first of a few improvements that will (hopefully soon) be released with PRAW version 3. If you want to start using HTTPS exclusively through PRAW please update via the following: pip install git+git://github.com/praw-dev/praw.git@praw3 If you experience any issues feel free to report them here, however filing a bug on github (https://github.com/praw-dev/praw/issues) would be ideal. Thanks!", "created_utc": 1410935671, "gilded": 1, "name": "t3_2gmzqe", "num_comments": 2, "score": 10, "title": "[PRAW] HTTPS enabled PRAW testing needed", "url": "https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/"}, {"author": "DAMN_it_Gary", "body": "using this simple code: r = praw.Reddit(user_agent='example') r.login(\"damn_it_gary\", \"hunter2\") r.send_message('zumicts', 'Subject Line', 'You are awesome!') I get this: praw.errors.NotLoggedIn: `please login to do that` on field `None`", "created_utc": 1410831323, "gilded": 0, "name": "t3_2gio0c", "num_comments": 9, "score": 1, "title": "Praw can't send messages (PM)", "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "Morgneer", "body": "For got to add [PRAW]", "created_utc": 1410827433, "gilded": 0, "name": "t3_2gihdt", "num_comments": 2, "score": 1, "title": "Is it possible to run a submission and comment stream in the same program?", "url": "https://www.reddit.com/r/redditdev/comments/2gihdt/is_it_possible_to_run_a_submission_and_comment/"}, {"author": "spinnelein", "body": "I'm having trouble going over 15,000 characters in self post text. For subs that are configured text posts only, the limit on reddit is something like 45,000 but praw errors out. praw.errors.APIException: (TOO_LONG) `this is too long (max: 15000.0)` on field `text` Ideas?", "created_utc": 1410759201, "gilded": 0, "name": "t3_2gfnzr", "num_comments": 2, "score": 1, "title": "15000 character limit on self posts in praw", "url": "https://www.reddit.com/r/redditdev/comments/2gfnzr/15000_character_limit_on_self_posts_in_praw/"}, {"author": "TheLazarbeam", "body": "1. I went through the PRAW tutorial and it mentioned that one of three big problems with bots is the issue of keeping it running all the time. Their remedy was seemingly to [keep the script open in my command prompt all the time and refresh it every 30 minutes.](https://praw.readthedocs.org/en/v2.1.16/pages/writing_a_bot.html) Am I missing something here, or is this what you have to do? Do people put their bots on personal servers? 2. Every time I run a PRAW script, every time no matter what, even if it encounters an exception, at the end it will print out: \"sys:1: ResourceWarning: unclosed \" \"C:\\Python34\\lib\\importlib\\_bootstrap.py:2127: ImportWarning: sys.meta_path is empty\" Should I be concerned? Am I failing to close my praw.Reddit() object? How can I get this to stop happening? I went to that spot in bootstrap.py and commented it out but to no avail. As you can see, I'm running Python 3.4, and I've heard PRAW has issues with 3.x. Could this be the issue? Thanks for reading all of that!", "created_utc": 1410742505, "gilded": 0, "name": "t3_2gezwe", "num_comments": 3, "score": 1, "title": "Two simple questions from a PRAW noob.", "url": "https://www.reddit.com/r/redditdev/comments/2gezwe/two_simple_questions_from_a_praw_noob/"}, {"author": "GoldenSights", "body": "I've found that I can't get users' information through their ID number, which I regularly do with submissions, comments, and even subreddits. [Comment](http://www.reddit.com/api/info.json?id=t1_ckgqal1) - t1_ckgqal1 [Submission](http://www.reddit.com/api/info.json?id=t3_2fvdyx) - t3_2fvdyx [Subreddit](http://www.reddit.com/api/info.json?id=t5_31x09) - t5_31x09 [Account?](http://www.reddit.com/api/info.json?id=t2_co7mq) - t2_co7mq Using praw, I can fetch a redditor and print his ID number, but using get_info() on that number returns nothing. I'm having a hard time finding someone else with the same problem. Can anyone point me in the right direction?", "created_utc": 1410589791, "gilded": 0, "name": "t3_2g9rig", "num_comments": 4, "score": 1, "title": "Why can't I get_info with an account fullname?", "url": "https://www.reddit.com/r/redditdev/comments/2g9rig/why_cant_i_get_info_with_an_account_fullname/"}, {"author": "ovooDE", "body": "I have been fiddling around to get a comment scraper running for a project of mine, and have started running with PRAW in python. The problem i have is that comment crawling slow down significantly after the initial 1000 comments alloted per subreddit, and i wonder if i can somehow speed it up. Currently it looks like this: r = praw.Reddit(user_agent = user_agent) comments = praw.helpers.comment_stream(r, subreddit, limit=None) for comment in comments: #do something with the comment As i said, the first 1000 comments per subreddit are fetched pretty quickly (100 per second) and after that it slows down to 5-10 ips, which should not be due to rate limitations by reddit. Maybe it would be faster to just use custom urls for get_content() with the comment ids?", "created_utc": 1410259273, "gilded": 0, "name": "t3_2fw9fz", "num_comments": 3, "score": 2, "title": "[PRAW] slowdown in helpers.get_comment_stream", "url": "https://www.reddit.com/r/redditdev/comments/2fw9fz/praw_slowdown_in_helpersget_comment_stream/"}, {"author": "GoldenSights", "body": "I am trying to pull all the items (or as many as I can) from /u/GoldenSights/liked. In the browser I can easily scroll through pages, but PRAW doesn't want to get more than 50 posts. I find that `get_liked()` does not accept the `limit=1000` argument that we usually use, nor `time='all'` What can I do to change the limit here? I'm probably missing something simple. Thank you in advance. [get_liked() documentation](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.Redditor.get_liked)", "created_utc": 1409958901, "gilded": 0, "name": "t3_2fle0e", "num_comments": 2, "score": 2, "title": "[PRAW] user.get_liked() only returns 50 items, does not support limit argument. How can I get more items?", "url": "https://www.reddit.com/r/redditdev/comments/2fle0e/praw_userget_liked_only_returns_50_items_does_not/"}, {"author": "Magzter", "body": "Using the following code: https://github.com/reddit/reddit/wiki/OAuth2-PHP-Example I've authorized the app on Reddit and updated the code with the correct client_id, client_secret and redirect_uri. The app succesfully redirects me to Reddit and after approving it takes me back with a json that says \"User Agent Required\". I can find a ton of posts showing how to set the user_agent string in Python using PRAW, but nothing on PHP. Anyone able to shed some light? I tried adding the following: $_SERVER['HTTP_USER_AGENT'] = \"MyTestApp/0.1 by /u/Magzter\"; But something tells me I'm wildly off.", "created_utc": 1409825800, "gilded": 0, "name": "t3_2ffxm9", "num_comments": 1, "score": 2, "title": "\"User Agent Required\" - PHP/OAuth", "url": "https://www.reddit.com/r/redditdev/comments/2ffxm9/user_agent_required_phpoauth/"}, {"author": "Leonlg", "body": "I started learning PRAW a while ago and I'm making a bot now, but I can't figure this out. Can someone explain how to do this? Thank you!", "created_utc": 1409672419, "gilded": 0, "name": "t3_2f9lyn", "num_comments": 4, "score": 3, "title": "[PRAW] How can I see if my bot already replied to a tread?", "url": "https://www.reddit.com/r/redditdev/comments/2f9lyn/praw_how_can_i_see_if_my_bot_already_replied_to_a/"}, {"author": "tryme1029", "body": "Using PRAW/Python", "created_utc": 1409517175, "gilded": 0, "name": "t3_2f3yz8", "num_comments": 1, "score": 0, "title": "How would I scan for all contents by a particular user and reply to them?", "url": "https://www.reddit.com/r/redditdev/comments/2f3yz8/how_would_i_scan_for_all_contents_by_a_particular/"}, {"author": "Lost_it", "body": "I am interested in the position(rank) of this post in the subreddit, not the score. edit: using praw", "created_utc": 1409458404, "gilded": 0, "name": "t3_2f265d", "num_comments": 2, "score": 1, "title": "Any way to get the rank of a submission in a subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/2f265d/any_way_to_get_the_rank_of_a_submission_in_a/"}, {"author": "kemitche", "body": "Hello everyone! I'm here to give some quick tips on what to put in your /r/redditdev post to make it easier for us to help you. \"XYZ isn't working\", on its own, is not enough information for us to help you! We need more specifics to be able to understand what exactly isn't working for you. Please try and include the following information in your posts. If you're not sure how to get some/all of it, google around or ask for additional assistance in your post. * The exact API URL you were hitting * The request headers (but obfuscate cookies and secrets!) * The POST data you're sending, if any * Any response headers * The response **body** In other words, include as much information about the raw HTTP request as you can. If you're using a library of some kind, be aware that not everyone is going to be familiar with whatever language, library, or SDK you're working with. That means that while including the code in your post can help, it's not *nearly* as helpful as the raw HTTP information. The main exception here is PRAW, as it has wide enough use that someone may be able to help without the raw HTTP info.", "created_utc": 1409081085, "gilded": 0, "name": "t3_2enl27", "num_comments": 1, "score": 10, "title": "Help us help you - include important debugging info!", "url": "https://www.reddit.com/r/redditdev/comments/2enl27/help_us_help_you_include_important_debugging_info/"}, {"author": "df27hswj95bdt3vr8gw2", "body": "[I want to pull stats from this page.](http://www.reddit.com/r/AskReddit/about/traffic/) I'm looking at PRAW and there doesn't seem to be a way, which isn't surprising because it seems to not even be available through the API. Is there a good way to do it?", "created_utc": 1409080911, "gilded": 0, "name": "t3_2enkr2", "num_comments": 2, "score": 2, "title": "PRAW/API: Is there a way to pull traffic stats from a subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/2enkr2/prawapi_is_there_a_way_to_pull_traffic_stats_from/"}, {"author": "Dobias", "body": "When using the following code import praw r = praw.Reddit('downs test') while True: subreddit = r.get_subreddit('programming') for submission in subreddit.get_hot(limit=10): print submission.ups, submission.downs the output is like 129 0 1274 0 19 0 78 0 15 0 22 0 22 0 53 0 5 0 157 0 So the downvote count is always zero. This can not be right, at least I guess. Am I doing something wrong or it this a bug in PRAW?", "created_utc": 1408537371, "gilded": 0, "name": "t3_2e2q2l", "num_comments": 7, "score": 3, "title": "PRAW - downvote count always zero?", "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": "hit_bot", "body": "Ok, so I've written some code that, for all intents and purposes, should work: def checkComments(comments): for comment in comments: print comment.body checkComments(comment.replies) def processSub(sub): sub.replace_more_comments(limit=None, threshold=0) checkComments(sub.comments) #login and subreddit init stuff here subs = mysubreddit.get_hot(limit=50) for sub in subs: processSub(sub) However, given a submission with a comment that has 50 nested replies like so: root comment -> 1st reply -> 2nd reply -> 3rd reply ... -> 50th reply The above code only prints: root comment 1st reply 2nd reply 3rd reply 4th reply 5th reply 6th reply 7th reply 8th reply 9th reply Any idea how I can get the remaining 41 levels of replies? Or is this a praw limitation?", "created_utc": 1408511791, "gilded": 0, "name": "t3_2e2191", "num_comments": 1, "score": 2, "title": "Praw - How to retrieve replies to a comment past 10 levels deep", "url": "https://www.reddit.com/r/redditdev/comments/2e2191/praw_how_to_retrieve_replies_to_a_comment_past_10/"}, {"author": "snaysler", "body": "Hi everyone, I am running an online course at /r/ludobots. The whole site is mediated and run by a group of bots that are always up and running on a remote server. Anyway, I've been getting more use recently and there is a huge problem. PRAW seems to no longer be able to edit the \"config/sidebar\" wiki page, which I use as a live feed for what people are doing on the site. As of about a day ago, all of the bots are no longer able to edit this wiki page. If I physically sign in to the bot accounts and navigate to the wiki page, I can edit it with no problem. In fact, I can edit it no problem on any mod account. But PRAW seems no longer capable of editing this page. I already have a connection failsafe with a try/except python loop that tries to connect to reddit 10 times with a 5 second break between attempts, then it quits the program. Because all my bots are trying to update \"config/sidebar\", somewhat regularly (every 30min or so) it seems like none of them can now. Why is this happening? Why is PRAW so damn unreliable? How can I fix this? The site is growing in popularity and I will only be getting more traffic, and I can't keep up anymore with all this live maintenance and apologies to users. Please Help! -Tayler B", "created_utc": 1408122660, "gilded": 0, "name": "t3_2dndq3", "num_comments": 10, "score": 3, "title": "Serious Issue with PRAW in my Sub (Urgent!!)", "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "irrestistable", "body": "I'm writing a reddit bot using PRAW for python and I'm having trouble going through the documentation to find the right commands. I'm specifically looking to find the score of a comment and a post but I can't seem to find it.", "created_utc": 1407971685, "gilded": 0, "name": "t3_2dhhrk", "num_comments": 7, "score": 2, "title": "PRAW command list?", "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "echocage", "body": "I'd like to know if anyone has a decent method for this using praw", "created_utc": 1407873301, "gilded": 0, "name": "t3_2dd6n8", "num_comments": 2, "score": 3, "title": "Get all new submissions, including subreddits who have excluded themselves from /r/all?", "url": "https://www.reddit.com/r/redditdev/comments/2dd6n8/get_all_new_submissions_including_subreddits_who/"}, {"author": "deletedLink", "body": "Hello everyone, I'd like to make a call out for a subreddit configuration file along the same lines as the internet maintains -- [the robots.txt file](http://en.wikipedia.org/wiki/Robots_exclusion_standard). It would work pretty simply. Just set up a page called /r//wiki/robots.txt with a line in it. 1. **Disallow all bots** User-agent: * Disallow: / 2. **Except my bot** User-agent: mybot Disallow: The \"Disallow\" field may be able to be customized to different categories. - Self-posts - Link-posts - Wiki - Fun (e.g. robots that are just fun) - Informational (e.g. unit conversion, wiki lookup) - Cross-linking Something along those lines. It would be pretty easy to create the functionality in PRAW to support and standardize this feature. Thoughts and input would be welcome. Cheers! DL", "created_utc": 1407749183, "gilded": 0, "name": "t3_2d7yqc", "num_comments": 9, "score": 4, "title": "Subreddit robots permission file -- analogous to robots.txt?", "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "bsturtle", "body": "I'm working on a bot to monitor author post frequency. The community only allows one submission per user every 7 days. I'm using sqlite to store submissions less than 7 days old, and then looking for new submissions every 30 min or so, comparing the author to the db. When creating the initial db and when checking for new submissions, I'd like to stop looking at the submissions if I hit one i've already seen, or if it is more than 7 days old. How does reddit or PRAW return the submission data? Is it in any order like oldest or newest first? Is it dependable? Thanks", "created_utc": 1407722409, "gilded": 0, "name": "t3_2d72ou", "num_comments": 6, "score": 2, "title": "[PRAW] How are submissions retrieved from a subreddit when get_new() is used?", "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "brucemo", "body": "I know the ID of a mail message, so I can append \"t4_\" in front of that to get its id, and use: o = rh.get_info(thing_id = \"t4_\" + msg) ... to return the thing. But this thing has no replies, even though in reality it has replies. This does nothing: for reply in o.replies: ... do stuff ... ... because o == []. It shouldn't be. How do I get the object's replies to populate? I don't want to waste people's time here, so am I supposed to be able to answer this question myself? The doc doesn't seem to do it because the objects aren't documented and I don't appear to have access to many functions that must exist under the hood. http://www.reddit.com/r/redditdev/comments/1kxd1n/how_can_i_get_the_replies_to_a_comment_with_praw/ I did find that but it didn't get me anywhere other than to tell me why my code doesn't work. I can't find an analogous means of solving the problem.", "created_utc": 1407392559, "gilded": 0, "name": "t3_2cv1ua", "num_comments": 2, "score": 1, "title": "How do I get this PRAW object to populate?", "url": "https://www.reddit.com/r/redditdev/comments/2cv1ua/how_do_i_get_this_praw_object_to_populate/"}, {"author": "mcLudobot", "body": "Hi, I'm a bot for an online course at /r/ludobots. I'm controlled by a python program using praw. All I have to do when my program runs is get my unread messages and reply to them. I use the get_unread function to do this. Unfortunately it seems that this method is unreliable and only works with about 60% of the messages I'm sent. For some reason, many of the messages I receive go unanswered by my program, though they are marked as read, which is strange. Does anyone know of this problem, or has anyone experienced inconsistent behavior from praw in terms of dealing with messages? The public launch of the course is this week, and we need me to be working correctly. Thanks!", "created_utc": 1407175412, "gilded": 0, "name": "t3_2clwr7", "num_comments": 2, "score": 1, "title": "PRAW get_unread() issue, please help", "url": "https://www.reddit.com/r/redditdev/comments/2clwr7/praw_get_unread_issue_please_help/"}, {"author": "tingmakpuk", "body": "Noob here, probably asking a stupid question. But I recently built a bot as a means of learning python, and in studying a handful of bots, they all seem to look for their trigger by just grabbing 100 comments at a time. Over and over. I can see how this is unavoidable for the wikibot, for example, because many posters would be unaware of its existence, but it still serves a useful function. But a lot of them are being called by name, or some other name related trigger. If there was an actual messaging system in the praw, the bots wouldn't have to grab comments haphazardly. I'm trying to make sense of dogetipbot now, and it almost seems like it's using such a messaging system -- but I can't be sure if it's that or a complex structure for grabbing comments. Does this exist? What's it called? Edit words.", "created_utc": 1407081405, "gilded": 0, "name": "t3_2ciblc", "num_comments": 2, "score": 1, "title": "Praw inefficiency? Or does this exist?", "url": "https://www.reddit.com/r/redditdev/comments/2ciblc/praw_inefficiency_or_does_this_exist/"}, {"author": "mathleet", "body": "Hey guys, I'm trying to write a function that checks to see if a specified user has already responded to a comment. This is what I have so far: def already_responded(comment, submission): \"\"\"Checks to see if user has already responded to this comment\"\"\" ## Parse replies to comment for replies in comment.replies: ## Check to see if it is a MoreComments object ## If it is, load more comments, and recurse this method if isinstance(replies, praw.objects.MoreComments): submission.replace_more_comments() already_responded(comment, submission) if not isinstance(replies, praw.objects.Comment): return False if str(replies.author) == \"fixed_username\": return True return False (FWIW, I know recursing the function probably isn't the most efficient way of getting more comments. Ideas there would be good too!) This function seems to have worked on some test cases, but when I try to use it on [this thread](http://www.reddit.com/r/videos/comments/2cfy08/a_couple_of_friends_of_mine_are_currently_on/cjf2dll) to see if haiku_robot has responded, it returns a False. The function can read that srpske is an author but nobody beyond that. Any ideas? Thanks! [EDIT] If it helps, here is a pastebin of some test code that I'm playing with to help illustrate the point. http://pastebin.com/yQQ1BbQV", "created_utc": 1407035742, "gilded": 0, "name": "t3_2ch33g", "num_comments": 3, "score": 2, "title": "Trying to use praw to see if a user has already responded to a comment. Any ideas?", "url": "https://www.reddit.com/r/redditdev/comments/2ch33g/trying_to_use_praw_to_see_if_a_user_has_already/"}, {"author": "Theemuts", "body": "Hi, I've written a script using PRAW to collect user data, but when I fetched the data of /u/stuff_and_crap AVG showed me this popup: http://i.imgur.com/BOLwFtt.png Is this a false positive? As far as I know, I'm only fetching text.", "created_utc": 1406973945, "gilded": 0, "name": "t3_2cex9y", "num_comments": 8, "score": 9, "title": "Virus detected in data returned by reddit", "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "haiguise1", "body": "Is there a way to get all comments in a thread without getting all MoreComments objects in PRAW? Or atleast all/most parent comments?", "created_utc": 1406814555, "gilded": 0, "name": "t3_2c8lem", "num_comments": 2, "score": 2, "title": "Is there a way to get all comments in a thread without getting all MoreComments objects in PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/2c8lem/is_there_a_way_to_get_all_comments_in_a_thread/"}, {"author": "FirestarterMethod", "body": "So I've written a bot in Python using PRAW. If certain conditions are met, the bot will post a comment. I have a cfg file with all the user-editable vars, and I read it in with ConfigParser. Right now it grabs all of the vars from the cfg file, but I've had to keep the message in the main bot file, because I can't figure out how to import the message from the cfg file and keep the correct markdown syntax. -------------- This is what's in the main body. MESSAGE = ''' Hello world. *I have multiple lines* ''' **This way displays the markdown correctly in the comment.** But when I make it: MESSAGE = configs.get('misc','message') and this in the .cfg file: [misc] message= ''' Hello world. *I have multiple lines* ''' it displays **badly (as 'code' in the comment).** ***Is there something I can do to fix this?***", "created_utc": 1406813762, "gilded": 0, "name": "t3_2c8k7m", "num_comments": 2, "score": 1, "title": "Using an external cfg file for multi-line markdown/comment?", "url": "https://www.reddit.com/r/redditdev/comments/2c8k7m/using_an_external_cfg_file_for_multiline/"}, {"author": "Dobias", "body": "Hi, as a test I wrote this small python code: import praw r = praw.Reddit('comment parser') comments = r.get_comments(\"redditdev\", limit=None) print len(list(comments)) The output is: 994 But I strongly guess there are more comments here. ;-) Are these only the top-level comments? How can I get all?", "created_utc": 1406641612, "gilded": 0, "name": "t3_2c1iv5", "num_comments": 2, "score": 1, "title": "How to get all comments ever written in a specific subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/2c1iv5/how_to_get_all_comments_ever_written_in_a/"}, {"author": "sworeiwouldntjoin", "body": "Hi all! I'm new to PRAW and I was just wondering, what's the most efficient way to process the source of a comment, and extract any links contained therein? I have a couple different ways in mind but I'm fairly certain I'm overlooking something. I've looked through the PRAW docs, and read through the comment parsing section, so I'm just wondering if I need to process the body of the comment as a string and pattern match until I get any links, or if there's a method that already exists for that? Please forgive me if this question is ignorant.", "created_utc": 1406370461, "gilded": 0, "name": "t3_2brrf9", "num_comments": 4, "score": 5, "title": "[PRAW] - How do I extract links from a comment?", "url": "https://www.reddit.com/r/redditdev/comments/2brrf9/praw_how_do_i_extract_links_from_a_comment/"}, {"author": "Eathed", "body": "I'm trying to grab the list of link flair templates from a subreddit my bot moderates. I've come across [get_flair_list](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModFlairMixin.get_flair_list) but it seems like that only returns a list of who made each post and what their user flair is. Am I overlooking something here or is there not a way to grab this?", "created_utc": 1406269606, "gilded": 0, "name": "t3_2bo5wq", "num_comments": 0, "score": 1, "title": "[PRAW] Getting a list of link flair templates", "url": "https://www.reddit.com/r/redditdev/comments/2bo5wq/praw_getting_a_list_of_link_flair_templates/"}, {"author": "naiyt", "body": "https://github.com/naiyt/reddit-replier Some prior knowledge of Python is expected. We have PRAW which abstracts the actual HTTP interactions with the Reddit API. Why not something on top of PRAW that simplifies creating reply bots? I've read the source for a lot of Reddit bots, and most weren't really designed to lend themselves well to reuse. redditreplier is intended to simplify the \"bot\" part of the process, and let you focus on what you actually want your bot to respond to and say. It will use PRAW to continually watch whatever subreddits you specify. It will then pass any new messages to your code, which decides whether it should reply and what it should reply with. You'll want to go through the README for more details, but you basically just create a class with a `parse` method. redditreplier will pass that any new messages. You then parse that message to decide if you want to respond, and act accordingly. This means creating new bots is a simple process that doesn't require you to rewrite all of the PRAW and Reddit interactions every time. It's still in beta, so there are likely some issues. The exception handling definitely needs some work, for example. Feel free to fork the repo and send in a pull request if you have any suggestions or fixes. I wrote a proof of concept [here](https://github.com/naiyt/autogithubbot) that is basically like AutoWikiBot, but for GitHub links. I'm not actually running it live anywhere because I didn't think anybody would really find it useful. But feel free to peruse that code if you want some ideas. Also, if there is interest in this, **please use it to make interesting, non spammy bots**. Nobody really likes uncalled for bots that respond to every message that contains a specific string. My favorite bots are those that only respond when specifically summoned (with specific keywords that won't accidentally be used) and provide the user with interesting information. If anybody has any questions or comments on this, feel free to PM me or open a GitHub issue.", "created_utc": 1406268671, "gilded": 0, "name": "t3_2bo4xn", "num_comments": 4, "score": 19, "title": "I wrote a Python module to make writing Reddit bots simpler (please use for good and not evil)", "url": "https://www.reddit.com/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/"}, {"author": "ThreeCorners", "body": "*I apologize in advance if this is the wrong subreddit for this, but I didn't know of a better one. This arose when I was coding a project with PRAW.* So I've been tracking score over time for various reddit submissions. I found that the most popular submissions often have this very strange \"stepping\" feature, as for example here: [http://imgur.com/saKf104](http://imgur.com/saKf104). Does anybody know if this some aspect of vote fuzzing (it looks as though batches of votes being dropped periodically), or if there is some other explanation? It looks artificial, so my guess is that it is some undocumented feature of reddit.", "created_utc": 1406235798, "gilded": 0, "name": "t3_2bmq3s", "num_comments": 3, "score": 2, "title": "Very bizarre submission score behavior", "url": "https://www.reddit.com/r/redditdev/comments/2bmq3s/very_bizarre_submission_score_behavior/"}, {"author": "tylerdurdenbot", "body": "Honestly, and imho, this is the single most annoying thing about the reddit api for bots. I haven't personally run into this error for years. I feel like there should be a way for me to authorize any bots that I create so that they can get over this **new user** limitation. But since there isn't any way around this as of yet, PRAW reply bot developers, how are you handling this error? Do you just wait out the 10 minutes or so, or do you somehow create some kind of queue and test the oldest item every cycle or so?", "created_utc": 1406235345, "gilded": 0, "name": "t3_2bmp83", "num_comments": 0, "score": 4, "title": "PRAW - \"You're Doing That too Much!\"", "url": "https://www.reddit.com/r/redditdev/comments/2bmp83/praw_youre_doing_that_too_much/"}, {"author": "DarkMio", "body": "Hi, my problem has to do with the praw.helpers_comment_stream, which seems to have more data than I can access to. import praw r = praw.Reddit(user_agent='Data-Poller for several bot-logins by /u/DarkMio') r_ss = praw.Reddit(user_agent='Small Subreddit Mention Bot by /u/DarkMio') r_ss.login('Botaccount', 'Botpassword') comment_stream = praw.helpers.comment_stream(r, 'dota2', limit=1, verbosity=3) comment = next(comment_stream) This is my general setup. Since I don't want to create more instances than needed, I would like to reply with multiple accounts. (As you may be aware, some subreddits ban particular bots, some don't.) The problem is, that the vars(comment) seems to lack data, where I could point my other praw-instance at. Sample data: {'_info_url': 'http://www.reddit.com/api/info/', '_replies': None, '_submission': None, '_underscore_names': ['replies'], 'approved_by': None, 'author': Redditor(user_name='fr0z3n_'), 'author_flair_css_class': u'thrall', 'author_flair_text': u'ill disrupt you', 'banned_by': None, 'body': u'He played Dota 1 for mouz. But i was talking about the Dota 2 scene :)\\n\\n', 'body_html': u'<div class=\"md\"><p>He played Dota 1 for mouz. But i was talking about the Dota 2 scene :)</p>\\n</div>', 'controversiality': 0, 'created': 1406233756.0, 'created_utc': 1406204956.0, 'distinguished': None, 'downs': 0, 'edited': False, 'gilded': 0, 'has_fetched': True, 'id': u'cj6dzxq', 'json_dict': None, 'likes': None, 'link_author': u'hoang_iee2000', 'link_id': u't3_2bl1t2', 'link_title': u'Puppey - The one-man army', 'link_url': u'http://imgur.com/EDD8o6j', 'name': u't1_cj6dzxq', 'num_reports': None, 'parent_id': u't1_cj6dw60', 'reddit_session': , 'saved': False, 'score': 1, 'score_hidden': True, 'subreddit': Subreddit(display_name='DotA2'), 'subreddit_id': u't5_2s580', 'ups': 1} Either I am completely missing here something (like generating permalinks out of the comment-id - or it isn't possible at all. Thanks for you interest. Edit: Appearantly I've found the most nastiest way to do that in PRAW: import praw from pprint import pprint r = praw.Reddit(user_agent='Data-Poller for several bot-logins by /u/DarkMio') r_ss = praw.Reddit(user_agent='SmallSubredditBot by /u/DarkMio') found = 0 comment_stream = praw.helpers.comment_stream(r, 'all', limit=1, verbosity=3) comment = next(comment_stream) #pprint(vars(comment)) #print comment.link_id thread = str(comment.link_id).replace('t3_', '') submission = r_ss.get_submission(submission_id=thread) submission.replace_more_comments(limit=None, threshold=0) flat_comments = praw.helpers.flatten_tree(submission.comments) print comment.name target_comment = comment.name.replace('t1_', '') for comments in flat_comments: if comments.id == target_comment: print comment.body.encode('utf-8') comment.reply(\"Thanks.\") the big problem with this example is, that I don't target the comment, I just point at the thread where the target can be found and try to load all comments if possible. This fails if there are many stacked comments in a row. How stated, this doesn't look nice nor do I think is it good or anything. A really nasty way to get from one instace the data - and reply with another instance.", "created_utc": 1406205446, "gilded": 0, "name": "t3_2bl8ew", "num_comments": 3, "score": 2, "title": "PRAW - loading with one instance - replying with another instance", "url": "https://www.reddit.com/r/redditdev/comments/2bl8ew/praw_loading_with_one_instance_replying_with/"}, {"author": "rreyv", "body": "Reddit Live is awesome! Are there any plans to support creating live threads in PRAW?", "created_utc": 1406147247, "gilded": 0, "name": "t3_2bj2h3", "num_comments": 2, "score": 2, "title": "PRAW and creating live threads?", "url": "https://www.reddit.com/r/redditdev/comments/2bj2h3/praw_and_creating_live_threads/"}, {"author": "DarkMio", "body": "G'day, I run into an inconsitency with PRAW when using def submission_stream(self): \"\"\"Checks all submissions. Either they're link.posts or self.posts. Either way, we catch both.\"\"\" while True: try: submission_stream = praw.helpers.submission_stream(self.r, self.subreddit, limit=None, verbosity=0) log.info(\"Opened submission stream successfully.\") while self.running == True: submission = next(submission_stream) # retrieve the next submission self.subreddit is 'all' in this case. It is common that I get this error every few minutes lately: Traceback (most recent call last): File \"massdrop_linkfix.py\", line 173, in submission_stream submission = next(submission_stream) # retrieve the next submission File \"/usr/local/lib/python2.7/dist-packages/praw/helpers.py\", line 118, in _stream_generator for i, item in gen: File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 471, in get_content page_data = self.request_json(url, params=params) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 517, in request_json delattr(self, '_request_url') AttributeError: _request_url I am running this on an Raspberry Pi with a Debian + Python 2.7.7 - what's happening here?", "created_utc": 1405785947, "gilded": 0, "name": "t3_2b50uq", "num_comments": 0, "score": 1, "title": "PRAW submission helper is running inconsistently?", "url": "https://www.reddit.com/r/redditdev/comments/2b50uq/praw_submission_helper_is_running_inconsistently/"}, {"author": "WilliamNyeTho", "body": "I understand that this is a very simple question, and I have been to the recommended sites to download it. I currently have pip installed. The next step says to type: $ pip install praw But it doesn't say where to type this (I'm running windows 8). Any quick advice is appreciated. Thanks!", "created_utc": 1405736490, "gilded": 0, "name": "t3_2b3nua", "num_comments": 9, "score": 2, "title": "How do I install PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "brucemo", "body": "The following is a minimal program that is supposed to go through your mod mail and print out the authors of each of your messages. The real program does more than this, but I removed a bunch of code in order to eliminate other explanations for the error I received. This code worked in previous versions of python and/or PRAW. I reinstalled Windows 8.1 recently so I got new everything. Python 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 10:38:22) [MSC v.1600 32 bit (Intel)] on win32 .. and what appears to be praw-2.1.17 I do not know what I had previously. Other PRAW scripts run, but this one does not. import praw, sys def main(argv): rh = praw.Reddit(\"whatever\") rh.login() for obj in rh.get_mod_mail(limit = 1000): if obj.author: name = obj.author.name else: name = \"[Deleted]\" print(name) if __name__ == \"__main__\": main(sys.argv[1:]) I am a mod of stuff and have mod mail. The output of the above program is nothing. It blows up as follows on my planet: Traceback (most recent call last): File \"testm.py\", line 15, in main(sys.argv[1:]) File \"testm.py\", line 7, in main for obj in rh.get_mod_mail(limit = 1000): File \"C:\\Python\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 295, in wrapped function.func_defaults[0]) AttributeError: 'function' object has no attribute 'func_defaults' sys:1: ResourceWarning: unclosed C:\\Python\\Python34\\lib\\importlib\\_bootstrap.py:2150: ImportWarning: sys.meta_path is empty", "created_utc": 1405649379, "gilded": 0, "name": "t3_2b0f51", "num_comments": 3, "score": 2, "title": "PRAW: Is get_mod_mail broken?", "url": "https://www.reddit.com/r/redditdev/comments/2b0f51/praw_is_get_mod_mail_broken/"}, {"author": "elihusmails", "body": "I was running into a SSL \"Certificate verify failed\" error in PRAW when logging in to Reddit. I found that if I use r = Reddit(user_agent=username) r.config._ssl_url = None r.login(username, password) things work, but when I log in, my username/password is sent in the clear. So how can I install the proper cert(s) into my Python environment to prevent this from happening?", "created_utc": 1405478149, "gilded": 0, "name": "t3_2atnid", "num_comments": 3, "score": 1, "title": "How to install Reddit SSL Cert?", "url": "https://www.reddit.com/r/redditdev/comments/2atnid/how_to_install_reddit_ssl_cert/"}, {"author": "elihusmails", "body": "I have a python bot that goes through my liked and saved links and parses comments. All was working fine until I noticed that when I call \"login\" from PRAW, I now get SSL certificate verify failed error. if __name__ == '__main__': username = sys.argv[1] password = sys.argv[2] r = Reddit(user_agent=username) r.login(username, password) print 'LOGIN SUCCESS' There are situations where I am re-posting data from comments to another subreddit, so I do need to login for the bot to be successful. Did something either change in PRAW or Reddit that would cause this and how do I fix it? EDIT: I'm going to try and verify this to get things working - http://stackoverflow.com/questions/21655478/sslerror-with-praw EDIT2: The link above did work, for anyone who may have run into the same problem.", "created_utc": 1405334094, "gilded": 0, "name": "t3_2anne2", "num_comments": 3, "score": 2, "title": "PRAW 2.1.17 - certificate verify failed", "url": "https://www.reddit.com/r/redditdev/comments/2anne2/praw_2117_certificate_verify_failed/"}, {"author": "honeyplease", "body": "I am building a bot in Python and I'm having trouble with the final stage in which it replies to certain comments. Suppose I know a comment in the 'all' subreddit says \"Apples are green.\", and I want the bot to reply to it, but I don't know its id. I am able to find the comment in the list of flattened comments: submissions = r.get_subreddit('all').get_hot(limit=100) submission = next(submissions) flat_comments = praw.helpers.flatten_tree(submission.comments) print(flat_comments[0]) # Apples are green But how could I retrieve the comment id from this? My goal is to reply to get the id for the comment that matches 'Apples are green', so I can reply to this comment specifically in the last step. I guess I should add that I've been extracting the comments directly from the json dictionary using regex, not using PRAW, so I ended up having a comment without an identifier in PRAW. I am able to see the id of all parent comments using vars(comments) as in the tutorial, but I haven't figured out how to see the vars for the child comments, maybe this is what would help me solve my problem. The tutorials and man pages online aren't too helpful for me because they assume more skills than I have. The nested nature of the json dict makes it kind of a pain to find the comment id using regex, that's why I turned to PRAW for this, but I'm stuck.", "created_utc": 1405326760, "gilded": 0, "name": "t3_2anhlc", "num_comments": 4, "score": 1, "title": "PRAW Retrieve comment id from body, then reply to it.", "url": "https://www.reddit.com/r/redditdev/comments/2anhlc/praw_retrieve_comment_id_from_body_then_reply_to/"}, {"author": "linpressionism", "body": "I'm setting up bots to moniter/update a set of wikis for a subreddit, but can't find a way to 1) set wikis to editable only by mods via praw (instead of by hand) or 2) restore a previous version of a wiki via praw. Does anyone know if/how these things are possible? Thanks!", "created_utc": 1404847975, "gilded": 0, "name": "t3_2a65gw", "num_comments": 0, "score": 2, "title": "Wiki restoration and editor control through praw?", "url": "https://www.reddit.com/r/redditdev/comments/2a65gw/wiki_restoration_and_editor_control_through_praw/"}, {"author": "theroflcoptr", "body": "Some subreddits allow normal users to set submission flair. Is there any way to get PRAW to do this? Seems like it immediately throws a ModeratorOrScopeRequired error without actually trying to set the flair.", "created_utc": 1404175198, "gilded": 0, "name": "t3_29isgn", "num_comments": 1, "score": 0, "title": "Using PRAW to set submission flair without moderator permissions", "url": "https://www.reddit.com/r/redditdev/comments/29isgn/using_praw_to_set_submission_flair_without/"}, {"author": "Cuisinier_Microsoft", "body": "Hello /r/redditdev, I've been looking around the doc and the internet but I can't seem to find how to get the date of a submission with praw. Is it even possible?", "created_utc": 1404101128, "gilded": 0, "name": "t3_29g2sd", "num_comments": 5, "score": 2, "title": "Getting the date of a submission", "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "DarkMio", "body": "##NVM - I was retarded. next(comment_stream) simply exhausted and stopped the iteration of the stream, since dota2moddingtesting has only very few comments. That stopped the further execution. --- ##Original Post: Running this: import praw r = praw.Reddit(user_agent='MASSDROP guest link poster') comment_stream = praw.helpers.comment_stream(r, 'dota2moddingtesting') submission_stream = praw.helpers.submission_stream(r, 'dota2moddingtesting') while True: comment = next(comment_stream) # Retrieve the next comment submission = next(submission_stream) # retrieve the next submission print submission returns this: Items: 32 (107.74 ips) Items: 36 (18.53 ips) 4 :: Custom Smell Powe r 2 :: Drow Ranger Nekkid 2 :: [Custom Gamemodes] Index for all relevant topics. 3 :: w3x-to-vmf now gets you VAC banned 3 :: Beep bop beep I AMZ A ROBOTZ beep 2 :: aaaa 2 :: Is this real life? 2 :: No pun intended 2 :: Spam 3 :: darkmio peen size confirmed 2 :: modding apply directly to the bobbin 6 :: Alchemist realism mod 2 :: maximum amounts of letters are hard to reach, but when you do, you know,... 2 :: Stylesheet Test for Stylesheet Kids 2 :: Custom Gamemodes are scientifically proven to make karma 2 :: Kappa Kappa & the Chameleons 2 :: Making Titanfall ... er ... what? 2 :: The \"oh the mods broke the subreddit\" thread. 2 :: The new design is now online! 1 :: this update test 0 :: Lorem Ipsum Dolor 2 :: lore dispum 2 :: need more brainless content 2 :: That's a bot message. 2 :: 05/22/14 02:59 PM 2 :: Dota 2 main client update for 05/22/14 03:14 PM 2 :: Dota 2 main client update for 05/22/14 03:18 PM 1 :: Dota 2 main client update for 05/24/14 10:02 AM 0 :: Dota 2 main client update for 05/24/14 11:07 AM 2 :: Stretch Goal #20 unlocked: New Upgraded Creeps 2 :: Stretch Goal #20 unlocked: New Upgraded Creeps 2 :: Stretch Goal #21 unlocked: A->Z Challenge Support Looking at /r/dota2moddingtesting, you'll see, that there are 3 posts missing: 2 about massdrop and one about Stretch Goal #22. Am I missing something here? It's pretty odd that 3 submissions are entirely missing. Edit: To give more detail, what I want to do, is to check recent comments and submissions for massdrop-links. Running something like comment_stream = praw.helpers.comment_stream(r, 'dota2moddingtesting') submission_stream = praw.helpers.submission_stream(r, 'dota2moddingtesting') for comments, submissions in zip(comment_stream, submission_stream): # do work seems to break the iterator-process entirely, ending in never running the for-loop itself.", "created_utc": 1403989462, "gilded": 0, "name": "t3_29chn0", "num_comments": 0, "score": 1, "title": "PRAW praw.helpers.submission_stream() works, but is missing some entries", "url": "https://www.reddit.com/r/redditdev/comments/29chn0/praw_prawhelperssubmission_stream_works_but_is/"}, {"author": "labtec901", "body": "I've been following this tutorial https://praw.readthedocs.org/en/v2.1.16/pages/writing_a_bot.html to create the question discover bot with /u/_Daimon_ 's tutorial, but now I'd like to do the same thing, but with the comments in a particular subeddit. How would I go about this?", "created_utc": 1403886291, "gilded": 0, "name": "t3_2991wo", "num_comments": 1, "score": 1, "title": "[PRAW] Writing a Question-Discover program for comments rather than self posts.", "url": "https://www.reddit.com/r/redditdev/comments/2991wo/praw_writing_a_questiondiscover_program_for/"}, {"author": "BadgerBalls", "body": "I'm using PRAW to delete and re-upload an image to my sub. It doesn't look like the image takes effect until I manually go in to about/stylesheet and click the \"save\" button. Is there a way to do this via PRAW?", "created_utc": 1403655826, "gilded": 0, "name": "t3_290jf7", "num_comments": 2, "score": 1, "title": "PRAW equivalent of clicking the \"Save\" button on the Stylesheet page?", "url": "https://www.reddit.com/r/redditdev/comments/290jf7/praw_equivalent_of_clicking_the_save_button_on/"}, {"author": "ThreeCorners", "body": "I want to add up a given user's link karma and comment karma. Let's take u/unidan, for example. This gives numbers that are quite different from the numbers reported at www.reddit.com/u/unidan. Here's a snippet: import praw r = praw.Reddit('Testing PRAW.') u = r.get_redditor('unidan') link_karma = 0 for p in u.get_submitted(limit = None): link_karma += p.score print link_karma His profile gives 158,784 link karma, 2,235,856 comment karma. PRAW gives 219,160 link karma, 85,198 comment karma.", "created_utc": 1403590581, "gilded": 0, "name": "t3_28y0c8", "num_comments": 11, "score": 3, "title": "Very inaccurate karma via PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "chocolatecloud12", "body": "When I type in 'pip install praw' in the terminal, I get a traceback. File \"(stdin)\", line 1 pip install praw ^ SyntaxError: invalid syntax I'm using python 3.3.5, and I have pip installed. Please explain to me 'noob friendly' as possible. Thanks.", "created_utc": 1403477378, "gilded": 0, "name": "t3_28tqod", "num_comments": 5, "score": 2, "title": "Having problems with installing PRAW using pip", "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": "baloneyeagle123", "body": "Here is the code I'm trying to run: def scan_submission(submission): for comment in submission.comments: #What throws the error scan_comment(submission, comment) Which is exactly how every place I've found has said to get comments from a submission. But when I try this to get the following error: AttributeError: '' has no attribute 'comments' Which is extremely confusing to me as the official PRAW documentation shows that the Submission object does contain a comments attribute.", "created_utc": 1403127692, "gilded": 0, "name": "t3_28hwaz", "num_comments": 2, "score": 0, "title": "Error trying to get comments from a submission in PRAW", "url": "https://www.reddit.com/r/redditdev/comments/28hwaz/error_trying_to_get_comments_from_a_submission_in/"}, {"author": "stickytruth", "body": "I'm suddenly getting inconsistent results using Submission.comments in praw where it is throwing exceptions. Python 3.3.0 Praw 2.1.16 and 2.1.15 Demo script: import sys, praw print('Python version:',sys.version_info) print('Praw Version:', praw.__version__) print('------------') reddit = praw.Reddit('/u/stickytruth bug hunt', log_requests=1) items = ['t3_28holl', # comments = [] # http://www.reddit.com/r/photoshopbattles/comments/28holl/this_parrot_skateboarding/ 't3_28hlqq', # comments = '' has no attribute 'comments' # http://www.reddit.com/r/photoshopbattles/comments/28hlqq/amazons_jeff_bezos/ 't3_28hl19', # comments = '' has no attribute 'comments' # http://www.reddit.com/r/photoshopbattles/comments/28hl19/italian_football_player_marchisio_levitating_in/ 't3_28hb7u' # comments = [] # http://www.reddit.com/r/photoshopbattles/comments/28hb7u/a_buddy_of_mine_midcatch_during_an_ultimate/ ] for thing in reddit.get_submissions(items): print('\\nLoading', thing.title) try: print('# Comments', thing.num_comments) print(thing.comments) except Exception as e: print(e) --- Output [2.1.16]: Python version: sys.version_info(major=3, minor=3, micro=0, releaselevel='final', serial=0) Praw Version: 2.1.16 ------------ retrieving: http://www.reddit.com/by_id/t3_28holl,t3_28hlqq,t3_28hl19,t3_28hb7u.json Loading This Parrot Skateboarding # Comments 0 retrieving: http://www.reddit.com/r/photoshopbattles/comments/28holl/this_parrot_skateboarding/.json [] Loading Amazon's Jeff Bezos # Comments 1 retrieving: http://www.reddit.com/r/photoshopbattles/comments/28hlqq/amazons_jeff_bezos/.json '' has no attribute 'comments' Loading Italian football player Marchisio levitating in training # Comments 2 retrieving: http://www.reddit.com/r/photoshopbattles/comments/28hl19/italian_football_player_marchisio_levitating_in/.json '' has no attribute 'comments' Loading A buddy of mine mid-catch during an Ultimate Frisbee league game # Comments 1 retrieving: http://www.reddit.com/r/photoshopbattles/comments/28hb7u/a_buddy_of_mine_midcatch_during_an_ultimate/.json [] --- Results with praw 2.1.15 are identical. Any idea what's causing this? Thanks", "created_utc": 1403125099, "gilded": 0, "name": "t3_28hrir", "num_comments": 3, "score": 0, "title": "PRAW Comment Errors", "url": "https://www.reddit.com/r/redditdev/comments/28hrir/praw_comment_errors/"}, {"author": "laptopdude90", "body": "Is there a way to submit a post with praw? I found a r.submit() function, but it's not in the documentation so I don't know what arguments it wants.", "created_utc": 1403045966, "gilded": 0, "name": "t3_28etuk", "num_comments": 2, "score": 2, "title": "Submit a post with Praw", "url": "https://www.reddit.com/r/redditdev/comments/28etuk/submit_a_post_with_praw/"}, {"author": "AnSq", "body": "PRAW has [`get_my_moderation`](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.MySubredditsMixin.get_my_moderation) that uses [`/subreddits/mine/moderator`](http://www.reddit.com/dev/api#GET_subreddits_mine_moderator), and of course there's [`get_moderators`](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_moderators)/[`/r/[subreddit]/about/moderators`](http://www.reddit.com/dev/api#GET_about_moderators), bit I can't seem to find anything to go the other way for any user. Did I just miss it, or does it really not exist? The list of subs someone mods is now public in the sidebar of their profile, so it seems like there should be an easy way to get it programmatically. [That was only added a few months ago](http://www.reddit.com/r/modnews/comments/1wem4f/moderators_the_list_of_subreddits_you_moderate_is/) though, so maybe there isn't.", "created_utc": 1402935946, "gilded": 0, "name": "t3_28aga6", "num_comments": 3, "score": 5, "title": "Is there any way to get a list of subreddits that someone else moderates?", "url": "https://www.reddit.com/r/redditdev/comments/28aga6/is_there_any_way_to_get_a_list_of_subreddits_that/"}, {"author": "F3AR3DLEGEND", "body": "Hey guys, I am working on a specific Reddit bot which will utilize PRAW. To minimize the amount of requests required (so that I do not have to wait one/two seconds between each request), am I able to send multiple replies through one request? If this is possible through PRAW, please tell me how! Thanks!", "created_utc": 1402868523, "gilded": 0, "name": "t3_288anc", "num_comments": 1, "score": 2, "title": "Reply to multiple messages in one request", "url": "https://www.reddit.com/r/redditdev/comments/288anc/reply_to_multiple_messages_in_one_request/"}, {"author": "Alexratman", "body": "So this bot is used on /r/nerdcubed, what is does is when someone says, \"he is banned from Germany\", it will reply with he is banned from many places. I've got two issues, first its not finding the comments or its not responding to them, secondly I don't think the \"already_done = []\" works, this might be the reason why comments are not being found or responded to. import traceback import time import sys import praw import re nerd_exp = re.compile(ur'(he|dan|nerd).{2,15}?(banned from)', re.IGNORECASE) r = praw.Reddit(user_agent='Responds to banned from jokes in /r/nerdcubed. Created by /u/echocage for /u/tokyorockz, then edited by /u/alexratman') username = 'username' r.login('username', 'password') already_done = [] try: subreddit = r.get_subreddit('nerdcubed') posts = subreddit.get_new() for submission in posts: flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: if not hasattr(comment, 'body'): continue if not comment in already_done: res = re.search(nerd_exp, comment.body) if res is not None: if not 'many' in comment.body: if not comment.author == username: if not any([x for x in comment.replies if (str(x.author) == username) or ('many' in x.body)]): print comment comment.reply(\"He's banned from many places\") time.sleep(5) already_done.append(comment) except praw.errors.RateLimitExceeded as err: print \"Rate Limit Exceeded:\\n\" + str(err), sys.stderr time.sleep(err.sleep_time + .05) except: print traceback.format_exc() I'm new to python so this is the most advanced thing I have done. Any help is appreciated!", "created_utc": 1402861360, "gilded": 0, "name": "t3_2880cz", "num_comments": 3, "score": 5, "title": "Comment bot not commenting, What have I done wrong?", "url": "https://www.reddit.com/r/redditdev/comments/2880cz/comment_bot_not_commenting_what_have_i_done_wrong/"}, {"author": "BananaPotion", "body": "Can't find anything in the praw documentation.", "created_utc": 1402701223, "gilded": 0, "name": "t3_283bqr", "num_comments": 2, "score": 2, "title": "[PRAW] How to get newest posts of last week from a given subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/283bqr/praw_how_to_get_newest_posts_of_last_week_from_a/"}, {"author": "nallen", "body": "In AskScience and Science we have a lot of flaired users, and we'd like to be able to add them to a private subreddit, right now it's done entirely by hand, which is a real chore if several people have added flair. Is there an existing script or bot that would automate the process, like the script that syncs flair between two subreddits? I'm not a programmer, but it seems like something that should be straight forward to do with PRAW.", "created_utc": 1402667325, "gilded": 0, "name": "t3_281u72", "num_comments": 6, "score": 2, "title": "Syncing a Flaired User List to Approved Submitter List", "url": "https://www.reddit.com/r/redditdev/comments/281u72/syncing_a_flaired_user_list_to_approved_submitter/"}, {"author": "polhemic", "body": "(Python 2.7 on ubuntu 12.04LTS) I've been looking at praw and it doesn't seem to be handle the after_field argument as I would expect. Have I made a dumbass mistake in here? import praw user_agent = 'Test dlr by /u/polhemic' subreddit = 'reactiongifs' r = praw.Reddit(user_agent=user_agent) sr = r.get_subreddit(subreddit) after_id=None while True: print after_id submissions = sr.get_new(limit=10,after_field=after_id) if submissions == None: break for sub in submissions: print sub.id + ' ' + sub.url after_id = subs.id The tty output comes back from this as: None 2801zr http://i.imgur.com/caqh0iY.gif 28014b http://i.imgur.com/eF8kh6w.gif 27zzm8 http://imgur.com/XUg3aqp 27zz6z http://i.imgur.com/oqh7Qt8.gif 27zya9 http://i.imgur.com/bZLtuUQ.gif 27zy24 http://i.imgur.com/meftV0q.gif 27zvkj http://i.imgur.com/OqM3JRT.gif 27zv2b http://i.imgur.com/TiBjpCN.gif 27zuq9 https://i.imgur.com/XujHL.gif 27ztpe http://i.imgur.com/NFGg4QP.gif 27ztpe 2801zr http://i.imgur.com/caqh0iY.gif 28014b http://i.imgur.com/eF8kh6w.gif 27zzm8 http://imgur.com/XUg3aqp 27zz6z http://i.imgur.com/oqh7Qt8.gif 27zya9 http://i.imgur.com/bZLtuUQ.gif 27zy24 http://i.imgur.com/meftV0q.gif 27zvkj http://i.imgur.com/OqM3JRT.gif 27zv2b http://i.imgur.com/TiBjpCN.gif 27zuq9 https://i.imgur.com/XujHL.gif 27ztpe http://i.imgur.com/NFGg4QP.gif 27ztpe 2801zr http://i.imgur.com/caqh0iY.gif 28014b http://i.imgur.com/eF8kh6w.gif 27zzm8 http://imgur.com/XUg3aqp 27zz6z http://i.imgur.com/oqh7Qt8.gif 27zya9 http://i.imgur.com/bZLtuUQ.gif 27zy24 http://i.imgur.com/meftV0q.gif 27zvkj http://i.imgur.com/OqM3JRT.gif 27zv2b http://i.imgur.com/TiBjpCN.gif 27zuq9 https://i.imgur.com/XujHL.gif 27ztpe http://i.imgur.com/NFGg4QP.gif Which then repeats these last blocks ad-infinitum, never progressing further back through history.", "created_utc": 1402611459, "gilded": 0, "name": "t3_28037u", "num_comments": 2, "score": 3, "title": "[praw] Can't get praw.Object.Subreddit.get_new to honor after_field argument", "url": "https://www.reddit.com/r/redditdev/comments/28037u/praw_cant_get_prawobjectsubredditget_new_to_honor/"}, {"author": "VoterBot", "body": "I'd be grateful for suggestions on how to debug this issue with PRAW. I don't have direct access to praw/helpers.py or praw/decorators.py (nor would I want to dick around with them), and as a novice with Python I don't know how to reliably replicate the intermittent network flaw which throws the ValueError, which I assume is being caused by a trashed JSON request or something. Here's the log: [0m Traceback (most recent call last): [0m File \"/app/.heroku/python/lib/python3.4/site-packages/praw/__init__.py\" , line 471, in get_content [0m page_data = self.request_json(url, params=params) [0m File \"/app/.heroku/python/lib/python3.4/site-packages/praw/__init__.py\", line 516, in request_json [0m data = json.loads(response, object_hook=hook) [0m File \"myTest.py\", line 426, in [0m for submission in submissionStream: [0m File \"/app/.heroku/python/lib/python3.4/site-packages/praw/helpers.py\", line 118, in _stream_generator [0m File \"/app/.heroku/python/lib/python3.4/json/decoder.py\", line 343, in decode [0m for i, item in gen: [0m File \"/app/.heroku/python/lib/python3.4/site-packages/praw/decorators.py\", line 161, in wrapped [0m ValueError: Unterminated string starting at: line 1 column 2153 (char 2152) [0m return_value = function(reddit_session, *args, **kwargs) [0m obj, end = self.raw_decode(s, idx=_w(s, 0).end()) [0m File \"/app/.heroku/python/lib/python3.4/json/__init__.py\", line 331, in loads [0m obj, end = self.scan_once(s, idx) [0m return cls(**kw).decode(s) [0m sys:1: ResourceWarning: unclosed [0m /app/.heroku/python/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty [0m File \"/app/.heroku/python/lib/python3.4/json/decoder.py\", line 359, in raw_decode When this crashes praw.helpers.submission_stream() it also gives the 'unlcosed socket warning' at the bottom there, which I complained about a couple of days ago here. Socket timeouts also generate that warning. I've worked around the problem by simply re-calling the submission_stream() when it crashes, but this is inelegant and results in a heavier load on the reddit server than is necessary, especially since it can happen quite often at certain times of day on Heroku. Is this a PRAW bug? Or would you think it's something I'm doing wrong? The call to submission_stream works most of the time and is the standard way of doing it, as far as I know: while True: try: submissionStream = praw.helpers.submission_stream(r, '+'.join(subreddits.getInputList()), maxPosts,0) for submission in submissionStream: except ValueError as err: print(\"Submission stream ValueError. Sleeping for 5 seconds and trying again. Here's the error: \"+str(err), file=sys.stderr) time.sleep(5) continue Thanks.", "created_utc": 1402490508, "gilded": 0, "name": "t3_27vebn", "num_comments": 2, "score": 1, "title": "[PRAW] ValueError in praw.helpers.submission_stream() on Heroku", "url": "https://www.reddit.com/r/redditdev/comments/27vebn/praw_valueerror_in_prawhelperssubmission_stream/"}, {"author": "VoterBot", "body": "Over the last three days comment_stream() and submission_stream() in Praw have been failing with socket.timeout errors. I can catch the exception easily enough and loop back to the submissions_stream() call without the script crashing, but I still get this error in the Heroku log: app[worker.1]:\u2190[0m bot.py:211: ResourceWarning: unclosed Line 211 in the script is a continue command in the socket.timeout exception handler which loops back to the submission_stream() call. It doesn't cause a performance problem so I'm not too bothered, but it bugs me and it would be nice if I could fix it so it doesn't dirty up my PRAW log. The timeouts don't happen on my local machine, only on Heroku. I'd be grateful for suggestions.", "created_utc": 1402311831, "gilded": 0, "name": "t3_27os73", "num_comments": 1, "score": 2, "title": "Recent socket timeouts under Heroku and PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/27os73/recent_socket_timeouts_under_heroku_and_praw/"}, {"author": "snaysler", "body": "Hi there. I'm designing an online course in /r/ludobots, and am using RedditBots (PRAW) to mediate everything. I have a pretty large program with main functions I'm trying to run successively in one execution of the program. First I'm scouring the \"new\" posts of the sub to look for ones that haven't been looked at already according to data stored in a secret wiki page in my sub. Then it finds the first url reference in these posts, and if the url references another post within my sub, it connects the two as \"connected nodes\" in a sort of \"tree\" visualization. Then it updates the tree data in /r/ludobots/wiki/data, updates the page listings in /r/ludobots/wiki/tree, and creates an image of the tree visualization. Then it uploads this image to my sub. Now this all works just fine...if I only run one function at a time in each execution of the program. ***BUT***, if I run all the functions back to back in one execution of the program, then none of it seems to work, or only the first function has any effect. It seems like uploading to Reddit has no effect, because nothing has changed in my sub that should have changed during execution. I figured maybe these functions are stressing the Reddit servers too much, so I put a time.sleep(20) call between functions so it would wait 20 seconds before doing another task of the process. This makes it seem to run the first and third of three functions, omitting the middle one. Is this something that just happens with large praw-based programs?", "created_utc": 1402001909, "gilded": 0, "name": "t3_27ey83", "num_comments": 3, "score": 1, "title": "I'm designing online class with Reddit; PRAW has started malfunctioning, what's going on?", "url": "https://www.reddit.com/r/redditdev/comments/27ey83/im_designing_online_class_with_reddit_praw_has/"}, {"author": "quadnix", "body": "I'd like to add a user as an approved submitter with PRAW, but after combing the docs, I couldn't find a way to do it. Am I missing out on something?", "created_utc": 1401860605, "gilded": 0, "name": "t3_279qlu", "num_comments": 2, "score": 3, "title": "Adding a user as an approved submitter with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/279qlu/adding_a_user_as_an_approved_submitter_with_praw/"}, {"author": "nuntipat", "body": "I'm trying to get submission and their comment from Reddit based on the keywords given. The code below is what I have done so far. I have a several question regarding the code. 1. Is there a way to reduce the number of API call? Now it seems that there is 1 API call per each submission and their comment. I think that it might be possible to issue API for multiple submissions once (default limit is 25 right?) 2. Is there a way to know the number of submission that is relevant to the keyword. Currently I could limit the search space using period (day, month, year, ...) but how can I know how many submission match in the period specify. Thanks f = open(file=filename, mode='w', encoding='utf-8') r = praw.Reddit(...) subreddit = r.search(query=keyword, sort='new', limit=submission_limit, period=period) print ('\\nStart crawling ' + str(submission_limit) + ' submissions to ' + filename) countSubmission = 1 for submission in subreddit: f.write(\"\\n\\nSUBMISSION : \" + str(countSubmission) + \" DATE : \" + time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(submission.created_utc)) + \"\\n\\n\") f.write(submission.selftext) #submission.replace_more_comments(limit=None, threshold=0) flat_comments = praw.helpers.flatten_tree(submission.comments) countComment = 1 for comment in flat_comments: if comment is praw.objects.Comment: f.write(\"\\n\\nCOMMENT : \" + str(countComment) + \"\\n\\n\") f.write(comment.body) countComment += 1 countSubmission += 1 f.close() print ('Done')", "created_utc": 1401461306, "gilded": 0, "name": "t3_26vmat", "num_comments": 0, "score": 3, "title": "[PRAW] Reddit search API", "url": "https://www.reddit.com/r/redditdev/comments/26vmat/praw_reddit_search_api/"}, {"author": "tehusername", "body": "I've been trying [this](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.Redditor.get_comments) to retrieve comments from a redditor but I can't seem to filter out comments only for today. I've tried setting *time* to *day* and *today* but still to no avail. Any suggestions?", "created_utc": 1401350794, "gilded": 0, "name": "t3_26rr50", "num_comments": 1, "score": 3, "title": "[PRAW] Get redditor's comments for today only", "url": "https://www.reddit.com/r/redditdev/comments/26rr50/praw_get_redditors_comments_for_today_only/"}, {"author": "DrJosh", "body": "[CommentTreeBot](http://www.reddit.com/user/CommentTreeBot)... * uses praw to re-construct the comment tree for a posting; * creates a tree visualization of it using GraphViz; * posts the resulting image to imgur; and * posts a comment in that posting pointing to the visualization.", "created_utc": 1401206255, "gilded": 0, "name": "t3_26m45h", "num_comments": 22, "score": 10, "title": "I created CommentTreeBot.", "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "gavin19", "body": "I just noticed that the past few days that my AutoMods and scripts are all getting the same error. I have never had this before. I run two instances of AutoMod for two separate subs, under two different accounts (/u/RFootballBot and /u/COYBot). I also run four small one-off scripts that just update the sidebars in four different subreddits. 2 of those are run through this account, and 2 through /u/RFootballBot. Each is set to run once per hour, but they're run at 10/25/40 and 55 minutes past the hour respectively. They're all run from the same Digital Ocean server. Also, I just tried one script on the server and it got the error, but then tried it locally from my Windows machine and it ran fine. Is this just me being rate-limited because they're all coming from the same IP? If so, can I just increase the sleep time in PRAW between passes, or introduce some delay? Cheers.", "created_utc": 1400979401, "gilded": 0, "name": "t3_26ex2a", "num_comments": 5, "score": 2, "title": "Rate limit exceeded errors with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/26ex2a/rate_limit_exceeded_errors_with_praw/"}, {"author": "tehusername", "body": "I've been sifting through the PRAW documentation and I can't seem to find anything about this. My last resort would be to somehow get sample submissions and if it fails, conclude that it must be a private one. Any thoughts on this?", "created_utc": 1400853291, "gilded": 0, "name": "t3_26aqel", "num_comments": 7, "score": 4, "title": "[PRAW] How to check if a subreddit is private/restricted?", "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "AnSq", "body": "I was searching for a way to get only unread mod mail messages like `get_unread()` (not `get_mod_mail()`) and I found [this post from 9 months ago](http://www.reddit.com/r/redditdev/comments/1jklbu/praw_is_there_a_function_for_getting_unread_mod/): >Does `get_unread()` also get unread mod mail (it doesn't seem like it should, because it's a part of `praw.__init__.PrivateMessagesMixin(*args, **kwargs)`)? Is there a way to only get unread mod mail through `get_mod_mail()`? > >Edit: Thinking about it further, I think there are some apps out there that provide notifications for new mod mail, so surely this must be possible somehow (?) with this reply from /u/_Daimon_: >Possible yes, implemented in PRAW no. I have some other coding matters to attend to first, but I should be able to add this functionality within the week. I'll be sure to add another comment to this post when I do. Does such a function actually exist now and I just missed it?", "created_utc": 1400757782, "gilded": 0, "name": "t3_2678bh", "num_comments": 2, "score": 0, "title": "RE: [PRAW] Is there a function for getting unread mod mail?", "url": "https://www.reddit.com/r/redditdev/comments/2678bh/re_praw_is_there_a_function_for_getting_unread/"}, {"author": "jsmcgd", "body": "Hi, I've just started to use PRAW. The documentation seems thorough but I can't seem to find a list of what functions are made available to me. Can someone provide a link. Apologies if I'm being blind.", "created_utc": 1400755100, "gilded": 0, "name": "t3_26762p", "num_comments": 1, "score": 2, "title": "Is there a documented list of PRAW's API functions?", "url": "https://www.reddit.com/r/redditdev/comments/26762p/is_there_a_documented_list_of_praws_api_functions/"}, {"author": "SyedAJafri", "body": "I working on a bot and apparently reddit doesn't let you pm multiple people in one message so I have to send a message for each person. Is there some way I could have all the pms sent at once? Would there be some kind of limit? The bot will send a pm to each user in a thread if the op requests it, so the list can get pretty huge. Edit: Just to clarify I meant using PRAW.", "created_utc": 1400720634, "gilded": 0, "name": "t3_2664jz", "num_comments": 8, "score": 1, "title": "Trying to pm multiple people without waiting 2 seconds every time.", "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "spike77wbs", "body": "I am getting a \"praw.errors.AlreadySubmitted: `that link has already been submitted` on field `url`\". So I tried this \"redd.submit('xyzsubreddit', title, url=url, resubmit='true')\" but that didn't recognize resubmit. Is it possible to resubmit? Sorry am a newbie trying to learn this stuff....", "created_utc": 1400394857, "gilded": 0, "name": "t3_25uii4", "num_comments": 7, "score": 1, "title": "Is it possible to use PRAW to resubmit?", "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "God-of-the-gaps", "body": "I don't even know at this point. I've reinstalled and installed every version of Python, 3.4, 2.7 all of them, and I can't even get Pip to work yet, let alone praw. My computer is windows Xp. On the 3.4 version, I put \"python\" whatever into the command thing, and it gives back that python isnt even a command, so I went to path and added that in, but whenever I do \"python distribute-setup.py\" or something, it says that the file doesn't exist So then I tried found the two files for pip and distribute in the downloads area and actually clicked open with python for both But now when I try to say \"pip install praw\", it just says syntax error. :( EDIT:tried deleting python and re-installing to get a fresh start, but now it won't even uninstall. ;_;", "created_utc": 1400382119, "gilded": 0, "name": "t3_25u5gx", "num_comments": 1, "score": 1, "title": "Unable to install Praw. Please Help!", "url": "https://www.reddit.com/r/redditdev/comments/25u5gx/unable_to_install_praw_please_help/"}, {"author": "redditratings", "body": "Relevant docs: * [Step 3: Getting authorization form the user. (from praw docs)](http://praw.readthedocs.org/en/latest/pages/oauth.html#step-3-getting-authorization-from-the-user) * [Authorization (from reddit api wiki)](https://github.com/reddit/reddit/wiki/OAuth2#authorization) In the reddit api wiki, it says: > You should generate a unique, possibly random, string for each authorization request. This value will be returned to you when the user visits your REDIRECT_URI after allowing your app access - you should verify that it matches the one you sent. This ensures that only authorization requests you've started are ones you finish. Few questions: 1. Does this mean every time an OAuth request is started, a unique key should be generated for that request? 2. It says \"possibly random\". Should the key have a \"unique\" component and a \"random\" component? 3. What is a good way to generate this key?", "created_utc": 1400200437, "gilded": 0, "name": "t3_25o8xq", "num_comments": 4, "score": 1, "title": "[praw] get_authorize_url method requires a \"unique key\", but docs don't say much about it. What is a good way to generate this key?", "url": "https://www.reddit.com/r/redditdev/comments/25o8xq/praw_get_authorize_url_method_requires_a_unique/"}, {"author": "Seeing_Eye_Bot", "body": "Hello World, I have been interested in Reddit bots for a little while now and I would like to make one for fun :) . I have zero coding knowledge, but I felt like learning a Reddit Bot would be a fun way to get some. I had a cool idea about a bot that would put ^this ^text into **bold** or something. Im not really sure how to do it and some guides I have seen have been a little confusing... this is what I have so far: import praw r = praw.Reddit(user_agent='My first bot...Put more info here when done') r.login('Seeing_Eye_Bot', '*password*') subreddit = r.get_subreddit('test') subreddit_comments = subreddit.get_comments() . I guess I am Wondering how I can do a search for ^this ^text and then replace it with **this text** Any and All Help is appreciated, I apologize if I wrote this out very poorly.", "created_utc": 1399863570, "gilded": 0, "name": "t3_25bvld", "num_comments": 4, "score": 1, "title": "Please Help On First Bot", "url": "https://www.reddit.com/r/redditdev/comments/25bvld/please_help_on_first_bot/"}, {"author": "BrandieBrands", "body": "As a complete beginner in Python I've been following the PRAW tutorials. Now when I run my python file that I wrote with help from [the third tutorial](https://praw.readthedocs.org/en/latest/pages/comment_parsing.html) I keep getting this error: http://imgur.com/d6yKbZw. The comment is, however, still being posted. Is there anyone who has gotten this error before and has resolved it, or knows what to do?", "created_utc": 1399532142, "gilded": 0, "name": "t3_250uqn", "num_comments": 3, "score": 1, "title": "HTML DeprecationWarning when replying to a comment", "url": "https://www.reddit.com/r/redditdev/comments/250uqn/html_deprecationwarning_when_replying_to_a_comment/"}, {"author": "ubccompscistudent", "body": "Originally I tried to install setup via the command line with: curl https://bootstrap.pypa.io/ez_setup.py -o - | python But then I got the following message: The following error occurred while trying to add or remove files in the installation directory: [Errno 13] Permission denied: '/Library/Python/2.7/site-packages/test-easy-install-20455.pth' So now I'm just trying to run get-pip.py in the python interpreter and now I'm getting the exact same message. So clearly I need to provide administrator access to the python directory. But I have no idea how to do that. At no point am I being prompted for root and password. Installing pip/PRAW has been the most frustrating experience in my two years of programming for something that should be so easy.", "created_utc": 1399436378, "gilded": 0, "name": "t3_24xdaj", "num_comments": 10, "score": 2, "title": "Trying to install pip, but getting administrator", "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "JBHUTT09", "body": "I was thinking of adding an account age check to the bot that helps me mod a subreddit since we're occasionally spammed by 0 day old accounts. It wasn't going to remove the posts, but rather send a modmail asking the mods to check to make sure it's a legit post. Anyway, I can't seem to find anything about account ages in either the PRAW code overview or the reddit API documentation. Am I just missing it, or does it not exist? Thank you.", "created_utc": 1399395392, "gilded": 0, "name": "t3_24vkho", "num_comments": 10, "score": 7, "title": "Is there a way to get the age of an account?", "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "brianjherman", "body": "code: https://gist.github.com/brianherman/e92f55cf3561397236b9 I get the error: Traceback (most recent call last): File \"karmadecay.py\", line 25, in submission.delete() File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 477, in upvote return self.vote(direction=1) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 502, in vote return self.reddit_session.request_json(url, data=data) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 177, in wrapped raise error_list[0] praw.errors.NotLoggedIn: `please login to do that` on field `None`", "created_utc": 1399352701, "gilded": 0, "name": "t3_24ua4d", "num_comments": 1, "score": 2, "title": "Can't get praw to delete a post", "url": "https://www.reddit.com/r/redditdev/comments/24ua4d/cant_get_praw_to_delete_a_post/"}, {"author": "jimgur", "body": "Hello, I made a [reddit bot](https://github.com/Carpetfizz/Jimgur/blob/master/jimgur.py) using PRAW that ran for a couple hours and just stopped. It didn't error out or anything, nor did it give a `RateLimitExceeded` exception. I'm guessing this is just the website itself temporarily stopping it? Thanks for any clarification! EDIT: Thanks for the replies. It turns out that it stopped because there simply weren't any more posts during that time. I changed it to get new submissions, so it's been running consistently for about 4-6 hours.", "created_utc": 1399260949, "gilded": 0, "name": "t3_24qx0w", "num_comments": 2, "score": 3, "title": "Is it normal for PRAW bots to just stop for some time?", "url": "https://www.reddit.com/r/redditdev/comments/24qx0w/is_it_normal_for_praw_bots_to_just_stop_for_some/"}, {"author": "pietrod21", "body": "This is what I get when installing praw at the beginning: C:\\Users\\Admin\\Desktop\\pip>pip install praw Downloading/unpacking praw Running setup.py egg_info for package praw C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'entry_points' warnings.warn(msg) C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'install_requires' warnings.warn(msg) C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'test_suite' warnings.warn(msg) C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'tests_require' warnings.warn(msg) usage: -c [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...] or: -c --help [cmd1 cmd2 ...] or: -c --help-commands or: -c cmd --help error: invalid command 'egg_info' Complete output from command python setup.py egg_info: C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\App\\appdata\\canopy-1.3.0.1715. win-x86_64\\lib\\distutils\\dist.py:267: UserWarning: Unknown distribution option: 'entry_points' warnings.warn(msg) and this is the log in C:\\Users\\Admin\\AppData\\Roaming\\pip: ------------------------------------------------------------ C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\Scripts\\pip-script.py run on 05/04/14 04:18:56 Downloading/unpacking https://github.com/praw-dev/praw Downloading from URL https://github.com/praw-dev/praw Cannot unpack file c:\\users\\admin\\appdata\\local\\temp\\pip-pcgqqj-unpack\\praw (downloaded from c:\\users\\admin\\appdata\\local\\temp\\pip-nekbib-build, content-type: text/html; charset=utf-8); cannot detect archive format Cannot determine archive format of c:\\users\\admin\\appdata\\local\\temp\\pip-nekbib-build Exception information: Traceback (most recent call last): File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\basecommand.py\", line 126, in main self.run(options, args) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\commands\\install.py\", line 223, in run requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\req.py\", line 961, in prepare_files self.unpack_url(url, location, self.is_download) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\req.py\", line 1079, in unpack_url return unpack_http_url(link, location, self.download_cache, only_download) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\download.py\", line 455, in unpack_http_url unpack_file(temp_location, location, content_type, link) File \"C:\\Users\\Admin\\AppData\\Local\\Enthought\\Canopy\\User\\lib\\site-packages\\pip\\util.py\", line 486, in unpack_file raise InstallationError('Cannot determine archive format of %s' % location) InstallationError: Cannot determine archive format of c:\\users\\admin\\appdata\\local\\temp\\pip-nekbib-build Then I solved the problem installing this: http://stackoverflow.com/questions/11425106/python-pip-install-fails-invalid-command-egg-info (pip install --upgrade setuptools) And rerunning \"pip install praw\" I got: C:\\Users\\Admin\\Desktop\\pip>pip install praw Requirement already satisfied (use --upgrade to upgrade): praw in c:\\users\\admin \\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages Requirement already satisfied (use --upgrade to upgrade): requests>=1.2.0 in c:\\ users\\admin\\appdata\\local\\enthought\\canopy\\app\\appdata\\canopy-1.3.0.1715.win-x86 _64\\lib\\site-packages (from praw) Requirement already satisfied (use --upgrade to upgrade): six>=1.4 in c:\\users\\a dmin\\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages (from praw) Requirement already satisfied (use --upgrade to upgrade): update-checker>=0.9 in c:\\users\\admin\\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages (from praw ) Cleaning up... Other maybe useful info: I have windows 8 I have python 3.2.2 and python 2.7 installed. Any help? ps I know I have to eradicate windows from my oc, the problem is that soon I have to return it...", "created_utc": 1399170653, "gilded": 0, "name": "t3_24o064", "num_comments": 6, "score": 1, "title": "I install praw via pip install, but doesn't work.", "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "Sc1F1", "body": "Hey guys, I'm trying to implement a parent check function, but I'm having a few problems. First, when I try: `if string in parent.body` I get the message: `AttributeError: '' has no attribute 'body'` It works perfectly find though when I use: `print parent.body`. Also, when It performs more than one check: `if parent.id in already_done and string in parent.body` It returns the error: `TypeError: argument of type 'Comment' is not iterable` Any help on these would be greatly appreciated. Edit: I used `parent = r.get_info(thing_id=comment.parent_id)` to get the parent. Edit2: I just tried `print parent.body` again and it failed, my guess is I'm getting this error because parent is a submission not a comment(*facepalm*). So now I need to figure out how to test if parent is belongs to praw.objects.Submission class. Edit3: Used isinstance() to check. While I didn't receive any answers, posting here did help me think of a solution, so thanks guys.", "created_utc": 1399159370, "gilded": 0, "name": "t3_24nm86", "num_comments": 1, "score": 3, "title": "[PRAW] Checking content of comment parent.", "url": "https://www.reddit.com/r/redditdev/comments/24nm86/praw_checking_content_of_comment_parent/"}, {"author": "actual_factual_bear", "body": "I have it running on Python 2.7, but I tried moving a script to an OS X 10.6 machine running Python 2.6.1, and I get an exception as soon as I try to construct an instance: r = praw.Reddit(user_agent='my_cool_application') Traceback (most recent call last): File \"\", line 1, in File \"/Library/Python/2.6/site-packages/praw-2.1.15-py2.6.egg/praw/__init__.py\", line 1059, in __init__ super(AuthenticatedReddit, self).__init__(*args, **kwargs) ... File \"/Library/Python/2.6/site-packages/requests-2.2.1-py2.6.egg/requests/packages/urllib3/connection.py\", line 83, in _prepare_conn if self._tunnel_host: AttributeError: 'HTTPConnection' object has no attribute '_tunnel_host' I tried searching for this \"tunnel_host\" error without any luck. I have already tried using \"easy_install\" to update to the latest version of praw.", "created_utc": 1399069293, "gilded": 0, "name": "t3_24ku36", "num_comments": 7, "score": 3, "title": "Is PRAW really compatible with Python 2.6?", "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": "GHETTO_CHiLD", "body": "if possible i would love to request the feature of 'reason' be added to the get_banned() function in praw. right now some users are on a temporary ban and some users are not. if i could pull in the reason field i would be able to determine if my scripts should delete the user from our subreddits tools/web apps database.", "created_utc": 1398959680, "gilded": 0, "name": "t3_24gpae", "num_comments": 2, "score": 3, "title": "feature request: add 'reason' to get_banned in praw.", "url": "https://www.reddit.com/r/redditdev/comments/24gpae/feature_request_add_reason_to_get_banned_in_praw/"}, {"author": "charredgrass", "body": "I don't have a computer that I can keep on 24/7, and I want to make a bot that can scan comments for certain words and reply to them. What is the best free way to host this? Also, sorry I'm such a noob at this, I just started learning PRAW a few days ago and Python a few months ago.", "created_utc": 1398904771, "gilded": 0, "name": "t3_24ezpi", "num_comments": 14, "score": 5, "title": "Where to host a simple PRAW bot that just scans comments of a sub?", "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "mgrieger", "body": "Hi everyone, I have a bot that I am experimenting with opening up to all of reddit, instead of just particular subreddits that have given me permission. I expect that inevitably the bot will be banned from subreddits that do not want the bot to post there anymore. So my question is the following: what is the best way to handle a bot trying to reply to a comment in a subreddit that it is banned in? Is this even something I have to worry about? I have never had an account banned before so I don't know if the comments even show up to the bot if it is banned. I did some googling on this question and I couldn't find anything, but I have my code setup like this for now: try: comment.reply(comment_text) except: pass I'm assuming that if there is an issue with bots attempting to reply to comments in a subreddit they are banned in that some sort of exception is thrown. I don't like having try/except blocks that catch every exception, so is there a certain exception that is thrown for this problem? Maybe I just overlooked something, but I couldn't find anything in PRAW's documentation or reddit's API documentation that specified an exception type. Thanks! **EDIT:** Thanks to /u/RemindMeBotWrangler, I got it working. For future reference, here is what worked for me: import requests try: comment.reply(comment_text) except requests.exceptions.HTTPError, err: if str(err) == '403 Client Error: Forbidden': pass", "created_utc": 1398882896, "gilded": 0, "name": "t3_24dzm3", "num_comments": 6, "score": 1, "title": "[PRAW] Best way to handle being banned from a subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/24dzm3/praw_best_way_to_handle_being_banned_from_a/"}, {"author": "elihusmails", "body": "Is there anything in the praw API? I didn't see anything and since there's some specific formatting in the comments when returned in praw, I was hoping there would be something. Thanks.", "created_utc": 1398740153, "gilded": 0, "name": "t3_248v0w", "num_comments": 4, "score": 3, "title": "Looking for a way to grab URL links out of comments", "url": "https://www.reddit.com/r/redditdev/comments/248v0w/looking_for_a_way_to_grab_url_links_out_of/"}, {"author": "HellFireKoder", "body": "I am going through the tutorial at https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html I had to `print(r.user.send_message(username, message))` just to find out it returned anything, it doesn't print any errors to the screen on it's own. The code I used is at http://pastebin.com/gTFpbTvz For every time it finds one of the words, it returns \"{'errors': []}\" I give it the right username and password, I can't figure out why it won't work. P.S. Even if I don't have the `print` in `print(r.user.send_message('MyUserName', msg))` it still doesn't send me a message. Any help is appreciated. I am using python 3.4", "created_utc": 1398733909, "gilded": 0, "name": "t3_248ku8", "num_comments": 16, "score": 3, "title": "user.send_message() Returning \"{'errors': []}\" And Not Sending Message", "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "Sc1F1", "body": "Hey guys, I want to write a bot, but every time I try to install PRAW I get this output: Downloading/unpacking praw Could not fetch URL https://pypi.python.org/simple/praw/: There was a problem confirming the ssl certificate: Will skip URL https://pypi.python.org/simple/praw/ when looking for download links for praw Could not fetch URL https://pypi.python.org/simple/: There was a problem confirming the ssl certificate: Will skip URL https://pypi.python.org/simple/ when looking for download links for praw Cannot fetch index base URL https://pypi.python.org/simple/ Could not fetch URL https://pypi.python.org/simple/praw/: There was a problem confirming the ssl certificate: Will skip URL https://pypi.python.org/simple/praw/ when looking for download links for praw Could not find any downloads that satisfy the requirement praw Cleaning up... No distributions at all found for praw Any help with this would be greatly appreciated. EDIT: A word", "created_utc": 1398610178, "gilded": 0, "name": "t3_243ueb", "num_comments": 2, "score": 1, "title": "Error when installing PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/243ueb/error_when_installing_praw/"}, {"author": "YouBetterGoFindIt", "body": "praw.errors.RateLimitExceeded: `you are doing that too much. try again in 9 minutes.` on field `ratelimit`", "created_utc": 1398575813, "gilded": 0, "name": "t3_2432ye", "num_comments": 4, "score": 1, "title": "What is the rate limit exceeded message and how can I avoid it?", "url": "https://www.reddit.com/r/redditdev/comments/2432ye/what_is_the_rate_limit_exceeded_message_and_how/"}, {"author": "5loon", "body": "import praw r = praw.Reddit(user_agent='example') r.login(' ', ' ') if message in r.get_unread(): text = message.body author = message.author message.mark_as_read() returns \"NameError: name 'message' is not defined\" I'm reasonably new to PRAW. I'm probably making some little silly mistake on defining \"message\". Thanks in advance. **Edit**: Found out what was wrong. It's for message in r.get_unread(): Thanks /u/gavin19!", "created_utc": 1398539116, "gilded": 0, "name": "t3_241rkt", "num_comments": 2, "score": 1, "title": "Having trouble parsing inbox messages. [Python 2.7] [PRAW]", "url": "https://www.reddit.com/r/redditdev/comments/241rkt/having_trouble_parsing_inbox_messages_python_27/"}, {"author": "CastleCorp", "body": "Hey guys! I am new to PRAW, and pretty new to python (I am experienced in Java) so I am trying to learn as I go, and so far, with tutorials and all that good stuff. Here is what I am trying to do: * Get a list of subreddits the user is subscribed to (this is working currently) * Get the top (hot) post from each of the subscribed subreddits (not working) subscribed_reddits = r.get_my_subreddits(limit=500) # get all subscribed reddits, up to 500 subbed_reddits = [] # Array to hold all subscribed subreddits hot_posts= [] # Array to hold all the hot posts def getSubscribedReddits(): # store each subscribed reddit in array, print the array for subreddit in subscribed_reddits: subbed_reddits.append(subreddit) # add the subreddit's name to the array print subreddit.display_name return subbed_reddits # return for function call pass def getHotPosts(): # Get the top \"Hot\" post from each of the user's subscribed reddits for subreddit in subbed_reddits: i = iter(subbed_reddits) item = i.next() current_sub = r.get_subreddit(item.display_name) hot_posts.append(current_sub.get_hot(limit=1)) submissions = current_sub.get_hot(limit=1) return hot_posts pass Those are both working fine, but no matter what I try, I can't seem to get the list of posts from hot_posts[] in human-readable form. Does anyone have suggestions on how I can do this? Thanks!", "created_utc": 1398522831, "gilded": 0, "name": "t3_2415uq", "num_comments": 12, "score": 1, "title": "Need some help iterating through submissions in an array", "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "Ambit", "body": "I'm attempting to use the Reddit API from Python using Google's OAuth2Client library. The issue I'm having is that when I get to `step2_exchange()`, Reddit responds with JSON that just says `{ \"error\": 401 }` The token URI I'm using is `https://ssl.reddit.com/api/v1/access_token`. OAuth2Client handles adding the parameters to the HTTP request. It adds some that the Reddit docs don't ask for, but I get the same response even after commenting them out. This was working when I was playing around with PRAW, so I'm guessing I'm just missing something small here.", "created_utc": 1398463033, "gilded": 0, "name": "t3_23ziu3", "num_comments": 7, "score": 3, "title": "API Returning 401 Response When Using OAuth2Client", "url": "https://www.reddit.com/r/redditdev/comments/23ziu3/api_returning_401_response_when_using_oauth2client/"}, {"author": "110011001100", "body": "Ideally a Python library, something I can use with PRAW I am trying to export reddit self text submissions to Kindle", "created_utc": 1398370004, "gilded": 0, "name": "t3_23w0jq", "num_comments": 5, "score": 4, "title": "Is there a library to translate markdown to HTML?", "url": "https://www.reddit.com/r/redditdev/comments/23w0jq/is_there_a_library_to_translate_markdown_to_html/"}, {"author": "Phteven_j", "body": "In the spirit of not doing the same work twice, I'm going to show you how to do something that helps you avoid doing the same work twice. It is surprisingly easy to set up an SQL database and interface it with your bot. ###The problem: Not doing the same work twice. If you followed the PRAW documentation, you may have used [this technique](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html#not-doing-the-same-work-twice) to prevent your bot doing something twice like leaving a comment, sending a message, or really anything. We don't want our bot spamming up reddit just because it doesn't know what it already did. So, we make use of a temporary local variable like already_done and keep track of what we have processed in it via: already_done.append(submission.id) ###The other problem: Losing your records of work done In my apps, I use a two-factor sanity check to avoid doing the same work: the above list method and a thorough search. The problem with the list method (already_done.append(id)) is that if the bot crashes, you lose the record of what work you have done. What I will often do is comb through my bot's history to make sure I didn't already reply to a comment or process a command, often using something like this: def replied(comment): replies = comment.replies replied = False for reply in replies: if reply.author.name == username: return True return False This makes a TON of reddit calls, which results in a TON of delay. Just to see if we already did something! ###Why databases? The issue with my two-factor system is that reddit requests take time and are not very efficient compared to a local look-up. On the other hand, I can have a local database for my 2nd factor which isn't limited in calls by the reddit API and makes use of super efficient search algorithms. ###Installing prerequisites So we need a few things for our python app to use a database. I'm going to be using MySQL because fuck it, why not. So you need these packages installed: sudo apt-get install python-mysqldb sudo apt-get install mysql-server You will need to pick a username for mysql, I use \"root\". I don't like to write raw SQL queries, so I use a wrapper called \"peewee\" for friendly commands. [Peewee documentation](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0CCkQFjAA&url=http%3A%2F%2Fpeewee.readthedocs.org%2Fen%2Flatest%2Fpeewee%2Fcookbook.html&ei=Lr9YU9pKhreSBbHjgIgP&usg=AFQjCNF8hSPe9PFvF-PeIt67QobCKT4Eow&sig2=IkJgo1KANGZjd2fZYz9SZw&bvm=bv.65397613,d.dGI) pip install peewee ###Basic set up You will need to create an SQL database, which is very easy. Enter MySQL with: mysql -u root -p mysql> create database redditbot In our example, we want to store submission IDs we have already processed in case our bot crashes and we lose our \"already_done\" list. In peewee, we need to connect to our database and create a \"model\" which represents our submission.id (or whatever we want to store): from peewee import * db = MySQLDatabase('redditbot', host='localhost', user='root', passwd='SQLPASS') class Submissions(Model): subid = TextField() class Meta: database = db db.connect() Submissions.create_table(True) #the True flag won't alarm if table exists Cool, so now we are connected to our database, we have established what a \"submission\" model is, and we created a table of submissions. Now, we can add submissions to our database, search for existing submissions, or even flush the database of all entries. I'll show you those functions here: def is_added(submission_id): #is ID already in database? try: submissions = Submissions.get(Submissions.subid == submission_id) return True except: return False def add_entry(submission_id): #add new entry if not is_added(submission_id): print \"Adding %s\" % submission_id Submissions(subid= submission_id).save() def flush_db(): #remove all entries subs = Submissions.select() for sub in subs: sub.delete_instance() Rather than typing out ANY queries when we want to add things, I have wrapped all of the peewee code in custom python methods. Now, we can simply use my function to add a submission to our database: submissions = reddit.get_new(limit =10) for submission in submissions: add_entry(submission.id) If the entry is already in the table, it won't be added again due to the is_added() method. This means at the \"reddit bot\" layer, we aren't doing any checks, which makes for much cleaner code. ###Integrating our 2-factor check Now we don't want to rely on the DB when we can use already_done *most* of the time, so let's put both together: already_done = [] submissions = reddit.get_new(limit =10) for submission in submissions: if submission.id not in already_done and not is_added(submission.id): #Do whatever to your submission on this line already_done.append(submission.id) add_entry(submission.id) Boom. So we will check already_done for our ID and if it isn't there (because it's new OR we crashed), it will check the database. If it isn't in the database, it will add it. This way, we don't have to do any reddit calls to check if we already did this work. This makes for a MASSIVE performance increase. If you want, you can create one function to handle already_done and your database, like so: def add_done(submission.id): already_done.append(submission.id) add_entry(submission.id) And a method to check if something is already done: def is_done(submission.id): if not submission.id in already_done and not is_added(submission.id): return True So our final code could be: already_done = [] submissions = reddit.get_new(limit =10) for submission in submissions: if not is_done(submission.id): #Do whatever to your submission on this line add_done(submission.id) --- I hope that all makes sense! Let me know if you have any questions. I'm not super experienced with databases, but this simple implementation works for most of my purposes.", "created_utc": 1398324898, "gilded": 0, "name": "t3_23ucco", "num_comments": 1, "score": 6, "title": "[Guide] Using databases with your reddit bot in python", "url": "https://www.reddit.com/r/redditdev/comments/23ucco/guide_using_databases_with_your_reddit_bot_in/"}, {"author": "jahjaylee", "body": "Hey guys, I've been using PRAW for an app I'm developing and I'm trying to get the breakdown of upvotes and downvotes of certain posts. I can easily get the total karma amount but is there any way to get the breakdown?", "created_utc": 1398316528, "gilded": 0, "name": "t3_23u4eh", "num_comments": 1, "score": 3, "title": "Getting the number of upvotes and downvotes of a post", "url": "https://www.reddit.com/r/redditdev/comments/23u4eh/getting_the_number_of_upvotes_and_downvotes_of_a/"}, {"author": "fedo_tip_bot", "body": "So, I have this stupid bot, and it posts replies to various subreddits but also including a private subreddit. Manual posts show up everywhere, but comments through PRAW seem to be filtered out (it's an approved submitter in that private subreddit, but replies still show up in the moderation queue and spam queue). Outside that private subreddit, none of its replies show up to other users, although they appear when I'm logged into the bot. What am I doing wrong?", "created_utc": 1398305987, "gilded": 0, "name": "t3_23tpns", "num_comments": 5, "score": 1, "title": "How to get bot out of moderation/spam queue?", "url": "https://www.reddit.com/r/redditdev/comments/23tpns/how_to_get_bot_out_of_moderationspam_queue/"}, {"author": "JackOfCandles", "body": "I wrote a bot and it seems to miss posts that should trigger it. I'm using the comment stream in the praw library to look at /r/all and it seems to be saying it's at about 25 comments per second. I assume that's what CPS means anyway. Is that normal? Edit: After changing it from /r/all to only 3 smaller subs, I noticed that the comments per second is being reported as 0 most of the time, so it seems that this isn't based on what my bot can handle, but rather that's simply all the comments that are currently currently available at that comment fetch iteration. Still, I'm not sure why it occasionally is missing valid comments. I've got limit set to none, like so: all_comments = praw.helpers.comment_stream(r, SUBREDDITS, limit=None)", "created_utc": 1398304274, "gilded": 0, "name": "t3_23tmto", "num_comments": 18, "score": 2, "title": "How many comments per second should a bot be processing?", "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "Plague_Bot", "body": "~~I'm trying to retrieve a list of posts that have recently been archived (6 months old). To do this I planned to use the search function and pass in something like \"6 months ago\" or \"November 2013\" to the [period parameter](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.search). But I have no idea what format it's expecting. Or even if that's its intended use.~~ ~~The [wiki](http://www.reddit.com/wiki/search) that the PRAW documentation links to doesn't even mention the period parameter, so I'm a little short on documentation.~~ Found it in the [Reddit API documentation](http://www.reddit.com/dev/api#GET_search). Should have thought to look there before. Whoops. It only accepts these, if anyone's wondering: one of (hour, day, week, month, year, all) ___ Also, since I am only concerned about the date and not the search term, is it possible to pass in an empty search term? I'm looking to do something like this: results = r.search(\"\", period=\"6 months ago\", limit=None) Thanks in advance.", "created_utc": 1398031807, "gilded": 0, "name": "t3_23jii6", "num_comments": 1, "score": 3, "title": "[PRAW/Reddit API] Using the period parameter of the search function (plus other stuff).", "url": "https://www.reddit.com/r/redditdev/comments/23jii6/prawreddit_api_using_the_period_parameter_of_the/"}, {"author": "bobdammit", "body": "I have been using praw for an api that I have been building and have been using oauth2. I have been having issues with submitting comments with an oauth2 login. When I check to see if is_oauth_session equals true, it always checks out, but when I try to add a comment, using submission.add_comment, I keep getting an error that says I am not logged in with an oauth session. If I do a real login with r.login(username='blah', password='blah'), it works. How am I getting this error, if the boolean checks out from above?", "created_utc": 1397939523, "gilded": 0, "name": "t3_23gl7i", "num_comments": 2, "score": 2, "title": "submitting comments with oauth2", "url": "https://www.reddit.com/r/redditdev/comments/23gl7i/submitting_comments_with_oauth2/"}, {"author": "KnightHawk3", "body": "I am really new to using PRAW, but I basically want a way of looking up the flair of a username on a specific subreddit. IE: What is the flair of /u/KnightHawk3 on /r/pokemontrades r = praw.Reddit(\"/u/KnightHawk3's IRC bot called Rhythm\") username = \"KnightHawk3\" user = r.get_redditor(username) for comment in user.get_comments(): print(comment.subreddit) if str(comment.subreddit) == \"pokemontrades\": print(\"I have found a comment in the subreddit!\") I can't find the documentation for variables in the comment object (ie: subreddit) Where is the documentation on this, and what is the way of finding the flair? EDIT: Without being a moderator.", "created_utc": 1397876956, "gilded": 0, "name": "t3_23es5v", "num_comments": 6, "score": 2, "title": "Getting the flair of a comment", "url": "https://www.reddit.com/r/redditdev/comments/23es5v/getting_the_flair_of_a_comment/"}, {"author": "testingpraw", "body": "I have noticed that reddit-stream.com can get comments in real time. However, when I try to hit the api to see if there are more comments, it seems the api is 30 seconds too slow and is sending me cached json objects. I am logged in when trying to retrieve the json data. I have tried multi-threading, using praw and changing the praw.ini chache time limit, but I haven't had any luck. Is there anyway that I can access a live stream of comments from a submission?", "created_utc": 1397667927, "gilded": 0, "name": "t3_2370hr", "num_comments": 5, "score": 0, "title": "Comment Stream: Is there anyway to bypass reddit's caching?", "url": "https://www.reddit.com/r/redditdev/comments/2370hr/comment_stream_is_there_anyway_to_bypass_reddits/"}, {"author": "SOTB-human", "body": "Since PRAW doesn't support setting moderator permissions, I've been posting a request directly via the Reddit API in order to diminish the permissions on moderator invites in /r/modeveryone (an operation known as \"hitlering invites\"). It used to work successfully every time; however, it recently stopped working, producing `[[u'USER_REQUIRED', u'please login to do that', None]]`. Was something changed in the authentication API recently? The change must have occurred between 2014-04-11 06:16:36 UTC (the last success) and 2014-04-12 05:32:15 UTC (the first failure), since I didn't alter or restart the script during that interval. The relevant code is as follows: import praw # etc. r = praw.Reddit(user_agent=USER_AGENT) r.login(USERNAME, PASSWORD) sss = r.http.post(\"http://www.reddit.com/api/login\", {'passwd': PASSWORD, 'rem': True, 'user': USERNAME, 'api_type': 'json'}) sssj = json.loads(sss.text) MODHASH = sssj[\"json\"][\"data\"][\"modhash\"] # etc. def hitlerInvite(username): params = { 'name': username, 'r': 'modeveryone', 'uh': MODHASH, 'api_type': 'json', 'type': 'moderator_invite', 'permissions': '-all,+access,+config,+flair,+mail,+posts,+wiki' } response = r.http.post(\"http://www.reddit.com/api/setpermissions\", params) responsej = json.loads(response.text) if responsej[\"json\"][\"errors\"]: printLog( \"Error in hitlering\", username, \":\", responsej[\"json\"][\"errors\"]) else: printLog( \"Successfully hitlered\", username)", "created_utc": 1397576944, "gilded": 0, "name": "t3_233n3n", "num_comments": 0, "score": 2, "title": "Did something change in the authentication API recently? I can no longer set moderator permissions through the API.", "url": "https://www.reddit.com/r/redditdev/comments/233n3n/did_something_change_in_the_authentication_api/"}, {"author": "testingpraw", "body": "I am making a live comment stream, and I have not really seen a direct answer to this on google or the reddit dev search. Is there a way that I can get the login session data, using praw, so the user does not have to re-login to reddit to comment? For example, instead of using a form to populate r.login(), is there anyway that I can use r.login(username=session.reddit_username, password = session.password), or something like that?", "created_utc": 1397482608, "gilded": 0, "name": "t3_2303hw", "num_comments": 5, "score": 3, "title": "grabbing login session data with praw", "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "Alexratman", "body": "I'm trying to develop a Bot to help with reddit raffles; it would pull the comments from the post, extract the body & author, then it would get the authors comment karma. It would then output all of this information into a CSV file. I'm looking at using python and PRAW, however I'm a total newbie to python. I think that doing okay but I cannot get any sort of output from the script; import praw r = praw.Reddit(user_agent='RaffleCommentBot by /u/alexratman') r.login('RaffleCommentBot', '##Password##') id = \"thread_id\" submission = r.get_submission(submission_id = \"id\") submission.reddit_session.config.more_comments_max = 5000 print \"submission get!\" subm.replace_more_comments() comments = submission.flatten_tree(subm.comments) users = set () for idx, comment in enumerate(comments): if type(comment) != praw.objects.MoreComments: users.add(comment.author.name) with open(\"output.csv, \"w\") as output: output.write(\"\\n\".join(users) + \"\\n\") Im basing this off a script a friend used in 2012 so its very outdated. Any help is greatly appreciated!", "created_utc": 1397175054, "gilded": 0, "name": "t3_22ql4t", "num_comments": 2, "score": 1, "title": "Is there a way to pull comments (and additional information) into a CSV", "url": "https://www.reddit.com/r/redditdev/comments/22ql4t/is_there_a_way_to_pull_comments_and_additional/"}, {"author": "Carsonbizotica", "body": "So all I really want to do is count the comments on a submission. Right now I'm relying on: submission.replace_more_comments(limit=None, threshold=0) flat_comments = praw.helpers.flatten_tree(submission.comments) len(flat_comments) The problem is that I don't care what's *in* the comments, only that they exist. I've been scouring the PRAW Docs and I can't seem to find, or invent, something that would give me access to this number without having to load all the comments. Anyone have any ideas? ***Related Question***: Does a deleted comment return `None` or something special?", "created_utc": 1397158680, "gilded": 0, "name": "t3_22pvic", "num_comments": 2, "score": 1, "title": "[PRAW] I've got a question about counting comments", "url": "https://www.reddit.com/r/redditdev/comments/22pvic/praw_ive_got_a_question_about_counting_comments/"}, {"author": "d2lp_test2", "body": "Here's the loop. http://pastebin.com/vDP7aWsL See here: http://www.reddit.com/r/bottest/comments/22m7hz/h_legacy_keys/ It replied once and then errord out with this exception. I wonder why. I wait 20 secs between each comment and 30 mins before fetching the topics. Edit: Error trace: Traceback (most recent call last): File \"bot.py\", line 27, in submission.add_comment(comment_text) File \"D:\\python3.3\\lib\\site-packages\\praw\\objects.py\", line 902, in add_commen t response = self.reddit_session._add_comment(self.fullname, text) File \"D:\\python3.3\\lib\\site-packages\\praw\\decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"D:\\python3.3\\lib\\site-packages\\praw\\__init__.py\", line 1977, in _add_com ment retry_on_error=False) File \"D:\\python3.3\\lib\\site-packages\\praw\\decorators.py\", line 177, in wrapped raise error_list[0] praw.errors.RateLimitExceeded: `Du versuchst es zu oft. Probier es in 19 Sekunde n nochmal` on field `ratelimit`", "created_utc": 1397061578, "gilded": 0, "name": "t3_22m9wv", "num_comments": 2, "score": 1, "title": "[PRAW] I keep getting RateLimitExceeded Exception and i do not understand why.", "url": "https://www.reddit.com/r/redditdev/comments/22m9wv/praw_i_keep_getting_ratelimitexceeded_exception/"}, {"author": "zynix", "body": "I've been working on a project to harvest date time's of every single electronic action I've made for as far back as possible. With Reddit, this is not my only account ( I have 3 active and another 6-7 that go back to late 2006). PRAW seems more ideal for building bots and my second glance through it didn't seem like it had much support for harvesting likes, submissions, and comments. Keep in mind, I just want the datetime stamps and nothing else. Am I mistaken? Would praw be a good fit for this? Alternatively I have a system called ransack that I used to harvest my Google search history ( 80K data points ) via running a managed browser instance ( Python owns the browser and can inject user input/js at will while also intercepting/collecting SSL protected pages ).", "created_utc": 1396883656, "gilded": 0, "name": "t3_22fhy9", "num_comments": 4, "score": 1, "title": "PRAW or just do direct harvesting for collecting metadata about myself?", "url": "https://www.reddit.com/r/redditdev/comments/22fhy9/praw_or_just_do_direct_harvesting_for_collecting/"}, {"author": "AlmostARockstar", "body": "Writing a new bot and I have been notified by praw that a new version is available. I decided to upgrade it with easy_install as it says in the title but since then I've been getting the following error: C:\\Python27\\lib\\pkgutil.py:186: ImportWarning: Not importing directory 'C:\\Python27\\lib\\site-packages\\mpl_toolkits': missing __init__.py file, filename, etc = imp.find_module(subname, path) C:\\Python27\\lib\\pkgutil.py:186: ImportWarning: Not importing directory 'c:\\python27\\lib\\site-packages\\mpl_toolkits': missing __init__.py file, filename, etc = imp.find_module(subname, path) Any thoughts?", "created_utc": 1396811402, "gilded": 0, "name": "t3_22d3hj", "num_comments": 0, "score": 1, "title": "[PRAW] Getting ImportWarning after easy_install --upgrade praw ... Any body else had this issue?", "url": "https://www.reddit.com/r/redditdev/comments/22d3hj/praw_getting_importwarning_after_easy_install/"}, {"author": "Codyd51", "body": "I'm really new to the Reddit API and PRAW..how can I use PRAW to check for new mail? Or is there a better way to do it? Thanks", "created_utc": 1396803407, "gilded": 0, "name": "t3_22crls", "num_comments": 2, "score": 3, "title": "[PRAW] Check for new mail?", "url": "https://www.reddit.com/r/redditdev/comments/22crls/praw_check_for_new_mail/"}, {"author": "shrayas", "body": "I moderate over at the /r/bengalurufc subreddit and wanted a way to update the sidebar at regular intervals as the standings change. So today i learnt how to update the sidebar via a python script using PRAW. Thought i'd share the knowledge here. [Here](https://gist.github.com/shrayasr/10005943) is the GIST. Feedback is appreciated. Thanks!", "created_utc": 1396790710, "gilded": 0, "name": "t3_22ccgn", "num_comments": 7, "score": 2, "title": "Wrote a small Gist on how to update a subreddit's sidebar contents.", "url": "https://www.reddit.com/r/redditdev/comments/22ccgn/wrote_a_small_gist_on_how_to_update_a_subreddits/"}, {"author": "205", "body": "As the title says, how would I go about pulling the author of the parent comment with praw. Thanks.", "created_utc": 1396731705, "gilded": 0, "name": "t3_22ap4k", "num_comments": 4, "score": 3, "title": "Author of parent comment", "url": "https://www.reddit.com/r/redditdev/comments/22ap4k/author_of_parent_comment/"}, {"author": "redditTopics", "body": "Is there any way of getting old Reddit content (submissions/comments) from the past year or more, or at least the last few months, via the API? I'm currently using praw, but I can't seem to find any functionality in praw or in the reddit API itself that allows you to access submissions or subreddits by date. Would Reddit provide a dump for academic research purposes? Are there other sites that would do the same? (I checked out and have contacted Gnip and DataSift.) I want to avoid scraping but if it comes down to that, how far back can you go just by following links? Would I have to resort to the wayback machine? Any help is appreciated. Thanks!", "created_utc": 1396579129, "gilded": 0, "name": "t3_225rhj", "num_comments": 4, "score": 4, "title": "How can I get historical Reddit data?", "url": "https://www.reddit.com/r/redditdev/comments/225rhj/how_can_i_get_historical_reddit_data/"}, {"author": "argutus1", "body": "I have written some code with Praw that uploads (overwrites) some already existing images in the stylesheet. This image is coded into the sidebar. The problem that I have is if I upload the the new image to the stylesheet, it won't be updated in the sidebar until I save the stylesheet. I had tried a method to fix this by getting the current stylesheet and saving it, and then updating the stylesheet, but I would get an error. Does anyone know a fix?", "created_utc": 1396454967, "gilded": 0, "name": "t3_220tbz", "num_comments": 2, "score": 1, "title": "Update Image in Sidebar", "url": "https://www.reddit.com/r/redditdev/comments/220tbz/update_image_in_sidebar/"}, {"author": "JBHUTT09", "body": "###Solved: Was a string issue. The escape character '\\\\' does not work on '%'. '%%' is required, instead. I wrote a bot a few months ago to help moderate /r/animesuggest, by checking post flair and taking certain actions if there is no flair on a post. I used a for loop and the get_new_by_date function to go through the most recent 20 posts in the subreddit: for submission in subreddit.get_new_by_date(limit=20): And it worked perfectly. But when I run the code it would give me a deprecation warning, telling me to use get_new instead. I brushed it off because it worked. But I'm now revamping the bot to address a few bugs, and I'd like to fix this, too. But I can't, for the life of me, figure out how to replace get_new_by_date with get_new. I thought it would be a simple matter of just changing the code to: for submission in subreddit.get_new(limit=20): But that doesn't seem to work. I've searched around and all I've been able to find are threads where people recommend using get_new_by_date instead of get_new. I've read the documentation, too, but I just can't seem to figure it out. Can anyone help me with this? **Edit:** I've dug around some more and I noticed that the [documentation entry for get_new](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Subreddit.get_new) is identical to the entry for get_new_by_date (directly below). Doesn't this mean I really should be able to simply replace get_new_by_date with get_new?", "created_utc": 1396452311, "gilded": 0, "name": "t3_220p0f", "num_comments": 2, "score": 1, "title": "Question about PRAW's get_new_by_date vs get_new", "url": "https://www.reddit.com/r/redditdev/comments/220p0f/question_about_praws_get_new_by_date_vs_get_new/"}, {"author": "vafrederico", "body": "Hello there, I'm currently building a few bots with a friend of mine. Since we don't plan to pay reddit gold for each bot, so that it would be possible to work with mentions, we were thinking of having a comment scrapper which would redirect each comment to it's respective bot, so that we could reduce our number of requests to /all/comments. We are using PRAW and basically, we have our parser, `RedditParser`, that adds each matched comment to a `Bot` class `Queue`, each bot class extends a `MetaBot` class, which extends `threading.Thread`. I'm trying to use comment.reply but it says that there is no authenticated reddit session, even though at the `__init__` of every bot, there is a self.r.login. I already tried to edit `comment.reddit_session` to `self.r`, but still no success. From PRAW documentation i thought i would only need to extend `Thread`, and create a `self.r = praw.Reddit` for each Bot. But it does not work. Any suggestions? EDIT: Managed to do this by re-fetching the comment on each Bot as suggested by /u/SOTB-human using `self.r.get_submission(url=permalink).comments[0]`. From the PRAW documentation, if you use `r.get_submission` with a comment permalink, it will return the submission, but the first comment will be the one belonging to the permalink.", "created_utc": 1396049796, "gilded": 0, "name": "t3_21mtuq", "num_comments": 4, "score": 1, "title": "PRAW + Multiple Bots + Multiple Threads", "url": "https://www.reddit.com/r/redditdev/comments/21mtuq/praw_multiple_bots_multiple_threads/"}, {"author": "labtec901", "body": "I'm trying to create a version of _Daimon_'s tutorial reddit bot: http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html But instead of scraping the self post texts of a subreddit, I'd like to scrape all the new comments in a subreddit. I've tried writing my own script to do this, but I don't think I'm even close to on the right track. Can you help?", "created_utc": 1395930836, "gilded": 0, "name": "t3_21i9iz", "num_comments": 2, "score": 0, "title": "Trying to write something simple, and it's devolved into a spaghetti code nightmare and I need to start over. Can you help?", "url": "https://www.reddit.com/r/redditdev/comments/21i9iz/trying_to_write_something_simple_and_its_devolved/"}, {"author": "glider_integral", "body": "Apparently I can't get my own flair in a subreddit (although this code in particular is to get anyone's flair in a subreddit). import praw r = praw.Reddit(user_agent='python_wrapper') flair = r.get_flair('subreddit', 'user') I hope I don't need to get mod privileges to know my own flair.", "created_utc": 1395720634, "gilded": 0, "name": "t3_21ann0", "num_comments": 2, "score": 2, "title": "PRAW get my flair in a particular subreddit.", "url": "https://www.reddit.com/r/redditdev/comments/21ann0/praw_get_my_flair_in_a_particular_subreddit/"}, {"author": "nissoPT", "body": "I installed praw (i guess) but when i try to import it in IDLE i get this message: import praw Traceback (most recent call last): File \"\", line 1, in import praw File \"/Library/Python/2.7/site-packages/praw-2.1.14-py2.7.egg/praw/__init__.py\", line 43, in from update_checker import update_check File \"build/bdist.macosx-10.9-intel/egg/update_checker.py\", line 15, in ImportError: No module named pkg_resources >>> can someone help me?", "created_utc": 1395638090, "gilded": 0, "name": "t3_217ivx", "num_comments": 4, "score": 3, "title": "I'm a noob that can't seem to be able to install praw", "url": "https://www.reddit.com/r/redditdev/comments/217ivx/im_a_noob_that_cant_seem_to_be_able_to_install/"}, {"author": "makeupdev", "body": "I'm doing a project for a data science class and I want to analyze a bunch of submissions to /r/makeupaddiction. I'm trying to write a scraper to grab the last 3000 (or more) posts with a certain tag. (It will only be run once, to collect the data!) I'm using PRAW. Just running a search gives me ~500 results. My query looks like: start = datetime.today() end = datetime.today() - timedelta(days=5) for x in range(0,50): postperiod = \"timestamp:%s..%s\" % (start.strftime(\"%s\"),end.strftime(\"%s\")) query = scraper.search('FOTD', 'MakeupAddiction','new', None, postperiod) self.do_query(query) start = start - timedelta(days=5) end = end - timedelta(days=5) time.sleep(2) The do_query() method does the processing of each query's results (filtering and saving to a csv file). The time.sleep(2) is to comply with reddit's API etiquette of not overloading their server although I'm not sure how necessary that is. Anyway, it just returns the same time period worth of results (the very newest) over and over. I'm pretty sure I'm following PRAW's [API for search](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Subreddit.search): search(query, subreddit=None, sort=None, syntax=None, period=None, *args, **kwargs) Not sure what I'm doing wrong. Going back in discrete time periods was my best idea to grab lots of records but let me know if there's a better way. Thanks for any insight! :(", "created_utc": 1395518370, "gilded": 0, "name": "t3_213ht6", "num_comments": 7, "score": 2, "title": "Want to grab last ~3000 records", "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "Harakou", "body": "Hey guys, I'm a bit inexperienced with Praw and Python in general, a little baffled with the trouble I'm having here. I'm writing a bot, and I want it to send my account a message when fails to fetch a page. However, when I pass my account's username and the message to the send_message function, the logged in account sends the message to itself, with the username as the message subject. Dummy script to show what I'm doing. (This replicates the problem in the Python shell as well.) bot = praw.Reddit(user_agent=\"user agent stuff\") bot.login(botUserName) bot.user.send_message('Harakou', \"Generic error report\") This seems to be in line with the usage of send_message in the documentation as well as the Getting Started tutorial, so I'm a bit confused. I've also tried specifying the arguments, but then Python complains that it's being supplied multiple values of 'recipient.' bot.user.send_message(recipient='Harakou', message=\"Generic error report\") I'm assuming this is a case of me doing something silly, but I'm not sure what. Any advice?", "created_utc": 1395366731, "gilded": 0, "name": "t3_20ykci", "num_comments": 2, "score": 3, "title": "Having some trouble with PRAW send_message?", "url": "https://www.reddit.com/r/redditdev/comments/20ykci/having_some_trouble_with_praw_send_message/"}, {"author": "editer63", "body": "Hi folks. I'm a grad student working on a project that involves downloading posts and comments from a couple of Reddit subs, and I'm a complete noob at Python and so on. I'm trying to self-teach, but it's slow going, and I need a boost. So if anyone in Austin can meet me and show me how to do the stuff I need to do (or explain that it can't be done, though I'm confident it can) in PRAW and how to manage the output so it's in a form I can use, I'd be happy to pay for a lesson or two. (My project isn't \"about\" scraping Reddit data, my adviser doesn't care how I get the stuff off the site, so this isn't a \"do my homework for me\" request.) Thanks!", "created_utc": 1395180566, "gilded": 0, "name": "t3_20r8nb", "num_comments": 4, "score": 4, "title": "Any PRAW wizards in Austin?", "url": "https://www.reddit.com/r/redditdev/comments/20r8nb/any_praw_wizards_in_austin/"}, {"author": "nnnannn", "body": "I was wondering if it was possible to coordinate RES and PRAW so that I could do things like: * Filter out a sub from /r/all if posts with a certain keyword began to dominate the sub, then erase the filter when those posts subsided. * Search a sub for regular submitters from the past month, then tag those users as such. How would I go about doing this? Can I post to #!settings/userTagger through PRAW or Requests? What kind of query would I have to make? Apologies if this is impractical/impossible.", "created_utc": 1394724037, "gilded": 0, "name": "t3_20bmac", "num_comments": 3, "score": 4, "title": "Coordinating RES and PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/20bmac/coordinating_res_and_praw/"}, {"author": "Actualacc", "body": "import time import praw # -*- coding: utf-8 -*- #i have no idea what this does r = praw.Reddit('') r.login('actualacc','notmypassword') already_done = [] prawWords = [u'\u00af\\_(\u30c4)_/\u00af',u'\u00af\\\\_(\u30c4)_/\u00af',] while True: subreddit = r.get_subreddit('all') all_comments = r.get_comments('all') for comment in all_comments: has_praw = any(face in comment.body for face in prawWords) if comment.id not in already_done and has_praw: msg = '[Misplaced Smiley](%s)' % comment.permalink print(msg) r.send_message('actualacc','smiley', msg) already_done.append(comment.id) print(comment.body) time.sleep(1) For some reason, this code does not detect anything and I receive no messages. It works fine if I set the prawWords to something that is just a string of letters. What should I change? And I am aware that I won't see the message as a \"new message\".", "created_utc": 1394678917, "gilded": 0, "name": "t3_20aehd", "num_comments": 6, "score": 5, "title": "Trying to get notified when a certain string (\"\u00af\\_(\u30c4)_/\u00af\") is used in a comment.", "url": "https://www.reddit.com/r/redditdev/comments/20aehd/trying_to_get_notified_when_a_certain_string_\u30c4_is/"}, {"author": "brucemo", "body": "I am sorry that I am posting this question again, but my project is blocked until I get an answer, and while I appreciate the answer I got [last time,](http://www.reddit.com/r/redditdev/comments/1zhcrq/enumerating_the_ban_list/) it was not one that I could use. Can someone please tell me how to use PRAW to get get successive pages of the ban list?", "created_utc": 1394675719, "gilded": 0, "name": "t3_20a9hl", "num_comments": 1, "score": 2, "title": "How to iterate the ban list with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/20a9hl/how_to_iterate_the_ban_list_with_praw/"}, {"author": "DarkMio", "body": "Hi, I am learning python right now and as a training to my learning-process (and something to work on where I am really interested in), I am writing a bot with some NLP (natural language pattern) analyzing. The biggest problem I am facing over and over again are [HTTPErrors - like in this screenshot.](http://i.imgur.com/N4XbO5m.png) My bot is atm only listening to threads in /r/dota2 to get some valuable data. # -*- coding: utf-8 -*- ### THIS BOT RUNS ON /r/dota2 FOR NOW. ### import time import praw from unidecode import unidecode from datetime import datetime r = praw.Reddit('Getting valuable wordlists, from /u/DarkMio') r.login() already_done = [] print \"You're sucessfully logged in.\" wordlist = open('wordlist.txt').readlines() lolWords = map(lambda it: it.strip(), wordlist) while True: subreddit = r.get_subreddit('dota2') for submission in subreddit.get_new(limit=15): # make it lowercase for now op_text = submission.selftext.lower() has_lol = any(string in op_text for string in lolWords) # test if it contains LoL content if submission.id not in already_done and has_lol: sub_title = unidecode(submission.title) msg = 'Maybe LoL content: [%s](%s)' % (sub_title, submission.short_link) print \"[%s] Sending of following thread '%s' to DotoBot.\" % (datetime.now().time(), sub_title) r.user.send_message('DotoBot', msg) with open('reddit.txt', 'a') as textfile: content = '\\n\\nThread: %s\\n\\nContent:\\n%s' % ( sub_title, unidecode(submission.selftext)) textfile.write(content) already_done.append(submission.id) time.sleep(120) The solution of the problem is something I can't wrap around, since I don't know how to catch those exceptions specificly. This is due to my inexpirience with programming and python in general. My try would've been: try: except: except: But how do I catch those errors (since I want / need a bot that is as verbose as possible) and return to it? I have tried [this before] (http://www.reddit.com/r/redditdev/comments/1dmdcz/praw_and_httperror_handling/) which somehow ended my while-loop.", "created_utc": 1394527304, "gilded": 0, "name": "t3_204i97", "num_comments": 7, "score": 2, "title": "Learning python - can't deal with HTTPErrors.", "url": "https://www.reddit.com/r/redditdev/comments/204i97/learning_python_cant_deal_with_httperrors/"}, {"author": "bassguitarman", "body": "I made this bot to annoy my friend and I can't get it to work. import time import praw from random import randint r=praw.Reddit(\"Annoybot\") r.login(\"annoysterninator\",\"secret\") message = (\"anything\") comment1 = \"lick\" comment2 = \"I found the vegan!\" comment3 = \"DdddddddDrop the bass\" comment4 = \"twerk eth twerk eth twerk\" comment5 = \"what nice leg muscles you have\" comment6 = \"The qua-a-adratic formula\" comment7 = \"it is ethjck twerk music swag money\" already_seen = set() while True: user = r.get_redditor(\"bassguitarman\") username = \"same as above\" comments = user.get_comments(limit=2) sub_comments = r.get_comments(\"all\") for comment in sub_comments: if comment.author.name == username and comment.id not in already_seen: comment.reply (\"Dudley!\") already_seen.add (comment.id) for comment in comments: reply = randint(1,7) if reply == 1: comment.reply (comment1) if reply == 2: comment.reply (comment2) if reply == 3: comment.reply (comment3) if reply == 4: comment.reply (comment4) if reply == 5: comment.reply (comment5) if reply == 6: comment.reply (comment6) if reply == 7: comment.reply (comment7) already_seen.add (comment.id)", "created_utc": 1394489710, "gilded": 0, "name": "t3_2032ul", "num_comments": 5, "score": 2, "title": "What is wrong with this bot?", "url": "https://www.reddit.com/r/redditdev/comments/2032ul/what_is_wrong_with_this_bot/"}, {"author": "myessail", "body": "# get onboard data, establish connection username = open(source_dir + \"username.txt\", \"r\").read().rstrip() password = open(source_dir + \"password.txt\", \"r\").read().rstrip() user_agent = (\"praw script which comments with information about pokemon. by Michael Yessaillian\") reddit = praw.Reddit(user_agent = user_agent) reddit.login(username = username, password = password) already_done = set() # keeps track of work already done #get data from comments, respond with information subreddit = reddit.get_subreddit('pokemon') subreddit_comments = subreddit.get_comments() for comment in subreddit_comments: # check for question if \"_pokedexbot\" in comment.body and comment.id not in already_done: comment.reply(\"working!\") already_done.add(comment.id) # adds to work done This is part of a bot I'm writing, modifying some code from [here](http://praw.readthedocs.org/en/latest/pages/comment_parsing.html). In theory, I think it should be replying \"working!\" to a comment that contains the bot's username, but when I run it, nothing happens. Do I have this set up correctly? The full project can be seen on [my github](https://github.com/myessail/pokedexbot).", "created_utc": 1394464492, "gilded": 0, "name": "t3_201vo4", "num_comments": 2, "score": 2, "title": "[PRAW] Having trouble with comment parsing.", "url": "https://www.reddit.com/r/redditdev/comments/201vo4/praw_having_trouble_with_comment_parsing/"}, {"author": "ravik78c", "body": "getting the error as shown below while using praw to submit more than one links to a subreddit in my local reddit instance. (QUOTA FILLED) `You've submitted too many links recently. Please try again in an hour.` on field `None`. What to do?", "created_utc": 1394239020, "gilded": 0, "name": "t3_1zurpt", "num_comments": 2, "score": 3, "title": "[Praw] : Facing quota error while submit more than one links one after the other,", "url": "https://www.reddit.com/r/redditdev/comments/1zurpt/praw_facing_quota_error_while_submit_more_than/"}, {"author": "ravik78c", "body": "I used praw to submit a link on a subreddit, but when I tried to add one more it asked me to do it after an hour. But when I do the same thing using the reddit site, It allows me to add as many links as I want. Does praw inhibits us to add more than one link per hour per user or is it checked by reddit api?", "created_utc": 1394159864, "gilded": 0, "name": "t3_1zrysj", "num_comments": 6, "score": 2, "title": "[PRAW] How many submissions can be made using praw at a time?", "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "ravik78c", "body": "Does anybody know how to configure praw to be used on the local reddit instance? I have my instance installed on my webserver and I can get the data which do not require loggin in but when I try to log-in it fails and throws error. Can you help me to run the praw api on my local instance?", "created_utc": 1394153215, "gilded": 0, "name": "t3_1zroo7", "num_comments": 2, "score": 5, "title": "[PRAW] Using praw for local reddit instance", "url": "https://www.reddit.com/r/redditdev/comments/1zroo7/praw_using_praw_for_local_reddit_instance/"}, {"author": "Abe_Linkin", "body": "I'm messing around with PRAW and was looking for a way to print if a tag is NSFW if it has been tagged that way, or if a subreddit is 18+. Any way to accomplish this? All I can find is how to mark a post NSFW", "created_utc": 1393961392, "gilded": 0, "name": "t3_1zk07r", "num_comments": 9, "score": 6, "title": "[PRAW] Does Praw allow you to see if a post is tagged as NSFW?", "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "brucemo", "body": "The following code prints \"kind\" and \"data\", those two text strings, that's all: import praw rh = praw.Reddit(\"Some descriptive name for this\") rh.login() sr = rh.get_subreddit(\"sub I'm a mod of\") for o in sr.get_banned(): print(o) If I examine \"o\" more closely, I can replace the \"for o in\" stuff with: it = sr.get_banned() for o in it['data']['children']: print(o) This gives me the first page of banned people, with no obvious clue about how to get more. Sticking the \"limit\" keyword in the obvious place doesn't work. I can't find examples of use of this call anywhere online. How do I more sensibly iterate this? This is obviously a category of object that I don't understand.", "created_utc": 1393892037, "gilded": 0, "name": "t3_1zhcrq", "num_comments": 2, "score": 4, "title": "Enumerating the ban list", "url": "https://www.reddit.com/r/redditdev/comments/1zhcrq/enumerating_the_ban_list/"}, {"author": "TheTretheway", "body": "I'm using PRAW to go through the comments on a particular submission, as per the example. I would like to make sure that, once it's been restarted and the *already_done* set cleared, it only responds to comments that were posted within the last x minutes. *comment.author.name* gets the name of the author, is there a way of doing time? Thanks!", "created_utc": 1393881158, "gilded": 0, "name": "t3_1zgt0d", "num_comments": 3, "score": 2, "title": "Basic PRAW Question: Getting comment posted time", "url": "https://www.reddit.com/r/redditdev/comments/1zgt0d/basic_praw_question_getting_comment_posted_time/"}, {"author": "unigee", "body": "Is there something wrong with the latest release of PRAW? I just upgraded and when I attempt to import praw I now get this error: http://i.imgur.com/fWNEDo5.png Any ideas?", "created_utc": 1393274329, "gilded": 0, "name": "t3_1ytpmx", "num_comments": 4, "score": 3, "title": "Latest PRAW not working?", "url": "https://www.reddit.com/r/redditdev/comments/1ytpmx/latest_praw_not_working/"}, {"author": "chancrescolex", "body": "So let's say I have a bot. This bot replies to comments that contain a trigger. If the bot tries to reply to a comment but is banned from that sub, it dies with the following error: Traceback (most recent call last): File \"Documents/Python3/bot.py\", line 46, in comment.reply(trigger_reply % (comment.author, parent_author)) File \"/usr/local/lib/python3.3/site-packages/praw/objects.py\", line 336, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"/usr/local/lib/python3.3/site-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python3.3/site-packages/praw/__init__.py\", line 1955, in _add_comment retry_on_error=False) File \"/usr/local/lib/python3.3/site-packages/praw/decorators.py\", line 161, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python3.3/site-packages/praw/__init__.py\", line 492, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.3/site-packages/praw/__init__.py\", line 362, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python3.3/site-packages/praw/internal.py\", line 173, in _raise_response_exceptions response.raise_for_status() File \"/usr/local/lib/python3.3/site-packages/requests/models.py\", line 773, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 403 Client Error: Forbidden So is there a way to either make it not crash and burn, or to check if it's banned from the sub the comment is in and not attempt to comment?", "created_utc": 1393095436, "gilded": 0, "name": "t3_1yn3ae", "num_comments": 5, "score": 5, "title": "[PRAW] how to keep bot from crashing when it fails to post a comment reply due to being banned from a particular sub?", "url": "https://www.reddit.com/r/redditdev/comments/1yn3ae/praw_how_to_keep_bot_from_crashing_when_it_fails/"}, {"author": "argutus1", "body": "First of all, I understand that my code is incorrect, .lower() cannot be used on another function and 'or' cannot be used in this manner, I'm just confused as to how I can go about fixing it. import praw r = praw.Reddit( user_agent = \"VertcoinBot v1.0: A bot to automate several activities on Vertcoin Subreddits\" ) r.login('...','...') submission = r.get_submission(submission_id='1wulry') post_comments = submission.comments already_done = set() trade_count = 0 conf_count = 0 uneven = 0 for comment in post_comments: if \"bought\" in comment.body.lower() or \"sold\" in comment.body.lower() and comment.id not in already_done: trade_count = str(comment.body).lower().count(\"bought\") + str(comment.body).lower().count(\"sold\") conf_count = str(comment.replies).lower().count(\"confirmed\") if trade_count != conf_count: uneven = 1 if 1 = 25: r.get_subreddit('vertmarket').set_flair(comment.author, '', 'Gold') #Gold CSS Flair if uneven == 0: comment.reply('Your flair has been updated.') print \"Updated user flair.\" else: comment.reply('Your flair has been updated, however some of your trades have not been confirmed, in the meantime your flair will not reflect these trades.') print \"Updated user flair with unconfirmed trades.\" already_done.add(comment.id) The specific error I get is: > Line 12: > for (\"bought\", \"sold\") in comment.body.lower(): > SyntaxError: can't assign to literal", "created_utc": 1393035945, "gilded": 0, "name": "t3_1ylb52", "num_comments": 7, "score": 2, "title": "Searching comments for two words regardless of case", "url": "https://www.reddit.com/r/redditdev/comments/1ylb52/searching_comments_for_two_words_regardless_of/"}, {"author": "chancrescolex", "body": "I'm a total noob. I'm trying to use the example bot [here](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) just to play around with making a reddit bot and when it tries to log in I get the following error: Warning (from warnings module): File \"C:\\Python33\\lib\\site-packages\\requests\\packages\\urllib3\\connection.py\", line 99 HTTPConnection.__init__(self, host, port, strict, timeout, source_address) DeprecationWarning: the 'strict' argument isn't supported anymore; http.client now always assumes HTTP/1.x compliant servers. After searching, the only advice I could find was to update requests, which I did, and I still get the same error. Any thoughts?", "created_utc": 1392926949, "gilded": 0, "name": "t3_1ygwc7", "num_comments": 7, "score": 1, "title": "Requests DeprecationWarning when trying to log in with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "sf_spoileralert", "body": "Hey everyone, I'm writing my first crawler and I've got a question about replace_more_comments(). I understand the rate limits and rules surrounding API calls to /morechildren, however I'm looking for a way to speed up my script. Currently it's taking me almost 25 minutes to pull down a popular submission comments alone (~4500 comments). Would this rate improve if I was authenticated? I'm a little confused from the PRAW docs how to go about that, so if anyone has any tips, I'd really appreciate it. Obviously I'm not looking to circumvent the rules or anything, I just need some advice on how to optimize my script. Thanks!", "created_utc": 1392751308, "gilded": 0, "name": "t3_1y9lu7", "num_comments": 2, "score": 1, "title": "[PRAW] replace_more_comments() rate limiting", "url": "https://www.reddit.com/r/redditdev/comments/1y9lu7/praw_replace_more_comments_rate_limiting/"}, {"author": "bobbonew", "body": "The information returned for certain API calls is *extremely* large. For example the **/subreddits/mine/** call is absolutely gigantic, when my use case is to only obtain the subreddit name and its ID. How can I limit the information returned? The only parameter I can see accomplishing this is 'show' - but the information listed on it is extremely limited: >**show** - optional parameter; if all is passed, filters such as \"hide links that I have voted on\" will be disabled. Can anybody point me in the right direction? (Note: I am not using PRAW for this.)", "created_utc": 1392749291, "gilded": 0, "name": "t3_1y9hzq", "num_comments": 1, "score": 2, "title": "How can I limit the information returned in an API call?", "url": "https://www.reddit.com/r/redditdev/comments/1y9hzq/how_can_i_limit_the_information_returned_in_an/"}, {"author": "MetricPleaseBot", "body": "I'm a bot being written by /u/Bur_Sangjun and I need to delete private messages if they don't match a certain criteria (don't match the format I will accept (/u/MetricPleaseBot ### U > U, where # is an integer and U is a unit). I can not for the life of my find a way to delete private messages through praw.", "created_utc": 1392724226, "gilded": 0, "name": "t3_1y8gt3", "num_comments": 3, "score": 0, "title": "What is the proper way to delete a message?", "url": "https://www.reddit.com/r/redditdev/comments/1y8gt3/what_is_the_proper_way_to_delete_a_message/"}, {"author": "jonathan_morgan", "body": "I am trying to use praw and python to retrieve all comments for a set of posts so I can build and compare comment cascades. I am testing on a post whose num_comments value is 22,014: * [http://www.reddit.com/r/AskReddit/comments/1bvkol/what_was_your_biggest_holy_shit_why_havent_i_done/](http://www.reddit.com/r/AskReddit/comments/1bvkol/what_was_your_biggest_holy_shit_why_havent_i_done/) I do the following: # make praw instance import praw r = praw.Reddit( user_agent = \"reddit comment collector with PRAW, v0.1 by /u/jonathan_morgan\" ) # log in - omitting actual values r.login( my_username, my_password ) # get post post_id = \"1bvkol\" post = r.get_submission( submission_id = post_id ) # use the replace_more_comments() method to pull in as many comments as possible. post.replace_more_comments( limit = None, threshold = 0 ) # flatten flat_comments = praw.helpers.flatten_tree( post.comments ) # count comments print( \"==> after flatten_tree(), comment count: \" + str ( len( flat_comments ) ) ) On repeated invocations, the count printed at the end has been either 12,970 or 13,364, neither is close to 22,014. Am I doing something wrong here? Is this possibly a limitation of the API, or because I am not a gold user? I've found that the \"num_comments\" count on the post doesn't always match the actual number of comments, but I've never seen it this far off. And, on posts with fewer comments, it seems to work just fine. For example, the following has 115 comments: * [http://www.reddit.com/r/programming/comments/1cp0i3/clang_c11_support_is_now_feature_complete/](http://www.reddit.com/r/programming/comments/1cp0i3/clang_c11_support_is_now_feature_complete/) Should I be expanding the \"More Comments\" by hand, as in this post: [http://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/](http://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/), instead of calling replace_more_comments()? A follow-on - what order is the comments list in? I did a quick loop over the 1st 15 things in \"flat_comments\" in a couple of different posts and outputted the created_utc value, converted to a readable date, and the comments are not in date order. Is there a parameter I can pass to comments() to tell it to sort oldest-first? If I can't get all comments, for whatever reason, I'd like to get as many comments as I can, earliest first, in order, so I am looking at the complete tree up to a given point in time, rather than having most recent comments.and not having the beginning of the cascade. Any help or advice will be greatly appreciated. Thanks, Jonathan Morgan", "created_utc": 1392698798, "gilded": 0, "name": "t3_1y7pq7", "num_comments": 2, "score": 2, "title": "help with retrieving all comments for a given post with praw/python", "url": "https://www.reddit.com/r/redditdev/comments/1y7pq7/help_with_retrieving_all_comments_for_a_given/"}, {"author": "timotab", "body": "While I'm not new to scripting (Unix sysadmin for 20+ years), I'm still relatively new to python. I have python 3.3.3 with praw installed. I'm looking to get the current ~~CSS code~~ sidebar, make a change, and submit that change. The full details of what I'm trying to do is in my [/r/requestabot post](http://www.reddit.com/r/RequestABot/comments/1xmj7q/need_a_bot_to_post_distinguish_then_update/), but I didn't get much feedback there. Edit: I'm stupid. I meant the sidebar, not the stylesheet", "created_utc": 1392659645, "gilded": 0, "name": "t3_1y5t6e", "num_comments": 5, "score": 0, "title": "need help getting/updating CSS", "url": "https://www.reddit.com/r/redditdev/comments/1y5t6e/need_help_gettingupdating_css/"}, {"author": "acini", "body": "I am seeing many users [abusing the bots](http://www.reddit.com/r/bestof/comments/1y5ga3/ufiguratively_hilter_gets_reddit_bots_stuck_in_an/) around different subs. So here are some [ready-made functions](https://github.com/acini/praw-antiabuse-functions) to prevent abuse of your bot/s. 1. `is_summon_chain(post)`: detects grandparent comment being bot's own. 1. `comment_limit_reached(post)`: stores comment counts per submission in memory (One should also maintain banned_users and banned_subreddits in a list object.) If you modify/add to code, please send a pull request on github. Happy coding!", "created_utc": 1392656216, "gilded": 0, "name": "t3_1y5o0i", "num_comments": 2, "score": 2, "title": "Anti-abuse functions for new botmasters using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1y5o0i/antiabuse_functions_for_new_botmasters_using_praw/"}, {"author": "MrBonez", "body": "I'm following [this tutorial](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) and seem to have run into a bit of an issue where it keeps replying to the same post. If someone could give me a bit of help I'd really appreciate it. import praw import time r = praw.Reddit('Auto replier' 'work in progress') r.login('username', 'password') already_done = [] HotWords = ['Word 1', 'Word 2', 'Word 3', 'Word 4'] while True: subreddit = r.get_subreddit('subreddit') for submission in subreddit.get_new(limit=10): op_title = submission.title.lower() has_hotword = any(string in op_title for string in HotWords) if submission.id not in already_done and has_hotword: print ('Found one...') msg = 'Lorem ipsum dolor sit amet...' submission.add_comment(msg) already_done.append(submission.id) time.sleep(60)", "created_utc": 1392627147, "gilded": 0, "name": "t3_1y4t0j", "num_comments": 2, "score": 1, "title": "[PRAW] Spamming issue", "url": "https://www.reddit.com/r/redditdev/comments/1y4t0j/praw_spamming_issue/"}, {"author": "globalglasnost", "body": "**UPDATE The answer I was looking for is MoreComments.comments(update=True), which returns an object of type list (more Comments or MoreComments) or an object of type NoneType, which I think is a deleted comment? Either way I'm able to recursively traverse and grab each Comment object now thanks for everyone's help!** Using PRAW in Python...the code below loads all the comments and counts what is loaded on the fly. The two /r/politics URLs I am loading each have over 1000 in reality, however the code is only loading 200-300? Do I need to collapse a MoreComment object down further? If so I did a dir(praw.objects.MoreComment) and I am kinda lost as how to proceed, I see a comment method but I can't call it? Thanks! **CODE** import praw r=praw.Reddit(user_agent='user_agent_example') iCount = 0 # submission = r.get_submission('http://www.reddit.com/r/politics/comments/1xzbcl/on_february_11the_day_we_fight_backour/') submission = r.get_submission('http://www.reddit.com/r/politics/comments/1ryfk0/americans_want_congress_members_to_pee_in_cups_to/') flat_comments = praw.helpers.flatten_tree(submission.comments(limit=none)) for comment in flat_comments: iCount = iCount + 1 if isinstance(comment, praw.objects.Comment): # code here print(iCount) **dir(praw.objects.MoreComment) returns:** > ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_get_json_dict', '_populate', '_update_submission', 'comments', 'from_api_response', 'fullname']", "created_utc": 1392510185, "gilded": 0, "name": "t3_1y0t1v", "num_comments": 4, "score": 5, "title": "I want to load *all* the comments from a specific URL, however, I'm not sure how to load from MoreComment objects?", "url": "https://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/"}, {"author": "mattster42", "body": "So I've been picking up PRAW and Python over the past week to build a bot to help moderate a subreddit. What I'm wanting is to have the script automatically post each link from a certain RSS feed. Easy enough, but with each new link I ALSO want the script to attach a certain link flair to the submission AND remove it from any previous submissions, so that it denotes the current entry for the feed. So far the script works perfectly, unless someone else posts the latest entry first. If that happens, the script correctly returns the ALREADY_SUB error, but then the whole flair thing goes out of whack. I could solve the problem if there was a way to get the Submission object for the submission that causes the ALREADY_SUB error, that is, the Submission that the other person posted. However, I don't know how to do that. I first tried the get_submission using the URL argument, but it seems like that argument is meant for the URL of the reddit thread, not the URL linked to. Any guidance?", "created_utc": 1392257819, "gilded": 0, "name": "t3_1xrkhi", "num_comments": 2, "score": 2, "title": "Get a Submission object based on the URL of the link?", "url": "https://www.reddit.com/r/redditdev/comments/1xrkhi/get_a_submission_object_based_on_the_url_of_the/"}, {"author": "metalayer", "body": "I'm trying to write a function to register new Reddit accounts. I've set the raise_captcha_exception flag so the API should respond with the CAPTCHA id but I'm not sure how to actually get it. Also, with raise_captcha_exception set to false I get no prompt in my terminal. Running python 3.3 Stack overflow question here - http://stackoverflow.com/questions/21617451/how-to-get-reddit-captcha-id-with-praw", "created_utc": 1392084352, "gilded": 0, "name": "t3_1xkp8n", "num_comments": 2, "score": 2, "title": "[PRAW] Captcha handling problem", "url": "https://www.reddit.com/r/redditdev/comments/1xkp8n/praw_captcha_handling_problem/"}, {"author": "JJJollyjim", "body": "Hi there, I'm new to Python and the reddit API in general, so apologies if I make any mistakes here. A bot I am working on is receiving messages, and then doing some non-reddit stuff which could take a day or two. I expect that the bot may restart in this time, and I don't want to be storing everything in miniscule amounts of RAM on my server. What I am currently doing is storing the sender's username in a database (Mongo) and sending a new message to that username later, which works fine: reddit.send_message(user_name, \"subject\", \"body\") However, I'd prefer for a couple of reasons to use a reply. Looking at the reddit API docs, I should store the id of the message and call `/api/comment` with the message id as the `thing_id`. Just using pure PRAW, I'm not sure how to do it. I can't create a `praw.objects.Message` with an id, as far as I can tell. Any suggestions?", "created_utc": 1392021468, "gilded": 0, "name": "t3_1xi8ol", "num_comments": 1, "score": 2, "title": "[PRAW] How to store an inbox message (in a database) for replying to later", "url": "https://www.reddit.com/r/redditdev/comments/1xi8ol/praw_how_to_store_an_inbox_message_in_a_database/"}, {"author": "zd9", "body": "Is there a way in PRAW to find out which sub a comment is from if I am getting comments from /r/all?", "created_utc": 1391899514, "gilded": 0, "name": "t3_1xe0fw", "num_comments": 3, "score": 1, "title": "[PRAW] Check what subreddit a comment is from", "url": "https://www.reddit.com/r/redditdev/comments/1xe0fw/praw_check_what_subreddit_a_comment_is_from/"}, {"author": "notverysmart_bot", "body": "First of all I am new to programming in general and I am sorry in advance for bothering all of you. I have however googled for solutions, but sadly I don't understand much. I was trying to make a reddit bot and log in to reddit. I am using the example code from the PRAW documentation: r = praw.Reddit(USER_AGENT) def login(): print (\"Logging in..\") r.login(username, password) However I am getting the following error: Warning (from warnings module): File \"C:\\Program Files (x86)\\Python\\lib\\site-packages\\requests\\packages\\urllib3\\connection.py\", line 99 HTTPConnection.__init__(self, host, port, strict, timeout, source_address) DeprecationWarning: the 'strict' argument isn't supported anymore; http.client now always assumes HTTP/1.x compliant servers. Any help and explanation of what's going on would be much appreciated.", "created_utc": 1391785324, "gilded": 0, "name": "t3_1x9wzh", "num_comments": 6, "score": 3, "title": "Having trouble logging in with PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/1x9wzh/having_trouble_logging_in_with_praw/"}, {"author": "Mustermind", "body": "In the praw \"tutorial\" for writing a bot, they store a list of comments ids for each time an action is performed. What if the bot needs to be restarted? Of course you can store the ids in a file, but is there any other way of protecting against replying to a comment twice? What about checking the replies to a comment to see if the bot has already replied? Is there anything that can go wrong there?", "created_utc": 1391773514, "gilded": 0, "name": "t3_1x9kzl", "num_comments": 7, "score": 3, "title": "Is there more than one way to make sure a bot will not do the same thing twice?", "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "FramesPerSushi", "body": "r.get_subreddit('example').get_moderators() works fine, but r.get_subreddit('example').get_modqueue() returns this error: AttributeError: '' has no attribute 'get_modqueue' Edit: Even though it's fairly obvious I should probably specify that I'm using PRAW (Python).", "created_utc": 1391743431, "gilded": 0, "name": "t3_1x8q1b", "num_comments": 2, "score": 1, "title": "Am I not using \"get_modqueue()\" correctly?", "url": "https://www.reddit.com/r/redditdev/comments/1x8q1b/am_i_not_using_get_modqueue_correctly/"}, {"author": "mattster42", "body": "So the past couple days have introduced me to Python and PRAW simultaneously. I'm getting the hang of the very basics so far, but I'm having trouble figuring out how to set link flair the way I want. So I've adapted newsfrbot to submit a link for every new entry in an RSS feed (is there a better script to do this? Because it's implementation of cPickle crashes if the process is ever interrupted). What I want to do is assign a certain link flair to the post upon submission, while clearing that flair from any previous posts (essentially so that the latest post from the bot has a flair marked \"current\"). I'm sure there's a simple four line solution to this, so can someone help a newbie get in the right direction?", "created_utc": 1391383742, "gilded": 0, "name": "t3_1wug8a", "num_comments": 6, "score": 2, "title": "Set link flair upon submission?", "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "newpong", "body": "Im not sure if I just need to sleep, or what. This seems like this would be a straight forward task, but I can't for the life of me figure out what I'm doing wrong. Here's the traceback: >>> a.author Redditor(user_name='xxxxxxxxxx') >>> a.body u'xxxxxxxxxxx' >>> a.score Traceback (most recent call last): File \"\", line 1, in File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 83, in __getattr__ attr)) AttributeError: '' has no attribute 'score' But 'score' seems to be available: >>> pprint(dir(a)) [ [...] 'approve', 'author', 'body', 'body_html', 'clear_vote', 'context', 'created', 'created_utc', 'delete', 'dest', 'distinguish', 'downvote', 'edit', 'first_message', 'first_message_name', 'from_api_response', 'fullname', 'has_fetched', 'id', 'ignore_reports', 'is_root', 'json_dict', 'likes', 'link_title', 'mark_as_read', 'mark_as_unread', 'name', 'new', 'parent_id', 'permalink', 'reddit_session', 'remove', 'replies', 'reply', 'report', 'score', 'subject', 'submission', 'subreddit', 'undistinguish', 'unignore_reports', 'upvote', 'vote', 'was_comment'] Praw Version: $ pip freeze | grep praw praw==2.1.12 Any idea what I'm doing wrong? Thanks.", "created_utc": 1391244101, "gilded": 0, "name": "t3_1wptfz", "num_comments": 2, "score": 2, "title": "What is the correct way to return the comment score? I'm getting an attribute error.", "url": "https://www.reddit.com/r/redditdev/comments/1wptfz/what_is_the_correct_way_to_return_the_comment/"}, {"author": "TeroTheTerror", "body": "Hi, newish to python and PRAW here. Looked at the PRAW docs, but couldn't find anything to help me. Basically I would like to check a post X minutes after being submitted and see if the user assigned link flair. If not I want to remove the post. Checking the link flair and removing posts seems straight forward, but the time variable is holding me up, any suggestions?", "created_utc": 1391158077, "gilded": 0, "name": "t3_1wmt4p", "num_comments": 2, "score": 1, "title": "[PRAW] Is it possible to use Praw to check on posts after a certain amount of time?", "url": "https://www.reddit.com/r/redditdev/comments/1wmt4p/praw_is_it_possible_to_use_praw_to_check_on_posts/"}, {"author": "chpwssn", "body": "Is it possible to point PRAW to a Reddit clone? I thought it may be easier/better to test a bot that scans all submissions on a clone than on here. Thanks.", "created_utc": 1391105088, "gilded": 0, "name": "t3_1wkn3s", "num_comments": 1, "score": 2, "title": "PRAW on a Reddit clone?", "url": "https://www.reddit.com/r/redditdev/comments/1wkn3s/praw_on_a_reddit_clone/"}, {"author": "IAMA_YOU_AMA", "body": "First off, I just want to say that PRAW is great and I really appreciate the effort the developers put into it! I also find it very helpful for my text mining and other analysis projects. I even hope to make a useful bot one day. That being said, as someone still relatively new to python, I am really struggling with the documentation. For example, it took me way longer than it should have to figure out comments have a body attribute that I should be using or an id attribute too. I'm not even sure what else I still might be missing. I know that comments come from the content generator and I'm more confident with using it now, but only after much trial and error and inspecting other scripts. I feel like I'm missing out on a lot of potential, by not knowing everything I can do. :-/ If anyone has any pointers or helpful advice on how to get through the documentation, I would really appreciate it. Thanks!", "created_utc": 1390959488, "gilded": 0, "name": "t3_1wfb55", "num_comments": 5, "score": 3, "title": "PRAW documentation", "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "WikipediaCitationBot", "body": "**Introducing /u/WikipediaCitationBot** ------ **What do you comment on?** > I comment on articles that: > * have *more than 7 problems*, and *exceed a 0.15 ratio* of the number of references to the number of [**problems**](http://en.wikipedia.org/wiki/Template:Citation_needed#Inline_templates), and though I scan for dead links, I don't count them to this sum of problems > or > * have *no references at all* and have at *least three problems*, although I will specify whether or not the page has external links > I'm fairly lenient; at the time of writing this, I have commented on 33 todayIlearned posts, but have scanned over 500 wikipedia submissions to the subreddit, so I tend to keep to myself unless it's a particularly egregious article. And since I was launched, I have only become more strict about what I comment on. **How do I get rid of you?** > If I have made a mistake or am bothering you, feel free to downvote my comments. Any comment I make with a *score of zero or below will be automatically deleted*. That means if it's just your word against mine, you win. **Where do you scan?** > I'm only concerned with subreddits focused on providing accurate information, so currently I only scan ~~/r/wikipedia~~ and /r/todayilearned (*please don't ban me*) **How do you operate?** >I'm less than a hundred lines of python thanks to praw and urllib; once this bot has more capabilities I'll make the code public on GitHub **Why is this necessary?** > I think it's good to keep citing and lack thereof in mind, but I also try to ensure the script isn't overzealous or annoying. Mostly, I hope if I point out that a wikipedia page is poorly cited, it will encourage someone knowledgeable about the subject to contribute to it or cite it themselves. **Who made you?** > A procrastinating second year EE student ------ **My questions:** Do you have any suggestions or desires for functionality? Do you think the numbers should be tweaked? More strict? Less strict? Should I look at wikipedia articles linked in comments? What other subreddits should I look in?", "created_utc": 1390709056, "gilded": 0, "name": "t3_1w642h", "num_comments": 1, "score": 1, "title": "About WikipediaCitationBot", "url": "https://www.reddit.com/r/redditdev/comments/1w642h/about_wikipediacitationbot/"}, {"author": "FramesPerSushi", "body": "both import praw def wut_the_fuck(): return 'idk' post = wut_the_fuck() print(post) and import requests def wut_the_fuck(): requests.get('http://example.com') return 'idk' post = wut_the_fuck() print(post) return \"idk\", but import requests import praw def wut_the_fuck(): requests.get('http://example.com') return 'idk' post = wut_the_fuck() print(post returns: testweb.py:5: ResourceWarning: unclosed requests.get('http://example.com') idk I have absolutely no idea what's going on. Does anyone know?", "created_utc": 1390589032, "gilded": 0, "name": "t3_1w1w1s", "num_comments": 12, "score": 2, "title": "Can't use requests module if I've imported praw", "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "HAEC_EST_SPARTA", "body": "I am using PRAW to program a Reddit bot. Assuming I have a Comment object, how do I determine which subreddit it is in?", "created_utc": 1390363794, "gilded": 0, "name": "t3_1vtpmp", "num_comments": 4, "score": 1, "title": "Get which Subreddit a Comment Is In [PRAW]", "url": "https://www.reddit.com/r/redditdev/comments/1vtpmp/get_which_subreddit_a_comment_is_in_praw/"}, {"author": "Dwarflord", "body": "Hi, I'm making a bot in python, 3.3.3 if it matters, and I'm coming to an issue where it can't reply to a comment. It reads everything fine and prepares the right output, but when it comes to the comment.reply('..') line I get, Traceback (most recent call last): File \"C:\\Python33\\Class\\RoseluckBot\\RoseBotv1.py\", line 51, in comment.reply(\"Looks like you love Roseluck too! \" + postrep) NameError: name 'comment' is not defined I can post code if needed. Does anyone else get this issue? Thank you for any help you can give! Edit: code #RoseBot import time import re import praw import random r = praw.Reddit('Roseluck and other pony Monitor V1.3 ' 'By Dwarflord, for the Plounge' 'Posts images of ponies based on character named') r.login(Login values) already_done = [] cache = [] def get_rose_img(): ###Returns a roseluck image based on random seeding roseImages = ['XyhgWeG','vAw1hwE','U9S6Uaq','gxsDDl4','MqVN9n2','0Sf9q6U','QkR5pz5','1F6p7ks','Ubii8iL','Gc5EjiU','KPdbXQ1'] ranInd = random.choice(roseImages) #Random list choice. thanks Python! fullRep = \"[You need this!](http://i.imgur.com/%s.jpg)\" % (ranInd) return fullRep def check_condition(comment): ###Chekcs whether or not the comment contains rose or roseluck, in lower. ###Returns True or False. r_pat = re.compile('rose') r_pat2 = re.compile('roseluck') text = comment.body result = re.search(r_pat, text.lower()) result2 = re.search(r_pat2, text.lower()) if result or result2: #comment.reply('Looks like you said Roseluck!') #No, savour it return True else: return False #Main while True: subreddit = r.get_subreddit('MLPLounge') subreddit_comments = subreddit.get_comments() for c in subreddit_comments: #For all comments in selected Subreddit Comments if c.id in cache: #If comment already commented on break cache.append(c.id) condition = check_condition(c) #condition can be True or False if condition: postrep = get_rose_img() print(c) print (\"Looks like you love Roseluck too! \" + postrep) comment.reply(\"Looks like you love Roseluck too! \" + postrep)", "created_utc": 1390331768, "gilded": 0, "name": "t3_1vs7v5", "num_comments": 4, "score": 2, "title": "Replying to a comment with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1vs7v5/replying_to_a_comment_with_praw/"}, {"author": "Jyrroe", "body": "I'm writing a bot that finds and removes posts/comments that violate my subreddit's rules. When a comment is deleted, the bot leaves a comment for the user explaining the removal. The problem is, I can't find a way to distinguish normal comments from ones that have already been removed, so every time the bot runs, it adds another explanation comment. Any ideas? *** **EDIT:** I have a pseudo-solution, I found out about praw.helpers.comment_stream() and I found get_spam(), so I'm basically checking that each comment is *not in* the spam list yet, and only then will the bot comment on it. The problem is, I'm not seeing what the limit is on returned spam comments/posts. Is there a chance I could end up checking a comment from comment_stream() that wasn't returned by get_spam()? Ideally I'd like something like *comment.is_removed* or *comment.is_spam* but I'm not finding anything like that... **Question part 2:** Using get_spam(), is there a way to return comments only (instead of comments and posts)?", "created_utc": 1390313869, "gilded": 0, "name": "t3_1vrfu3", "num_comments": 3, "score": 6, "title": "[PRAW] How can I tell if a post/comment has been removed?", "url": "https://www.reddit.com/r/redditdev/comments/1vrfu3/praw_how_can_i_tell_if_a_postcomment_has_been/"}, {"author": "HAEC_EST_SPARTA", "body": "I am currently developing a Reddit bot using PRAW. It is working for the most part, but I am having one problem. When I fetch the newest comments from all of Reddit, it continually returns the same comments. Please help me to rectify this problem. Here is my source code: `while True: comments = r.get_comments('all', limit=100) for comment in comments: for word in comment.body.split(): (process the word)` Sorry for the large number of edits, but I have no idea how to properly format code for this site.", "created_utc": 1390292759, "gilded": 0, "name": "t3_1vqywg", "num_comments": 3, "score": 2, "title": "PRAW Comment Fetching Problem", "url": "https://www.reddit.com/r/redditdev/comments/1vqywg/praw_comment_fetching_problem/"}, {"author": "5loon", "body": "I'm trying to make a bot that will read a message with a username, and then input that username along with a CSS class to add flair to that person's username. I can't find anything on praw reading incoming messages. Does anyone know how to do this, or know any bots that already do this? I remember /r/pokemon having something similar earlier on. Thanks in advance.", "created_utc": 1390205869, "gilded": 0, "name": "t3_1vnq5y", "num_comments": 6, "score": 2, "title": "I'm new to using praw on python, I have one simple question.", "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "blpst", "body": "Hi, I could not find a function to accomplish this in praw. Reddit does have an api for it, though. (http://www.reddit.com/api/multi/mine) I have taken a look at the source code, it shouldn't be that hard to implement except I do not have much time on my hands recently. Is there anything I'm missing? Is there a way using the current version of praw? Thanks.", "created_utc": 1390187223, "gilded": 0, "name": "t3_1vn371", "num_comments": 0, "score": 1, "title": "Praw: Get list of multis?", "url": "https://www.reddit.com/r/redditdev/comments/1vn371/praw_get_list_of_multis/"}, {"author": "redguy13", "body": "I was wondering if anyone could assist me with checking to see if \"get_redditor\" returns an error or not. I have used the \"fetch=True\" argument and it still returns. However, if you go to the user \"Alaska88\" page then it does not exist. The error happens when the program reaches the \"for comment in comments\" line and I am assuming the try-except doesn't work due to it being a lazy object. Thank you in advance for any time or help. import praw import urllib2 r = praw.Reddit('testing scraper') r.login() account = r.get_redditor('Alaska88',fetch=True) comments = account.get_comments(sort ='new',time ='all') print 'before comment loop' try: for comment in comments: print 'in comment loop' print(comment.body.encode('utf-8')) print('/////////////////////////') except urllib2.HTTPError: print 'In Except' time.sleep(60) pass File \"reddit_bot.py\", line 9, in This is where the error starts => for comment in comments: This is where it finally ends => raise HTTPError(http_error_msg, response=self)requests.exceptions.HTTPError: 404 Client Error: Not Found", "created_utc": 1390186084, "gilded": 0, "name": "t3_1vn25q", "num_comments": 2, "score": 1, "title": "Praw: How to do a try-except for a lazy object", "url": "https://www.reddit.com/r/redditdev/comments/1vn25q/praw_how_to_do_a_tryexcept_for_a_lazy_object/"}, {"author": "Rosco_the_Dude", "body": "I've been playing with praw for the past week or two, and I began writing so many functions with it that I naturally began making a class to represent the bot. Here's a short summary of what I'm doing so far: The bot scans its friends' posts and the posts in its private subreddit for predetermined tags and keywords so that it can be controlled from Reddit's normal interface by my friends and me. Right now it only has two main functions: advanced keyword searches, and reposting things to the private subreddit by commenting on a post and including a special tag in the comment body. I have a few of other ideas for it, but I'm still learning my way around praw (and still learning my way around Python, to some extent). I'm still in experimentation phase, so I don't keep the program running indefinitely yet. However, I'd like some data to persist after the program shuts down, e.g. when I save posts' IDs so that my program doesn't perform the same functions on them twice, I want the IDs to be around even if I need to restart the program. I've looked up documentation on Pickle, cPickle, and Shelve. So far Shelve seems like the best idea, but I'm still new enough at this that I'd like to hear other people's opinions. If I read the docs correctly, it seems that I can't just save an instance of an entire class in one fell swoop; it seems like I'd need to save (and eventually load) each value/array/dictionary seperately. Am I correct in this? I guess my question isn't about whether things like Shelve, Pickle, etc. exist, but it's whether there is a preferred way to use them when writing Reddit bots. I'm still new enough that I'd love to hear how everyone keeps their data persistent in their own bots/applications.", "created_utc": 1390108677, "gilded": 0, "name": "t3_1vkqib", "num_comments": 9, "score": 2, "title": "I'm writing my first Reddit bot, and I was wondering what is the best technique for saving instances of objects so that my bot doesn't lose progress when it shuts down.", "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "stats94", "body": "When I run 'pip install praw' in cmd it comes up with: Downloading/unpacking praw Cannot fetch index base URL https://pypi.python.org/simple/ Could not find any downloads that satisfy the requirement praw Cleaning up... No distributions at all found for praw Storing debug log for failure in C:\\ ... My python scripts seem to always fail to download any content, and I have absolutely no idea why this is but I guess this may be the problem here too. I've not found anybody that's had this problem yet - anybody know how to fix? **Edit: PROBLEM SOLVED, THANKS EVERYONE!**", "created_utc": 1390046566, "gilded": 0, "name": "t3_1viomp", "num_comments": 5, "score": 0, "title": "pip unable to download praw", "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": "idProQuo", "body": "I'm currently working on a reddit bot that's gotten quite complex. I'd like to implement a testing framework just to restore some sanity. However, I'm finding it difficult to write useful tests when PRAW is involved. Does anyone know of a good testing framework for working with PRAW, or a good way of mocking its functionality? If you know of a project on GitHub that has good tests, a link would be awesome! Ideally, I'd like to not need a local reddit instance running when I test.", "created_utc": 1389753108, "gilded": 0, "name": "t3_1v8sbx", "num_comments": 4, "score": 3, "title": "Using PRAW with a unit testing framework and mocks", "url": "https://www.reddit.com/r/redditdev/comments/1v8sbx/using_praw_with_a_unit_testing_framework_and_mocks/"}, {"author": "PointsOutTrains", "body": "I have a list of users for a network project I'm working on. I want to connect the users to the subreddits they have commented in or posted in, and get the number of times they have commented or posted in that subreddit. I thought the best way to do this would be by scraping the comments and getting the subreddit from there, but I don't know how using PRAW. Is there a way using get_comments, or is this not possible?", "created_utc": 1389573165, "gilded": 0, "name": "t3_1v2ho0", "num_comments": 9, "score": 1, "title": "Is it possible to get the subreddit of a comment with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "SwedishBoatlover", "body": "I've tried >for post in self.redditor.get_overview(): But it will only give me the 25 latest posts. How do I go ahead to get every single post the user has made to iterate through them? Ninjaedit: All posts that are not archived yet. Maybe it's obvious, but I'm using PRAW, Python 3.3, on W7.", "created_utc": 1389103411, "gilded": 0, "name": "t3_1umg87", "num_comments": 4, "score": 0, "title": "How to get all posts from a user?", "url": "https://www.reddit.com/r/redditdev/comments/1umg87/how_to_get_all_posts_from_a_user/"}, {"author": "Justinsaccount", "body": "Hi everyone. I was just tracking down why links to phoronix.com in /r/linux never work right when using reddit sync. Turns out it is related to [a two year old issue](https://github.com/reddit/reddit/issues/283) that is not going to be fixed. As far as I can tell, Reddit uses [This function](https://github.com/reddit/reddit/blob/master/r2/r2/lib/filters.py?source=cc#L56) for encoding json, which escapes &,. https://github.com/reddit/reddit/wiki/JSON Mentions that a few fields are escaped, but it would appear that all data is escaped. From what I tested, clients don't url decode the url field that reddit returns. for example: In [1]: import praw In [2]: r=praw.Reddit(user_agent='test') In [3]: thing=r.get_subreddit('linux').get_hot(limit=5).next() In [4]: thing.url Out[4]: u'http://www.phoronix.com/scan.php?page=news_item&px=MTU1NzA' Not sure what to do about this... minimally the documentation should be updated..", "created_utc": 1388628104, "gilded": 0, "name": "t3_1u7553", "num_comments": 6, "score": 13, "title": "Client developers: I think most (all?) of your clients are subtly broken.", "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "pachufir", "body": "So, I've been following the PRAW tutorials (wonderfully written btw) to write my own reddit bot, and I saw many times in the published code the login line is just r.login() or r.login(USERNAME, PASSWORD). I was wondering if this was just so that the code can be published without revealing the credentials, or is there some way that the fields are being dynamically filled in?", "created_utc": 1388441298, "gilded": 0, "name": "t3_1u1pwy", "num_comments": 10, "score": 6, "title": "open sourcing code with login credentials", "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "imareddituserhooray", "body": "Hey everyone, I have a best practice question related to PRAW's get_mentions function. How do you normally keep track of what has been seen; for example, can you control the read/unread status of mentions with PRAW? Do you normally write the id to a permanent storage (file/db, not a variable) for what you have already processed? If my bot crashes, I just don't want to double reply to mentions. Thanks!", "created_utc": 1388382800, "gilded": 0, "name": "t3_1tzwqd", "num_comments": 1, "score": 2, "title": "Python (PRAW) get_mentions and old mentions after crash", "url": "https://www.reddit.com/r/redditdev/comments/1tzwqd/python_praw_get_mentions_and_old_mentions_after/"}, {"author": "pachufir", "body": "I was wondering if there was a quick way through praw to get all the urls in a comment? Specifically I want to get all the gifs linked in a comment, but I feel if I can just easily get all the links, finding the gifs shouldn't be too hard. Thanks in advance!", "created_utc": 1388347220, "gilded": 0, "name": "t3_1tyk0o", "num_comments": 9, "score": 1, "title": "Anyway to extract links from a comment?", "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "UWillAlwaysBALoser", "body": "I apologize if this has been asked before, but it's very difficult to search for questions about R on reddit. I want to pull the subreddits in which a specified user has commented, just as /u/chicken_bridges has [using Praw](https://github.com/chickenbridges/reddit_cluster_scripts/blob/master/collect_data.py). Ideally, I'd like to do this in R, without having to use Python (or the rPython package). I'm curious if anyone has used R to scrape reddit through the API.", "created_utc": 1388045160, "gilded": 0, "name": "t3_1tq6kg", "num_comments": 6, "score": 2, "title": "Any documentation or tutorials for using the API in R?", "url": "https://www.reddit.com/r/redditdev/comments/1tq6kg/any_documentation_or_tutorials_for_using_the_api/"}, {"author": "Ph0X", "body": "Looking around, I couldn't really find a good answer for this. Closest I found was [this thread from almost 2 years ago](http://www.reddit.com/r/redditdev/comments/pz4xj/how_can_i_get_a_list_of_all_posts_in_a_subreddit/). I was wondering if there has been any update on this. I would like a dump of all the submissions on a subreddit I help run for statistical purposes, and I was going to write a script using PRAW to go through and save all the posts, but from what I understand, the normal methods are capped at 1000 submissions. Is there no way to get an enumeration of all posts. How long it takes doesn't really matter since it would be a one time thing.", "created_utc": 1387882951, "gilded": 0, "name": "t3_1tlmrf", "num_comments": 8, "score": 1, "title": "List of all submissions in a subreddit", "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "spinnelein", "body": "On /r/BackYardChickens we have a set of images that go on the sidebar. Currently I'm using PRAW and set_stylesheet to change the stylesheet every minute or so to cycle through the images. Is there a better way to do this? My method works fine, but should I be concerned about making that many stylesheet edits?", "created_utc": 1387740839, "gilded": 0, "name": "t3_1th5ar", "num_comments": 1, "score": 2, "title": "Cycling through a set of images on the sidebar - is there a better way to do it?", "url": "https://www.reddit.com/r/redditdev/comments/1th5ar/cycling_through_a_set_of_images_on_the_sidebar_is/"}, {"author": "rhiever", "body": "When was this API call added, and how does it work behind the scenes? Example: >import praw >r = praw.Reddit(user_agent=\"bot by /u/{0}\".format(\"rhiever\")) >r.get_subreddit_recommendations([\"redditdev\"]) >>[Subreddit(display_name='badkarma'), Subreddit(display_name='ModerationLog'), Subreddit(display_name='help'), Subreddit(display_name='ideasfortheadmins'), Subreddit(display_name='goldbenefits'), Subreddit(display_name='modhelp'), Subreddit(display_name='redditrequest'), Subreddit(display_name='webgl')] From the subreddits I frequent, the recommendations seem pretty nice. Is reddit finally implementing a recommendation system?! :-D", "created_utc": 1387514492, "gilded": 0, "name": "t3_1tay79", "num_comments": 2, "score": 5, "title": "Noticed new API call: get_subreddit_recommendations()", "url": "https://www.reddit.com/r/redditdev/comments/1tay79/noticed_new_api_call_get_subreddit_recommendations/"}, {"author": "AngusDWilliams", "body": "I found a PRAW example for tracking karma by subreddit, but it only looks at the last N submissions. I want a way to continuously track multiple user's cumulative karma per subreddit. I plan to hit the reddit API once per day to update this total. How would I go about implementing this? Is it possible?", "created_utc": 1387480621, "gilded": 0, "name": "t3_1t9l74", "num_comments": 2, "score": 3, "title": "How should I implement tracking karma by subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/1t9l74/how_should_i_implement_tracking_karma_by_subreddit/"}, {"author": "pranavrc", "body": "Hi! I have a bot over at /r/cricket called /u/howstat, which seems to reply twice to comments often. Is this an issue with PRAW? [Here](http://github.com/pranavrc/howstat)'s the code for the bot. If not, I suppose I could use comment timestamps to avoid that, so is there a way to get a comment's timestamp with PRAW? I couldn't find a suitable method for that, in the documentation. Thanks!", "created_utc": 1386868480, "gilded": 0, "name": "t3_1sq7yl", "num_comments": 8, "score": 2, "title": "[Praw] Bot double-posts often.", "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "Innokin_Paul", "body": "Hi, I hope you can help me with this. We are running an [ECig Christmas giveaway at /r/Innokin.](http://www.reddit.com/r/Innokin/comments/1rrnpp/innokinreddit_contest_2_reboot_new_rules_and_a/) I have installed and tested Praw and I would like to use it to collect the usernames from the contest thread. If possible I would like to also have them listed in a way which is easy to export into excel. Is this possible and can you help us? Thank you!", "created_utc": 1386577257, "gilded": 0, "name": "t3_1sg934", "num_comments": 7, "score": 1, "title": "Help with a Praw Script to collect usernames from a contest thread.", "url": "https://www.reddit.com/r/redditdev/comments/1sg934/help_with_a_praw_script_to_collect_usernames_from/"}, {"author": "im14", "body": "These Python PRAW calls work, but they process messages from newest to oldest. I want to process messages in the order they arrived to inbox. Looked up info on \"get_content generator\" but didn't find any obvious ways to do this. Anyone know of a solution? for m in reddit.get_unread(limit=1000): //process message.. m.mark_as_read() In my case order of messages is actually important. Will tip good answers!", "created_utc": 1386392194, "gilded": 0, "name": "t3_1saqs9", "num_comments": 4, "score": 1, "title": "I want to process messages from the inbox from oldest to newest. How?", "url": "https://www.reddit.com/r/redditdev/comments/1saqs9/i_want_to_process_messages_from_the_inbox_from/"}, {"author": "Plague_Bot", "body": "My bot just threw an internal server error and stopped running. What are some of the possible causes for this, and how can I avoid it? Or is there a way to catch it and continue the program? The full error that python threw: Traceback (most recent call last): File \"C:\\Python27\\programs\\redditPlague.py\", line 95, in for comment in comments: File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 440, in get_content page_data = self.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 158, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 476, in request_json response = self._request(url, params, data) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 349, in _request _raise_response_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 173, in _raise_response_exceptions response.raise_for_status() File \"C:\\Python27\\lib\\site-packages\\requests\\models.py\", line 725, in raise_for_status raise HTTPError(http_error_msg, response=self) HTTPError: 500 Server Error: Internal Server Error", "created_utc": 1385909626, "gilded": 0, "name": "t3_1ru8jl", "num_comments": 6, "score": 1, "title": "HTTPError: 500 Server Error: Internal Server Error \u2013 Possible causes and solutions? [PRAW]", "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "echblog", "body": "I'm trying to parse the 'all' comments feed, however I seem to be running up against either the cache or the 2 second limit, although from what I can tell I shouldn't be. Relevant code: r = praw.Reddit(user_agent='testing_praw/0.1 by echblog') r.login() submissions_list = [] while True: comments = r.get_comments('all', limit=None) newSubmission = stuff #I've stripped out the code relevant to this for comment in comments: if newSubmission not in submissions_list: submissions_list.append(newSubmission) time.sleep(60) What's happening is the first time I call r.get_comments, it will work as expected and retrieve as many as it can. After waiting for the sleep period, it tries again, but this time it retrieves the same comments one at a time, with a two second interval between each one. If the cache has a 30 second timeout, having a sleep value of 60 should be more than enough time for it to clear, shouldn't it? I've tried for different values of both *limit* and *time.sleep*, but to no avail. What I don't get if it *is* the cache, why is it only returning one comment at a time?", "created_utc": 1385827045, "gilded": 0, "name": "t3_1rs00u", "num_comments": 2, "score": 4, "title": "[PRAW] Retrieving 'all' comments works once, then returns 1 every 2 seconds.", "url": "https://www.reddit.com/r/redditdev/comments/1rs00u/praw_retrieving_all_comments_works_once_then/"}, {"author": "SeaCowVengeance", "body": "Hi, I'm making a conversion bot that'll be specific to a few select subreddits. It'll search for comments that match criteria and reply to them. Obviously you won't want to a bot to reply to the same thing more than once in this situation. My first thought to solving this was to go though the replies for the match to see if my bot has already reached it but that's slow and takes to many API calls. My second thought was to make a hash table containing the IDs of the comments that have been replied to, but I would have to stores them in an external file when the bot goes down and reload them. Anyone have any efficient tricks for doing this? I'm using python/praw if that matters.", "created_utc": 1385670181, "gilded": 0, "name": "t3_1rnznl", "num_comments": 5, "score": 11, "title": "Best way for a bot to keep track of the replies it's already made.", "url": "https://www.reddit.com/r/redditdev/comments/1rnznl/best_way_for_a_bot_to_keep_track_of_the_replies/"}, {"author": "deten", "body": "I am really new to PRAW and I just cant find simple documentation on how to do the things I am trying... so maybe I can learn a bit from here. I wanted to get the newest X (10 for now) submissions and store their URL and the name of the submitter to an object or a file. I am not very good with python, but I figure it is easiest to make an array of objects that includes this information. Any advice on where to start?", "created_utc": 1385584629, "gilded": 0, "name": "t3_1rlhzs", "num_comments": 3, "score": 1, "title": "[PRAW] Retrieving the newest 10 submissions in a sub and storing their link and the username of the poster", "url": "https://www.reddit.com/r/redditdev/comments/1rlhzs/praw_retrieving_the_newest_10_submissions_in_a/"}, {"author": "WaitForItTheMongols", "body": "I'm trying to install PRAW, but PRAW requires PIP, which itself requires SetupTools, and there's just too much happening here, and I have honestly no idea what I'm doing. I've never installed external additions, so I'm unfamiliar with all this. I'm running Windows 7, if that's relevant. The main issue is that all the tutorials say \"Do this\" followed by some form of code. I don't know where to run any of that. Command prompt? In python? If someone could walk me through the steps, that'd be awesome.", "created_utc": 1385569601, "gilded": 0, "name": "t3_1rkv8u", "num_comments": 6, "score": 2, "title": "Yet another PRAW installation issue...", "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": "swollen_pickle", "body": "I have a long string which is close to, but less than 40,000 characters, which I believe is the limit for a self post in a subreddit which only allows self posts. However when I try and submit this post to a self post only subreddit using PRAW I am getting `praw.errors.APIException: (TOO_LONG) 'this is too long (max: 15000)' on field 'text'`. When I output the string `text` to a text file and then copy and paste it and post it manually it works. Does anyone know why this might be happening?", "created_utc": 1385562287, "gilded": 0, "name": "t3_1rkmao", "num_comments": 2, "score": 5, "title": "[PRAW] I'm getting APIException TOO_LONG when trying to submit a long self post using PRAW, however when I try and post the same string manually by copy and pasting it, it works.", "url": "https://www.reddit.com/r/redditdev/comments/1rkmao/praw_im_getting_apiexception_too_long_when_trying/"}, {"author": "unixfan", "body": "Hey guys! I'm working on a small experiment and I'm using Praw. When I try to get the subscribed subreddits for the logged user it throws me an ArgumentError, even though the method exists and it's on the [docs](https://praw.readthedocs.org/en/PRAW-1.0.9/praw.html#praw.objects.LoggedInRedditor.my_reddits). This is the code: >>> import praw >>> r = praw.Reddit('ffdf') >>> r.user >>> r.login('unixfan', 'mypassword') >>> r.user.name 'unixfan' >>> r.user.my_reddits() Traceback (most recent call last): File \"\", line 1, in File \"/usr/local/lib/python2.7/site-packages/praw/objects.py\", line 81, in __getattr__ attr)) AttributeError: '' has no attribute 'my_reddits' Also, all other `LoggedInRedditor` methods throw the same exception, so IDK what's going on.", "created_utc": 1385303498, "gilded": 0, "name": "t3_1rcl4u", "num_comments": 1, "score": 1, "title": "[PRAW] Praw throwing an AttributeError when trying to call 'my_reddits()'", "url": "https://www.reddit.com/r/redditdev/comments/1rcl4u/praw_praw_throwing_an_attributeerror_when_trying/"}, {"author": "atomicUpdate", "body": "Here is the relevant snippet of code I'm using: info = rGetUserData.refresh_access_information(refresh_token=dbUserRow[\"refresh_token\"], update_session=True) user = rGetUserData.get_me() name = '%s' % (user.name) linkKarma = '%u' % (user.link_karma) commentKarma = '%u' % (user.comment_karma) orangered = '%s' % (False) numMessages = 0 numComments = 0 numModMail = 0 for message in rGetUserData.get_mod_mail(): orangered = '%s' % (True) numModMail += 1 for message in rGetUserData.get_unread(): orangered = '%s' % (True) if message.was_comment == True: numComments += 1 else: numMessages += 1 It's able to perform the get_mod_mail() call successfully, however, it dies at get_unread() and returns the 403 error. If I comment out the get_unread() block, everything completes successfully. I've tried adding in a delay between get_mod_mail() and get_unread() of 3 seconds for the rate limiting (which I believe Praw handles automatically anyway), but that didn't help either. Doing the get_unread() before the get_mod_mail() doesn't help either. Here is the full traceback for the error: 192.168.1.1 - - [22/Nov/2013 18:21:20] \"GET /get_user_data/atomicUpdate HTTP/1.1\" 500 - Traceback (most recent call last): File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1836, in __call__ return self.wsgi_app(environ, start_response) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1820, in wsgi_app response = self.make_response(self.handle_exception(e)) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1403, in handle_exception reraise(exc_type, exc_value, tb) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1817, in wsgi_app response = self.full_dispatch_request() File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1477, in full_dispatch_request rv = self.handle_user_exception(e) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1381, in handle_user_exception reraise(exc_type, exc_value, tb) File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1475, in full_dispatch_request rv = self.dispatch_request() File \"/usr/lib/python2.7/site-packages/flask/app.py\", line 1461, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File \"/home/tjvanpat/Programs/redditIWidget/redditIWidget_Server.py\", line 92, in getUserData for message in rGetUserData.get_unread(): File \"/usr/lib/python2.7/site-packages/praw/__init__.py\", line 440, in get_content page_data = self.request_json(url, params=params) File \"/usr/lib/python2.7/site-packages/praw/decorators.py\", line 158, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/lib/python2.7/site-packages/praw/__init__.py\", line 476, in request_json response = self._request(url, params, data) File \"/usr/lib/python2.7/site-packages/praw/__init__.py\", line 349, in _request _raise_response_exceptions(response) File \"/usr/lib/python2.7/site-packages/praw/internal.py\", line 173, in _raise_response_exceptions response.raise_for_status() File \"/usr/lib/python2.7/site-packages/requests/models.py\", line 725, in raise_for_status raise HTTPError(http_error_msg, response=self) HTTPError: 403 Client Error: Forbidden So...any ideas why get_mod_mail() completes successfully, while get_unread() returns the 403 when they both require the same privatemessages OAuth authentication (assuming that's what the 403 is trying to tell me)?", "created_utc": 1385170265, "gilded": 0, "name": "t3_1r9ci4", "num_comments": 2, "score": 2, "title": "get_unread() Giving HTTPError: 403 Client Error: Forbidden", "url": "https://www.reddit.com/r/redditdev/comments/1r9ci4/get_unread_giving_httperror_403_client_error/"}, {"author": "NotSinceYesterday", "body": "Hi all Trying to get a bot to post a thread and sticky it. However I keep receiving the error: >AttributeError: '' has no attribute 'sticky' But it is [listed in the documentation](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.sticky). Relevant part of code: >submission = r.submit('pkmntrades4mods', title, text=body) >submission.distinguish() >submission.add_comment(\"Holy shit, did this work?\") >submission.sticky()", "created_utc": 1384775614, "gilded": 0, "name": "t3_1qw2u3", "num_comments": 8, "score": 2, "title": "Sticky posts", "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "DAsSNipez", "body": "Is it possible to do this? So instead of: reddit_object.get_subreddit('redditdev') I would have something like: reddit_object.get_userpage('DAsSNipez') I've looked at the documentation and the code overview but I find for the code overview at least it's incredibly hard to process and as I don't know if what I want to do is supported I have idea how to search for it. Edit: I've tried using: reddit_object.get_redditor(username) What I get back is this: {'_authentication': None, '_random_count': 0, '_use_oauth': False, 'access_token': None, 'client_id': None, 'client_secret': None, 'config': , 'handler': , 'http': , 'modhash': None, 'redirect_uri': None, 'refresh_token': None, 'update_checked': True, 'user': None} I don't *think* that's what I want, if it is then I have no idea how to get information from it.", "created_utc": 1384770349, "gilded": 0, "name": "t3_1qvzkd", "num_comments": 3, "score": 2, "title": "Praw, opening a user page.", "url": "https://www.reddit.com/r/redditdev/comments/1qvzkd/praw_opening_a_user_page/"}, {"author": "NEBRASSKICKER", "body": "I'm trying to make a bot for automatically posting game threads for my sports subreddit. I have praw installed and i'm trying to go off of [this guide] (https://praw.readthedocs.org/en/latest/) (but it's kinda confusing). Can anyone guide me through what code I need to write to make this happen? I've already figured out how to login with to the bot account, but I can't figure how to post, and I need to figure out how to have this code execute so it can post threads at certain times.. Any help?? Thank You for you time.", "created_utc": 1384328378, "gilded": 0, "name": "t3_1qiu59", "num_comments": 13, "score": 2, "title": "Help with making auto posting bot.", "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "davidystephenson", "body": "I am attempting to use the `place_holder` argument to `get_new` to return only items after the latest post previously found. However, it seems to still return items created before the specified post. The full source of my script: import os import praw r = praw.Reddit('test 1.0 by /u/davidystephenson') terms = ['ukraine'] if os.path.exists('last.txt'): with open('last.txt', 'r') as file: last = file.read() last_created = r.get_submission(submission_id=last).created print('last test', last, last_created) else: print('Warning: no last.txt file') last = None subreddits = ['geopolitics'] submissions = [] for subreddit in subreddits: submissions += list( r.get_subreddit(subreddit).get_new(limit=None, place_holder=last) ) print('length test', len(submissions)) for submission in submissions: text = submission.selftext.lower() title = submission.title.lower() matches = [term for term in terms if term in text or term in title] print( 'after test', submission.created, last_created, submission.created > last_created ) if matches: print('matches test', submission.short_link, matches) else: pass # no matches if submission.created > last_created: with open('last.txt', 'w') as file: file.write(submission.id) However, even after running this script several times, items older than the \"last_created\" value continue to be returned, confirmed by individual testing as shown in line 34. For example, running the above code, even several times at intervals, repeatedly returns: last test 1q0zkw 1383778127.0 length test 20 after test 1383743049.0 1383778127.0 False after test 1383738593.0 1383778127.0 False after test 1383720420.0 1383778127.0 False matches test http://redd.it/1pzesl ['ukraine'] after test 1383672394.0 1383778127.0 False after test 1383657110.0 1383778127.0 False after test 1383596146.0 1383778127.0 False after test 1383572155.0 1383778127.0 False after test 1383561922.0 1383778127.0 False after test 1383546313.0 1383778127.0 False after test 1383540940.0 1383778127.0 False after test 1383522016.0 1383778127.0 False after test 1383487187.0 1383778127.0 False after test 1383309728.0 1383778127.0 False after test 1383294259.0 1383778127.0 False after test 1383291938.0 1383778127.0 False after test 1383266995.0 1383778127.0 False after test 1383240858.0 1383778127.0 False after test 1383230463.0 1383778127.0 False after test 1383155985.0 1383778127.0 False after test 1383099494.0 1383778127.0 False Thoughts?", "created_utc": 1383752562, "gilded": 0, "name": "t3_1q13hd", "num_comments": 6, "score": 2, "title": "PRAW: Using place_holder still returns posts older than the place_holder", "url": "https://www.reddit.com/r/redditdev/comments/1q13hd/praw_using_place_holder_still_returns_posts_older/"}, {"author": "rog1121", "body": "Ive got a script that iterates trhough comments and flairs people. There are several conditions it includes and right now some of the actions in the script are making more api calls than it should be. On line 25 it makes an API requst to get the comments which it uses in line 27: flat_comments = praw.helpers.flatten_tree(submission.comments) And then on line 33, 70 and 77 it references to line 25 again so for each comment it's making 3 api requests. With posts that have 300+ comments this gets very inefficient and slow. Is there any way to store the api results as a string and use that as a reference? Also if you have any ideas on how to cleanup the code that would be great as well Here is the code: import sys, os from ConfigParser import SafeConfigParser import logging import praw # load config file containing_dir = os.path.abspath(os.path.dirname(sys.argv[0])) cfg_file = SafeConfigParser() path_to_cfg = os.path.join(containing_dir, 'config.cfg') cfg_file.read(path_to_cfg) #configure logging logging.basicConfig(level=getattr(logging, cfg_file.get('logging', 'level'))) reddit_username = cfg_file.get('reddit', 'username') logging.info('Logging in as /u/'+reddit_username) r = praw.Reddit(user_agent=cfg_file.get('reddit', 'user_agent')) r.login(cfg_file.get('reddit', 'username'), cfg_file.get('reddit', 'password')) subreddit = cfg_file.get('reddit', 'subreddit') submission = r.get_submission(submission_id=cfg_file.get('reddit', 'link_id')) with open (\"id.txt\", \"r\") as myfile: completed=myfile.read() flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: content = comment.body if 'confirm' in content.lower() and comment.is_root == False and comment.id not in completed: parent = [com for com in flat_comments if com.fullname == comment.parent_id][0] if comment.author_flair_css_class: child_css = str(int(comment.author_flair_css_class) + 1) if comment.author_flair_text: child_text = comment.author_flair_text if parent.author_flair_css_class: parent_css = str(int(parent.author_flair_css_class) + 1) if parent.author_flair_text: parent_text = parent.author_flair_text if not comment.author_flair_css_class: child_css = '1' if not comment.author_flair_text: child_text = '' if not parent.author_flair_css_class: parent_css = '1' if not parent.author_flair_text: parent_text = '' # Prevent users from confirming under their own comments if comment.author == parent.author: comment.reply('You have confirmed a trade under your own post, this action has been reported to the Moderators') comment.report() parent.report() # Karma check verification elif comment.author.link_karma + comment.author.comment_karma", "created_utc": 1383665671, "gilded": 0, "name": "t3_1pyczr", "num_comments": 2, "score": 2, "title": "Store API call and code cleanup help", "url": "https://www.reddit.com/r/redditdev/comments/1pyczr/store_api_call_and_code_cleanup_help/"}, {"author": "The_Gingey", "body": "I am trying to learn how to use the API, but am running into a problem. When I run the tutorial code through the terminal using python it works. But when I run it in a python script, it fails, saying: AttributeError: 'module' object has no attribute 'Reddit' Any advice? This is the code I am running. import praw r = praw.Reddit(user_agent='my_cool_application') submissions = r.get_subreddit('opensource').get_hot(limit=5) [str(x) for x in submissions] Edit: I am sorry if this is a trivial question. I have used multiple python libraries before but never had a problem like this. I am also running on Mac 10.8, with python 2.7", "created_utc": 1383597799, "gilded": 0, "name": "t3_1pwda4", "num_comments": 6, "score": 2, "title": "Problem using reddit API", "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "MURICAN_BOT", "body": "I am aware that you can do this with PRAW import praw r = praw.Reddit('placeholder example') subreddit = r.get_subreddit('redditdev') for submission in subreddit.get_new(limit=100, place_holder='1bu7ak'): print submission.title However, I would like to get submissions *older* than the placeholder. This is used for when I'm scanning a LARGE amount of comments, and I get an error for too many requests. I would like to stop and wait, and then start again from the last comment (getting older and older comments) How can I do this?", "created_utc": 1382838447, "gilded": 0, "name": "t3_1pag1x", "num_comments": 1, "score": 2, "title": "Get comments older than placeholder", "url": "https://www.reddit.com/r/redditdev/comments/1pag1x/get_comments_older_than_placeholder/"}, {"author": "kirbz1692", "body": "I'm trying to learn how to use praw, and when I run [this program](http://pastebin.com/VBq5yAej) it throws an keyerror because when it looks for `self.sleep_time = self.response['ratelimit']` it can't find it. Anyone know whats up? I'm a beginner with this so its possible I'm doing something extremely wrong. **EDIT:** here's the traceback: Traceback (most recent call last): File \"C:/Users/ME/PycharmProjects/RedditBot/tutorial\", line 9, in r.login(\"MYUSERNAME\",\"MYPASSWORD\"); File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\__init__.py\", line 1157, in login self.request_json(self.config['login'], data=data) File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\decorators.py\", line 172, in wrapped return_value)) File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\errors.py\", line 325, in __init__ self.sleep_time = self.response['ratelimit'] KeyError: 'ratelimit' KeyError: 'ratelimit'", "created_utc": 1382837335, "gilded": 0, "name": "t3_1paezw", "num_comments": 4, "score": 1, "title": "Help with a keyerror using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1paezw/help_with_a_keyerror_using_praw/"}, {"author": "Zarqu0n", "body": "I am using praw and i need the subscriber count of some subreddits,", "created_utc": 1382737710, "gilded": 0, "name": "t3_1p7zhy", "num_comments": 6, "score": 1, "title": "How do you find out the subscriber count of a specific subreddit?", "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "ascetica", "body": "I'm creating a bot that responds upon request with a random quote from Jurassic Park. I'm trying to have my bot reply with a multi-line comment. I am able to do it for a single comment [using this method](http://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/car4p6j). However I'd like to have my bot select a reply from a large number of possible replies, and I want to store these in a python list or an XML file. I tried the following list: jurassic_park_quote_list = ['''Dr. Alan Grant: T-Rex doesn't want to be fed. He wants to hunt. Can't just suppress 65 million years of gut instinct.''', '''Dr. Alan Grant: [finding egg shells] Oh my God. Do you know what this is? This is a dinosaur egg. The dinosaurs are breeding. Tim: But Grandpa said all the dinosaurs were girls. Dr. Alan Grant: Amphibian DNA. Lex: What's that? Dr. Alan Grant: Well, on the tour, the film said they used frog DNA to fill in the gene sequence gaps. They mutated the dinosaur genetic code and blended it with that of a frog's. Now, some West African frogs have been known to spontaneously change sex from male to female in a single sex environment. Malcolm was right. Look... [we see a trail of baby dinosaur footprints Dr. Alan Grant: Life found a way.''', '''Dr. Ian Malcolm: [while being chased by the T-Rex] Must go faster.'''] There are three entries in the list. However, when the bot replies with the second multi-line comment it ignores the formatting. I also haven't had any luck importing from an XML file and using \\n\\n for a line break. Any ideas?", "created_utc": 1382665632, "gilded": 0, "name": "t3_1p5z3c", "num_comments": 2, "score": 1, "title": "[PRAW] Creating multi-line comment from list of strings or XML file", "url": "https://www.reddit.com/r/redditdev/comments/1p5z3c/praw_creating_multiline_comment_from_list_of/"}, {"author": "hinayu", "body": "I'm doing some comment scraping for a certain redditor. For each comment that I get, I'd like to construct the URL that points back to the comment. Here's an example of what I'm doing currently. r = praw.Reddit(user_agent) username = r.get_redditor('username') comments = username.get_comments(limit=1) for comment in comments: pprint(vars(comment)) So I'm looking at what I can do with a comment object, and I see id, link_id, name, parent_id, and subreddit_id. I'm not sure if I can use one of those to potentially construct a link back to the comment or not, but I wasn't able to figure anything out. Thanks for the help in advance!", "created_utc": 1382588495, "gilded": 0, "name": "t3_1p3qau", "num_comments": 13, "score": 3, "title": "[PRAW] Constructing url from a comment", "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "rog1121", "body": "I have this script setup to track trades on a subreddit and incremental add to a flair every time. The problem is that if there are multiple child comments from the same user it counts all 3 comments as one. http://i.imgur.com/wqKNFWk.png Here are the flairs after I run the script http://i.imgur.com/1XLHeqK.png As you can see the Parent post is flaired correctly but the 3 child posts from the same user are all counted as one. import sys, os import praw from ConfigParser import SafeConfigParser # load config file containing_dir = os.path.abspath(os.path.dirname(sys.argv[0])) cfg_file = SafeConfigParser() path_to_cfg = os.path.join(containing_dir, 'config.cfg') cfg_file.read(path_to_cfg) r = praw.Reddit(user_agent=cfg_file.get('reddit', 'user_agent')) r.login(cfg_file.get('reddit', 'username'), cfg_file.get('reddit', 'password')) subreddit = cfg_file.get('reddit', 'subreddit') submission = r.get_submission(submission_id=cfg_file.get('reddit', 'link_id')) flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: if 'confirm' in comment.body and comment.is_root == False: parent = [com for com in flat_comments if com.fullname == comment.parent_id][0] if comment.author_flair_css_class or comment.author_flair_text: child_css = str(int(comment.author_flair_css_class) + 1) child_text = comment.author_flair_text if not comment.author_flair_css_class: child_css = '1' if not comment.author_flair_text: child_text = '' if parent.author_flair_css_class or parent.author_flair_text: parent_css = str(int(parent.author_flair_css_class) + 1) parent_text = parent.author_flair_text if not parent.author_flair_css_class: parent_css = '1' if not parent.author_flair_text: parent_text = '' comment.subreddit.set_flair(comment.author, child_text, child_css) comment.author_flair_css_class = child_css print 'Changed Child CSS' parent.subreddit.set_flair(parent.author, parent_text, parent_css) parent.author_flair_css_class = parent_css print 'Changed Parent CSS'", "created_utc": 1382497673, "gilded": 0, "name": "t3_1p0xc9", "num_comments": 4, "score": 4, "title": "Script counting multiple comments as one", "url": "https://www.reddit.com/r/redditdev/comments/1p0xc9/script_counting_multiple_comments_as_one/"}, {"author": "nareik15", "body": "Further details [here](http://www.reddit.com/r/redditdev/comments/1oi5fb/a_number_of_questions_about_the_replace_more/) I am trying to download, using PRAW, the comments from submissions which have many comments (2000+) and this is proving to be very time consuming when fetching the default 200 comments or even 500 (when changing this in preferences) and then using the \"replace_more_comments\" method. So, seeing that you can fetch 1500 comments with reddit gold, I went ahead and bought this but now can't figure out how to make my default number of comments fetched equal 1500. If anyone has any idea how to do this, that would be great. P.S I have already tried to do the following but get a JSON error (if anyone knows how to fix this that would also be incredibly useful): r.get_submission(\"url_of_a_submission?limit=1500\")", "created_utc": 1382321738, "gilded": 0, "name": "t3_1ovi6a", "num_comments": 4, "score": 3, "title": "Just bought Reddit Gold. How do I change default No of comments fetched to 1500.", "url": "https://www.reddit.com/r/redditdev/comments/1ovi6a/just_bought_reddit_gold_how_do_i_change_default/"}, {"author": "rog1121", "body": "I'm making a bot to handle the reputation system over at /r/hardwareswap We basically have one top level comment and then a user replies under it to verify the trade. This is what I have so far, obviously the bottom part of the code doesn't work import praw r = praw.Reddit('rog1121') r.login('rog1121', '') subreddit = 'hardwareswap' submission = r.get_submission(submission_id='1od22u') flat_comments = praw.helpers.flatten_tree(submission.comments) for comment in flat_comments: if comment.body == 'second' and comment.is_root == False: new_flair = str(int(comment.author_flair_css_class) + 1) # Update the child flair if not comment.author_flair_text: comment.subreddit.set_flair(comment.author, '', new_flair) elif not comment.author_flair_css_class: comment.subreddit.set_flair(comment.author, comment.author_flair_text, '1') elif not comment.author_flair_text and not comment.author_flair_css_class: comment.subreddit.set_flair(comment.author, '', '1') else: comment.subreddit.set_flair(comment.author, comment.author_flair_text, new_flair) # Update the parent flair if not comment.parent_id.author_flair_text: comment.parent_id.subreddit.set_flair(comment.author, '', new_flair) elif not comment.parent_id.author_flair_css_class: comment.parent_id.subreddit.set_flair(comment.author, comment.author_flair_text, '1') elif not comment.parent_id.author_flair_text and not comment.author_flair_css_class: comment.parent_id.subreddit.set_flair(comment.author, '', '1') else: comment.parent_id.subreddit.set_flair(comment.author, comment.author_flair_text, new_flair)", "created_utc": 1382205019, "gilded": 0, "name": "t3_1osc2c", "num_comments": 4, "score": 1, "title": "Parent_id of comment actions?", "url": "https://www.reddit.com/r/redditdev/comments/1osc2c/parent_id_of_comment_actions/"}, {"author": "Admsugar", "body": "I meant importing not installing. woops. Im trying to get praw installed so I can start messing around with reddits API and learn some new things, I have succesfully installed pip I think and in terminal (mac) I entered 'pip install praw' I got this result in terminal: ------ Downloading/unpacking praw Downloading praw-2.1.10.tar.gz (83kB): 83kB downloaded Running setup.py egg_info for package praw Downloading/unpacking requests>=1.2.0 (from praw) Downloading requests-2.0.0.tar.gz (362kB): 362kB downloaded Running setup.py egg_info for package requests Downloading/unpacking six (from praw) Downloading six-1.4.1.tar.gz Running setup.py egg_info for package six Downloading/unpacking update-checker>=0.6 (from praw) Downloading update_checker-0.6.tar.gz Running setup.py egg_info for package update-checker Requirement already satisfied (use --upgrade to upgrade): setuptools in /Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/setuptools-1.1.6-py2.7.egg (from update-checker>=0.6->praw) Installing collected packages: praw, requests, six, update-checker Running setup.py install for praw Installing praw-multiprocess script to /Library/Frameworks/Python.framework/Versions/2.7/bin Running setup.py install for requests Running setup.py install for six Running setup.py install for update-checker Successfully installed praw requests six update-checker Cleaning up... ---- I figured it had worked but then when I try 'import praw' in python I'm getting an ImportError: No module named praw. I know this is probably a simple fix, just wanted to get this cleared up. Thanks", "created_utc": 1381978256, "gilded": 0, "name": "t3_1omaox", "num_comments": 1, "score": 1, "title": "Installing praw into python", "url": "https://www.reddit.com/r/redditdev/comments/1omaox/installing_praw_into_python/"}, {"author": "nareik15", "body": "So I've been extracting comments from a number of submissions recently for a project i'm doing which focusses on conversations and engagement. For those comment trees that have more than 200 and therefore are littered with \"more_comments\", I have been using the *replace_more_comments* function in praw, which as I understand it, makes a new API call for each of these it comes across . So i had a few questions: 1. Is there anyway to group together fetched comments to reduce the number of api calls required? i.e. at the moment if there are 30 \"more_comments\" markers, that means there are 30 api calls and thus a minute to wait for all comments. but if under each of these \"more_comments\", there is only 5 comments then that is a somewhat wasteful api call (given that each api call can retreive 100 comments) 2. Is there any way to mark the progress of the *replace_more_comments* functions? i.e. if there are 200 \"more_comments\" and the functions has retreived 100, being able to say that half have been retreived the purpose of this is really just to allow me to get some sense of how long I have left to wait for some of these things to load. :) Thanks in advance for any advice. :)", "created_utc": 1381848852, "gilded": 0, "name": "t3_1oi5fb", "num_comments": 2, "score": 3, "title": "A number of questions about the \"replace_more_comments\" function in PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1oi5fb/a_number_of_questions_about_the_replace_more/"}, {"author": "JBHUTT09", "body": "This is a bot that I built today in order to help moderate /r/AnimeSuggest. It checks new posts and pm the author if they do not tag their post with flair within 3 minutes. Here is my code: import praw import time import re import sys def main(): #log in username=open(\"Username.txt\",\"r\").read().rstrip() password=open(\"Password.txt\",\"r\").read().rstrip() user_agent=(\"Auto flair moderator for /r/AnimeSuggest\") r=praw.Reddit(user_agent=user_agent) r.login(username=username,password=password) JBHUTT09=\"JBHUTT09\" #initialize array to hold checked posts already_done=[] #specify subreddit subreddit=r.get_subreddit('animesuggesttesting') #start endless loop while True: for submission in subreddit.get_new_by_date(limit=5): if submission.id not in already_done: time.sleep(180) if (submission.link_flair_text is None): author=submission.author subLine='You have not tagged your post.' msg=\"[Your recent post](%s) in /r/AnimeSuggest does not have any flair. Please add flair to your post. \\n\\n If you are unsure of how to add flair, refer to [this post](http://redd.it/1ml8km).\\n\\n*This is a brand new bot I wrote to help mod. It's also the first bot I've written. If shit is broken and everything is going to hell, please pm me: /u/JBHUTT09.\" % submission.short_link r.send_message(author,subLine,msg) JBSubLine=\"Message sent to /u/%s\"%author JBmsg=\"Message sent concerning [this post.](%s)\"%submission.short_link r.send_message(JBHUTT09,JBSubLine,JBmsg) print \"Sent message to: %s\" % author already_done.append(submission.id) print \"Posts checked: %d\" % len(already_done) main() Now, this code was running absolutely fine this afternoon. No problems at all. I had to shut down my computer so I had to stop the bot. That's when the trouble started. Ever since I reran the bot, it's been crashing when sending PMs. But it only crashes sometimes and I think it's based on the time delay. I had set it to a 20 second delay for testing purposes when trying to figure out the problem, because I wasn't going to wait 3 minutes. It suddenly worked. So I thought it was a weird one time thing. But when I put the delay back to 3 minutes, it started crashing again. Does anyone have any idea why this happens? **Edit:** Crash message: Traceback (most recent call last): File \"C:\\Python27\\botTest.py\", line 34, in main() File \"C:\\Python27\\botTest.py\", line 27, in main r.send_message(author,subLine,msg) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 303, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 205, in wrapped return function(obj, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 1909, in send_message retry_on_error=False) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 141, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 479, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 348, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 321, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\adapters.py\", line 354, in send raise ConnectionError(e) ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /api/compose/.json (Caused by : [Errno 10054] An existing connection was forcibly closed by the remote host)", "created_utc": 1381632011, "gilded": 0, "name": "t3_1obxel", "num_comments": 17, "score": 7, "title": "Having issues with a time delay in a bot", "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "sixteenmiles", "body": "I don't really have any idea what I'm doing but I'm trying to get my head around python/praw with a simple (I hope) little project. The idea is to parse the entire history of a specific sub and return percentage statistics based on the prefix of each post, but I can't figure out the method for doing this correctly. get_new(limit=None) This only goes back so far (about 29 pages by my count), but ideally what I would like to do is start at the newest post and iterate backwards until I hit the beginning or as far back as I can go. Is this even possible? Any help appreciated, thanks.", "created_utc": 1381581077, "gilded": 0, "name": "t3_1oabiy", "num_comments": 6, "score": 1, "title": "[PRAW] Is it possible to get the content of an entire sub?", "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": "Lauren_of_Lore", "body": "I am trying to write a bot for my subreddit that will automatically assign flair to new posts, and allow mobile users to change the flair by posting a comment in their thread. I was wondering two things: * How do I set flair on a submission? * How do I get a submission's current flair? I read a list of things you can do with PRAW on the PRAW website but didn't see anything about thread flair listed, only user flair. I also tried searching this subreddit and didn't find anything :/ Thank you :)", "created_utc": 1381266898, "gilded": 0, "name": "t3_1o0hcg", "num_comments": 6, "score": 3, "title": "PRAW thread flair?", "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "Gambit89", "body": "Hello, I'm having a caching issue and was hoping you guys can shed some light as to why PRAW is behaving like this. I have an PRAW bot that polls /r/all/comments/ every 30 seconds, as recommended per the docs, but it takes every 4th request to get a new batch of comments (approx. 2 minutes) and thus I'm missing ~1.5 minutes of comments. Yet, if I navigate to it in my browser, I get the newest comment for that second. I estimate there are about 100 new comments every 15 secs, so that's good if /r/all/comments/ updates every fetch - which it doesn't with PRAW. To operate with the 2 minute cache, I'll have to fetch 2 mins/15 secs = ~8 times, or almost to the 1000 limit. I'd rather not do this because I don't quite know where the cache cutoff is exactly, and I might miss some comments in between requests. E.g. comments revealed by cache |------ ~2 mins ------| margin of error |------ ~2 mins -----| |++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++| actual comments i.e, there could be some comments in the margin of error which aren't being revealed. But I suppose a majority of comments are better than the 25% I'm getting with my current method. But maybe my method can be fixed... is there a way to get PRAW to fetch new comments for /r/all/comments/ every 30 seconds?", "created_utc": 1381220386, "gilded": 0, "name": "t3_1nyznn", "num_comments": 2, "score": 1, "title": "PRAW is caching comments for 2m when configured for 30s?", "url": "https://www.reddit.com/r/redditdev/comments/1nyznn/praw_is_caching_comments_for_2m_when_configured/"}, {"author": "rreyv", "body": "I've written a bot that is supposed to update just the sidebar description at regular intervals. However, the code is also updating the spam settings to go from [this](http://i.imgur.com/yFHEXlU.png) to [this](http://i.imgur.com/6HfvFW2.png). Initially, I felt that this was just cosmetic and not really changing anything in the backend of reddit, but then I viewed the moderation log and I noticed that the changes were being logged so something was probably changed. [Screenshot](http://i.imgur.com/4gWgbcU.png). This is code I ran to test this. All I want to do is add a line at the end of the description. Could you guys tell me if something is wrong with my code here? If not, is this some sort of a bug in PRAW? What are the spam settings being changed to? Also note that none of the other settings are changed that I am aware of. It's just the spam settings. import praw import HTMLParser if __name__==\"__main__\": r = praw.Reddit('/r/cricket sidebar updating and match thread creating bot by /u/rreyv. Version 1.0') #reddit stuff r.login() #sign in! subredditName='rreyv' settings=r.get_settings(subredditName) description_html=settings['description'] html_parser = HTMLParser.HTMLParser() description = html_parser.unescape(description_html) description=description + \"\\n\\nAdded Something\" settings=r.update_settings(r.get_subreddit(subredditName),description=description)", "created_utc": 1380776725, "gilded": 0, "name": "t3_1nmwta", "num_comments": 6, "score": 3, "title": "Potential bug with updating sidebar in PRAW, or am I doing something wrong?", "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "bVector", "body": "I'm looking to setup OAuth on my app using praw, but I'm a little confused as to what I need to persist the OAuth tokens across app restarts. After using the code I'm given an access key and a refresh token. praw documentation states that the access code is only valid for 60 minutes, but doesn't explicitly state that the refresh token is permanent. Is this what I should save? Do I need anything else to refresh access after a restart of the app?", "created_utc": 1380738765, "gilded": 0, "name": "t3_1nll3d", "num_comments": 4, "score": 3, "title": "Saving OAuth codes in db", "url": "https://www.reddit.com/r/redditdev/comments/1nll3d/saving_oauth_codes_in_db/"}, {"author": "im14", "body": "I need to scrape certain bot's comments to create a statistics page (I don't control the bot). I need to be able to parse every single comment of the bot. I know there's a 'limit' variable that can't go higher than 1,000. Through Reddit UI, I can continuously press \"Next\" to get all user's comments. Is there an equivalent in PRAW? Thanks.", "created_utc": 1380222181, "gilded": 0, "name": "t3_1n7164", "num_comments": 6, "score": 6, "title": "Can I get all comments of a user by calling something like user.get_comments().next() repeatedly?", "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "titusjan", "body": "A have two related questions about PRAW: 1. Is it possible to access the raw JSON response that is send by the reddit server so that I can archive it? 2. Is it possible to create praw objects, for instance a *praw.objects.Comment* object, without a reddit_session? When I look at the [praw Comment documentation](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Comment) it seems that a reddit_session is mandatory. However I'd like to instantiate a comment off-line, preferably using the JSON that i've archived earlier.", "created_utc": 1379754094, "gilded": 0, "name": "t3_1mtvu2", "num_comments": 1, "score": 6, "title": "PRAW - saving JSON response and using that to instantiate praw objects", "url": "https://www.reddit.com/r/redditdev/comments/1mtvu2/praw_saving_json_response_and_using_that_to/"}, {"author": "Delocaz", "body": "So, I made a bot a while ago that'd stat an account, and upload the result to a self post in its own subreddit. Now, that bot doesn't work, and I have no idea why. Here's the code: http://pastebin.com/ngK2AQgt The error is at this line: r = praw.Reddit(user_agent=\"RedStatsBot by u/Delocaz\") And the error is: Traceback (most recent call last): File \"E:\\Filer\\Kode\\Python\\RedStatsWeb\\RedStats_Bot.py\", line 21, in user = r.get_redditor(USER_TO_TEST) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 839, in get_redditor return objects.Redditor(self, user_name, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 617, in __init__ fetch, info_url) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 71, in __init__ self._populated = self._populate(json_dict, fetch) File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 125, in _populate json_dict = self._get_json_dict() File \"C:\\Python33\\lib\\site-packages\\praw\\objects.py\", line 117, in _get_json_dict as_objects=False) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 342, in _request response = handle_redirect() File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 315, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python33\\lib\\site-packages\\requests\\sessions.py\", line 438, in send r = adapter.send(request, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests\\adapters.py\", line 292, in send timeout=timeout File \"C:\\Python33\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 423, in urlopen conn = self._get_conn(timeout=pool_timeout) File \"C:\\Python33\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 238, in _get_conn return conn or self._new_conn() File \"C:\\Python33\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 205, in _new_conn strict=self.strict) File \"C:\\Python33\\lib\\http\\client.py\", line 737, in __init__ DeprecationWarning, 2) File \"C:\\Python33\\lib\\idlelib\\PyShell.py\", line 60, in idle_showwarning file.write(warnings.formatwarning(message, category, filename, AttributeError: 'NoneType' object has no attribute 'write' If I copy the code up to line 20, it apparently works, for some reason.", "created_utc": 1379751735, "gilded": 0, "name": "t3_1mtup9", "num_comments": 1, "score": 2, "title": "PRAW - Code errors at line 20 (r = praw.Reddit(blahblah))", "url": "https://www.reddit.com/r/redditdev/comments/1mtup9/praw_code_errors_at_line_20_r_prawredditblahblah/"}, {"author": "mgrieger", "body": "Hey everyone, I am the dev of /u/VerseBot (which runs on /r/Christianity and /r/TrueChristian), and I have come across a very strange issue lately. The bot was working perfectly fine for over two weeks, and then all of a sudden it started spamming, replying to the same comment multiple times. Here's what seems to happen when the problem arises: - Bot finds a comment that it needs to reply to - Bot replies to comment with appropriate response - PRAW throws a HTTPError exception (usually a 502 Bad Gateway error) from the PRAW reply calls within commenter.py - Comment id of comment bot replies to does not get stored in the PostgreSQL database, OR a Python set - Bot starts spamming on the comment I am really clueless on why this is happening as it still sometimes works fine, but most of the time just spams like crazy. Has something changed in the reddit API that requires a PRAW update? Does anybody else running a bot with PRAW and/or Heroku have this problem? I have tried several variations of try/except blocks on the PRAW reply calls, but nothing seems to work. [Here is my code on GitHub.](https://github.com/matthieugrieger/versebot) Hopefully somebody knows a solution to this annoying problem that has cropped up recently. Thanks!", "created_utc": 1378958367, "gilded": 0, "name": "t3_1m85d0", "num_comments": 34, "score": 14, "title": "Bot randomly started spamming with no changes to the code", "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "qviri", "body": "While working on an idea I had, I tried getting the contents of http://www.reddit.com/r/polandball/about/contributors/ programmatically. That particular subreddit's contributors page is public. For a contrary example, http://www.reddit.com/r/vancouver/about/contributors gives me a \"page not found\" error (I'm not a mod there). Using stock PRAW, I can't get get\\_subreddit('polandball').get\\_contributors() without being logged in as a moderator for that subreddit. If I modify PRAW source to have mod=False decorator on get\\_contributors() in \\_\\_init\\_\\_.py (ugly I know), it comes through just fine, even without r.login(): >>> import praw >>> r = praw.Reddit(user_agent='polandballscraper/0.01') >>> r.get_subreddit('polandball').get_contributors() If I try the changed PRAW with get\\_subreddit('vancouver').get\\_contributors(), I get a 404 from requests (\"requests.exceptions.HTTPError: 404 Client Error: Not Found\"). If I had mod permissions on /r/vancouver, would I be able to see http://www.reddit.com/r/vancouver/about/contributors and do get\\_subreddit('vancouver').get\\_contributors() after an r.login()? Is changing get\\_contributors() so that mod permissions are only necessary for subreddits that don't make the contributors list public feasible and desirable? It would cause a runtime error if you don't have permissions just as it does now if you don't have mod, I'm just not sure about the elegance about doing it in the function somehow rather than in the decorator.", "created_utc": 1377570958, "gilded": 0, "name": "t3_1l5tsa", "num_comments": 6, "score": 2, "title": "PRAW: why is Subreddit.get_contributors() mod-only?", "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "Eliminioa", "body": "So I'm just delving into PRAW, and I'm having trouble getting, well, everything to work. It seems that many of the basic functions refuse to take input. The most pressing one is the r.login() function. No matter how i pass it data, it returns a huge error report that ends with: File \"C:\\Python33\\lib\\idlelib\\PyShell.py\", line 60, in idle_showwarning file.write(warnings.formatwarning(message, category, filename, AttributeError: 'NoneType' object has no attribute 'write' Any idea what's going wrong?", "created_utc": 1377488954, "gilded": 0, "name": "t3_1l3hkp", "num_comments": 2, "score": 1, "title": "PRAW help: NoneType has no attribute write", "url": "https://www.reddit.com/r/redditdev/comments/1l3hkp/praw_help_nonetype_has_no_attribute_write/"}, {"author": "BJ2094", "body": "I use this code to get the comments from a page: comments = praw.helpers.flatten_tree(post.comments) c = str(comments[0]) but when i try to print c, it only displays the first few words and an ellipsis. How can I fix this?", "created_utc": 1377393857, "gilded": 0, "name": "t3_1l185p", "num_comments": 1, "score": 1, "title": "Printing submission comments doesn't print entire comment.", "url": "https://www.reddit.com/r/redditdev/comments/1l185p/printing_submission_comments_doesnt_print_entire/"}, {"author": "SauceProviderBot", "body": "When I try and run: comments = r.get_all_comments() I get: /Library/Python/2.7/site-packages/praw/__init__.py:639: DeprecationWarning: Please use `get_comments('all', ...)` instead DeprecationWarning) So instead i'm using : comments = r.get_comments('all',limit = 5000) But I feel this is inefficient. Ideas? This is my first reddit bot.", "created_utc": 1377305702, "gilded": 0, "name": "t3_1kz820", "num_comments": 4, "score": 2, "title": "Having an issue using get_all_comments in PRAW.", "url": "https://www.reddit.com/r/redditdev/comments/1kz820/having_an_issue_using_get_all_comments_in_praw/"}, {"author": "Amablue", "body": "This seems like it ought to be really basic. Lets say I have a comment, like [this one](http://www.reddit.com/r/funny/comments/1kx27i/i_started_chatting_with_a_local_police_officer/cbthe8d). In my script I run this: import praw reddit = praw.Reddit(\"Misbhaving robot :(\") comment = reddit.get_info(thing_id = \"t1_cbthe8d\") print type(comment), comment print comment.replies When I run it, this is my output: Have you gotten a reply yet? Or the bail money? [] That comment clearly has a bunch of replies, why am I getting nothing?", "created_utc": 1377235845, "gilded": 0, "name": "t3_1kxd1n", "num_comments": 1, "score": 1, "title": "How can I get the replies to a comment with praw?", "url": "https://www.reddit.com/r/redditdev/comments/1kxd1n/how_can_i_get_the_replies_to_a_comment_with_praw/"}, {"author": "Squidifier", "body": "Hey! Just wondering, is there a way to use PRAW to set the subreddit sticky, as is outlined in [THIS post.](http://redd.it/1jr429) Thanks!", "created_utc": 1377102385, "gilded": 0, "name": "t3_1kt95a", "num_comments": 3, "score": 2, "title": "Using PRAW to set Subreddit Sticky?", "url": "https://www.reddit.com/r/redditdev/comments/1kt95a/using_praw_to_set_subreddit_sticky/"}, {"author": "MrFanzyPanz", "body": "Hi, redditdev! I'm trying to use PRAW to build an H index score for a user based on their combined link/comment history. The code treats every comment/link submitted by the user as a publication, and every reply/root comment on each comment/link as a citation. The problem is that the code makes too many requests. I'm using get_ comments() and get_ submitted() to pull a user's history (I wrote the code before seeing get_ overview() -___- ). My code is pasted below. while True: h_index_list = list() try: for comment in r.get_redditor(account).get_comments(limit = None): h_index_list.append(len(comment.replies)) print(len(comment.replies)) except requests.exceptions.HTTPError: print('Server Error') continue except IndexError: h_index_list.append(0) try: for link in r.get_redditor(account).get_submitted(limit = None): h_index_list.append(len(link.comments)) print(len(link.comments)) except requests.exceptions.HTTPError: print('Server Error') continue except IndexError: h_index_list.append(0) break In both the 'for comment' and 'for link' loops, the 'comment.replies' and 'link.comments' calls are treated as requests. I have a different script which makes this call: for post in r.get_subreddit(subreddit).get_hot(limit=500): unique_id = post.name subred = post.subreddit.display_name if not post.author: user = '*' else: user = post.author.name createdtime = post.created_utc This code pulls items in groups of 100, despite the numerous 'post.___' calls it makes. I've updated praw to the newest version. Are .replies and .comments simply slower?", "created_utc": 1376978060, "gilded": 0, "name": "t3_1kpuzo", "num_comments": 6, "score": 6, "title": "My code is making too many requests.", "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "CMThF", "body": "Hello everyone! I hope you can help me. I'm trying to get a dataset of all submissions to reddit for research purposes at my University. I use python and praw for this task. My approach is that I get a generator object from genObj = r.get_subreddit('all').get_new(limit=None) and want to iterate it via for s in genObj: Works perfectly fine, for about 500k requests. Then I get a HTTPError: 504 Server Error: Gateway Time-out, which is raised when accessing the generator object. When catching the exception and further accessing the generator, two different things happen: Either the generator object died due to a killed session, or I can continue for another ~500k requests until again the generator is killed. I tried to time.sleep every few requests or when catching an exception, thinking too many requests might have been the problem - but this doesn't help. Since recreating the generator would result in restarting from the most recent post, and skipping to a certain submission id and continuing from there is not supported by praw, the closed session pretty much ends all efforts. The most basic version (without the exception catching) is [here](https://gist.github.com/CMThF/f5745c1971743c5f755c) to be found. It runs on an Ubuntu 12.04.2 server. Running it on a different machine and different internet connection did not change this behavior, so possible parallel working Reddit API or praw scripts should not be the problem. My advisor used the same script for crawling some time ago, where it worked perfectly fine. Thank you for your time, regards, C.", "created_utc": 1376923121, "gilded": 0, "name": "t3_1ko256", "num_comments": 6, "score": 1, "title": "Using PRAW, I get a Gateway Timeout when crawling Reddit submissions", "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "MrFanzyPanz", "body": "I wanted to rebuild a comment tree to make some pretty graphs. I've been using comment.replies to pull child posts, and most of these can requests can be funneled through one call. However, while the code works, after 5 levels down the comment tree it stops pulling comments unless a \"more_comments\" call is made. This call has greatly increased the number of queries my code makes, and following PRAW's 2 second delay, larger comment trees take forever to pull. Is there a more efficient way to go through the whole comment tree in PRAW? Thanks, /r/redditdev!", "created_utc": 1376609016, "gilded": 0, "name": "t3_1kgauh", "num_comments": 3, "score": 5, "title": "In PRAW, is there a way to request an entire comment tree without using more_comments?", "url": "https://www.reddit.com/r/redditdev/comments/1kgauh/in_praw_is_there_a_way_to_request_an_entire/"}, {"author": "cosileone", "body": "I've had a look through the praw docs but trying to post things (comments/image links) requires the user to manually enter a captcha. Is there an automated way to post images or is there some requirement I'm missing?", "created_utc": 1376599278, "gilded": 0, "name": "t3_1kfy6s", "num_comments": 5, "score": 1, "title": "How do you post images using praw?", "url": "https://www.reddit.com/r/redditdev/comments/1kfy6s/how_do_you_post_images_using_praw/"}, {"author": "pipsqueaker117", "body": "I wrote a simple bot for reddit which needs to run every ten minutes or so to adequately perform its job. As I didnt want to leave my computer turned on forever in order to let the script I decided to try hosting it on google app engine. Now I'm a newb to both app engine and praw, so bear with me here. When I uploaded my script to the app engine it threw me an error saying that it could not find/import any packages named \"praw.\" That's fine. To fix this, and all the later dependency errors that came up, I just ran the following commands in a python shell import dependencypackage print(dependencypackage) I then copied all the files/folders that the shell spit out into my app's src directory. By doing this a few times I managed to trade one error for another, but now I'm running into the following error: Traceback (most recent call last): File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/SpelunkyDailyBot.py\", line 2, in import praw File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369458891331523467/praw/__init__.py\", line 34, in from praw import decorators, errors ImportError: cannot import name decorators I'm stumped by this one, because when I check the praw folder in my app's src dir there exist both a decorators.py and a decorators.pyc. So the question here is twofold- 1) What am I doing wrong here? Is the decorators module not supported by App Engine or something? Can I even use praw in a GAE script? I'd really love to get this bot on the cloud, so this question is the main one 2) Is there any easier way to do this? I feel like the process I've followed to get all the dependencies in the src is one of the least efficient possible. Is there some sort of tool I can use, some sort of magic button I can use in my IDE (pycharm or pydev for eclipse) which will take care of uploading all these dependencies for me? Answers and explanations are welcome (duh) Looking forward to your answers! EDIT: I made some changes to how the packages were imported, and now I'm getting the following error Traceback (most recent call last): File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/SpelunkyDailyBot.py\", line 2, in import praw File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/praw/__init__.py\", line 43, in from update_checker import update_check File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\", line 87, in class UpdateChecker(object): File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\", line 93, in UpdateChecker @cache_results File \"/base/data/home/apps/s~reddit-spelunky-bot/1.369461431267618641/update_checker.py\", line 43, in cache_results filename = os.path.join(gettempdir(), 'update_checker_cache.pkl') File \"/base/data/home/runtimes/python27/python27_dist/lib/python2.7/tempfile.py\", line 45, in PlaceHolder raise NotImplementedError(\"Only tempfile.TemporaryFile is available for use\") NotImplementedError: Only tempfile.TemporaryFile is available for use", "created_utc": 1376349543, "gilded": 0, "name": "t3_1k8ocm", "num_comments": 8, "score": 6, "title": "Is there any way to easily host a reddit bot using praw on Google App Engine?", "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "TheNoodlyOne", "body": "I'm working on a basic bot which I call the \"Six Degrees of Karmanut.\" Basically, it loads however many comments of the first user, gets the authors of any replies, and repeats the process for each of them. Even if I'm only loading five comments, it takes at least twelve seconds for each redditor, because you have to make a separate request for the children. Is there a way to include these children in the request for comments? Code: import praw USERNAME_1 = \"TheNoodlyOne\" USERNAME_2 = \"Karmanut\" MAX_DEPTH = 5 MAX_COMMENTS = 5 r = praw.Reddit(user_agent=\"six degrees of Karmanut\") queue = [(USERNAME_1,0)] processed = [] while True: if len(queue) == 0: break u = queue.pop(0) if u[1] > MAX_DEPTH: break done = False for p in processed: if u[0] == p[0]: done = True print \" \"+u[0]+\" (\"+str(u[1])+\")\" if done == True: continue processed.append(u) red = r.get_redditor(u[0]) for c in red.get_comments(limit=MAX_COMMENTS): rep = c.replies for c2 in rep: queue.append((str(c2.author), u[1]+1))", "created_utc": 1375779086, "gilded": 0, "name": "t3_1jsuht", "num_comments": 3, "score": 0, "title": "Get comment and replies in one request?", "url": "https://www.reddit.com/r/redditdev/comments/1jsuht/get_comment_and_replies_in_one_request/"}, {"author": "IAmAnAnonymousCoward", "body": "Didn't find anything about it in the reddit API or PRAW documentation. Is this something the admins object to?", "created_utc": 1375555569, "gilded": 0, "name": "t3_1jmzm7", "num_comments": 15, "score": 7, "title": "[PRAW] Can a bot give gold?", "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "isolani", "body": "Does `get_unread()` also get unread mod mail (it doesn't seem like it should, because it's a part of `praw.__init__.PrivateMessagesMixin(*args, **kwargs)`)? Is there a way to only get unread mod mail through `get_mod_mail()`? Edit: Thinking about it further, I think there are some apps out there that provide notifications for new mod mail, so surely this must be possible somehow (?)", "created_utc": 1375460604, "gilded": 0, "name": "t3_1jklbu", "num_comments": 1, "score": 3, "title": "[PRAW] Is there a function for getting unread mod mail?", "url": "https://www.reddit.com/r/redditdev/comments/1jklbu/praw_is_there_a_function_for_getting_unread_mod/"}, {"author": "droidBehavior", "body": "I want to go to a subreddit, get all the comments, and view the 'more comments' section. If the id is 't3_1jce6p', what do i put for children? it says put in the children id's delimited by a comma, but I am not supposed to actually put in children 1 by 1? there are hundreds of them. this post helped a bit but I am still lost, http://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/ I want to return json of hidden comments, so what is the correct way to do this? and what if there are more comments after clicking 'more comments' here is the non working code I have come up with data = { 'link_id' : 't3_1jce6p', 'children' : 'cbdcft5', #not sure what to put here, but i think its a list of the children id's 'api_type': 'json' } client = requests.session() r = client.post('http://www.reddit.com/api/morechildren', data=data) j = json.loads(r.content) print j['data']['children'] right now I have a working program to type in a user name and return all their posts, but I want to go a step further and give the post as well as the question it answers(not even sure how to do this yet)", "created_utc": 1375226850, "gilded": 0, "name": "t3_1jduep", "num_comments": 0, "score": 2, "title": "confused about morechildren", "url": "https://www.reddit.com/r/redditdev/comments/1jduep/confused_about_morechildren/"}, {"author": "naffarama", "body": "I'm pretty new so apologies if this is a stupid. When I try to get a list of friends from the logged in user I end up getting a redirect exception. Inputting: import praw r = praw.Reddit('blah blah blah') r.login('User','****') r.user.get_friends() gives: RedirectException: Unexpected redirect from http://www.reddit.com/prefs/friends/.json to https://ssl.reddit.com/prefs/friends/.json Any ideas what's going on? Any help greatly appreciated.", "created_utc": 1375189943, "gilded": 0, "name": "t3_1jcgsw", "num_comments": 2, "score": 1, "title": "Unexpected redirect getting friends list", "url": "https://www.reddit.com/r/redditdev/comments/1jcgsw/unexpected_redirect_getting_friends_list/"}, {"author": "MonkeyNin", "body": "There are a list of subs, (ex: /r/earthporn , /r/spaceporn/ ) that if I upvote an image I want to download it using praw. Do I have to 1. Grab the list like normal submissions = r.get_subreddit('EarthPorn').get_hot(limit=10) 2. Another query [LoggedInRedditor.get_liked()](http://python-reddit-api-wrapper.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.LoggedInRedditor.get_liked) and check for collisions? Or am I missing a function somewhere on submissions?", "created_utc": 1375059806, "gilded": 0, "name": "t3_1j8wfe", "num_comments": 9, "score": 2, "title": "[PRAW] download images in subs, if you upvote them?", "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "IAmAnAnonymousCoward", "body": "Example: Searching for http://www.livememe.com/vg972qp in /r/AdviceAnimals. Is it possible to get the submission object in such a case? RedirectException: Unexpected redirect from http://www.reddit.com/r/all/search/.json?q=http%3A%2F%2Fwww.livememe.com%2Fvg972qp&sort=new to http://www.reddit.com/submit.json?url=http%3A%2F%2Fwww.livememe.com%2Fvg972qp P.S.: I'm still looking for an answer to [this question](http://www.reddit.com/r/redditdev/comments/1hvgjf/praw_actually_getting_the_top_200_submissions/) as well. It was caught by the spam filter and unfortunately to this day, the mods didn't find the time to approve it.", "created_utc": 1374775886, "gilded": 0, "name": "t3_1j1j6q", "num_comments": 2, "score": 3, "title": "[PRAW] RedirectException when searching for URL if there's only one result.", "url": "https://www.reddit.com/r/redditdev/comments/1j1j6q/praw_redirectexception_when_searching_for_url_if/"}, {"author": "Dominoed", "body": "Okay, so, I would like help with installing PRAW. At this point, I need step-by-step instructions. I went on the PRAW website, and it said that the reccomended way to install PRAW was to use \"pip\". I went and tried installing pip manually, but it didn't work well. So I decided to go on the website to find installation instructions. It said to use \"virtualenv\" to install pip. On the virtualenv website, it says to use pip to install virtualenv. So, what is the easiest, quickest way to install PRAW? At this point all I have is Python 3.3.", "created_utc": 1374709743, "gilded": 0, "name": "t3_1izqfy", "num_comments": 21, "score": 5, "title": "Help with installing PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "isolani", "body": "Do you guys run your PRAW bots on a VPS or heroku or...?", "created_utc": 1374641360, "gilded": 0, "name": "t3_1ixqu0", "num_comments": 24, "score": 13, "title": "[PRAW] Where do you all host your python-based bots?", "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "xiggy", "body": "This bot grabs 1000 submissions from /r/gifs. It then filters out the submissions that are from 'i.imgur.com' and adds them to a list. After that it cycles through the list and embeds one image into a HTML file as a background-image. The webpage then refreshes every X amount of seconds to display the new image. The bot also grabs the submission title and overlays it at the bottom. I had an old monitor I wasn't using and was looking for a project to work on with my RPi. I figured I would just use the monitor as a sort of digital picture frame. Then I thought, lets make it more interesting by using gifs instead of images. If you have any suggestions on making it more efficient, I'd be glad to hear them. This is my first time working with Python and PRAW, so I'm sure there are better ways of doing this. import praw import time r=praw.Reddit(user_agent='Dynamic gif Slideshow by /u/xiggy') subreddit = r.get_subreddit('gifs') submissions = subreddit.get_hot(limit=1000) imglinks = [] used = [] c = 0 while True: for submission in submissions: if submission.id not in used: if submission.domain == 'i.imgur.com': tit = submission.title #tit... lol imglinks.append(submission.url) used.append(submission.id) html_file = open(\"style.css\", \"w\") html_file.write(\"body { margin:0px; padding: 0px;width:100%; height:100%; background:url(\\'\" + imglinks[c] + \"\\') center center no-repeat; background-size:contain; overflow:hidden; background-color:#121211; position:relative; font-family: sans-serif; } #title { position:absolute; width:100%; min-height:60px; text-align:center; color:#FFF; bottom:0px;v left:0px; background:rgba(0,0,0,0.6); line-height:60px; align:middle; font-weight: bold; font-size: 125%; }\") html_file.close() html_file = open(\"bg-test.html\", \"w\") html_file.write(\"\" + tit + \"\") html_file.close() print imglinks[c] c += 1 time.sleep(60) time.sleep(1300)", "created_utc": 1374430339, "gilded": 0, "name": "t3_1iredp", "num_comments": 9, "score": 20, "title": "It's not much, but it's my first bot. I made it to run on my Raspberry Pi that's connected to a display.", "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "Amablue", "body": "So I've been trying to run a script and I keep getting this error: Traceback (most recent call last): File \"__main__.py\", line 14, in main() File \"__main__.py\", line 11, in main bot.go() File \"c:\\path\\to\\my\\script.py\", line 381, in go before_id = self.scan(before_id) File \"c:\\path\\to\\my\\script.py\", line 161, in scan if self.do_stuff(comment): File \"c:\\path\\to\\my\\script.py\", line 254, in do_stuff comments = self.get_thread_comments(orig_comment) File \"c:\\path\\to\\my\\script.py\", line 315, in get_thread_comments new_comments = reply.comments() File \"C:\\Program Files\\Python27\\lib\\site-packages\\praw\\objects.py\", line 583, in comments not in self.submission._comments_by_id] AttributeError: 'NoneType' object has no attribute '_comments_by_id' The lines in question are as follows: for reply in item.replies: if type(reply) is praw.objects.MoreComments: new_comments = reply.comments() #Crashes here for comment in new_comments: stack.append(comment) stack.append(reply) The call to comments() is dying for some reason that's a complete mystery to me. What's going on?", "created_utc": 1374118831, "gilded": 0, "name": "t3_1ijb3m", "num_comments": 2, "score": 2, "title": "Error when running a PRAW script", "url": "https://www.reddit.com/r/redditdev/comments/1ijb3m/error_when_running_a_praw_script/"}, {"author": "_Skrillex_", "body": "I'm new to Python and PRAW. I just installed PRAW today and was following some example code I found online. I keep getting an error when it reaches the line r.login(). Traceback (most recent call last): File \"C:/Users/Sam/Documents/learning.py\", line 7, in r.login() File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 1120, in login self.request_json(self.config['login'], data=data) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 342, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 313, in handle_redirect response = self.handler.request(request=request.prepare(), AttributeError: 'Request' object has no attribute 'prepare' I took out the r.login() part and it ran until it got to: for submission in subreddit.get_hot(limit=10): Then it gave me the same AttributeError: 'Request' object has no attribute 'prepare'. I'm sure this is something simple, but I'm a complete beginner and clueless as to what it is.", "created_utc": 1374103351, "gilded": 0, "name": "t3_1iirrw", "num_comments": 2, "score": 2, "title": "Just installed PRAW. Keep getting error when trying to running a script.", "url": "https://www.reddit.com/r/redditdev/comments/1iirrw/just_installed_praw_keep_getting_error_when/"}, {"author": "NH4ClO4", "body": "I can't seem to find anything in the PRAW docs detailing a variable that holds the text of a message retrieved with get_unread(). I tried selftext in hopes that would work, as it was the closest I could find, but it didn't work. What is the magic variable?", "created_utc": 1373921311, "gilded": 0, "name": "t3_1id53y", "num_comments": 5, "score": 4, "title": "[PRAW] How to get the full text of a message?", "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "MrFanzyPanz", "body": "A picture of the traceback can be found [here](http://imgur.com/lY8vSLd). My code simply pulls data regarding submissions from a large number of subreddits and places them in a database. I'm not entirely sure what's happened here, but it seems like an internal error from praw. Maybe one of you guys can help me figure it out? Thanks!", "created_utc": 1373612799, "gilded": 0, "name": "t3_1i51by", "num_comments": 9, "score": 5, "title": "My PRAW code ran for 3 days then died.", "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "MrFanzyPanz", "body": "Hello, Redditdev! I am a UCLA student, and my roommates and I are conducting a large research project on reddit communities. We are planning on taking data hourly from reddit using praw and about 10 VMs for 1 month. The code will not upvote or downvote anything, or interact with reddit in any way other than simply pulling data. The user-agent names will follow the format user_agent = 'UCLA Reddit Research Project: __' with the __ being filled by the VM number. I just wanted to let you guys know that these bots are harmless, and also to say thank you so much for your help over the last 6 months as I worked on getting this off the ground! You guys are the best!", "created_utc": 1373063895, "gilded": 0, "name": "t3_1hptt3", "num_comments": 9, "score": 7, "title": "Large Reddit Research Project", "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "zzpza", "body": "I am a noob at both PRAW, python and CSS, so please bear with me... I am writing a script to post several weekly community threads. I am also working on a CSS mod that generates a menu at the top of the page, so I can link to the the latest version of each of the three weekly community threads, effectively making a 'sticky' post at the top of the page. The CSS menu takes the menu content (names and links to each post) from the text in the sidebar. Is there anyway to get PRAW to replace the link in the sidebar text for last weeks thread with the new one? I have tested the basic posting script in a test subreddit and it works OK. I created a new account to use to post the new community threads from, but each time I run the script I get asked to type in a captcha. How would this work when I run it from a cron job? Many thanks :)", "created_utc": 1372679822, "gilded": 0, "name": "t3_1hf6fi", "num_comments": 4, "score": 2, "title": "[PRAW] Can it change the sidebar (description) text? Also, PRAW keeps asking for a captcha when I test the basic script.", "url": "https://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/"}, {"author": "TEST_BRAVERY", "body": "So, if you say e.g. `subreddit.add_moderator(\"someusername\")`, then it adds that user as a moderator to the subreddit (assuming you're logged in as someone with sufficient permissions). How do you restrict permissions upon adding a moderator? E.g., suppose we want to add only \"mail\" permissions. I couldn't figure out how to do this from [reading the docs](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Subreddit.add_moderator). Thanks", "created_utc": 1372487213, "gilded": 0, "name": "t3_1has5q", "num_comments": 1, "score": 3, "title": "(PRAW) How do you add moderators with non-full permissions?", "url": "https://www.reddit.com/r/redditdev/comments/1has5q/praw_how_do_you_add_moderators_with_nonfull/"}, {"author": "tgdm", "body": "I'm really new to all of this but I finally got my bot to work. It's over 1100 lines of code (a good chunk of it is from the 25+ varied responses it chooses from), all VERY organized but I'm sure there are ways I could condense it... but that's not the problem here. I have no idea how to get it to keep running. After it runs once, it just stops. When I run in via command prompt (by loading it with python), it runs to completion but then no new command line comes up. It just sort of hangs I guess. I'm using: import time import praw r = praw.Reddit(user_agent = '') subreddit = r.get_subreddit() subreddit_comments = subreddit.get_comments(limit = 10) subreddit_submissions = subreddit.get_new(limit = 10) r.login() already_doneComment = [] already_doneSubmission = [] ############# #skipping a bunch of strings and definitions that go here ############# while True: for comment in subreddit_comments: print comment.body if comment.id not in already_doneComment and definedFunction1(comment.body.lower()): comment.reply(STRING1) already_doneComment.append(comment.id) elif comment.id not in already_doneComment and definedFunction2(comment.body.lower()): comment.reply(STRING2) already_doneComment.append(comment.id) #submissions for comment in subreddit_submissions: print submission.title print submission.selftext if comment.id not in already_doneSubmission and (definedFunction1(submission.selftext.lower()) or definedFunction1(submission.title.lower())): submission.add_comment(STRING1) already_doneSubmission.append(submission.id) elif comment.id not in already_doneSubmission and (definedFunction2(submission.selftext.lower()) or definedFunction2(submission.title.lower())): submission.add_comment(STRING2) already_doneSubmission.append(submission.id) time.sleep(300) If that isn't enough context, I could try post more on pastebin I guess. I need help figuring out why the `while True:` and `time.sleep(300)` aren't working like I thought they would.", "created_utc": 1372451336, "gilded": 0, "name": "t3_1h9rda", "num_comments": 6, "score": 1, "title": "Need help getting bot to keep running", "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": "osmotischen", "body": "Hi, i'm very interested in using PRAW and i think its great, but i can't figure out how to get the actual text out of a submission. For example, if i do >>>t = list(r.get_content('http://www.reddit.com/r/redditdev', limit = 10)) >>>str(t[0]) '1:: [Praw] [Request] Use timestamps in submission query' however, i don't want the title of the post, i want the text which is > I recently had good luck using timestamps in an undocumented manner. It would be awesome to be able to use them in Praw and it might even get around the 1k assuming 'new' sorts chronologically (?). I've searched through the documentation and have read through a good portion of it, but i still haven't figured out how to do this, if it's possible. Can anyone help? Thank You! example post from here: http://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/ If it makes any difference i'm trying using this with a subreddit where all posts are text posts", "created_utc": 1372384633, "gilded": 0, "name": "t3_1h7yno", "num_comments": 3, "score": 0, "title": "how to get text of a submission", "url": "https://www.reddit.com/r/redditdev/comments/1h7yno/how_to_get_text_of_a_submission/"}, {"author": "DrewRWx", "body": "I recently had good luck using [timestamps in an undocumented manner](http://www.reddit.com/r/bugs/comments/1h6mse/timestamps_not_recognized_in_searchbox/). It would be awesome to be able to use them in Praw and it might even get around the 1k assuming 'new' sorts chronologically (?).", "created_utc": 1372370217, "gilded": 0, "name": "t3_1h7hbr", "num_comments": 1, "score": 0, "title": "[Praw] [Request] Use timestamps in submission query", "url": "https://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/"}, {"author": "SOTB-human", "body": "I noticed that special characters are sometimes handled inconsistently in PRAW; they appear as HTML entities in some places and as normal Python characters in other places. To demonstrate this, I'll execute some code to use PRAW to repost the selftext of this submission as a comment on the submission. > quoted text \u0ca0_\u0ca0", "created_utc": 1372362688, "gilded": 0, "name": "t3_1h7722", "num_comments": 4, "score": 1, "title": "(PRAW) HTML entities and special characters in selftext/body fields?", "url": "https://www.reddit.com/r/redditdev/comments/1h7722/praw_html_entities_and_special_characters_in/"}, {"author": "cgillett", "body": "I'm trying to write a comment with PRAW, but I've realized that I don't know how to make comments multi-line. I want to use a variable in it too. Something like this: username = \"cgillett\" submission.add_comment(\"Username: \" + username + \"\\n hey\") Thanks for your help!", "created_utc": 1372311200, "gilded": 0, "name": "t3_1h5u3a", "num_comments": 9, "score": 5, "title": "[PRAW] Creating multi-line comments", "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "jonathan_morgan", "body": "I am writing a collector for a research project, and I am trying to find a good way to pull in more than 500 comments related to a given post. PRAW does this (replace_more_comments()), but I inspected the attributes of a Comment object, and it doesn't look like it keeps the upvotes, downvotes, etc. around (output below). I did look at the output of _get_json_dict(), but at least for the comments I tested with, it just had this: {u'after': None, u'before': None, u'children': [], u'modhash': u''} Thanks for helping me understand! **attributes of Comment object** {'__class__': , '__delattr__': , '__dict__': , '__doc__': 'A class that represents a reddit comments.', '__eq__': , '__format__': , '__getattr__': , '__getattribute__': , '__hash__': , '__init__': , '__module__': 'praw.objects', '__ne__': , '__new__': , '__reduce__': , '__reduce_ex__': , '__repr__': , '__setattr__': , '__sizeof__': , '__str__': , '__subclasshook__': , '__unicode__': , '__weakref__': , '_get_json_dict': , '_populate': , '_update_submission': , 'approve': , 'clear_vote': , 'delete': , 'distinguish': , 'downvote': , 'edit': , 'from_api_response': , 'fullname': , 'is_root': , 'mark_as_nsfw': , 'mark_as_read': , 'mark_as_unread': , 'permalink': , 'remove': , 'replies': , 'reply': , 'report': , 'score': , 'submission': , 'undistinguish': , 'unmark_as_nsfw': , 'upvote': , 'vote': }", "created_utc": 1372205865, "gilded": 0, "name": "t3_1h2p60", "num_comments": 1, "score": 1, "title": "Does PRAW provide access to traits of comments like upvotes, downvotes and created date?", "url": "https://www.reddit.com/r/redditdev/comments/1h2p60/does_praw_provide_access_to_traits_of_comments/"}, {"author": "Acebulf", "body": "File \"C:\\Documents and Settings\\Owner\\Desktop\\cmvbot.py\", line 316, in get_thread_comments new_comments = reply.comments() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 583, in comments not in self.submission._comments_by_id] AttributeError: 'NoneType' object has no attribute '_comments_by_id' Caused by this bit of code: if type(reply) is praw.objects.MoreComments: new_comments = reply.comments() Is there a reason why a MoreComments object would throw this error?", "created_utc": 1372190620, "gilded": 0, "name": "t3_1h2586", "num_comments": 0, "score": 0, "title": "Praw AttributeError in MoreComments.comments()?", "url": "https://www.reddit.com/r/redditdev/comments/1h2586/praw_attributeerror_in_morecommentscomments/"}, {"author": "_deffer_", "body": "I am very new to this, so bear with me. My goal: I want to create a bot that finds the following information from a specific subreddit: * Username * Title * Date and once it has that information, posts that information to a different subreddit, preferably into a new post every day. ______________________________ Where I am now: I've installed python, praw and pip and can run basic things like: >>> import praw >>> r = praw.Reddit(user_agent='_deffer_') >>> submissions = r.get_subreddit('gameswap').get_hot(limit=10) >>> [str(x) for x in submissions] and it outputs the title names. A gigantic success for me considering I have a hard time making a calculator add. A few questions: * What am I writing the code in? Just notepad? When I ran the above, I typed it in manually, but I'm assuming there's a much better/easier way. * How do I run the code once I have it where it should be stored? I'm sure I'll have a lot more questions, but I should probably stick with the very basics for now.", "created_utc": 1372171398, "gilded": 0, "name": "t3_1h1fi9", "num_comments": 9, "score": 0, "title": "Okay. I've installed python, pip and praw (if it's even called 'installing.' What now?", "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "peteyMIT", "body": "I'm running a script which sucks down new and top posts to /r/pics. It works fine for new but has recently stopped working for pics. Compare, for example, these results via the interpreter, after connecting via PRAW new_submissions_generator = r.get_subreddit('pics').get_new(limit=100) for submission in new_submissions_generator: name = submission.author.name name u'anoteduser' Works as expected (assigns and returns username). Similarly: hot_submissions_generator = r.get_subreddit('pics').get_hot(limit=25) for submission in hot_submissions_generator: nameh = submission.author.name nameh u'preggit' Works as expected (assigns and returns username). But: top_submissions_generator = r.get_subreddit('pics').get_top(limit=25) for submission in top_submissions_generator: namet = submission.author.name Traceback (most recent call last): File \"\", line 2, in AttributeError: 'NoneType' object has no attribute 'name' Breaks with an error that there is no attribute name for the object. What is going on? I am 99% sure this wasn't happening a few days ago...", "created_utc": 1371872739, "gilded": 0, "name": "t3_1gu6y5", "num_comments": 5, "score": 6, "title": "Did something just break with PRAW's get_top method?", "url": "https://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/"}, {"author": "brucemo", "body": "[This ancient thread is related.](http://www.reddit.com/r/redditdev/comments/yd2c3/praw_question_resolving_a_comment_object_from/) That guy knew he had a comment, so he just did `submission = rh.get_submission(url = )`, and then looked at `submission._comments[0]`, and that was his comment. Assume I don't know whether my thing is a submission or a comment. If I don't want to parse the URL, presumably I can do this: submission = rh.get_submission(url = my_url) if submission.permalink == my_url: object = submission elif submission._comments[0].permalink == my_url: object = submission._comments[0] else So, is this the proper way to do this? I'm concerned because I don't have total control over the URL, and the URL might not be normalized. edit: I'm concerned this would crash if the URL is not normalized and there are no comments, but that would presumably be an easy fix. It may also make sense to test the comment first and see if its ID is present in the URL, then test the submission ID against the URL: submission = rh.get_submission(url = my_url) if len(submission._comments) > 0 and my_url.find(submission._comments[0].id) >= 0: object = submission._comments[0] elif my_url.find(submission.id) >= 0: object = submission else But this doesn't sound like the right way to do something that I would think should be easy to do: \"Here's a URL -- get the object.\"", "created_utc": 1371109938, "gilded": 0, "name": "t3_1g98a4", "num_comments": 5, "score": 5, "title": "PRAW: Looking for the proper way to get a submission or comment object from a URL", "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "redditpad", "body": "Does this exist in the API? using praw I would like to have something like this. submissions = r.get_subreddit('askreddit').get_new(since=date/last request thing) Thanks", "created_utc": 1370927934, "gilded": 0, "name": "t3_1g3p8y", "num_comments": 6, "score": 2, "title": "Getting only new submissions since last request.", "url": "https://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/"}, {"author": "brucemo", "body": "It is difficult to find an example of this code anywhere, but what I came up with is: def ParentObj(self, obj): assert type(obj) == praw.objects.Comment submission = self.rh.get_submission(url = obj.permalink) if obj.is_root: return submission return self.rh.get_submission(url = submission.permalink + obj.parent_id[3:])._comments[0] This scares me for three reasons: 1. I am using _comments, which has an underscore in front of it, which implies to me that it's dubious or marginal or internal. 2. I am converting from an ID of the form \"t1_cag5h2j\" to one of the form \"cag5h2j\" by flushing the first three characters down the toilet, which seems a scary way to convert from type A to type B. 3. I would think there would be something explicit somewhere that allows you to move up and down the tree, but I can't find it. Did I implement this in a sane fashion? Should I scrape the comment tree or something instead? I promise that I'm not an idiot, really.", "created_utc": 1370909392, "gilded": 0, "name": "t3_1g308t", "num_comments": 3, "score": 4, "title": "In PRAW, getting the object that is the parent of this comment", "url": "https://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/"}, {"author": "BittyTang", "body": "I am getting this error: File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 82, in __getattr__ attr)) AttributeError: '' has no attribute 'body' From this script: import praw r = praw.Reddit(user_agent = \"comment_sift_bot\") sub = r.get_subreddit(\"games\") siftWords = [\"reddit\"] for post in sub.get_hot(limit = 10): flatComments = praw.helpers.flatten_tree(post.comments) for comment in flatComments: if any(string in comment.body for string in siftWords): print(comment.body) I'm following the PRAW tutorials, and they used comment.body, so I'm not sure how I'm using it wrong.", "created_utc": 1370761086, "gilded": 0, "name": "t3_1fyz52", "num_comments": 2, "score": 1, "title": "Problem viewing comments with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1fyz52/problem_viewing_comments_with_praw/"}, {"author": "Memeifier", "body": "I am currently running [here](http://www.reddit.com/r/memeifier/), with an extremely long sleep time. Was built with PRAW and am using the PRAW defaults for timeouts.", "created_utc": 1370758967, "gilded": 0, "name": "t3_1fyxv7", "num_comments": 4, "score": 10, "title": "Am I an evil bot? (Generates memes from comments)", "url": "https://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/"}, {"author": "202halffound", "body": "I have a simple bot written in Python (PRAW) that automatically flairs posts on a subreddit. However, the bot crashes when my internet drops out, reddit.com crashes, or I have a blackout for a few hours. How can I make the bot account for these and survive the problem?", "created_utc": 1370695032, "gilded": 0, "name": "t3_1fx8ws", "num_comments": 5, "score": 2, "title": "How can I keep a bot running continuously when reddit.com crashes?", "url": "https://www.reddit.com/r/redditdev/comments/1fx8ws/how_can_i_keep_a_bot_running_continuously_when/"}, {"author": "JimmyRecard", "body": "I'm not sure if this is the best subreddit, but I've seen similar posts here so I guess I'll try it. If there is a better subreddit, please point me to it. I wanted to run /u/AndrewNeo's [groompbot](https://github.com/AndrewNeo/groompbot). However, I don't have a 24/7 server to run it on. But then it hit me. I actually do. My router. I have ASUS RT-N66U running [Toastman](http://toastmanfirmware.yolasite.com/)'s [TomatoUSB](http://tomatousb.org/) and it is super rock solid and quite beefy in terms of specs. It also includes [busybox](http://www.busybox.net/). I added [Entware](https://code.google.com/p/wl500g-repo/) and added Python 2.7.3. So, now I'm stuck. I can SSH into the router and run Python, but past that I can't figure out how to install the dependencies ([praw](https://github.com/praw-dev/praw/) and [gdata](https://code.google.com/p/gdata-python-client/)). They call for [pip](https://pypi.python.org/pypi/pip), but I can't figure out how to install that either. I've read all the documentation and most of it goes over my head. Can anyone point me in the right direction? If I'm able to successfully figure this out, I plan to put together a little tutorial/guide about running reddit bots on routers, just to kind of give back to the community and consolidate the knowledge for next person who stumbles on this while Googling.", "created_utc": 1370679017, "gilded": 0, "name": "t3_1fx1qr", "num_comments": 4, "score": 3, "title": "Running a reddit bot on my router.", "url": "https://www.reddit.com/r/redditdev/comments/1fx1qr/running_a_reddit_bot_on_my_router/"}, {"author": "stickytruth", "body": "Hi, Is there a way to store an instance of a Reddit object? I've tried pickle, but when I load the pickled data I get a 'maximum recursion depth exceeded' exception: File \"praw/objects.py\", line 78, in __getattr__ if not self._populated: RuntimeError: maximum recursion depth exceeded while calling a Python object Is there another way to store and reuse a logged in session? Thanks Edit: Writing out the question got me to think about it, and solve it. Sorry for jumping the gun. Does this look right? def init(): global r handler = MultiprocessHandler('127.0.0.1', 6000) r = praw.Reddit(user_agent=conf.Get('useragent'), site_name='reddit_nossl', handler=handler) def login_store(): init() r.login(conf.Get('username'), conf.Get('password')) store = {'http':r.http, 'modhash':r.modhash} pickleFile = open(pickleFileName, 'wb') pickle.dump(store, pickleFile, pickle.HIGHEST_PROTOCOL) pickleFile.close() def read_load(): pickleFile = open(pickleFileName, 'rb') data = pickle.load(pickleFile) pickleFile.close() init() r.http = data['http'] r.modhash = data['modhash'] r._authentication = True msgs = r.get_inbox() for msg in msgs: print msg.body", "created_utc": 1370541310, "gilded": 0, "name": "t3_1fsz2m", "num_comments": 5, "score": 3, "title": "Storing PRAW objects?", "url": "https://www.reddit.com/r/redditdev/comments/1fsz2m/storing_praw_objects/"}, {"author": "brucemo", "body": "Assume please that I have called \"get_stylesheet\" on a sub that I moderate, and I have that object. It's a dict, and an element of the dict is the CSS for the sub, and it's correct, so I know the \"get_stylesheet\" call worked properly. What can I pass to \"set_stylesheet\" that will not result in this error: > praw.errors.APIException: (BAD_CSS) `invalid css` on field `stylesheet_contents` I have tried passing the dictionary, and dictionary[\"stylesheet\"], and both produce this error. rh = praw.Reddit(\"whatever\") sh = rh.get_subreddit(sub) ss = sh.get_stylesheet() sh.set_stylesheet() ss, stylesheet = ss, ss[\"stylesheet\"], stylesheet=ss[\"stylesheet\"], all of these result in the above error. I am logged on and a moderator of the sub. I have also tried: ss = rh.get_stylesheet(sub) rh.set_stylesheet(sub, ) ... with the same results. I would like to eventually be able to make changes to the stylesheet, but for now I'd settle for a no-op.", "created_utc": 1370421887, "gilded": 0, "name": "t3_1fplrx", "num_comments": 10, "score": 2, "title": "How do you use PRAW's \"set_stylesheet\"?", "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "stickytruth", "body": "Solved -- It was indeed the proxy. The following praw.ini configuration is working for me on PythonAnywhere: [reddit_nossl] domain: www.reddit.com oauth_domain: oauth.reddit.com short_domain: redd.it check_for_updates: false http_proxy: http://proxy.server:3128 Hi, I'm trying to get PRAW working over at PythonAnywhere.com and have run up against a wall. **log**: retrieving: http://www.reddit.com/r/opensource/.json params: {'limit': 5} data: None Traceback (most recent call last): File \"test.py\", line 4, in print [str(x) for x in submissions] File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py\", line 438, in get_content page_data = self.request_json(url, params=params) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py\", line 473, in request_json response = self._request(url, params, data) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py\", line 346, in _request response = handle_redirect() File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/__init__.py\", line 317, in handle_redirect timeout=timeout, **kwargs) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"/home/mikenon/.local/lib/python2.7/site-packages/praw/handlers.py\", line 90, in request return self.http.send(request, proxies=proxies, timeout=timeout, allow_redirects=False) File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/site-packages/requests/adapters.py\", line 246, in send raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /r/opensource/.json?limit=5 (Caused by : [Errno 111] Connection refused) **test.py**: import praw r = praw.Reddit(user_agent='Trying out PRAW on PythonAnywhere',site_name='reddit_nossl') submissions = r.get_subreddit('opensource').get_hot(limit=5) print [str(x) for x in submissions] The reddit_nossl settings are as the name implies, and log_requests: 2, check_for_updates: false Using wget to access the same url works, [pastie link](http://pastie.org/pastes/7998257/text?key=9gpwt88jid8zww3ykt0yg) I also tried using just requests.get, which also works. [script](http://pastie.org/pastes/7998286/text?key=49wrr8rknnn4gkignsqflq), [output](http://pastie.org/pastes/7998283/text?key=t1qfmdty0jkkxtq5uj2svg) An admin at PythonAnywhere has said it's not a proxy issue, which is showing other users accessing reddit. I'm stumped, but that's not saying a whole lot. Is something staring me in the face? **Edit**: I added some debugging, and modified the request pool as suggested. The new script looks like: #!/usr/bin/python import praw import requests import logging import httplib httplib.HTTPConnection.debuglevel = 1 logging.basicConfig() logging.getLogger().setLevel(logging.DEBUG) requests_log = logging.getLogger(\"requests.packages.urllib3\") requests_log.setLevel(logging.DEBUG) requests_log.propagate = True # Works print 'Making basic Requests get' requests.get('http://www.reddit.com/r/opensource/.json?limit=5') # Doesn't work print '\\n\\nImporting PRAW and requesting submissions' r = praw.Reddit(user_agent='Trying out PRAW on PythonAnywhere',site_name='reddit_nossl') submissions = r.get_subreddit('opensource').get_hot(limit=5) print [str(x) for x in submissions] I manually edited PRAW's handlers.py to modify the pool, the first 2 lines of the PRAW output shows the settings. The output is rather long, so I'll [link to it here](http://pastie.org/pastes/7998858/text?key=pzb01ahx79yzmbsp9k7na) Interestingly, with just requests.get(url), the debugging shows it connecting to proxy.server, but with PRAW it says it is connecting to www.reddit.com. How would I go about setting PRAW to use the proxy?", "created_utc": 1370212144, "gilded": 0, "name": "t3_1fjl11", "num_comments": 8, "score": 4, "title": "PRAW Exception: Connection refused", "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "jcannon98188", "body": "I am getting the following error when attempting to follow the parsing comment example on the praw wiki: Traceback (most recent call last): File \"C:\\Users\\Jason\\Desktop\\utahbot.py\", line 6, in flat_comments = praw.helpers.flatten_tree(submission.comments_flat) File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 82, in __getattr__ attr)) AttributeError: '' has no attribute 'comments_flat' The following is my code: import time import praw r = praw.Reddit('Test Bot by /u/jcannon98188') r.login('BOTUSERNAME','BOTPASSWORD') submission = r.get_submission(submission_id='1fhhxs') flat_comments = praw.helpers.flatten_tree(submission.comments_flat) already_done = [] for comment in flat_comments: if comment.body == \"Utah\" and comment.id not in already_done: comment.reply(' world!') already_done.append(comment.id) Any ideas on what is happening?", "created_utc": 1370125281, "gilded": 0, "name": "t3_1fhjbt", "num_comments": 2, "score": 2, "title": "PRAW error when using flat_comments", "url": "https://www.reddit.com/r/redditdev/comments/1fhjbt/praw_error_when_using_flat_comments/"}, {"author": "brucemo", "body": "In particular I want to maintain state via a Reddit Wiki page. I'm sorry if this something obvious but when I google \"praw wiki\" the signal to noise ratio is zero. If doing this via a Wiki is insane, is there another obvious way to maintain state? I could do it via a thread in a subreddit, but there are length limits on self-posts and comments, and stuff gets archived, etc. Thank you,", "created_utc": 1369990651, "gilded": 0, "name": "t3_1fe7pj", "num_comments": 4, "score": 2, "title": "Does PRAW support Wiki stuff?", "url": "https://www.reddit.com/r/redditdev/comments/1fe7pj/does_praw_support_wiki_stuff/"}, {"author": "im14", "body": "I run /r/ALTcointip bot, and beginning this morning, bot began processing the messages in its inbox multiple times. It seems PRAW m.mark_as_read() call doesn't have proper effect. I'm also seeing this issue through browser, where messages in my inbox stay unread even though I've already clicked on them.", "created_utc": 1369938834, "gilded": 0, "name": "t3_1fcolg", "num_comments": 23, "score": 3, "title": "m.mark_as_read() is broken since this morning - known/old issue?", "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "jdp407", "body": "When is praw.errors.RateLimitExceeded actually thrown? The API docs say it's 'An exception for when something has happened too frequently', but doesn't PRAW just delay API calls, rather than throwing an exception if the 2 second rule isn't obeyed? Or do I have to write a 2 second delay into my bot?", "created_utc": 1369840675, "gilded": 0, "name": "t3_1f9t5s", "num_comments": 4, "score": 5, "title": "When is praw.errors.RateLimitExceeded thrown?", "url": "https://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/"}, {"author": "whodunit28", "body": "My praw.ini is located at: /usr/local/lib/python2.7/dist-packages/praw As given here (https://praw.readthedocs.org/en/latest/pages/configuration_files.html), I added following to my praw.ini: [reddit.com] http_proxy:username:pwd@server:port https_proxy:username:pwd@server:port Now when I try to run the example given on praw home page, I get the following error: Traceback (most recent call last): File \"reddit.py\", line 4, in [str(x) for x in submissions] File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 434, in get_content page_data = self.request_json(url, params=params) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 342, in _request response = handle_redirect() File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 315, in handle_redirect timeout=timeout, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/handlers.py\", line 90, in request allow_redirects=False) File \"/usr/local/lib/python2.7/dist-packages/requests/sessions.py\", line 438, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 272, in send conn = self.get_connection(request.url, proxies) File \"/usr/local/lib/python2.7/dist-packages/requests/adapters.py\", line 197, in get_connection conn = ProxyManager(self.poolmanager.connection_from_url(proxy)) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/poolmanager.py\", line 123, in connection_from_url return self.connection_from_host(u.host, port=u.port, scheme=u.scheme) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/poolmanager.py\", line 109, in connection_from_host pool = self._new_pool(scheme, host, port) File \"/usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/poolmanager.py\", line 72, in _new_pool pool_cls = pool_classes_by_scheme[scheme] KeyError: 'username' What am I doing wrong?", "created_utc": 1369738181, "gilded": 0, "name": "t3_1f6xw4", "num_comments": 1, "score": 1, "title": "Not able to configure proxy settings in praw.ini", "url": "https://www.reddit.com/r/redditdev/comments/1f6xw4/not_able_to_configure_proxy_settings_in_prawini/"}, {"author": "I_SLEEP_NORMALLY", "body": "For some research I'm working on, I was contemplating looking for random threads posted within the previous 7 days. After some tooling around with PRAW, I've hit a bit of a wall. Has anyone managed to pull off grabbing random threads within a series of given constraints? (e.g. someone else may be interested in looking at a random thread within the previous year, or above a given vote threshold.)", "created_utc": 1369699430, "gilded": 0, "name": "t3_1f61mp", "num_comments": 3, "score": 5, "title": "Is it possible to roll random threads with given constraints?", "url": "https://www.reddit.com/r/redditdev/comments/1f61mp/is_it_possible_to_roll_random_threads_with_given/"}, {"author": "whodunit28", "body": "I want to configure proxy settings in PRAW. Documentation says praw.ini should be in \"/home/foobar/.config/praw.ini\" but there is no foobar folder in my home directory. There is a .config folder but it doesn't contain praw.ini.", "created_utc": 1369643025, "gilded": 0, "name": "t3_1f4mtc", "num_comments": 4, "score": 2, "title": "Where do I find my praw.ini in Ubuntu 12.04?", "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "Meeshu", "body": "When I use r.login('username','password') in the python shell, it works fine and logs me in. However, in a script it gives an error. This is the only code in my script. Help? import praw r = praw.Reddit(user_agent = 'An automated reddit project - Meeshu') r.login('Meeshu','password')", "created_utc": 1369429389, "gilded": 0, "name": "t3_1ezssi", "num_comments": 6, "score": 3, "title": "Error with r.login()", "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": "im14", "body": "So the exception usually looks something like this: ERROR 2013-05-24 00:08:14,296 _check_inbox(): couldn't mark message as read: 502 Server Error: Bad Gateway ERROR 2013-05-24 00:08:14,297 Caught exception in main() loop: 502 Server Error: Bad Gateway Traceback (most recent call last): File \"/home/dv/git/cointipbot/src/cointipbot.py\", line 457, in main self._check_inbox() File \"/home/dv/git/cointipbot/src/cointipbot.py\", line 346, in _check_inbox m.mark_as_read() File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 313, in mark_as_read return self.reddit_session.user.mark_as_read(self) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 663, in mark_as_read retval = self.reddit_session._mark_as_read(ids, unread=unread) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 262, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 1721, in _mark_as_read response = self.request_json(self.config[key], data=data) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 343, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python2.7/dist-packages/praw/internal.py\", line 182, in _raise_response_exceptions response.raise_for_status() File \"/usr/local/lib/python2.7/dist-packages/requests/models.py\", line 689, in raise_for_status raise HTTPError(http_error_msg, response=self) HTTPError: 502 Server Error: Bad Gateway The code I have in place to catch it is: # Mark message as read while True: try: m.mark_as_read() break except urllib2.HTTPError, e: if e.code in [429, 500, 502, 503, 504]: lg.warning(\"_check_inbox(): Reddit is down (error %s), sleeping...\", e.code) time.sleep(60) pass else: raise except Exception, e: lg.error(\"_check_inbox(): couldn't mark message as read: %s\", str(e)) raise Am I not catching the right HTTPError? From stack trace, I've found exception definition to be at __/usr/local/lib/python2.7/dist-packages/requests/exceptions.py__: class HTTPError(RequestException): \"\"\"An HTTP error occurred.\"\"\" def __init__(self, *args, **kwargs): \"\"\" Initializes HTTPError with optional `response` object. \"\"\" self.response = kwargs.pop('response', None) super(HTTPError, self).__init__(*args, **kwargs) No mention of urllib2 in that file. So, am I catching the wrong type of HTTPError?", "created_utc": 1369356357, "gilded": 0, "name": "t3_1exvqb", "num_comments": 8, "score": 2, "title": "After weeks of trial and error I still can't catch Reddit downtime exceptions (HTTP 429, 502, etc). Help?", "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "RampagingKoala", "body": "I'm making a bot, and whenever r.login runs, I get this error: raise SSLError(e) requests.exceptions.SSLError: Can't connect to HTTPS URL because the SSL module is not available My user_agent string doesn't contain 'bot', and it was working before the praw update. Are there any packages I'm missing or something I should be doing?", "created_utc": 1369291932, "gilded": 0, "name": "t3_1ew2gk", "num_comments": 3, "score": 2, "title": "r.login failing", "url": "https://www.reddit.com/r/redditdev/comments/1ew2gk/rlogin_failing/"}, {"author": "feblehober123", "body": "I have been using praw to make a bot that makes comments. I got everything to work, but 2 or 3 days ago there was a new update for praw. I installed this, and since then I have had many errors. I login without getting errors, but at the point when it comments it tells me that I am not logged in. I ran this many times in the shell using different accounts, user agents, and messages. It always does the same thing. Is this just a mistake, or is there something I need to do to fix this?", "created_utc": 1369259633, "gilded": 0, "name": "t3_1ev2ex", "num_comments": 2, "score": 2, "title": "unexplained login errors using PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1ev2ex/unexplained_login_errors_using_praw/"}, {"author": "alphanovember", "body": "I have a number value on line 3 of my sidebar that needs to be decremented once a day. That's all it should do. I'll schedule it with a cron job, I just need someone to write the script for me. This sounds like something that could be whipped up in a few lines of PRAW, but I don't know enough to do it myself. There actually [exists](https://github.com/matchu/reddit-countdown) a reddit bot that does does just this, the only problem is that it was written to edit HTML (`54`) in the sidebar rather than markdown for some reason, so I can't use it. It also does way more than I need it to. **Edit:** The author of that script has updated it for MD. I'm now using that.", "created_utc": 1369169525, "gilded": 0, "name": "t3_1ese01", "num_comments": 3, "score": 1, "title": "Seeking a very simple countdown/value decrement Python script for my sidebar", "url": "https://www.reddit.com/r/redditdev/comments/1ese01/seeking_a_very_simple_countdownvalue_decrement/"}, {"author": "77739", "body": "This seems like it would be pretty basic so there's probably an easy answer, but I couldn't find it on the PRAW website or in the help() docs. Basically, I want to get only links submitted to a particular subreddit within a time frame (and their scores, comments, etc.). I found a way to do it constructing the API calls without PRAW, but I'd like to know if this is possible (and easier) using PRAW.", "created_utc": 1369152234, "gilded": 0, "name": "t3_1erqio", "num_comments": 1, "score": 2, "title": "Get submissions by date and subreddit with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1erqio/get_submissions_by_date_and_subreddit_with_praw/"}, {"author": "Biggerontheinside", "body": "I'm just beginning to learn how to use PRAW and python, so this may be something very obvious that I'm missing, but I keep getting this when I try to log in using PRAW: C:\\Python33\\lib\\http\\client.py:1172: DeprecationWarning: the 'strict' argument isn't supported anymore; http.client now always assumes HTTP/1.x compliant servers. source_address) Traceback (most recent call last): File \"C:\\Python33\\Files\\Test5.py\", line 11, in r.login(USERNAME,PASSWORD) # necessary if your bot will talk to people File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 1120, in login self.request_json(self.config['login'], data=data) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 95, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 469, in request_json response = self._request(url, params, data) File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 342, in _request response = handle_redirect() File \"C:\\Python33\\lib\\site-packages\\praw\\__init__.py\", line 315, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python33\\lib\\site-packages\\requests\\sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests\\adapters.py\", line 256, in send r = self.build_response(request, resp) File \"C:\\Python33\\lib\\site-packages\\requests\\adapters.py\", line 125, in build_response extract_cookies_to_jar(response.cookies, req, resp) File \"C:\\Python33\\lib\\site-packages\\requests\\cookies.py\", line 105, in extract_cookies_to_jar jar.extract_cookies(res, req) File \"C:\\Python33\\lib\\http\\cookiejar.py\", line 1647, in extract_cookies if self._policy.set_ok(cookie, request): File \"C:\\Python33\\lib\\http\\cookiejar.py\", line 931, in set_ok if not fn(cookie, request): File \"C:\\Python33\\lib\\http\\cookiejar.py\", line 952, in set_ok_verifiability if request.unverifiable and is_third_party(request): File \"C:\\Python33\\lib\\http\\cookiejar.py\", line 707, in is_third_party if not domain_match(req_host, reach(request.origin_req_host)): AttributeError: 'MockRequest' object has no attribute 'origin_req_host' I did some searching, and the best I could come up with is a problem with requests, but I've installed it and all its requirements and I'm just at a loss.", "created_utc": 1369072114, "gilded": 0, "name": "t3_1epfrj", "num_comments": 5, "score": 4, "title": "Can't sort this one out: trying to log in using PRAW, keep getting \"AttributeError: 'MockRequest' object has no attribute 'origin_req_host'\"", "url": "https://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/"}, {"author": "Buffer_Underflow", "body": "I'm trying to comment on a submission, then comment on a comment inside the submission. I made the bot sleep 9 minutes in between both comments, but I still can't seem to shake the RateLimitExceeded exception. So what exactly causes the RateLimitExceeded exception? And how can I avoid it? I was trying to do this on /r/test if it matters at all. btw I'm using praw.", "created_utc": 1368995351, "gilded": 0, "name": "t3_1enepj", "num_comments": 3, "score": 3, "title": "RateLimitExceeded problem", "url": "https://www.reddit.com/r/redditdev/comments/1enepj/ratelimitexceeded_problem/"}, {"author": "undergroundmonorail", "body": "I'm too new to Python to pin down the problem exactly (it might not even be praw), but when I try to send a message I get the error in the title. This is the offending line of code: r.send_message(\"undergroundmonorail\", \"Comment reply from {}.\".format(message.author.name), \"{}\\n\\n[[x]](http://www.reddit.com{})\".format(message.body, message.context)) Here's all of the error text I get from the interpreter: Traceback (most recent call last): File \"C:\\Users\\undergroundmonorail\\Desktop\\python\\redditbot\\youtube.py\", line 74, in \"{}\\n\\n[[x]](http://www.reddit.com{})\".format(message.body, message.context)) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.2-py2.7.egg\\praw\\decorators.py\", line 261, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.2-py2.7.egg\\praw\\decorators.py\", line 160, in wrapped if sys.stdin.closed or raise_captcha_exception: File \"\", line 523, in __getattr__ File \"C:\\Program Files\\PyScripter\\Lib\\rpyc.zip\\rpyc\\core\\netref.py\", line 150, in __getattr__ return syncreq(self, consts.HANDLE_GETATTR, name) File \"C:\\Program Files\\PyScripter\\Lib\\rpyc.zip\\rpyc\\core\\netref.py\", line 71, in syncreq return conn.sync_request(handler, oid, *args) File \"C:\\Program Files\\PyScripter\\Lib\\rpyc.zip\\rpyc\\core\\protocol.py\", line 434, in sync_request raise obj AttributeError: DebugOutput instance has no attribute 'closed' I have no idea where to even start *looking* for a solution. Any help would be greatly appreciated.", "created_utc": 1368738515, "gilded": 0, "name": "t3_1eh4ih", "num_comments": 5, "score": 1, "title": "I'm about 60% sure that this is a [PRAW] problem, but maybe it's just Python: I get \"AttributeError: DebugOutput instance has no attribute 'closed'\" when I try to send a message.", "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "noun_exchanger", "body": "Another nooby question from me.. but I can't seem to find the answer. I'm using Windows and have installed PIP, Distribute, and PRAW. I had reddit bot code working fine.. but then a couple days after the most recent PRAW update, there are a bunch of errors and the codes won't run. So the question is, how do I update PRAW with the command prompt using PIP? Thanks", "created_utc": 1368514314, "gilded": 0, "name": "t3_1eauwc", "num_comments": 6, "score": 1, "title": "How to update PRAW with PIP?", "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "shaggorama", "body": "There ought to be a subreddit-level flag (and perhaps a user-level flag) to instruct bots not to engage on that subreddit (or with that user). Obviously this would only be effective for those bots that respected such a flag, but I believe something like this could reduce the amount the moderators need to fight with bots. There seem to be a lot of subreddits that have a \"we see em, we ban em\" attitude towards reddit bots: why not just modify the ecosystem so the bots already get the message without evening needing the mods to ban them? Here's how I imagine this being implemented: 1. The option to set this flag needs to be made available in subreddit settings. I think this should default to \"bots are ok here.\" 2. The main reddit API wrappers (e.g. praw) should be set to respect this flag by default. Overriding this flag should be easy, but this way people just messing around with bots as hobby projects (which I assume is most of the population of praw users) will respect the flag by default. EDIT: Some great ideas in here, but I'm thinking such a feature would be more trouble than it's worth and probably actually cause more work for everyone (mods, bot creators, and app developers). There's nothing really wrong with the current system of just banning bots when mods find them. Maybe if we work this idea through more we could come up with a better implementation, but I'm concerned that the options we've considered would make people's lives harder instead of easier. Maybe we should bring more mods into the discussion?", "created_utc": 1368476670, "gilded": 0, "name": "t3_1e9lwp", "num_comments": 24, "score": 18, "title": "Proposal: robots.txt equivalent for reddit bots.", "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "testuser12345678", "body": "I've been running a script on PythonAnywhere which logs in using PRAW. I've run it every day, and it hasn't failed until just now, which is strange because I didn't change anything in my code or my PRAW installation. I therefore can only conclude that the Reddit API must have changed, or PythonAnywhere must have changed something. Does anyone know how I might fix this? Traceback (most recent call last): File \"bravery20.py\", line 1314, in r.login(username=username, password=password) File \"/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\", line 906, in login self.request_json(self.config['login'], data=data) File \"/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\", line 223, in error_checked_function return_value = function(cls, *args, **kwargs) File \"/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\", line 407, in request_json response = self._request(url, params, data) File \"/home/person/.local/lib/python2.7/site-packages/praw/__init__.py\", line 294, in _request timeout=timeout) File \"/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\", line 64, in __call__ result = self.function(reddit_session, url, *args, **kwargs) File \"/home/person/.local/lib/python2.7/site-packages/praw/decorators.py\", line 167, in __call__ return self.function(*args, **kwargs) File \"/home/person/.local/lib/python2.7/site-packages/praw/helpers.py\", line 137, in _request allow_redirects=False, auth=auth) File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 399, in post return self.request('POST', url, data=data, **kwargs) File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 354, in request resp = self.send(prep, **send_kwargs) File \"/usr/local/lib/python2.7/site-packages/requests/sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"/usr/local/lib/python2.7/site-packages/requests/adapters.py\", line 246, in send raise ConnectionError(e) requests.exceptions.ConnectionError: HTTPConnectionPool(host='proxy.server', port=3128): Max retries exceeded with url: http://www.reddit.com/api/login/.json (Caused by : [Errno 111] Connection refused) I'm using PRAW 2.0.15 because the latest version doesn't work for some reason; also, I've removed `\"login\"` from the `SSL_PATHS` list in `__init__.py` in order to prevent PRAW from attempting to connect securely (which PythonAnywhere doesn't support). Thanks in advance.", "created_utc": 1368119517, "gilded": 0, "name": "t3_1e0ffk", "num_comments": 2, "score": 6, "title": "Did something in the Reddit API change last night? I can no longer log in insecurely with PRAW 2.0.15 hosted on PythonAnywhere", "url": "https://www.reddit.com/r/redditdev/comments/1e0ffk/did_something_in_the_reddit_api_change_last_night/"}, {"author": "rapture_survivor", "body": "I'm pretty sure this is a newbie question; I've just started using the API today. I want to be able to retrieve a comment given a url for the comment without searching through the submission for a matching ID. I tried using the get_content method with the full URL of the target comment, but when I try to use the generator it creates, the API throws an error: >>> gen = r.get_content('http://www.reddit.com/r/pics/comments/1dplco/took_a_ride_and_suddenly_found_myself_in_a/c9sqtmx') >>> data = gen.next() Traceback (most recent call last): File \"\", line 1, in data = gen.next() File \"C:\\Python27\\lib\\site-packages\\praw-2.1.1-py2.7.egg\\praw\\__init__.py\", line 438, in get_content root = page_data[root_field] TypeError: list indices must be integers, not str As I have my program set up now, I take the submission ID from the url, and then get the submission using praw. I then proceed to search through all if the comments if the submission, checking each to see if their ID matches the one in the url. This usually doesn't take too long, but if the comment I'm looking for is in one of the 'more comments' areas, then it takes forever to open all of the sections to look for it. I know there must be some way to access a comment in that way, but I must not be doing it right.", "created_utc": 1367807310, "gilded": 0, "name": "t3_1dro2b", "num_comments": 2, "score": 4, "title": "Problems with the PRAW get_content() method", "url": "https://www.reddit.com/r/redditdev/comments/1dro2b/problems_with_the_praw_get_content_method/"}, {"author": "johnflim", "body": "What would be the best method for allowing a bot/program to keep track of read and unread private messages? I was planning on having my program mark unread messages in the inbox as 'read' after it finishes processing the message, but it seems like there are flaws in this plan. For example, if I accidentally log on to the reddit account associated with the bot, I might accidentally open up a message and it wouldn't be processed by the bot. The PRAW tutorials have a method in which processed submissions are kept track of using the submission ID. Would this be the best method for handling messages? Thanks!", "created_utc": 1367691633, "gilded": 0, "name": "t3_1doto3", "num_comments": 8, "score": 4, "title": "Reddit bot reading inbox", "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "killver", "body": "Hey! One thing I have not yet completely figured out is how tags work on Reddit. Are they assigned to submissions by users? Can they \"invent\" new tags or do they have to use existing ones? Is there a way to crawl the tags? E.g., via PRAW? Thanks!", "created_utc": 1367237115, "gilded": 0, "name": "t3_1dbzvd", "num_comments": 6, "score": 6, "title": "Tags on Reddit", "url": "https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/"}, {"author": "acidzest", "body": "Hi, I've made an App.net (like Twitter) bot that's meant to post new submissions there every ~hour, which it does but not as efficiently as I want it to. I have an array called already_done, if a submission's id is in that array, it's meant to go to the next submission after than, using place_holder, which works but then it just loops through again and again. I'm not really sure how to get it working, I'm sure it's simple enough, just a logic problem. Here's the code; import praw, time r = praw.Reddit(user_agent='ADNPost') already_done = [] while True: for submission in r.get_top(limit=1): id = submission.id title = submission.title url = submission.short_link save_state = (id) if id not in already_done: already_done.append(submission.id) post = title + \" | \" + url print post print save_state if id in already_done: for submission in r.get_front_page(limit=1, place_holder=submission.id): id = submission.id title = submission.title url = submission.short_link print title, url save_state = (id) already_done.append(submission.id) time.sleep(2) Current Output: Post 1 Post 2 Post 2 Post 2 When it should be: Post 1 Post 2 Post 3 It successfully uses place_holder=Post 1 but doesn't do it for Post 2, Post 3, etc. Any help would be greatly appreciated.", "created_utc": 1367151766, "gilded": 0, "name": "t3_1d9ops", "num_comments": 6, "score": 1, "title": "Trouble with an if loop/while loop combination", "url": "https://www.reddit.com/r/redditdev/comments/1d9ops/trouble_with_an_if_loopwhile_loop_combination/"}, {"author": "bboe", "body": "Hey everyone, I spent a little time over the last few days refactoring how PRAW actually handles requests including how PRAW performs both rate limiting and caching. The new approach is modular allowing you to more easily change PRAW's request handling behavior by providing your own handler class, or utilizing a non-default handler provided by PRAW. One such handler I wrote is a multiprocess handler that interfaces with a request-handler server that is now included in PRAW. Before I push this code out with version 2.1.0 it would be awesome if some of the regular PRAW users could test the multiprocess handler with your programs. The primary benefit of the multiprocess handler is you no longer have to worry about rate limiting when running multiple versions of your PRAW programs. To get started, fetch PRAW from github. While using git is recommended, here's a [zip](https://github.com/praw-dev/praw/archive/master.zip) of the latest source. Installing PRAW this way requires you to run `python setup.py install` from the root of the source tree (where setup.py lives). Once up and running give this script a try: https://gist.github.com/bboe/5458377 At this point you'll probably notice warnings like: Cannot connect to multiprocess server. Is it running? Retrying in 2 seconds. From a separate terminal, start PRAW's multiprocess request handling server by running `praw-multiprocess`. From here you should be able to run as many PRAW instances you want so long as you pass in an instance of the `MultiprocessHandler` when creating the Reddit instance. What I'm looking for is any issues you encounter. I've handled most of the obvious connection issues on my ubuntu OS, but perhaps the socket error messages are different on other OSes, or I completely neglected to test something. That's where you can help! At the moment the only unresolved issue that I am aware of is when the request times out, the server fails to pickle the resulting TimeOut object. This in turn will cause an EOFError on the client that it already handles. However, after 3 consecutive EOFErrors PRAW will raise a ClientException. Any feedback you have would be greatly appreciated. Thanks!", "created_utc": 1366882240, "gilded": 0, "name": "t3_1d2nr4", "num_comments": 1, "score": 11, "title": "Multiprocess PRAW -- testing needed", "url": "https://www.reddit.com/r/redditdev/comments/1d2nr4/multiprocess_praw_testing_needed/"}, {"author": "SOTB-human", "body": "I'm using PRAW, but if the only way to do this is by bypassing PRAW, then that'll work too. For example, suppose we're given a comment ID \"c9m894d\" and we need to figure out some more information about it: its content, author, score, etc. However, we don't know the ID of its submission, so we can't do the `praw.objects.Submission.get_info(r, \"http://www.reddit.com/r/all/comments/\"+submission_id+\"/_/\"+comment_id).comments[0]` thing. Comment IDs are unique, right? So is there a way to retrieve a comment using only its ID?", "created_utc": 1366873369, "gilded": 0, "name": "t3_1d2ino", "num_comments": 4, "score": 3, "title": "Is it possible to fetch a Comment using only its ID, not knowing its submission ID?", "url": "https://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/"}, {"author": "killver", "body": "Hey there! I am currently trying to permanently store submission objects to have later access to them. I also want to leave the option open to later easily have possibilities to retrieve further data like comments or user infos. My code looks the following way: import praw import datetime r = praw.Reddit('void') submissions_per_day = list() curr_date = None i = 0 for submission in r.get_subreddit('all').get_new(limit=None): date = datetime.datetime.fromtimestamp(int(submission.created_utc)).strftime('%Y-%m-%d') submissions_per_day.append(submission) #or vars(submission) if i == 0: curr_date = date if date != curr_date: SOMEHOW STORE THE LIST OF SUBMISSION OBJECTS So my goal is to get all submissions of one day store them, then for the next day and store them again. I have tried various things like the most obvious cPickle solution. But this is very slow, even when just crawling a few submissions and using the newest protocol. I have also tried to only take the dictionary of the object (i.e. vars(submission)) without any improvements. I know that I could just go ahead and manually parse the object and e.g. store the values to a csv file. But this seems so odd to me and I want a more fluent solution. I could also think about getting the json objects and storing them. Another solution would be to use databases but this is also not my prefered method. I hope anyone of you can give me some hints of how to cope with my problems. Thanks, Philipp", "created_utc": 1366812202, "gilded": 0, "name": "t3_1d0cln", "num_comments": 4, "score": 4, "title": "PRAW store submission data", "url": "https://www.reddit.com/r/redditdev/comments/1d0cln/praw_store_submission_data/"}, {"author": "alexleavitt", "body": "Been trying to get user data from PRAW, but keep getting this error: Traceback (most recent call last): File \"redditusers.py\", line 126, in for each in user_comments: File \"/usr/local/lib/python2.6/dist-packages/praw/__init__.py\", line 290, in get_content page_data = self.request_json(page_url, url_data=url_data) File \"/usr/local/lib/python2.6/dist-packages/praw/decorators.py\", line 164, in error_checked_function return_value = function(self, *args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw/__init__.py\", line 325, in request_json response = self._request(page_url, params, url_data) File \"/usr/local/lib/python2.6/dist-packages/praw/__init__.py\", line 217, in _request url_data, timeout) File \"/usr/local/lib/python2.6/dist-packages/praw/decorators.py\", line 59, in __call__ result = self.function(reddit_session, page_url, *args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw/decorators.py\", line 144, in __call__ return self.function(*args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw/helpers.py\", line 101, in _request response = reddit_session._opener.open(request, timeout=timeout) File \"/usr/lib/python2.6/urllib2.py\", line 391, in open response = self._open(req, data) File \"/usr/lib/python2.6/urllib2.py\", line 409, in _open '_open', req) File \"/usr/lib/python2.6/urllib2.py\", line 369, in _call_chain result = func(*args) File \"/usr/lib/python2.6/urllib2.py\", line 1172, in http_open return self.do_open(httplib.HTTPConnection, req) File \"/usr/lib/python2.6/urllib2.py\", line 1147, in do_open raise URLError(err) urllib2.URLError: from this code (which I adapted from the PRAW examples): #get recent karma & subreddit activity got_user = r.get_redditor(user) #THIS IS LINE 111 user_submissions = got_user.get_submitted(limit=None) user_comments = got_user.get_comments(limit=None) submission_karma_by_subreddit = {} submission_count_by_subreddit = {} for each in user_submissions: subreddit = each.subreddit.display_name submission_karma_by_subreddit[subreddit] = (submission_karma_by_subreddit.get(subreddit, 0) + each.score) submission_count_by_subreddit[subreddit] = (submission_count_by_subreddit.get(subreddit, 0) + 1) comment_karma_by_subreddit = {} comment_count_by_subreddit = {} for each in user_comments: #THIS IS LINE 126 subreddit = each.subreddit.display_name comment_karma_by_subreddit[subreddit] = (comment_karma_by_subreddit.get(subreddit, 0) + each.score) comment_count_by_subreddit[subreddit] = (comment_count_by_subreddit.get(subreddit, 0) + 1) I've run this code in the past, and it worked fine then. Not sure why the timeouts are happening now... Any thoughts?", "created_utc": 1366758403, "gilded": 0, "name": "t3_1cyxjr", "num_comments": 11, "score": 1, "title": "PRAW Timeouts?", "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "lamerx", "body": "Every time i try to upvote or downvote i get this user='name' passwd='mypassword' reddit.login(user,passwd) m=reddit.get_submission(submission_id='1crjy2') m.upvote() ##OUTPUT Traceback (most recent call last): File \"C:\\Python27\\Scripts\\testB0T2.py\", line 61, in reddit.get_submission(submission_id='19zce4').upvote() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 447, in upvote return self.vote(direction=1) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 341, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 464, in vote return self.reddit_session.request_json(url, data=data) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 239, in error_checked_function raise error_list[0] NotLoggedIn: `please login to do that` on field `None` And when I try to remove something that I am the author of with this code user='name' passwd='mypassword' reddit.login(user,passwd) m=reddit.get_submission(submission_id='1crjy2') m.remove() ##OUTPUT Traceback (most recent call last): File \"C:\\Python27\\Scripts\\sheboon\\testB0T2.py\", line 73, in m.remove() File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 331, in wrapped if mod_req and not is_mod_of_all(obj.user, subreddit): File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 288, in is_mod_of_all mod_subs = user.get_cached_moderated_reddits() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 688, in get_cached_moderated_reddits for sub in self.reddit_session.get_my_moderation(limit=None): File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 372, in get_content page_data = self.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 223, in error_checked_function return_value = function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 407, in request_json response = self._request(url, params, data) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 294, in _request timeout=timeout) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 64, in __call__ result = self.function(reddit_session, url, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 167, in __call__ return self.function(*args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\helpers.py\", line 154, in _request .format(prev_url, url)) ClientException: Unexpected redirect from http://www.reddit.com/reddits /mine/moderator/.json to http://www.reddit.com/subreddits/login.json?dest=%2Freddits%2Fmine%2Fmoderator%2F.json%3Flimit%3D1024", "created_utc": 1366613200, "gilded": 0, "name": "t3_1cumng", "num_comments": 12, "score": 6, "title": "PRAW + Voting & Removing", "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "frumious", "body": "I know praw already cache's results from reddit for 30 seconds but I'm looking for a way to add a more persistent data store so that I only hit reddit once for any \"thing\" in its API, ever. For now, I don't care about seeing any changes due to edits or votes but if one did, I guess that would complicate things. I can come up with something explicit that is layered on top of praw but having this store insinuated into praw so it's transparent to any existing praw-using code seems like a good design. Any thoughts on this? Any work in this direction yet? Edit: I'm looking for something that persists between executions.", "created_utc": 1366556582, "gilded": 0, "name": "t3_1csvar", "num_comments": 9, "score": 6, "title": "Persistent store for praw?", "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "ben444422", "body": "I've been using PRAW to crawl through all of the subreddits, but I've found that some subreddits have been banned (thereby not existing), abruptly stopping the crawling process via a 404 error. Is there a way to test whether a subreddit exists?", "created_utc": 1366436841, "gilded": 0, "name": "t3_1cq7g1", "num_comments": 7, "score": 8, "title": "How to know if subreddit exists?", "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": "wtf_are_my_initials", "body": "I'm currently using PRAW to make a bot, but I don't want the bot to ever respond to itself, so before I the bot comments, I want to check if the comment it is replying to was posted by the bot's username. I have searched through the docs and have not found it in the source so im asking here. How do I get the username of a commenter from a comment object? Thanks", "created_utc": 1365825097, "gilded": 0, "name": "t3_1c931l", "num_comments": 7, "score": 2, "title": "PRAW and comments.", "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "nannal", "body": "hey guys, I'm playing about with praw and am a total python nubbin. I've got it to login, make posts and read page titles and self post text but for what I'm planing I'm going to need to find a way of replying to a users last comment and reading the bot's inbox. any help you guys would could offer would be muchly appreciated.", "created_utc": 1365695338, "gilded": 0, "name": "t3_1c52gt", "num_comments": 15, "score": 7, "title": "Submit reply to users most recent comment.", "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "yowmamasita", "body": "The script is watching and fetching submissions on a subreddit's \"new\" feed every 30 minutes and I'm stuck thinking of a way to only fetch new posts. I'm using PRAW and thinking of storing the reddit id of the first post (ex. dtg4j), everytime I fetch, in a flat file. (since the first post is always the newest) But my program fetches 50 posts everytime, what if only 1 of that is new, then the other 49 posts I fetched are useless, right? So I'm not sure yet with that. And even if there's only a small chance, when I fetch 50 posts, what if there were 60 new submissions. That means I will miss the other 10. Please help me. Thanks!", "created_utc": 1365355195, "gilded": 0, "name": "t3_1bv06t", "num_comments": 1, "score": 7, "title": "Stuck at processing each submission on a subreddit without repeating", "url": "https://www.reddit.com/r/redditdev/comments/1bv06t/stuck_at_processing_each_submission_on_a/"}, {"author": "naive_babes", "body": "PRAW being a third-party module makes it hard to use it on Google App engine. There seem to be two options to be able to use it on app engine - install it in the local directory or use buildout. I'm really really confused about how to use buildout for this on app engine and there dont seem to be informative tutorials for this online yet. I extracted the egg files and copied them onto my app's directory, but there are persistent problems with dependencies. I've done this method for a lot of other third-party libraries, but i can't seem to get this going with PRAW. Has anyone done either of these things? Any tips on what to try next? Update: I did manage to get this going. I unzipped all the egg files of the praw installation and put them in the local directory of my app. There is some trouble with the update_checker dependency, and i got rid of it completely. There's an additional problem with obtaining the platform of google app engine while constructing the user agent string. I got rid of that bit as well. Now i have it working fine.", "created_utc": 1365311334, "gilded": 0, "name": "t3_1bu7ak", "num_comments": 3, "score": 3, "title": "How do i run PRAW on google appengine?", "url": "https://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/"}, {"author": "ritonlajoie", "body": "Hey ! I wondered if someone had the idea to create a proxy for reddit but which would cache the queries results for some time ? This would allow getting at least data from reddit without hitting it. Does this exist ? Would you need it ? I could code something around PRAW that would first hit a public cache then if the result is not there, would hit reddit and fill the cache.. Would you use that ?", "created_utc": 1365178172, "gilded": 0, "name": "t3_1bqnri", "num_comments": 0, "score": 0, "title": "any api proxy for reddit ?", "url": "https://www.reddit.com/r/redditdev/comments/1bqnri/any_api_proxy_for_reddit/"}, {"author": "utopiah", "body": "Else what about using https://github.com/praw-dev/praw as a Jabber bot like https://github.com/fritzy/SleekXMPP/wiki ?", "created_utc": 1365176680, "gilded": 0, "name": "t3_1bqlqv", "num_comments": 5, "score": 8, "title": "Is there a solution (e.g. irssi script or bitblee plugin) to have Reddit inbox and messages in an IRC client?", "url": "https://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/"}, {"author": "ipitythefoobar", "body": "I'm trying to run some tests on a shared web server and but I get a permissions error when I try to get praw on the server. \\# easy_install praw >[Errno 13] Permission denied: '/usr/lib/python2.6/site-packages/test-easy-install-20367.write-test' >The installation directory you specified (via --install-dir, --prefix, or the distutils default setting) was: > /usr/lib/python2.6/site-packages/ >Perhaps your account does not have write access to this directory? If the installation directory is a system-owned directory, you may need to sign in as the administrator or \"root\" account. If you do not have administrative access to this machine, you may wish to choose a different installation directory, preferably one that is listed in your PYTHONPATH environment variable. I assume this is because I'm on a shared server and don't have permissions into the communal Python directory. How would I go about getting around this issue?", "created_utc": 1364109411, "gilded": 0, "name": "t3_1awkvu", "num_comments": 2, "score": 2, "title": "Permissions issue when trying to easy-install praw on a hosted web server", "url": "https://www.reddit.com/r/redditdev/comments/1awkvu/permissions_issue_when_trying_to_easyinstall_praw/"}, {"author": "StealsTopComments", "body": "I'm trying to use PRAW's get_info() method (a wrapper for the /api/info API method) to get posts for some URLs. For some reason the behavior seems to be inconsistent. I've been testing some URLs while learning just to see the format of responses and whatnot, but some URLs (that I got from actual posts) give responses indicating that there are no posts for that URL. For example, the URL: http://i.imgur.com/jOG9a.jpg is used in [this post](http://www.reddit.com/r/gaming/comments/10no3v/the_pokemon_holy_grail/). If I fire up Python / PRAW in my terminal, this is the result: >>> import praw >>> r = praw.Reddit(user_agent='testing /u/StealsTopComments') >>> r.get_info(url='http://i.imgur.com/jOG9a.jpg') [] Can anyone shed some light on this? Am I misunderstanding what /info is for?", "created_utc": 1364092860, "gilded": 0, "name": "t3_1aw7jc", "num_comments": 3, "score": 3, "title": "PRAW get_info(): not working?", "url": "https://www.reddit.com/r/redditdev/comments/1aw7jc/praw_get_info_not_working/"}, {"author": "AlexanderSalamander", "body": "A form exists somewhere (I could host it on a webserver). The form has a single field: \"username\". You enter your username. Through PRAW, a reddit account bot makes you a moderator of /r/godmode. Simple as that. That way people can goof around and anyone can be re-added as a moderator at any time. What is entailed here? Is there a tutorial for little things like this? Is it actually complicated?", "created_utc": 1363978474, "gilded": 0, "name": "t3_1atckm", "num_comments": 1, "score": 0, "title": "A seemingly simple task, but I have no clue. Any help?", "url": "https://www.reddit.com/r/redditdev/comments/1atckm/a_seemingly_simple_task_but_i_have_no_clue_any/"}, {"author": "shrayas", "body": "Hi, Im trying to make an app just for learning purposes. I want to get the list of subreddits that a user is subscribed to. Of course i can use the `.login` method within PRAW but i felt that using oauth would give the user better confidence. I managed to get this going and you can find it over [here](https://github.com/shrayas/slashsub) on my Github. But my question is this: Why does one have to authorize the app every time he tries to login with reddit? isn't that a one time thing? If he's already logged in to Reddit it should automatically redirect back to the callback URL is what i feel. Maybe i'm doing something wrong.. In a gist here's what im doing #get object r = praw.Reddit(...) # set oauth info r.set_oauth_app_info(...) # get URL r.get_authorize_url(...) # Open a web browser and go to URL # Here is where the authorization page is shown everytime #on callback accessInfo = r.get_access_details(...) r.set_access_credentials(...) #do whatever i want r.get_me() #for instance Any help would be appreciate. Please excuse the n00bness in the question (if any)", "created_utc": 1363520211, "gilded": 0, "name": "t3_1agkmm", "num_comments": 2, "score": 7, "title": "PRAW and Oauth", "url": "https://www.reddit.com/r/redditdev/comments/1agkmm/praw_and_oauth/"}, {"author": "bboe", "body": "`pip install -U praw` will update you to the latest version that supports listing wiki pages via `r.get_wiki_pages('subreddit')`, creating / editing wiki pages via `r.edit_wiki_page('subreddit', 'page_title', 'content', 'reason')`. Read the [2.0.13 changelog](http://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-0-13) for additional changes. If there is additional functionality that you would like to see added, please [create an issue](https://github.com/praw-dev/praw/issues?state=open).", "created_utc": 1363368466, "gilded": 0, "name": "t3_1aczcr", "num_comments": 4, "score": 11, "title": "PRAW 2.0.13 adds basic wiki editing support", "url": "https://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/"}, {"author": "mrg3_2013", "body": "I am getting started w/ praw-dev and from other posts it looks like there is a limit of 1000 results that cannot be violated. What I am trying to do is to crawl for posts with title string containing key word (say \"happy dog\") along with an image. I don't need the comments. Can anyone please recommend (or point me to sample code) for this ? I am wondering if there is a way to get just the title + image associated without burdening the backend (and hopefully somehow get past the 1000 limit). Thanks! I don't need to do this real-time - so pacing requests are perfectly fine.", "created_utc": 1362624200, "gilded": 0, "name": "t3_19tiyz", "num_comments": 4, "score": 8, "title": "Using PRAW to search posts with a title keyword", "url": "https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/"}, {"author": "SN4T14", "body": "A few days ago, I added a YouTube bot, [Groompbot](https://github.com/AndrewNeo/groompbot), to the /r/nerdcubed subreddit, I made it run every minute via cron job, so it would always be the first to post, and it worked great. Then one day it stops posting, I take a look to find that it throws an error 429 (Too many requests) every time, I contacted the maker of it a few days ago, but he hasn't replied yet. Here's the error: ERROR:root:Error logging into Reddit. Traceback (most recent call last): File \"groompbot.py\", line 27, in getReddit r.login(settings[\"reddit_username\"], settings[\"reddit_password\"]) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\", line 857, in login self.request_json(self.config['login'], data=data) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\", line 223, in error_checked_function return_value = function(cls, *args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\", line 396, in request_json response = self._request(url, params, data) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/__init__.py\", line 283, in _request timeout=timeout) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\", line 64, in __call__ result = self.function(reddit_session, url, *args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/decorators.py\", line 167, in __call__ return self.function(*args, **kwargs) File \"/usr/local/lib/python2.6/dist-packages/praw-2.0.12-py2.6.egg/praw/helpers.py\", line 161, in _request response.raise_for_status() File \"/usr/local/lib/python2.6/dist-packages/requests/models.py\", line 638, in raise_for_status raise http_error HTTPError: 429 Client Error: Too Many Requests Any ideas, Reddit?", "created_utc": 1362575533, "gilded": 0, "name": "t3_19rubs", "num_comments": 14, "score": 7, "title": "Issues with YouTube bot", "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "MrFanzyPanz", "body": "Hello, Redditdev! I want to pull the score and author of the comments attached to a link. Preferably on a timely basis. Can get_comments be used on a submission? Is there a way to do this using PRAW that I'm missing? Thanks! :D **P.S.** Shout out to bboe and Deimorz for all your help!", "created_utc": 1362121650, "gilded": 0, "name": "t3_19g943", "num_comments": 2, "score": 8, "title": "Any way to glean comments from a link?", "url": "https://www.reddit.com/r/redditdev/comments/19g943/any_way_to_glean_comments_from_a_link/"}, {"author": "johnflim", "body": "Hi! I'm new at using PRAW and When using the below code, I can print my messages, but if a message is long enough, some of the message text is cut off. Am I using the generator wrong or is there a way to read the whole message? Thanks! import praw r = praw.Reddit(\"new bot by /u/johnflim\") r.login() inbox = r.get_inbox(11) for message in inbox: print message", "created_utc": 1361943598, "gilded": 0, "name": "t3_19b8mp", "num_comments": 2, "score": 3, "title": "PRAW get_inbox(), printing whole message from inbox", "url": "https://www.reddit.com/r/redditdev/comments/19b8mp/praw_get_inbox_printing_whole_message_from_inbox/"}, {"author": "MrFanzyPanz", "body": "Hello, redditdev! I'm using PRAW to pull some basic information from Reddit, namely the top 500 posts from /r/pics. My code is very straightforward: import praw r = praw.Reddit(user_agent='MrFanzyPanz Datascraper :D') for post in r.get_subreddit('pics').get_hot(limit=500, url_data={'limit': 100}): *post.name *post.score *post.author *post.created_utc I'm writing it out to a csv to play around with. My problem is that, while this worked a couple weeks ago, I've revisited the scraper, and I'm now getting this error when I run it: TypeError: get_content() got an unexpected keyword argument 'url_data' Was url_data removed in an update? I've update PRAW to the newest version. Thanks!", "created_utc": 1361830594, "gilded": 0, "name": "t3_197uz8", "num_comments": 3, "score": 7, "title": "PRAW: url_data removed from get_content()", "url": "https://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/"}, {"author": "lamerx", "body": "I cannot seem to get a bot to remove or delete a submission even when both the submission and the subreddit being posted to belongs to the account im using. Consider reddit = praw.Reddit(user_agent='BotScript 1.0, by u/LamerX') print \"Logging in...\" user='LamerX' passwd='password' reddit.login(user,passwd) print \"Logged In\" reddit.get_submission(submission_id=id of submission i own) submission.remove() This come ALWAYS draws the following error NotLoggedIn: `please login to do that` on field `None`", "created_utc": 1361818048, "gilded": 0, "name": "t3_197erw", "num_comments": 5, "score": 6, "title": "Praw Moderator Bot & Remove Method", "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "atomicUpdate", "body": "I followed the example for how to use OAuth with PRAW to gain access to a user's account that's [here](https://github.com/praw-dev/praw/wiki/OAuth). However, I'm wondering what is the right way to reuse that access once I've gotten the tokens from Reddit. I created a 'get_user_data()' function to reuse the token, and within that I perform an r.refresh_access_information(refresh_token). Unfortunately, this doesn't seem to immediately update the user information for the user that the refresh token is for. Instead, it seems to take 2 calls to get_user_data() in order to get the current information for the user I'm interested in. If r.refresh_access_information() is the correct function to use, how to I force PRAW to block long enough to get the information for the user that the refresh token is for, rather than returning the information for the last user that get_user_data() was called for? [My code is available here.](http://pastebin.com/tib9Hm5z) As you can probably tell from looking at it, the idea is to be able to have multiple widgets use the same server to request data from Reddit. However, I'm pretty new to all of this, so I may be going about it all wrong...", "created_utc": 1361508544, "gilded": 0, "name": "t3_1905ed", "num_comments": 8, "score": 5, "title": "How to reuse OAuth Tokens with PRAW", "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "JRDubstepcom", "body": "it seems everyone knows how to add certain functions to there programs yet i cant find a standard place to view examples. for instance \"submission.selftext.lower()\" how or where can i find information on this function? from what i can tell is that this code snipbit returns a links text. but what could i change? aka submission.titletext()? submission.linkurl()? or something of the sort. where would i be able to find information on different functions? i guess im having a hard time understanding praw because i dont know what i can do with it. thanks guys, sorry for newb question.", "created_utc": 1361176344, "gilded": 0, "name": "t3_18qq0d", "num_comments": 8, "score": 7, "title": "is there a praw library of some sort?", "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "ipitythefoobar", "body": "I want to use PRAW and do some automated data collection on a regular basis. I could run this from home but I'm pretty OCD and don't want an Internet hiccup, computer crash, or power outage to miss one of the polling times so ideally I could run this on a server. I have a cheap hosting account but I don't know if I can run a bot script from there. I'm fine writing PHP and JavaScript and stuff but running command line type stuff gives me pause especially when it involves importing stuff. I don't want the hosting company to get mad at me if I'm not releasing resources correctly. So what are some options and best practices for writing a Reddit bot? Thanks!", "created_utc": 1361107451, "gilded": 0, "name": "t3_18ov4m", "num_comments": 16, "score": 8, "title": "Advice on running a Reddit bot written in Python on a shared server?", "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "Bornhuetter", "body": "I recently ported moderatorbot to PRAW 2, and had a couple of issues. bboe was helpful as always, and I thought I'd reproduce here what I did to get my code working, in case others can learn from this. **Issue 1 - my_moderation** In PRAW 1.x the way to get a list of subreddits that the logged in user moderates, you use: For subreddit in r.user.my_moderation(): In PRAW 2.x the correct way is now: For subreddit in r.get_my_moderator(): (and don't forget the () after r.get_my_moderator as I initially did) **Issue 2 - _request** In PRAW 1.x, I used the following to make HTTP requests response = r._request(url, params) In PRAW 2.x params has been changed to data, and the following needs to be used. response = r._request(url, data=data) In this particular case I was trying to make an HTTP request to remove a comment. bboe recommended that I use r.request_json(url, data=data) > *you should use r.request_json as you can't count on r._request to always be around.* In this case that will work (even though I am not actually trying to get json data). In other cases I believe that I still need to use _request. I believe that some pages do not return a json properly (or the json does not have all the data I need), but the result of _request can be parsed using beatifulsoup. I don't have any code currently in production that uses beautifulsoup on _request, so I can't remember the exact situations I used it for. So, I'd like to ask bboe to consider putting some sort of _request functionality into supported \"user space\"", "created_utc": 1360703448, "gilded": 0, "name": "t3_18eeep", "num_comments": 2, "score": 8, "title": "Resolved issues porting to PRAW 2", "url": "https://www.reddit.com/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/"}, {"author": "shaggorama", "body": "Hey, I have a bot that uses all_comments_flat to parse a submission's comments. I just set up a new development station and noticed that the newest version of praw doesn't have that method on the Submission object anymore, and the comment attribute is a tree. What's the easiest way to just get all the comments on a submission? I'm suspecting that I'll need to use some kind of \"load more comments\" function to get all the available comments but I'm not sure where it is. Thanks,", "created_utc": 1359923103, "gilded": 0, "name": "t3_17tn7r", "num_comments": 5, "score": 8, "title": "Replacement for all_comments_flat", "url": "https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/"}, {"author": "MrFanzyPanz", "body": "Hello! I'm using PRAW to get some of the links/comments from a list of users' history and their data, but after only two are three accounts I'll be returned a 404 error. I check if a user account still exists before pulling the data, so I know it's not that the account was deleted. The 404 error doesn't appear to be connected to a problem in my code: it fails at random times while running the script. Is it possible that I'm getting kicked from my connection to the server? I'm using get_comments() and get_submitted() to pull the information.", "created_utc": 1359765151, "gilded": 0, "name": "t3_17q7b3", "num_comments": 13, "score": 5, "title": "404 Error during account scraping", "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "IcyRespawn", "body": "Hi guys, I've just started using PRAW today and I'm absolutely loving it. I did run into some issues while writing a bot. It's supposed to check a specific subreddit for the newest posts, read their contents and alert me whenever it finds something that matches a certain regular expression within them. I'm checking for new submissions every 10 seconds, by calling s.get_new_by_date(limit=5) on my subreddit's Subreddit object. The problem is that after the initial call, new posts are usually only fetched after anywhere from 30 seconds to a few minutes AFTER they were created, where I expect them to show up within a maximal amount of 10 seconds from the moment they were created. They do appear when I refresh the page in my browser. Things I tried doing to resolve this: - My user agent follows the API guidelines and isn't generic. - Setting my local praw.ini file so that cache_timeout=5 to ensure I'm not reading from the cache. - Calling s.refresh() on my Subreddit object after every sleep(10). Do you guys have any idea what might be causing this to happen? Is it because pages are cached server-side, and so I cannot force a refresh on the user's side? If so, why do pages usually take more than a minute (which is far more than the forced 30-second limitation if there is in fact one) to be detected by my algorithm? Thanks for reading, I appreciate your time. Icy", "created_utc": 1359163692, "gilded": 0, "name": "t3_17aiml", "num_comments": 6, "score": 8, "title": "[PRAW] Getting newest posts to show up as quickly as  possible", "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "jokoon", "body": "import praw r = praw.Reddit(user_agent='just to dl my comments and likes') r.login('jokoon', 'xxxxxx') f = open(\"comments.txt\",'w') r.set_oauth_app_info(client_id='xxxxxxxx-xx', client_secret='xxxxxxxxxxxxxxxxxx', redirect_url='http://127.0.0.1:65010/authorize_callback') url = r.get_authorize_url('xxxxxxxx', 'xxxxxx-xx', True) import webbrowser webbrowser.open(url) # user = r.get_redditor('jokoon') # f.write(r.get_me().get_liked()) I get the error set_oauth_app_info() got an expected keyword argument 'redirect_url'", "created_utc": 1359130013, "gilded": 0, "name": "t3_179di4", "num_comments": 6, "score": 6, "title": "Trying to get my comments and likes", "url": "https://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/"}, {"author": "takitesi", "body": "I tried posting this at an earlier date but received no responses so I'm trying again. I'm not sure if anyone here takes requests, but here it goes. I'd like a web app or chrome extension similar to [Group Reddit Saved Links] (https://chrome.google.com/webstore/detail/group-reddit-saved-links/gkchebpjpehcnlbjamhfgoconkmoalaf?hl=en) (which doesn't work for me for some reason). In a recent thread, /u/ButtCrackFTW provided a simple program to [download all saved links] (http://www.reddit.com/r/redditdev/comments/161odw/wrote_a_simple_script_in_python_using_praw_to/). In addition to the saved link's title, URL, and comments, I'd like to get the subreddit and, if possible, the date on which it was saved. I'd also like to be able to add my own \"tag\" to a post and change its title. I now need a way to organize the information for each link. I'm thinking if it was exported to a table structure with (sortable) headers something like this: Link Title (editable) | URL | Subreddit | Comments Link | Tags | Date It may be a longshot, but if anyone could make this a reality it would be much appreciated! Bonus functionality: Any saved comments that are in a saved link get displayed in the Comments Link column field", "created_utc": 1358894052, "gilded": 0, "name": "t3_1730dr", "num_comments": 3, "score": 0, "title": "A request for saved links", "url": "https://www.reddit.com/r/redditdev/comments/1730dr/a_request_for_saved_links/"}, {"author": "red_foot", "body": "I keep getting a JSONDecodeError, anyone gotten PRAW to work on Pythonanywhere? **EDIT:** Got past the first hurdle, but then experienced this error when using PRAW login. Traceback (most recent call last): File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1687, in wsgi_app response = self.full_dispatch_request() File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1360, in full_dispatch_request rv = self.handle_user_exception(e) File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1358, in full_dispatch_request rv = self.dispatch_request() File \"/usr/local/lib/python2.7/site-packages/flask/app.py\", line 1344, in dispatch_request return self.view_functions[rule.endpoint](**req.view_args) File \"/home/myapp.py\", line 21, in login return saved_links_page(fetch_Links(str(form.username.data), str(form.password.data))) File \"/home/myapp.py\", line 33, in fetch_Links r.login(str(username), str(password)) File \"/home/.local/lib/python2.7/site-packages/praw/__init__.py\", line 804, in login self.request_json(self.config['login'], data=data) File \"/home/.local/lib/python2.7/site-packages/praw/decorators.py\", line 211, in error_checked_function return_value = function(cls, *args, **kwargs) File \"/home/.local/lib/python2.7/site-packages/praw/__init__.py\", line 375, in request_json response = self._request(url, params, data) File \"/home/.local/lib/python2.7/site-packages/praw/__init__.py\", line 266, in _request timeout=timeout) File \"/home/.local/lib/python2.7/site-packages/praw/decorators.py\", line 64, in __call__ result = self.function(reddit_session, url, *args, **kwargs) File \"/home/.local/lib/python2.7/site-packages/praw/decorators.py\", line 155, in __call__ return self.function(*args, **kwargs) File \"/home/.local/lib/python2.7/site-packages/praw/helpers.py\", line 149, in _request response.raise_for_status() File \"/home/.local/lib/python2.7/site-packages/requests/models.py\", line 638, in raise_for_status raise http_error HTTPError: 501 Server Error: Not Implemented", "created_utc": 1358712705, "gilded": 0, "name": "t3_16y13b", "num_comments": 9, "score": 10, "title": "Pythonanywhere.com and PRAW", "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "takitesi", "body": "I'm not sure if anyone here takes requests, but here it goes. I'd like a web app or chrome extension similar to [Group Reddit Saved Links] (https://chrome.google.com/webstore/detail/group-reddit-saved-links/gkchebpjpehcnlbjamhfgoconkmoalaf?hl=en) (which doesn't work for me for some reason). In a recent thread, /u/ButtCrackFTW provided a simple program to [download all saved links] (http://www.reddit.com/r/redditdev/comments/161odw/wrote_a_simple_script_in_python_using_praw_to/). In addition to the saved link's title, URL, and comments, I'd like to get the subreddit and, if possible, the date on which it was saved. I'd also like to be able to add my own \"tag\" to a post and change its title. I now need a way to organize the information for each link. I'm thinking if it was exported to a table structure with (sortable) headers something like this: Link Title (editable) | URL | Subreddit | Comments Link | Tags | Date It may be a longshot, but if anyone could make this a reality it would be much appreciated! Bonus functionality: Any saved comments that are in a saved link get displayed in the Comments Link column field", "created_utc": 1358552912, "gilded": 0, "name": "t3_16uio8", "num_comments": 0, "score": 4, "title": "Does /r/redditdev take small requests?", "url": "https://www.reddit.com/r/redditdev/comments/16uio8/does_rredditdev_take_small_requests/"}, {"author": "takitesi", "body": "Basically, I'd like to be able to go to a website that has an online Python compiler/IDE that will run small python scripts that use PRAW, rather than having to set up a local environment. Does something like this exist? Or is there a way to do it fairly easily? Any help is much appreciated.", "created_utc": 1358429203, "gilded": 0, "name": "t3_16r0m0", "num_comments": 3, "score": 4, "title": "Has anyone built a web app that works with PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/16r0m0/has_anyone_built_a_web_app_that_works_with_praw/"}, {"author": "lamerx", "body": "Im sorry to have to come back here and ask yet another question. The praw documentation is too complicated and I dont really understand it. I cant find any information on the methods, functions, etc. I cant seem to figure out how to get a comments karma bu its id, consider the following for submission in reddit.get_subreddit(sub).get_top(limit=40): flat_comments = submission.all_comments_flat for comment in flat_comments: ident = str(comment.id) karma=comment.?? I cant seem to find the karma score of the comment", "created_utc": 1358385471, "gilded": 0, "name": "t3_16q02d", "num_comments": 2, "score": 3, "title": "Praw + karma", "url": "https://www.reddit.com/r/redditdev/comments/16q02d/praw_karma/"}, {"author": "bboe", "body": "PRAW users, I've been working on-and-off for the last few weeks (with significant help from /u/_Daimon_) on extending the initial OAuth2 support added to PRAW by /u/intortus. My original plan was for addition-only changes thus resulting in a backward-compatible 1.1 release. However, properly handling reddit's OAuth2 scopes necessitated a number of backwards incompatible namespace changes (which in turn prompted numerous other namespace changes) thus I am bumping the major version number. To minimize the amount of code from breaking that has a PRAW dependency, I wanted to give everyone a little time to update their projects before I add PRAW 2.0 to the cheeseshop (pypi). If you have code that depends on PRAW and you don't want it to break for other users of your projects you have two primary options: The first option is to simply update your `setup.py` or `requirements.txt` file to depend on any PRAW version less than 2.0 (1.0.16 will be the last 1.0 version). If your package installation does not automatically handle dependencies then you can inform your users to run `pip install praw==1.0.16` to get an appropriate version, or point them to [this source tarball](https://github.com/praw-dev/praw/archive/praw-1.0.16.tar.gz) or [this source zipfile](https://github.com/praw-dev/praw/archive/praw-1.0.16.zip). The second option, of course, is to update your package to work with PRAW versions >= 2.0. To help you get started you'll want to checkout the PRAW 2.0 [Change Log](https://github.com/praw-dev/praw/wiki/Changelog). Until I actually release the package on pypi, you'll need to clone the github repository (`git clone git://github.com/praw-dev/praw.git`), or manually download the source ([zipfile](https://github.com/praw-dev/praw/archive/master.zip)). I'm happy to answer any questions you have as replies to this submission. Assuming no major issues are discovered, I will release PRAW 2.0 Wednesday evening (PST). Edit: I should add that if you are interested in using PRAW via OAuth2, check out the [PRAW OAuth wiki page](https://github.com/praw-dev/praw/wiki/OAuth).", "created_utc": 1358245022, "gilded": 0, "name": "t3_16m0uu", "num_comments": 4, "score": 23, "title": "PRAW 2.0 is Coming (release in ~2 days)", "url": "https://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/"}, {"author": "mpheus", "body": "I have a little obsession of checking /r/gamedeals frequently for new game deals. Just wrote my first useful python script to make my life a little easier. The script checks for new posts made to that sub-reddit and pushes a Growl notification. The Growl notification looks like - http://i.imgur.com/ET9vL.png #!/usr/bin/env python # # This script looks up /r/gamedeals/new every 120 seconds and pushes the notification # for new posts to Growl app on Mac OS # # Uses PRAW - https://github.com/praw-dev/praw (for easy access to Reddit API) # and GNTP - https://github.com/kfdm/gntp (for pushing Growl notification) # import gntp.notifier import praw import time # icon for use with growl notification ICON_URL = \"http://cdn2.iconfinder.com/data/icons/crystalproject/128x128/apps/package_games.png\" USER_AGENT = 'new /r/gamedeals notifier by /u/mpheus' # reddit doesn't shows new posts made to a subreddit without logging in # REDDIT_ID = '' # not required # REDDIT_PASS = '' # not required growl = gntp.notifier.GrowlNotifier( applicationName = \"/r/gamedeals notifier\", notifications = [\"New Deal\"], defaultNotifications = [\"New Deal\"], # hostname = \"computer.example.com\", # Defaults to localhost # password = \"abc123\" # Defaults to a blank password ) growl.register() r = praw.Reddit(user_agent=USER_AGENT) # r.login(REDDIT_ID, REDDIT_PASS) already_done = [] # for storing the uids of posts already notified while True: data = r.get_subreddit('gamedeals').get_new_by_date(limit=10) for x in data: if x.id not in already_done: already_done.append(x.id) growl.notify( noteType = \"New Deal\", title = x.domain, description = x.title, icon = ICON_URL, sticky = True, # so that notification remains on the screen until closed priority = 1, callback = x.permalink ) print \"Notified about\", x.title, \"at\", time.strftime(\"%d %b - %I:%M:%S %p\") print \"Last checked for game deals at\", time.strftime(\"%d %b - %I:%M:%S %p\") time.sleep(120) I just launch this script in a terminal window and leave it running. I'm learning python right now and plan on improving the script by storing the uids of posts that already have been notified about in a database and leave the script running on Raspberry Pi that runs 24/7. Growl can be set to receive notification pushed from remote computer (in this case, RPi) as well. Thanks to reddit dev and PRAW dev for all their amazing work! EDIT: Removed login call and replaced `get_new` function with `get_new_by_date`.", "created_utc": 1357833724, "gilded": 0, "name": "t3_16bh8j", "num_comments": 5, "score": 23, "title": "Wrote my first python script using PRAW to check /r/gamedeals for new posts and notify me", "url": "https://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/"}, {"author": "MrFanzyPanz", "body": "Hi, redditdev! I'm having some trouble with my code. I'm collecting a large amount of data for a survey experiment and I'm new to Python as well as Stata, so I'm just writing all the data out to .txt files in CSV. Usually my code works fine, but I've tried running it recently and keep encountering an error related to post author's names. The error is: \"'None-type' object has no attribute 'name'\". I've looked at the posts which it corresponds to and they seem like normal posts. I'm at a loss for what is wrong, can anybody help? I've attached my code for posterity, although I don't think it's needed for this fix. from datetime import datetime import praw import time r = praw.Reddit(user_agent='your unique user agent') def datascraper(counter, subreddit): #Open up the files to which the data will be written filename = 'dataset_' + subreddit + '.txt' filename_titles = 'dataset_' + subreddit + 'titles.txt' namelist = [] txt = open(str(filename), 'a') title = open(str(filename_titles), 'a') # If this is the first time that this document is being # opened, we need to insert the headings first (for CSV format) if counter == 0: txt.write(\"rank , subreddit , num_comments , net_score , is_self, over_18 , downs , url , is_imgur , author , time_created , time_current , ups \\n\") title.write('rank, title \\n') counter = 1 for post in r.get_subreddit(subreddit).get_hot(limit=500, url_data={'limit': 100}): # write to CSV/titles_subreddit files # data listed as needed is in order: txt.write(str(counter) + ' , ') txt.write(post.subreddit.display_name + ' , ') txt.write(str(post.num_comments) + ' , ') txt.write(str(post.score) + ' , ') txt.write(str(post.is_self) + ' , ') txt.write(str(post.over_18) + ' , ') txt.write(str(post.downs) + ' , ') txt.write(post.url + ' , ') txt.write(str(post.domain.endswith('imgur.com')) + ' , ') name = post.author.name **This is where the problem is** namelist.append(name) txt.write(name + ' , ') txt.write(str(post.created_utc) + ' , ') txt.write(str(int(time.time())) + ' , ') txt.write(str(post.ups) + '\\n') post_title = post.title.encode('ascii', 'ignore') title.write(post_title + '\\n') counter += 1 txt.close() title.close() return counter, namelist Thanks for your input!", "created_utc": 1357512160, "gilded": 0, "name": "t3_1630jj", "num_comments": 6, "score": 6, "title": "PRAW is returning post.author.name errors", "url": "https://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/"}, {"author": "bheklilr", "body": "I'm completely new to `praw`, so please excuse my ignorance. I want to write a simple bot that reads from an RSS feed and submits posts based on that feed. I already have permission from the RSS owner to post to their subreddit this way. Is there a quick example for this somewhere? I just need to submit a link, no self posts. Thanks!", "created_utc": 1357225935, "gilded": 0, "name": "t3_15w2cs", "num_comments": 3, "score": 4, "title": "Help with submitting with Praw", "url": "https://www.reddit.com/r/redditdev/comments/15w2cs/help_with_submitting_with_praw/"}, {"author": "burntsushi", "body": "The title says it all. I have some hope that a workaround is possible, particularly since I can still *see* older comments (from old saved links or old submissions), they just don't seem to be tied to the user page. My instinct says that the comments still exist in the database some where, but is there any way to get to them? N.B. I'm using PRAW.", "created_utc": 1357183583, "gilded": 0, "name": "t3_15v7l4", "num_comments": 3, "score": 7, "title": "Retrieving a user's comments seems to be limited to 1000... is there a way around this?", "url": "https://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/"}, {"author": "lamerx", "body": "Could someone be so kind as to direct me to any documentation that may exist for using praw to create a new or even write to an existing CSS of a subreddit?", "created_utc": 1356846633, "gilded": 0, "name": "t3_15nuau", "num_comments": 4, "score": 10, "title": "Praw + CSS", "url": "https://www.reddit.com/r/redditdev/comments/15nuau/praw_css/"}, {"author": "downbound", "body": "Honestly this new fangled API structure is a bit foreign to me but I am learning some. However, I have hit a roadblock that is certain to ruin all my programmer cred. I program in python and have gotten OAuth working. I am getting a access_token. Now I feel stupid. I have not the foggiest how to make an API call through PRAW or anything else that uses that access_token. I am simply trying to find the username (eventually some other things but not now) of the person who got the access_token. It really must be something so easy that I am missing that there isn't even a guide. import requests url=\"https://ssl.reddit.com/api/me.json?\" data= { 'access_token': 'access_token', 'scope' : 'http://www.my.url' } print requests.get(url, data=data).text This gets me an empty JSON response {} but, HTTP return of 200. . .I dunno", "created_utc": 1356681257, "gilded": 0, "name": "t3_15kc2j", "num_comments": 3, "score": 15, "title": "Embarrased I need the \"Hello World\" of API", "url": "https://www.reddit.com/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/"}, {"author": "expiredtofu", "body": "~~I was playing with PRAW earlier on my computer, and I think I ran my script (which just displays new posts from r/all) a bit too fast. Now, I cannot get any information with PRAW, and when I go to ssl.reddit.com, I am greeted with the message \"you appear to be a bad robot\". Is it possible for this ban to be lifted?~~ EDIT: Tried again with a new script, the errors were just my fault. Thanks for the help!", "created_utc": 1356583886, "gilded": 0, "name": "t3_15i5c8", "num_comments": 6, "score": 0, "title": "Is it possible for my bot to be unbanned?", "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "MrFanzyPanz", "body": "Hello! Silly question here, sorry for the noobacity. I'm trying to use praw's get_content() generator in order to pull data from subreddits, however I don't know how to iterate over this object. I keep receiving an error that pics.json is an unknown url type: import praw import pprint r = praw.Reddit('Test Code - MrFanzyPanz') content_scrape = r.get_content(\"pics\", limit = 10) for submission in content_scrape: print submission **or** print submission.title If there is an alternative method which will allow me to simply parse www.reddit.com/.json as text, that would be also fine, although I need to be able to do it through python requests (preferably praw), rather than copy-pasting the text from the page manually.", "created_utc": 1355858609, "gilded": 0, "name": "t3_152d2w", "num_comments": 2, "score": 6, "title": "Praw URL parameters", "url": "https://www.reddit.com/r/redditdev/comments/152d2w/praw_url_parameters/"}, {"author": "LudoA", "body": "I'm using PRAW and trying to access the saved links of a logged in user. Basically, I'm following the steps mentioned under \"[A Few Short Examples](https://github.com/praw-dev/praw/wiki)\", which include an example on how to get the saved links. Unfortunately, I'm getting an error (\"has no attribute 'get_saved'\"). C:\\>python Python 2.7.2 (default, Jun 12 2011, 15:08:59) [MSC v.1500 32 bit (Intel)] on win32 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import praw >>> r = praw.Reddit(user_agent='example') >>> r.login('LudoA', 'foobar') >>> r.user.link_karma 2592 >>> r.user.get_saved() Traceback (most recent call last): File \"\", line 1, in File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 70, in __getattr__ attr)) AttributeError: '' has no attribute 'get_saved' >>> r.user Redditor(user_name='LudoA') >>> user = r.get_redditor('ludoa') >>> user Redditor(user_name='ludoa') >>> user.link_karma 2592 >>> user.get_saved() Traceback (most recent call last): File \"\", line 1, in File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 70, in __getattr__ attr)) AttributeError: '' has no attribute 'get_saved' >>> So it does see *r.user* as being of the right type (i.e. *LoggedInRedditor*), but it can't find the method *get_saved()*, just like with the non-logged in user object. Any idea as to what I'm doing wrong? Thanks!", "created_utc": 1355857718, "gilded": 0, "name": "t3_152c5b", "num_comments": 9, "score": 3, "title": "PRAW: can't access saved links - no attribute get_saved", "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "esacteksab", "body": "Doing a quick r.user.has_mail, it continues to return false, even after sending myself a message. End the session, poll Reddit again, I have a message. Check messages, mark as read via web...poll via ipython/cli and it still continues to return true. Anyone have any experience with this? I normally just use requests + json to interact with Reddit, but thought I'd use PRAW this time. Any thoughts, advice, etc would be greatly appreciated! Thanks!", "created_utc": 1355086020, "gilded": 0, "name": "t3_14kakv", "num_comments": 6, "score": 3, "title": "Does PRAW cache results with iPython? ", "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "rhiever", "body": "Hi all, I'm trying to implement global ratelimiting for my bot so I can run multiple PRAW bots in parallel, but apparently I'm not catching every time PRAW makes a request to reddit. Can someone please clarify how many requests are being made in the following code snippets? I count this as 1 request: r.get_subreddit(\"all\").get_top(limit=100) I count r.get_front_page() as 1 request, then each submission.comments as another request. posts = [] for submission in r.get_front_page(): for comment in submission.comments: posts.append(comment) I count this as 1 request: r.get_redditor(user).get_overview(limit=2000)", "created_utc": 1348954947, "gilded": 0, "name": "t3_10omtd", "num_comments": 3, "score": 8, "title": "PRAW: when are requests being made? (code included)", "url": "https://www.reddit.com/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/"}, {"author": "binaryechoes", "body": "Just looking over some data for a project. I've processed over 2.5 million comments and banned_by and num_reports is always blank. Although the project isn't using these fields I could see how they could be of use. Surely, of all those comments one of these would have something (maybe). Is this mod only territory? Anything I'm missing or think my code is flawed? Does someone have a comment/submission handy for me to verify? http://www.reddit.com/r/redditdev/comments/o4op9/accessing_moderatorrestricted_data_via_api/ https://github.com/reddit/reddit/issues/429 PS I'm using praw. Thanks", "created_utc": 1348457271, "gilded": 0, "name": "t3_10dizi", "num_comments": 4, "score": 3, "title": "banned_by and num_reports fields", "url": "https://www.reddit.com/r/redditdev/comments/10dizi/banned_by_and_num_reports_fields/"}, {"author": "TankorSmash", "body": "There have been several answers over the last two years, (all by /u/bboe) in regards to this question, but I'm just not able to figure it out for myself. http://www.reddit.com/r/redditdev/comments/icbh1/how_do_i_use_the_more_entries_in_the_json_reply/ http://www.reddit.com/r/redditdev/comments/o4jq0/is_it_possible_to_get_a_complete_comment_when/ http://www.reddit.com/r/redditdev/comments/aql1m/some_questions_on_morechildren_api_or_load_more/ Basically I should know what I'm doing but the official docs are a bit iffy, and looking at the PRAW code isn't helping. Apparently I need to send the following parameters: params = {\"link_id\": 't3_w5iwp', \"children\": 'c5afy83', \"depth\": '', \"id\": 'c5ag775', \"pv_hex\":\"\", \"r\":'gaming', \"renderstyle\":\"\", \"api_type\":'json'} Then I send it like this: url = r'http://reddit.com/api/morechildren' requests.get(url,params=params) #it's actually a requests.session object, logged in. But I'm getting a `not found` error page. Even copying the url from the Apigee page doesn't seem to be working", "created_utc": 1341637471, "gilded": 0, "name": "t3_w60cs", "num_comments": 11, "score": 3, "title": "Using morechildren without PRAW?", "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "bboe", "body": "Three things: With /u/mellort's support, I have moved what was previously [mellort/reddit_api](https://github.com/mellort/reddit_api) to [praw-dev/praw](https://github.com/praw-dev/praw) on github. /u/mellort has made a clone from the new location, so that existing links to the repository are not broken. Additionally I've official updated the name to __PRAW__ which is an acronym for \"Python Reddit API Wrapper\". The primary impetus for the name change was to resolve the confusion I see from many people regarding what to call the package. I admit it was very confusing that the repository was called `reddit_api` and the package was called `reddit`. Hopefully it should be clear what name to use now: PRAW. Finally, to clearly distinguish the python package from reddit's own source, I have renamed the package from `reddit` to `praw` and restarted PRAWs version number at 1.0. Aside from the readthedocs documentation, all the documentation (on github) should be updated to reflect the change. Making the switch is pretty simple, here are the steps: 0. Install the `praw` package (instructions on github) 0. Replace `import reddit` with `import praw` in your code and of course update any `reddit.NAME` references with `praw.NAME` such as `reddit.Reddit` with `praw.Reddit`. 0. If you had a user-level, or script level `reddit_api.cfg` file, please replace that with `praw.ini`. There is more information [here](https://github.com/praw-dev/praw/wiki/The-Configuration-Files). If you have any questions / comments please don't hesitate to ask / share.", "created_utc": 1341113171, "gilded": 0, "name": "t3_vv4tg", "num_comments": 2, "score": 10, "title": "Python Reddit API Wrapper package rename and repository move", "url": "https://www.reddit.com/r/redditdev/comments/vv4tg/python_reddit_api_wrapper_package_rename_and/"}, {"author": "RedditSrc4Research", "body": "Two things I noticed earlier tonight while using the python reddit api wrapper [PRAW](https://github.com/mellort/reddit_api): 1) The domain in .config/reddit_api/reddit_api.cfg has to match the .ini config for the server, even if they both point to the same IP. I had a couple DNS names configured for my site, and PRAW started complaining that I hadn't logged in even though I had. E.G., if the .ini file has reddit1.example.com and your reddit_api.cfg file has reddit2.example.com (both of which point to your server), PRAW will not recognize your login (and not throw an error either until you try to do something that requires the login). It took a while to debug :/ 2) Here is an example for make_moderator, which is not shown on the wiki: import reddit r = reddit.Reddit('some user agent','myreddit') sub = r.get_subreddit('mysubreddit') r.login('mymoderator','password') sub.make_moderator('thenewmoderator')", "created_utc": 1337070114, "gilded": 0, "name": "t3_tnxsj", "num_comments": 5, "score": 6, "title": "A couple PRAW notes (domain issue and make_moderator)", "url": "https://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/"}, {"author": "bboe", "body": "Edit: 1.3.0 is official now. `pip install reddit` or `pip install --upgrade reddit` will install six if it is not already installed. I recently finished adding hybrid support for both Python 2.6+ and 3.2+ and I would really appreciate if some PRAW users would upgrade to the 1.3.0dev version of PRAW to ensure everything works as expected. I have extended PRAW's tests to include unicode testing and thus given that all the tests pass on both 2.6, and 3.2 I don't expect there to be any issues. Nevertheless it doesn't hurt to be sure. 1.3.0dev is not available via pip, thus you'll need to manually fetch it from [github](https://github.com/bboe/reddit_api/tree/python3). Going forward PRAW will depend on the `six` module which can be obtained via `pip install six`. To make sure you are running the correct version you can run: `python -c 'import reddit; print(reddit.VERSION)'` The output should be `1.3.0dev`. If you run into any issues, please follow up either here, or on #reddit-dev in irc.freenode.net. Thanks!", "created_utc": 1332267227, "gilded": 0, "name": "t3_r5e5l", "num_comments": 5, "score": 7, "title": "Python Reddit API Developers: Python 2/3 hybrid beta testing needed", "url": "https://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/"}, {"author": "13steinj", "body": "I can't understand most of that cause you didn't format properly. Can you please cut and paste that to pastebin / a github gist", "created_utc": 1455018405, "gilded": 0, "name": "t1_cztc3jf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44v87n/forbidden_http_error/"}, {"author": "TheEnigmaBlade", "body": "Unfortunately, that's not how OAuth works. Right now you're setting basic authentication information in PRAW, but PRAW doesn't automatically perform any authentication. You need to retrieve a token to allow the bot/script to act on behalf of a Reddit account. I recommend using a helper library like [this one](https://github.com/SmBe19/praw-OAuth2Util) to use OAuth with PRAW. Alternatively, [this is a simplified version of the code I use for OAuth](https://gist.github.com/TheEnigmaBlade/8e93087de8b82bea6b78), but it can only be used with script apps.", "created_utc": 1455047264, "gilded": 0, "name": "t1_cztt44i", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44v87n/forbidden_http_error/"}, {"author": "DrBaily", "body": "Thanks for your reply. I actually omitted those lines when copying and pasting, but they're actually there at the top. I guess what I don't understand is why it worked fine last week, then when I run it today without having altered any of the code, it gives me this Forbidden: HTTP Error.", "created_utc": 1455102705, "gilded": 0, "name": "t1_czumra1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44v87n/forbidden_http_error/"}, {"author": "Pokechu22", "body": "Seems like the API result got changed slightly - rather than being `{\"kind\": \"Listing\", \"data\": {...}}`, it is now `[{\"kind\": \"Listing\", \"data\": {...}}]`. This is probably a bug. It affects __all__ listings.", "created_utc": 1454972441, "gilded": 0, "name": "t1_czsrwta", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/"}, {"author": "boib", "body": "Fixed now.", "created_utc": 1454972662, "gilded": 0, "name": "t1_czss0ma", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/"}, {"author": "Pokechu22", "body": "What's your \u03c0 on the bottom of the site say right now? Mine currently says it's running `86d9223`, and it's still broken. EDIT: Now my \u03c0 is `ce9e203` and things are working again.", "created_utc": 1454972811, "gilded": 0, "name": "t1_czss36q", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/"}, {"author": "boib", "body": "It was incremental. Some things start working before others. Still not completely working though. Edit: everything seems ok now. My pi is the same as yours.", "created_utc": 1454973017, "gilded": 0, "name": "t1_czss709", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/"}, {"author": "SayFuckYeah", "body": "Same here. > Traceback (most recent call last): File \"C:\\Users\\ \\Desktop\\bot.py\", line 33, in submissions = list(submissions) File \"C:\\Program Files (x86)\\Python35-32\\lib\\site-packages\\praw-3.3.0-py3.5.egg\\praw\\__init__.py\", line 564, in get_content root = page_data.get(root_field, page_data) AttributeError: 'list' object has no attribute 'get'", "created_utc": 1454972354, "gilded": 0, "name": "t1_czsrvax", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/"}, {"author": "genitaliban", "body": "Bug. It's being reverted.", "created_utc": 1454972417, "gilded": 0, "name": "t1_czsrwe7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/"}, {"author": "thabonch", "body": "Same problem. The line: for submission in submissions: raises the exception for me. submissions is the return value of a subreddit.get_new() call.", "created_utc": 1454972569, "gilded": 0, "name": "t1_czsrz0k", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44tixp/praw_search_suddenly_stopped_working/"}, {"author": "13steinj", "body": "The problem is the fact that the /api/info endpoint (among others) obviously doesn't build the comment tree. Because of that, the replies aren't loaded. You'll want to do c = r.get_info(thing_id=\"t1_czrxngy\") c.refresh() Obviously, this counts as a second request. Now that in thinking about it a good way to solve this issue is with something similar that reddit does with attributes on their objects (albeit the ideology is a bit different since the defaults are preloaded into the database) If in objects.py a private attribute named `_needs_refresh` which would be a tuple of attribute names that potentially need a refresh to do, in the `__getattribute__` method: if attr in self._needs_refresh and not getattr(self, attr, None) and not self.has_refreshed: self.refresh() return getattr(self, attr)", "created_utc": 1454949872, "gilded": 0, "name": "t1_czsc92k", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/44qh61/unable_to_see_comment_replies_unless_i_get_them/"}, {"author": "Phteven_j", "body": "PRAW is great. Super easy to use. The tutorials are very thorough and easy to follow. You can use Heroku to deploy it for free once you get it going.", "created_utc": 1454642071, "gilded": 0, "name": "t1_czoekeb", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/447pp6/what_to_use_for_a_bot/"}, {"author": "SirCutRy", "body": "Thanks!", "created_utc": 1454643092, "gilded": 0, "name": "t1_czof899", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/447pp6/what_to_use_for_a_bot/"}, {"author": "BBQLays", "body": "If you're comfortable with REST, just use whatever you want. If you have C# experience, you'd probably be comfortable writing a bot in Node using TypeScript. [Here's an example of a bot I made with that setup.](https://github.com/martellaj/running-repostr)", "created_utc": 1454625803, "gilded": 0, "name": "t1_czo4eiw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/447pp6/what_to_use_for_a_bot/"}, {"author": "NewAlexandria", "body": "Isn't there a whole subreddit dedicated to bot design? I forget its name", "created_utc": 1454774353, "gilded": 0, "name": "t1_czq1tqw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/447pp6/what_to_use_for_a_bot/"}, {"author": "earth-tone", "body": "If you prefer Perl, there's [Reddit::Client](http://redditclient.readthedocs.org/en/latest/).", "created_utc": 1454625942, "gilded": 0, "name": "t1_czo4huo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/447pp6/what_to_use_for_a_bot/"}, {"author": "GoldenSights", "body": "I've got a tutorial about switching to OAuth [here](https://www.reddit.com/r/GoldTesting/comments/3cm1p8/how_to_make_your_bot_use_oauth2/). You only need to set up OAuth once, then you can put the credentials into a file that every other bot will import. Makes it super easy because each bot only needs `r = obot.login()` and the rest is handled automatically.", "created_utc": 1453877324, "gilded": 0, "name": "t1_czdlnj7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "allthefoxes", "body": "The bots use different accounts though so I am guessing that last bit won't work?", "created_utc": 1453877464, "gilded": 0, "name": "t1_czdlpco", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "GoldenSights", "body": "You could have a bunch of seperate \"login\" methods in the credential file, so each bot can call it's proper login. Or you could have one function that takes a username parameter and uses a dictionary to decide which credentials to use. Whatever the case, there should be some way of simplifying.", "created_utc": 1453905292, "gilded": 0, "name": "t1_czdug48", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "13steinj", "body": "If you want something even simpler (IMO), /u/SmBe19 made [`praw-OAuth2Util`](https://github.com/SmBe19/praw-OAuth2Util)", "created_utc": 1453907367, "gilded": 0, "name": "t1_czdvngk", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "lecherous_hump", "body": "There's another Oauth-mageddon? It was supposed to happen in August.", "created_utc": 1453917258, "gilded": 0, "name": "t1_cze2jju", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "allthefoxes", "body": "Now its this march", "created_utc": 1453917336, "gilded": 0, "name": "t1_cze2lr0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "mathguy54", "body": "What is the OAuth-mageddon?", "created_utc": 1453997801, "gilded": 0, "name": "t1_czf8bx7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "13steinj", "body": "People who haven't made their scripts and bots work with oauth (for whatever reason, there's a lot of tools and it's easy enough anyway) will be ratelimited after March 17th", "created_utc": 1454000417, "gilded": 0, "name": "t1_czfa72l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42w4l7/with_the_oauthmageddon_approaching_what_is_a/"}, {"author": "bsimpson", "body": "This is a known issue, should be fixed shortly.", "created_utc": 1453751192, "gilded": 0, "name": "t1_czbku54", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42lzes/praw_subredditget_top_from_week_isnt_returning/"}, {"author": "brain_emesis", "body": "Ok great thanks!", "created_utc": 1453752828, "gilded": 0, "name": "t1_czbm0nf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42lzes/praw_subredditget_top_from_week_isnt_returning/"}, {"author": "theonefoster", "body": "Have you tried testing it on multiple subreddits? It could just be that the top posts on the subreddit you're using are all from", "created_utc": 1453749829, "gilded": 0, "name": "t1_czbjuf8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42lzes/praw_subredditget_top_from_week_isnt_returning/"}, {"author": "brain_emesis", "body": "I've found it to be the same on all subreddits", "created_utc": 1453752860, "gilded": 0, "name": "t1_czbm1iw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42lzes/praw_subredditget_top_from_week_isnt_returning/"}, {"author": "zurtex", "body": "In general with Python you might want to do something like: try: ... except Exception as e: print(e) print(dir(e)) raise And that way you can see what the string of e says and what the properties associated with the exception are. Then you'll be able to figure out what is available and what isn't.", "created_utc": 1453587840, "gilded": 0, "name": "t1_cz9dfl0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "theonefoster", "body": "I tried print(e) and just got \"HTTPException\" and nothing more. I'll try print(dir(e)) - what does that do?", "created_utc": 1453588012, "gilded": 0, "name": "t1_cz9dj6e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "zurtex", "body": "dir(obj) prints out all the attributes of an object: https://docs.python.org/2/library/functions.html#dir This way you can see if it has \"response\" or \"response_code\" or whatever the exception object might contain. Equally you can then do dir(e.response) and see what attributes it has.", "created_utc": 1453588159, "gilded": 0, "name": "t1_cz9dm7w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "theonefoster", "body": "dir(e) returns ['__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__getitem__', '__getslice__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_raw', 'args', 'message'] `Message` is just \"HTTP Error.\"", "created_utc": 1453589180, "gilded": 0, "name": "t1_cz9e7p7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "GoldenSights", "body": "Check out the _raw attribute. This should be the actual exception raised by the `requests` module. `e._raw.status_code`, etc. The thing is, PRAW has custom exceptions for 404 and 403 -- `praw.errors.NotFound`, `praw.errors.Forbidden`. That might be something you missed while testing, because you don't need to use the response code to distinguish those.", "created_utc": 1453604982, "gilded": 0, "name": "t1_cz9mvz6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "zurtex", "body": "Then the praw exception hasn't been given a response code. Make sure it's up to date and otherwise think about filling a bug with them.", "created_utc": 1453592144, "gilded": 0, "name": "t1_cz9fwk4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/42d03i/how_can_i_access_the_specific_http_code_in_an/"}, {"author": "13steinj", "body": "For what /u/GoldenSights while you can used that to get the immediate parent; it unfortunately won't give you the full amount of info (replies for one, among something else that I am forgetting). To do that, you'd need to do # normally its safe to not split comment.permalink and instead just use comment.submission.permalink, but that property can be bugged, yet it's rare url = \"{0}/{1}\".format(comment.permalink.rsplit('/', 1)[0], comment.parent_id[3:] parent_comment = r.get_submission(url).comments[0] If you want to get multiple ancestor comments at once, you can get up to nine via ancestor_comment = r.get_submission(comment.permalink.rsplit('/', 1)[0], params={'context': 9}[0]).comments[0] (It's up to 9 cause 9 is the max in a context, and if you go overboard it gives you the max possible.) The ancestor comment, or simply the direct parent, would have the next \"generation\" younger as `replies[0]`, however, in a context link, you won't be able to navigate sibling branches. EX A --b ----e --c ----d If the original comment was d, and you got the context, you'd have a uniform branch of a, b, e, and you'd be unable to access c or d. If you need to access c or d, you can get the partial tree via ancestor_comment = r.get_submission(comment.permalink.rsplit('/', 1)[0], params={'context': 9}[0]).comments[0] ancestor_comment = r.get_submission(ancestor_comment.permalink).comments[0] However, that would make 2 requests.", "created_utc": 1453529191, "gilded": 0, "name": "t1_cz8pdqp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "Joshjayk", "body": "Would /u/GoldenSights method work if I know that I only need the parent comment and the parent comment above that? That is, I know for a fact that I will only need two parent comments above it. So: A --b ----c Those three comments are all I will need.", "created_utc": 1453533838, "gilded": 0, "name": "t1_cz8qxmj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "13steinj", "body": "Yes, but again some attributes aren't set, and I forget which ones aren't set. You may just want to do the context method, but replace 9 with 2", "created_utc": 1453555957, "gilded": 0, "name": "t1_cz8wap1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "Joshjayk", "body": "Alright. In that case, could you clarify a few things in your example code for me? 1. What exactly would your `url` variable look like? I can see it's joining the comment's permalink and the parent's ID, but what would an example of a comment permalink be, and why do you only want the `parent_id` after the first 3 characters? 2. I'm not really understanding what you meant when you were talking about sibling branches and what not. So if the comment that I had received from the comment stream was `d`, and I wanted to look at `c` then `A`, how exactly would I do that? In the example code you provided, you get the submission like you did before, but this time you don't include the `parent_id`. What's the reason for that?", "created_utc": 1453569002, "gilded": 0, "name": "t1_cz92amt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "13steinj", "body": "1. It would look like a normal comment permalink, and would be the permalink of the parent comment. The reason why you disregard the first 3 chars is because permalinks don't work with the tX\\_ prefix. Further more, you should make sure that the parent id isn't the id of the submission itself, because it theoretically can be and then you'd be grabbing a non existent parent. 2. You'd have to parse the replies backwards, recursively, so something like: def flat_branch(comment, reverse_root_id): if comment.replies[0].id == reverse_root_id: return [comment, comment.replies[0]] listed= [comment] listed.extend(flat_branch(comment.replies[0], reverse_root_id) return listed And then use `reversed(flat_branch(yourupmostcomment, therootyouarelpokingforsid))`. Keep in mind I haven't tested this so my recursive function may be wrong. This all said and done I have a longstanding pr because I unfortunately had to do this often and wanted to make this easier for people ([#512](https://github.com/praw-dev/praw/pull/512)), however combined with me being busy, it taking forever cause there's also a bug caused by an inconsistency on reddit's side, and my vm breaking a few days ago (but that last bit is just an excuse at this point), I haven't been able to continue.", "created_utc": 1453570484, "gilded": 0, "name": "t1_cz935mw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "GoldenSights", "body": "To get the immediate parent, check out the `parent_id` attribute, which will be a t1_ fullname. You can use that to get the parent object like this: parent = r.get_info(thing_id=comment.parent_id) If you want to go another layer up, you'd have to do the same thing again, using the parent this time. I think there's also be a way to create a ?context url so that you get multiple parent layers in a single request. /u/13steinj should know more about this, I don't remember how to do it.", "created_utc": 1453523709, "gilded": 0, "name": "t1_cz8n436", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "Joshjayk", "body": "So... comments = praw.helpers.comment_stream(randombot, 'all') for comment in comments: parent = r.get_info(thing_id=comment.parent_id) parent2 = r.get_info(thing_id=parent.parent_id) print('%s' % parent.body) if parent2 is not None: print('%s' % parent2.body) Would that snippet of code get me the parent of the current comment, and the parent to that parent, if it exists?", "created_utc": 1453533278, "gilded": 0, "name": "t1_cz8qrg6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "GoldenSights", "body": "That looks right to me. Just keep in mind that fetching the two parents above *every* comment in /r/all is not realistic. You won't be able to keep up with the stream. Give that a try on a smaller subreddit like /r/test and make sure it does what you're looking for.", "created_utc": 1453533727, "gilded": 0, "name": "t1_cz8qwe8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "Joshjayk", "body": "Yeah this is for a subreddit specific script! Also I just realized you're the same person who helped me with my last project, so thanks a bunch!!", "created_utc": 1453533893, "gilded": 0, "name": "t1_cz8qy7x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "GoldenSights", "body": "Sounds great! You're very welcome.", "created_utc": 1453534138, "gilded": 0, "name": "t1_cz8r0vn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/429bps/praw_getting_parent_comments/"}, {"author": "13steinj", "body": "GAME_THREAD.sticky()", "created_utc": 1453344515, "gilded": 0, "name": "t1_cz61pbb", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "spookyyz", "body": "Sorry, I shoulda included this, I tried that and it was throwing a HTTP error for me. Maybe I messed up somewhere, I'll give it another shot. Thanks.", "created_utc": 1453348063, "gilded": 0, "name": "t1_cz63wsk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "13steinj", "body": "What's the error?", "created_utc": 1453348403, "gilded": 0, "name": "t1_cz6440a", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "spookyyz", "body": "This is what it's throwing. Traceback (most recent call last): File \"test.py\", line 32, in GAME_THREAD.sticky() AttributeError: 'unicode' object has no attribute 'sticky' Here is the test code. login() submission_title = \"TO STICKY!\" submission_text = \"Pls sticky :(\" GAME_THREAD = REDDIT.submit(SUBREDDIT, submission_title, text=submission_text) GAME_THREAD.sticky()", "created_utc": 1453349669, "gilded": 0, "name": "t1_cz64tzr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "13steinj", "body": "It...shouldn't be a unicode object. Unless that's a custom submit function. What happens when you try to `print GAME_THREAD`?", "created_utc": 1453349879, "gilded": 0, "name": "t1_cz64yb0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "spookyyz", "body": "Returns: https://www.reddit.com/r/spookyyz/comments/41y9tq/to_sticky/ The URL to the submission, obviously.", "created_utc": 1453350606, "gilded": 0, "name": "t1_cz65cu5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "13steinj", "body": "Can you view that link in a browser under the account you are logged in as? If you can, and you are logged in via oauth on praw, do you have the `read` scope?", "created_utc": 1453350878, "gilded": 0, "name": "t1_cz65i4m", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "spookyyz", "body": "Ah crap, it might be a scope thing with oauth, I think I just gave it submit and identity. I think you're right. I'll give that a shot and report back. Edit: You are brilliant my friend, I don't know why I didn't think of checking my scope. Added read and modposts and it's good to go. Thanks so much.", "created_utc": 1453351108, "gilded": 0, "name": "t1_cz65mig", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "13steinj", "body": "No problemo", "created_utc": 1453351988, "gilded": 0, "name": "t1_cz662vs", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41x45c/praw_stickying_a_submission_posted/"}, {"author": "13steinj", "body": "It would help if you could give us your code exactly (GitHub gist preffered or pastebin) so we can actually deduce the issue instead of drawing from potentially imaginary straws.", "created_utc": 1453252233, "gilded": 0, "name": "t1_cz4nf0q", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41roji/praw_python_not_retrieving_info_when_executed/"}, {"author": "bAndkAllDay", "body": "Updated", "created_utc": 1453253620, "gilded": 0, "name": "t1_cz4oao4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41roji/praw_python_not_retrieving_info_when_executed/"}, {"author": "13steinj", "body": "I believe it's because you're trying to manually modify sys.stdout. Add what you want to print to a string in the for loop, and just do f = open(\"output.txt\",\"w\"), and at the end f.write(outstr), f.close() E: also, using print as a keyword is unreliable in my experience. You should import print_function from __future__ and then use it as a function in any case, then manually set the output file to print to.", "created_utc": 1453254961, "gilded": 0, "name": "t1_cz4p52e", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41roji/praw_python_not_retrieving_info_when_executed/"}, {"author": "bAndkAllDay", "body": "Thank you! That was the problem!", "created_utc": 1453255780, "gilded": 0, "name": "t1_cz4pnew", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41roji/praw_python_not_retrieving_info_when_executed/"}, {"author": "wanderingbilby", "body": "Python is popular for Reddit bots because there's a library already written specifically for Reddit and Python is a popular and relatively easy to use scripting language. I wrote my bot implementation directly in PHP - everything is handled by standard REST API queries so it's not difficult if you can find a REST library or are otherwise used to dealing with those sorts of calls. It looks like there's a Java REST client library from Apache called [HttpComponents](http://hc.apache.org/httpcomponents-client-ga/) that should simplify things for you. The hardest part for most people seems to be handling the initial OAuth query, once that's done it's just standard API calls.", "created_utc": 1453222294, "gilded": 0, "name": "t1_cz42ign", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "markerz", "body": "I would personally recommend OkHttp by Square over HttpComponents by Apache. OkHttp has a much simpler API in my opinion. Either way, I agree with what you said though rate limiting yourself can be tough if you're just starting out.", "created_utc": 1453275176, "gilded": 0, "name": "t1_cz4zaye", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "wanderingbilby", "body": "yay for real life experience :) I just grabbed the first likely google result. the times i've tried to learn java i've given up in a drunken rage at the end of three days, a half-finished \"hello wo\" on my screen.", "created_utc": 1453292093, "gilded": 0, "name": "t1_cz53tcl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "elihusmails", "body": "> The hardest part for most people seems to be handling the initial OAuth query 100% agree", "created_utc": 1453226630, "gilded": 0, "name": "t1_cz45jkk", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "SmBe19", "body": "If you don't want to handle OAuth yourself you can use something like [this](https://github.com/SmBe19/praw-OAuth2Util).", "created_utc": 1453580085, "gilded": 0, "name": "t1_cz98x9m", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "itsthejoker-bot", "body": "I found your comment at exactly the right time as I was getting so irritated with the OAuth part. Thank you so much!", "created_utc": 1453601108, "gilded": 0, "name": "t1_cz9kure", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "13steinj", "body": "1. No idea 2. [JRAW?](https://github.com/reddit/reddit/wiki/API-Wrappers) 3. I recommend python since you don't need the power. If you learn, try to learn both at the same time, as werird as that sounds. For the most part they are the same, or syntax in 3 is fully backwards compatible into 2, so focus on 3, and anything else do via `six`, which is a compatibility library. If you wish of course.", "created_utc": 1453218138, "gilded": 0, "name": "t1_cz3zsrg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "elihusmails", "body": "Thanks for the response. My first bot is going to mine the saved posts in my account. I'm not sure that JRAW supports that. After thinking about it some more, I would rather do this in Python so that I can learn it.", "created_utc": 1453218625, "gilded": 0, "name": "t1_cz403kp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "kemitche", "body": "If you're familiar with HTTP, you can spin up your own PRAW-like library for personal use (minus some bells and whistles) fairly easily. The trickiest part is figuring out the response format of API calls (since only inputs are documented on [/dev/api](/dev/api)) but a few example calls will get you most of the way (and peeking at PRAW source could help you too). OAuth 2 can be a tricky beast, but if it's just for a bot (i.e., you don't need access to more than a single account, and you control the credentials to that account), then you can use the password grant type to get your access token, which is a single call and should be straightforward.", "created_utc": 1453240131, "gilded": 0, "name": "t1_cz4fbvv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "markerz", "body": "For the responses, there's this: https://github.com/reddit/reddit/wiki/JSON For Oauth, there's this: https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example", "created_utc": 1453275334, "gilded": 0, "name": "t1_cz4zcvm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "kemitche", "body": "Thanks for picking up the slack on my laziness :)", "created_utc": 1453314490, "gilded": 0, "name": "t1_cz5gtgl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "methamp", "body": "I started with Python about 45 days ago. Would recommend. I now play with reddit bots and twitter bots in py.", "created_utc": 1453244955, "gilded": 0, "name": "t1_cz4iq0i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "earth-tone", "body": "[Perl!](http://redditclient.readthedocs.org/en/latest/)", "created_utc": 1453225203, "gilded": 0, "name": "t1_cz44iwf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/41otyb/python_or_java_for_bot_development/"}, {"author": "13steinj", "body": "You'd need to check the score attribute of the comment object. The individual up/down votes stopped being part of the api response sometime last year.", "created_utc": 1452919419, "gilded": 0, "name": "t1_cz03n8u", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/416r43/praw_how_to_get_the_most_downvoted_comments_from/"}, {"author": "rubynew", "body": "It'll be something like this for overall comment score: import praw r = praw.Reddit(user_agent='my_cool_application') submissions = r.get_subreddit('learnprogramming').get_top_from_day(limit=15) for submission in submissions: print submission.title print submission.url print \"=\" * 30 submission.comments = sorted(submission.comments, key=lambda x: x.score) for comment in submission.comments[0:6]: print comment.body print \"score: \" + str(comment.score) print \"#\" * 10 print \"\\n\" * 3 output will look like this http://hastebin.com/duduyiquve.txt You can also use flat_comments = praw.helpers.flatten_tree(submission.comments) if you want more than just top level comments.", "created_utc": 1452999536, "gilded": 0, "name": "t1_cz13wt1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/416r43/praw_how_to_get_the_most_downvoted_comments_from/"}, {"author": "shaggorama", "body": "There is no package on pypi called \"nsfw\". Go back to github and look for a file or folder called \"nsfw\". This is probably a module specific to the code you are copying. This isn't a redditdev-specific question. If you need further assistance, you should post questions to /r/learnpython. EDIT: If you're copying [this code](https://github.com/Timidger/autowikiabot-py/blob/master/autowikiabot-commenter.py), it looks like the line that's giving you problems is actually commented out in the current version. Also... I don't know what you are trying to accomplish, but just from scanning this code, I don't think you want to be copying this repository, especially if you're just now learning. This \"smells\" like bad code. I'm seeing a lot of unidiomatic stuff going on. What are you trying to accomplish?", "created_utc": 1452697300, "gilded": 0, "name": "t1_cywneea", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "PieCrafted", "body": "I am attempting to restart the autowikibot. I am using this source: https://github.com/acini/autowikibot-py", "created_utc": 1452698383, "gilded": 0, "name": "t1_cywo15x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "13steinj", "body": "Also slowly attempting to restrat the bot; but I'm updating it first. See: /r/autowikipedia.", "created_utc": 1452712432, "gilded": 0, "name": "t1_cywy0a2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "shaggorama", "body": "I'm guessing that \"nsfw\" is a file that this person just never saw fit to commit to the repo. Frankly, you might be better off just starting from scratch. Maybe use this code as inspiration or to help you solve unexpected problems you might encounter (cause this guy encountered them already). But I think the fact that you've encountered a problem like this literally on the second line of this script is a hint that maybe you should bail. Or contact acini on github: looks like he's still active on that account, even if he hasn't updated that repo in a while.", "created_utc": 1452699767, "gilded": 0, "name": "t1_cywowfr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "13steinj", "body": "The autowikiabot repo in general is a tad effed up. I'm currently updating it locally from the original branch in an attempt for /r/autowikipedia", "created_utc": 1452697955, "gilded": 0, "name": "t1_cywns0a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "shaggorama", "body": "Yeah, no offense (if this was originally yours?) but that code is a little gross. Here are a few tips to help you clean it up: * You could replace that entire \"find_link()\" function with something like this: link_pat = re.compile('href=\"(.*?)\"') links = link_pat.findall(text) * Those globals everywhere are concerning. It would almost certainly make more sense to attach those globals and the functions that call them to a class. * The long strings of \"if-elif-elif-elif...\" could probably be simplified by either using a dict to handle different conditional behaviors, or using demorgan's law to simplify the actual conditional tests. * Regular expressions that get used in multiple places could just get compiled once and then passed around. This would both speed up your code and also make it a lot easier to read. Even if you're not compiling them, just attaching them to named variables would significantly improve readability. * That long sequence of \"html.replace\" calls in the \"reddify\" function could be simplified by looping over a dict or a list of 2-tuples, such that each key/value pair (or item in the list) defines the target of replacement and what it should be replaced with. something like: reddify_map = {'<b>': '__', '</b>': '__', ...} for k, v in reddify_map.iteritems(): html = html.replace(k,v, html)", "created_utc": 1452699215, "gilded": 0, "name": "t1_cywojr1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "13steinj", "body": "It's not mine. It's acini's. Whoever acini is. I think he dropped off of reddit a long time ago. And yes I know; it gives me headaches as well. I'm just a guy slowly updating the original repository to work properly again for /r/Autowikipedia; and just reading it is a bit difficult. On top of that wikipedia has changed their API responses slightly so I need to update those to. ANd use oauth, and move to python3.", "created_utc": 1452701758, "gilded": 0, "name": "t1_cywq89f", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "merreborn", "body": "https://www.reddit.com/r/autowikibot/comments/3dc0af/autowikibot_has_retired/ > It started as an experiment for me to learn python and get introduced to programming That explains a lot. I have to agree: a first-timer is probably better off starting from scratch, rather than trying to take on code written by another first-timer.", "created_utc": 1452702464, "gilded": 0, "name": "t1_cywqpvm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "13steinj", "body": "Probably need to change `from nsfw import getnsfw` to `from .nsfw import getnsfw`", "created_utc": 1452697078, "gilded": 0, "name": "t1_cywn9vo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40s7gb/help_with_reddit_bot_importerror_no_module_named/"}, {"author": "GoldenSights", "body": "Basically, what's happening is this: - _use_oauth is False - User makes a request - If the request needs OAuth, set _use_oauth to True - Request is made with OAuth headers - Set _use_oauth back to False - _use_oauth is False The problem is that certain exceptions during the request can break this system such that _use_oauth is never set back to False, and the assertion fails every time after that. The bandaid solution is to place `r._use_oauth = False` above the line that is raising the assertionerror. The better solution is to figure out which exceptions are breaking through, and making sure PRAW resets this variable automatically. This is something I've been meaning to do but haven't dedicated the time yet.", "created_utc": 1452649286, "gilded": 0, "name": "t1_cyw3akw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "tusing", "body": "Thank you!", "created_utc": 1452649698, "gilded": 0, "name": "t1_cyw3jzz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "GoldenSights", "body": "I haven't fully read through your code, and I don't have a lot of experience with making a threadsafe PRAW script, but there's also the possibility that one request is starting while the other one is still working, meaning the variable hasn't been toggled back off yet. If that sounds like something your bot might be doing, you should look into preventing that with custom locks or [praw-multiprocess](http://praw.readthedocs.org/en/stable/pages/multiprocess.html).", "created_utc": 1452650107, "gilded": 0, "name": "t1_cyw3tlw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "tusing", "body": "Thanks. It seems to happen even when using praw-multiprocess. I'm kinda new to programming on Python on this level, and I'm not sure how to set _use_oauth to False in this file, since ```r``` is not passed along. Do you have any ideas on how I might implement it?", "created_utc": 1452652780, "gilded": 0, "name": "t1_cyw5jp0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "GoldenSights", "body": "Well, I notice that the QueueStrategy class has a `self.r` attribute created from the `r` parameter, but the QueueThread class does not. Why not add that as a parameter to QueueThread, and pass it along on [this line](https://github.com/tusing/reddit-ffn-bot/blob/master/ffn_bot/reddit/queues.py#L123)? Then you could put `self.r._use_oauth = False` in between [these two lines](https://github.com/tusing/reddit-ffn-bot/blob/master/ffn_bot/reddit/queues.py#L92-L93).", "created_utc": 1452654079, "gilded": 0, "name": "t1_cyw6efk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "tusing", "body": "Thanks! On that note, how can I determine what I need to set _use_oauth false for? Should it be set to true after that? Sorry if these questions are a bit basic. I'm kinda new to it all.", "created_utc": 1452654296, "gilded": 0, "name": "t1_cyw6jja", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "GoldenSights", "body": "No, it shouldn't be set to True after that. Like my bullet points above, the variable is supposed to be false at all times until PRAW temporarily sets it while making a request. When it's True, you get these AssertionErrors. As far as where you should put it... anywhere that makes the crashes stop. This is something you shouldn't have to be dealing with in the first place, just use it to seal up the cracks. I can see you've updated the GitHub copy, so as long as that works you don't need to be doing it anywhere else yet. Again, the only time I get these AssertionErrors is when I'm having connectivity issues and the requests bail out unexpectedly. If you're getting them in otherwise good circumstances, it might be a result of a non-threadsafe program.", "created_utc": 1452655180, "gilded": 0, "name": "t1_cyw73yk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "LunarMist2", "body": "How can an exception break though? The `_use_oauth = True` is part of finally statements. Or am I missing something?", "created_utc": 1453881067, "gilded": 0, "name": "t1_czdmx30", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "GoldenSights", "body": "To be honest, I haven't actually pinpointed which exceptions cause this error to break out :/ It seems to be very rare, and when it happens it's hard to catch what happened before that put it into the wrong state. If it happens to you, definitely let me know!", "created_utc": 1453923736, "gilded": 0, "name": "t1_cze7e98", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "LunarMist2", "body": "Well, I came upon this thread because it's happening for me :P. I am trying to pinpoint the problem as well, though it's rather difficult to debug as it happens rarely, as you've said. I DO run multiple instances of it though, all running concurrently with gevent. Though it should not pose a problem since i use separate Reddit() objects for each one.", "created_utc": 1453926941, "gilded": 0, "name": "t1_cze9sug", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40pehq/bot_freezing_due_to_oauthrelated_assertionerror/"}, {"author": "GoldenSights", "body": ">If I change the 'all' to a specific subreddit it'll work In that case, it sounds like the bot simply can't keep up with the /r/all/comments stream, which moves super fast. If you're using OAuth, you're allowed to make 1 request per second, though PRAW still uses the default of 2. You can override it like this: wordbot.config.api_request_delay = 1 Also, since you're using the same compiled regex each time, why not store it in a variable? Calling `findWholeWord` so many times kind of defeats the purpose of compiling. If it still can't find comments, what subreddits have you been testing on? edit: Oh! I just realized you're doing `str(comment)`. Well, take a look at this: >>> str(c) 'This makes me curious, how do electronic tickets work for will call? I feel l...' >>> c.body 'This makes me curious, how do electronic tickets work for will call? I feel like an email system for will call could be quite the hassle.' >>> You're not getting the full text when you do that.", "created_utc": 1452553577, "gilded": 0, "name": "t1_cyum3w0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40jfcf/praw_matching_a_keyword_in_comment_stream/"}, {"author": "Joshjayk", "body": "Okay I tried changing the delay and changing str(c) to c.body. I was just testing on i think /r/testingground4bots. Also, how would I store it in a variable? Would I just do: wordFound = re.compile(r'\\b({0})\\b'.format(word), flags=re.IGNORECASE).search Where word = the variable with the assigned word?", "created_utc": 1452554596, "gilded": 0, "name": "t1_cyumr4h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40jfcf/praw_matching_a_keyword_in_comment_stream/"}, {"author": "GoldenSights", "body": "Yeah, then you can use `wordFound(c.body)`. Is it working better after those changes?", "created_utc": 1452555345, "gilded": 0, "name": "t1_cyun7ws", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40jfcf/praw_matching_a_keyword_in_comment_stream/"}, {"author": "Joshjayk", "body": "No for the regular expression line. You said store the \"re.compile(r'\\b({0})\\b'.format(w), flags=re.IGNORECASE).search\" in a variable. How would that look?", "created_utc": 1452563443, "gilded": 0, "name": "t1_cyus6qz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40jfcf/praw_matching_a_keyword_in_comment_stream/"}, {"author": "GoldenSights", "body": "You had it! Put it above the for loop, like this: word = \"random\" wordFound = re.compile(r'\\b({0})\\b'.format(word), flags=re.IGNORECASE).search comments = praw.helpers.comment_stream(wordbot, 'all') for comment in comments: if wordFound(comment.body) is not None:", "created_utc": 1452563781, "gilded": 0, "name": "t1_cyusewn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40jfcf/praw_matching_a_keyword_in_comment_stream/"}, {"author": "kamac496", "body": "Nevermind. Found it. Do this: submission = self.r.get_submission(submission_id=self.submissionId, comment_sort=\"new\") instead of this: submission = self.r.get_submission(submission_id=self.submissionId) Ta daaah.", "created_utc": 1452544780, "gilded": 0, "name": "t1_cyufzi3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/40ipg4/praw_get_recent_comments_from_a_submission/"}, {"author": "kemitche", "body": "Something along these lines: [https://oauth.reddit.com/r/redditdev/search?sort=top&q=timestamp%3A1451606400..1454284800&restrict_sr=on&syntax=cloudsearch](https://www.reddit.com/r/redditdev/search?sort=top&q=timestamp%3A1451606400..1454284800&restrict_sr=on&syntax=cloudsearch) \\* Note: I linked to the www version so you can click through and see the results on the website.", "created_utc": 1452126833, "gilded": 0, "name": "t1_cyoqqgq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zs8un/not_praw_whats_the_query_to_get_all_posts_between/"}, {"author": "Pokechu22", "body": "The important part is that you need to add `&syntax=cloudsearch` to the end. Then you just search for `timestamp:time1..time2` and it works (note that the timezone is a bit strange - it's not UTC IIRC).", "created_utc": 1452490764, "gilded": 0, "name": "t1_cytrelp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zs8un/not_praw_whats_the_query_to_get_all_posts_between/"}, {"author": "GoldenSights", "body": "Out of curiosity, does this have anything to do with PyCharm? Does running the file in a terminal / cmd change things? Also, does this script crash immediately, or does it choke on a certain subreddit? If it's a certain subreddit, can you print the names to find out what it is?", "created_utc": 1452116007, "gilded": 0, "name": "t1_cyoj4nh", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "Almenon", "body": "No, i tested it outside of pycharm too. I don't think the subreddit matters since json.load loads them in random order (no idea why) and the error happens around the same time. I'll check to make sure. EDIT: not subreddit dependent. seems to happen at random times early on in the iteration EDIT 2: I found it. It happens when I pass in a subreddit with a question mark at the end of the name. PRAW should have caught the error however. I think i'll submit it as a bug to the github page.", "created_utc": 1452116450, "gilded": 0, "name": "t1_cyojh8t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "GoldenSights", "body": "Hmm, I don't know. Using Python 3.5.0, PRAW 3.3.0, I'm not having any trouble fetching subreddit objects like this. Maybe someone else can toss in some ideas. As for the json order, Dictionaries are called unordered types, where the elements are ordered based on a hash, which can change from session to session. Check out these SO threads: http://stackoverflow.com/questions/526125/why-is-python-ordering-my-dictionary-like-so http://stackoverflow.com/questions/15479928/why-is-the-order-in-python-dictionaries-and-sets-arbitrary", "created_utc": 1452117370, "gilded": 0, "name": "t1_cyok6mm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "Almenon", "body": "Really? All it took for me was r.get_subreddit('csimiami?',fetch=True). I read up on python hash tables, they use random probing when a collision happens, which explains the randomness. Thanks for the links.", "created_utc": 1452119029, "gilded": 0, "name": "t1_cyoleoh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "bboe", "body": "You've stumbled upon some undefined behavior as a subreddit cannot have a `?` in its name. I'm not sure why it's doing what it's doing, but if you want to make a PR with a proposed fix I'd be happy to look at it.", "created_utc": 1452139603, "gilded": 0, "name": "t1_cyoyiz7", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "13steinj", "body": "To be clear, is this code being imported somewhere (used as a separate thread it seems too)? Since you are using pycharm can you attach the debugger in the active console and enable breakpoints on any python exception, then see the local variables in __getattr__ and the stack trace?", "created_utc": 1452137989, "gilded": 0, "name": "t1_cyoxks4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "Almenon", "body": "**Variables**: attr = {str} '_has_fetched' self = {subreddit} Unable to get repr for **Stack Trace** get\\_\\_subreddit in init.py -> objects line 1528 -> objects line 71 -> \\_populate in objects.py -> \\_post\\_populate in objects .py -> \\_\\_getattr__ (I copied each frame from the frames panel, but is there a way to copy the entire trace at once or to print it out?) **Summary** has\\_fetch is initialized to the result of \\_populate, which calls \\_post\\_populate, which calls \\_\\_getattr__, which tries to find the value of the nonexistent has_fetch by calling itself.", "created_utc": 1452143921, "gilded": 0, "name": "t1_cyp0td7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "13steinj", "body": "Hmmm. Does this happen on the very first subreddit in the list or a later one? In any case if you don't mind telling, which subreddit? E: oh I'm blind. It's the ? In the name, which can't actually occur. Definitely the wrong error though. Perhaps an elegant fix would be to check if invalid characters are in the subreddit name, and if they are, raise InvalidSubreddit on initialization.", "created_utc": 1452144484, "gilded": 0, "name": "t1_cyp1346", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "Almenon", "body": "It doesn't matter. You can call any subreddit with a ? and it will happen. In the list it happens with the first subreddit that has a ?. For example, you can invoke the error with just r.get_subreddit('csimiami?',fetch=True)", "created_utc": 1452144669, "gilded": 0, "name": "t1_cyp16ax", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "13steinj", "body": "Yes, but actual subreddits can't have a ? In their name.", "created_utc": 1452145847, "gilded": 0, "name": "t1_cyp1q1t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "GoldenSights", "body": "You're right about that, but it should be raising a 404 instead of causing this recursion error. Personally I'm still not sure what's causing it.", "created_utc": 1452183659, "gilded": 0, "name": "t1_cypftre", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "Almenon", "body": "Of course. I'm not sure why the list i was using had a question mark in it.", "created_utc": 1452146672, "gilded": 0, "name": "t1_cyp234r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrwso/recursion_error_when_using_praw_to_iterate/"}, {"author": "GoldenSights", "body": "PRAW actually includes a config variable for watching request activity: `r.config.log_requests`. By default it is 0, but setting it to 1 or 2 will print a message every time a request is being made (1 tells you the URL being used, 2 also tells you the status code when it's done). Here's an example that looks kind of like your scenario: >>> r.config.log_requests=1 >>> subreddit = r.get_subreddit('redditdev') >>> >>> hot = subreddit.get_hot() >>> >>> submissions = list(hot) substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/redditdev/.json >>> >>> submissions[0] >>> submissions[0].id '3zrucp' >>> submissions[0].comments substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/redditdev/comments/3zrucp/praw_having_trouble_figuring_out_when_requests/.json [] >>> Notice that 1. You're correct that `get_hot` simply prepares the generator, but does not create requests right away. 2. You're incorrect about the comments. The json for /hot, /new, etc do NOT include any comment data. You can view it in your browser here: https://www.reddit.com/r/redditdev/hot.json. PRAW must fetch the submission page directly. Hope that helps!", "created_utc": 1452115295, "gilded": 0, "name": "t1_cyoikih", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrucp/praw_having_trouble_figuring_out_when_requests/"}, {"author": "NotCalledBill", "body": "Thanks, this is a perfect breakdown for me. I suspect the `log_requests` variable will come in handy to me again in the future too.", "created_utc": 1452116038, "gilded": 0, "name": "t1_cyoj5jg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zrucp/praw_having_trouble_figuring_out_when_requests/"}, {"author": "GoldenSights", "body": "Does `r.get_subreddit('my_private_subreddit', fetch=True)` work? It's unusual to use `r.request` with actual URLs in PRAW. I also notice that the request which 403'd did not print the \"substituting oauth...\" message like the other ones did, so this may give us a hint as to what the problem is. But first, let's see if the other methods work normally.", "created_utc": 1451928834, "gilded": 0, "name": "t1_cylp357", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "body": "Thanks for helping. `r.get_subreddit('my_private_subreddit', fetch=True)` does work: >>> import mybot >>> r=mybot.login() substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> r.get_subreddit(\"\", fetch=True) substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r//about/.json status: 200 Subreddit(subreddit_name='') ---- >It's unusual to use r.request with actual URLs in PRAW. Oh, I didn't know that. I thought that was the only way to use it: https://praw.readthedocs.org/en/stable/pages/code_overview.html#praw.__init__.BaseReddit.request https://praw.readthedocs.org/en/stable/pages/code_overview.html#praw.__init__.BaseReddit.request_json I need to use `request_json()` with an actual URL because one of the JSON feeds I want is `https://www.reddit.com/r//about/traffic/.json`, which does not have a method like `get_contributors()` which `https://www.reddit.com/r//about/contributors/.json`has. >I also notice that the request which 403'd did not print the \"substituting oauth...\" message like the other ones did I noticed that too. Hopefully that will help.", "created_utc": 1451929974, "gilded": 0, "name": "t1_cylpuot", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "GoldenSights", "body": "Great, I'm glad that worked! I've actually never used the json version of /traffic before. I'm assuming those numbers are [timestamp, uniques, pageviews, subscribers], right? When I get home I'll see if it's something we can add to PRAW, I feel like it won't be too much trouble to implement the basics. `request` and `request_json` are used internally by other methods, and if you have to use it manually it probably means we're missing a feature. If you scroll around some more on that docs page you'll see all kinds of other functions. Just FYI, you don't need `fetch=True` for most cases. It's useful here because `r.get_subreddit` does not actually make any web requests until you do something on the subreddit object. The Fetch parameter forces the request right away. Let me know if you have any other questions!", "created_utc": 1451931126, "gilded": 0, "name": "t1_cylqmzg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "body": "That would be amazing! Thank you! I assumed there was a reason a `get_traffic()` method wasn't provided, and I didn't want to bother anyone with feature requests. `r.request_json(\"https://www.reddit.com/r/AskReddit/about/traffic/.json\")` works without errors for public subreddits which make traffic public. For some reason traffic URLs aren't documented on https://www.reddit.com/dev/api, but you're right about the format for the first key, `day`. There is also `hour` and `month`. You can compare these links in your browser to work out the values: - https://www.reddit.com/r/askreddit/about/traffic - https://www.reddit.com/r/askreddit/about/traffic.json (with something like [JSONView](https://chrome.google.com/webstore/detail/jsonview/chklaanhfefbnpoihckbnefhakgolnmc)) I did it below. The JSON response is one object with three keys, `day`, `hour`, and `month`. Each key has an array of arrays. Each inner array holds 3 or 4 integers. { day: [ [ 1451865600, // Timestamp, midnight of day (Mon, 04 Jan 2016 00:00:00 GMT) 750711, // Uniques for day 2733228, // Pageviews for day 9554 // Subscriptions for day ], [ 1451779200, // Timestamp, midnight of day (Sun, 03 Jan 2016 00:00:00 GMT) 1316557, // Uniques for day 5072635, // Pageviews for day 13566 // Subscriptions for day ], ... ], hour: [ [ 1451934000, // Timestamp, start of hour (Mon, 04 Jan 2016 19:00:00 GMT) 0, // Uniques for hour (0 until hour ends) 0 // Pageviews (0 until hour ends) ], [ 1451930400, // Timestamp, start of hour (Mon, 04 Jan 2016 18:00:00 GMT) 122922, // Uniques for hour 282349 // Pageviews for hour ], ... ], month: [ [ 1451606400, // Timestamp, midnight first day of month (Fri, 01 Jan 2016 00:00:00 GMT) 2829756, // Uniques for month 15342830 // Pageviews for month ], [ 1448928000, // Timestamp, midnight first day of month (Tue, 01 Dec 2015 00:00:00 GMT) 19867073, // Uniques for month 157554074 // Pageviews for month ], ... ] } ---- All I want is the raw JSON response. But other PRAW users might want the object functionality the other get_() methods have. For what it's worth, I've added a third Interpreter example to my original post. In case you or anyone else wants to investigate the original problem. Thank you once again.", "created_utc": 1451940828, "gilded": 0, "name": "t1_cylxjwn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "GoldenSights", "body": "There we go: https://github.com/praw-dev/praw/pull/575 Your explanation of the endpoint helped, thank you!", "created_utc": 1452113952, "gilded": 0, "name": "t1_cyohjiu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "body": "Thanks for taking the time to do this! Sadly, the new `get_traffic()` method is affected by the problem this thread mentions. It works perfectly for public subreddits (not requiring OAuth) but the first call that requires OAuth (e.g. private subreddits) still returns the 403 error. Subsequent calls work fine. >>> import mybot >>> r=mybot.login() substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json status: 200 >>> mybot.get_traffic(\"AskReddit\") # Public subreddit, works fine GET: https://api.reddit.com/r/askreddit/about/traffic/.json status: 200 {'hour': [[ >>> mybot.get_traffic(\"\") # Private subreddit moderated by account, does not work, returns 403 on first call GET: https://api.reddit.com/r//about/traffic/.json status: 403 >>> mybot.get_traffic(\"\") # Exact same call requiring OAuth, second time, works fine substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r//about/traffic/.json status: 200 {'hour': [[ Like you mentioned earlier, the first call requiring OAuth (which returned 403) does not substitute the subdomain. It made the call to `https://api.reddit.com`. The second call substituted properly and made the call to `https://oauth.reddit.com` ---- In your pull request you mentioned you couldn't determine if it's meant to be used with a particular OAuth scope. I investigated this, and there is an undocumented scope called `modtraffic`. On the authorization page the description is \"Access traffic stats in subreddits I moderate.\", but it doesn't work. Requesting the traffic for a private subreddit the account moderates using this scope returns `praw.errors.OAuthInsufficientScope`. There is another scope called `modconfig`. This is the OAuth scope that authorizes access to the traffic, even though the description is \"Manage the configuration, sidebar, and CSS of subreddits I moderate.\".", "created_utc": 1452172450, "gilded": 0, "name": "t1_cyp9s2o", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "GoldenSights", "body": "Hi, sorry for the delay, Thanks for showing me that `modconfig` was the correct scope after all. I tried a whole bunch of different scopes, but I must have forgotten that or had some other problem when I tried it. I've opened [a new PR](https://github.com/praw-dev/praw/pull/576) to add that in. This also makes it so the request passes on the first try because PRAW is sending the right oauth headers now. You should start dissecting the PRAW source code, I think you would make some great pull requests! You're a good troubleshooter and you like to explain things, we can always use more contributors.", "created_utc": 1452283090, "gilded": 0, "name": "t1_cyqzd10", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "body": "No need to apologize for any delay, your help is very much appreciated. `get_traffic()` works perfectly for the first OAuth request now. It took me a few minutes to figure out that the scope is `modconfig`. I don't know if that's intentional. Thank you for the compliment! I believe if someone is helping you with open source, the least you can do is give them as much detail as possible. To be honest, I have taken on far too many commitments recently and I have no extra time. Even this PRAW script is too much right now. I'm very impressed by the responsiveness and helpfulness of PRAW contributors, and I've added it to my list of open source projects to dive into when I have more time. You're a great representative for the project. Thanks again for your help. Have a great weekend!", "created_utc": 1452302545, "gilded": 0, "name": "t1_cyrbwiz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "kemitche", "body": "~~You're setting the **wrong domain**. Use oauth.reddit.com, not www.reddit.com.~~ Edit: Didn't see the second example on my first read. It looks then like praw or oauth2util aren't setting headers properly until the first request has failed.", "created_utc": 1451933414, "gilded": 0, "name": "t1_cyls7v2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "body": "Thanks for helping. Unfortunately that doesn't solve the problem. In example **2** above, you'll see I get the same 403 error when requesting `https://oauth.reddit.com`. The same happens with full URLS like `https://oauth.reddit.com/r/`. And the same happens when I substitute `oauth` for every `www` in example **1**. I also added example **3** to the original post, if it helps.", "created_utc": 1451941170, "gilded": 0, "name": "t1_cylxsoo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "kemitche", "body": "Ah I see now. Sorry for mis-reading! There's definitely something else going on in PRAW or OAuth2Util there.", "created_utc": 1451946259, "gilded": 0, "name": "t1_cym1fyj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "body": "No problem at all! Thanks for looking. I have tested without OAuth2Util (using the tutorial linked in the original post) and it's exactly the same, so it's something with PRAW. /u/GoldenSights mentioned above that `request()` is usually used internally, so I guess PRAW does some other internal set up before calling it. It's just unfortunate because it works perfectly after that first 403 error. Maybe there is an easy fix. Out of curiosity, [regarding my comment above](https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/cylxjwn), is there a reason the subreddit traffic endpoint isn't documented at https://www.reddit.com/dev/api?", "created_utc": 1451948903, "gilded": 0, "name": "t1_cym38l3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "kemitche", "body": "The only reason things don't get documented is someone forgot, didn't take the time, or it's an older endpoint that was missed when adding the decorators for documentation. Add docs is a pretty trivial PR, so I'm sure it'd be easy to get updated if you wanted to add the proper decorators to the traffic endpoint function.", "created_utc": 1451949174, "gilded": 0, "name": "t1_cym3ewl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "Developx", "body": "I see. I just wondered if it was deprecated or not intended for public use, but that makes sense. Thanks again!", "created_utc": 1451949623, "gilded": 0, "name": "t1_cym3pgf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "kemitche", "body": "API endpoints have to be explicitly whitelisted for use via OAuth and given an appropriate OAuth scope, so if it's OAuth accessible it's allowed (barring any announcements about deprecations)", "created_utc": 1451950480, "gilded": 0, "name": "t1_cym49ja", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3zfnsf/403_forbidden_on_first_praw_request_or_request/"}, {"author": "GoldenSights", "body": "I'm not having any trouble getting a stickied link, so I think the issue must be something else. Can you share the subreddit / submission you're trying to retrieve?", "created_utc": 1451761176, "gilded": 0, "name": "t1_cyjircz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "askLubich", "body": "I was accessing a sub called '/r/asozialesnetzwerk' and [this](https://www.reddit.com/r/asozialesnetzwerk/comments/3ypn6b/schnapspralinen_f\u00fcrs_volk_fr\u00fch_\u00fcbt_sich/) is the post which was sticky when the error occurred. In the meantime, the sticky post has changed and the bot is running again.", "created_utc": 1451761711, "gilded": 0, "name": "t1_cyjj2i1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "13steinj", "body": "In addition to what u\\/GoldenSights asked, what python version are you running this on?", "created_utc": 1451761310, "gilded": 0, "name": "t1_cyjiu63", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "askLubich", "body": "I am using Python 2.7.11 (PC) and 2.7.9 (Raspberry Pi) together with Praw 3.3.0 (both). There error occurs on both devices.", "created_utc": 1451762060, "gilded": 0, "name": "t1_cyjj9xj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "13steinj", "body": "I see you've already solved the issue, but since I'm curious, what was the full traceback of the error?", "created_utc": 1451769718, "gilded": 0, "name": "t1_cyjnllw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "GoldenSights", "body": "I'm guessing it's a Python 2 issue then, because I purposely tested a submission \"\u00fc\" in the title, but I use Python 3. As far as workarounds go, the two stickies are always the first two items in the /hot listing, so you can access them that way if you need to.", "created_utc": 1451763591, "gilded": 0, "name": "t1_cyjk625", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "askLubich", "body": "Thanks, this works! It must have been due to the umlaut.", "created_utc": 1451764569, "gilded": 0, "name": "t1_cyjkq0e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "pcjonathan", "body": "This is very similar to the [issue](https://github.com/praw-dev/praw/issues/473) I reported a while ago when dealing with unicode characters.", "created_utc": 1451762364, "gilded": 0, "name": "t1_cyjjgdp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "askLubich", "body": "I am starting to think, this might rather be related to the use of an umlaut (\u00e4\u00fc\u00f6) in the post's title than the fact that it was a link.", "created_utc": 1451763610, "gilded": 0, "name": "t1_cyjk6hc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z57b4/praw_using_get_sticky_if_a_link_is_sticked_causes/"}, {"author": "micwallace", "body": "Use separate oauth ids and they will be separate.", "created_utc": 1451707227, "gilded": 0, "name": "t1_cyiynan", "num_comments": null, "score": 10, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z3ff3/two_bots_one_ip/"}, {"author": "lecherous_hump", "body": "This is another reason to use Oauth because I'm pretty sure without it, all requests from that machine will share the same cap.", "created_utc": 1451740882, "gilded": 0, "name": "t1_cyj99lm", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z3ff3/two_bots_one_ip/"}, {"author": "micwallace", "body": "I can't confirm but yes, the caps are probably done on a per-IP or per-session basis.", "created_utc": 1451741197, "gilded": 0, "name": "t1_cyj9ctp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z3ff3/two_bots_one_ip/"}, {"author": "esquilax", "body": "> Two bots, one IP? Dude, NSFW...", "created_utc": 1451778457, "gilded": 0, "name": "t1_cyjsfi2", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z3ff3/two_bots_one_ip/"}, {"author": "RemindMeBotWrangler", "body": "If you're going to use praw and worry about doing too many requests, you can do this http://praw.readthedocs.org/en/stable/pages/multiprocess.html", "created_utc": 1451720732, "gilded": 0, "name": "t1_cyj4akv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z3ff3/two_bots_one_ip/"}, {"author": "13steinj", "body": "Have you tried specifying the string as unicode, e.g., `u'Hol\u00e0 blah blah`?", "created_utc": 1451693445, "gilded": 0, "name": "t1_cyiroxh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z2pgs/praw_send_message_special_characters/"}, {"author": "RemindMeBotWrangler", "body": "The issue is that the character changes(at least in print function). `\u00fc` becomes `\u00c3\u00bc` for example when I do .encode(\"utf-8\"). Which will break URLs.", "created_utc": 1451694700, "gilded": 0, "name": "t1_cyiscft", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z2pgs/praw_send_message_special_characters/"}, {"author": "13steinj", "body": "Instead of `.encode(\"utf-8\")`, have you tried something similar to reddit's `_force_unicode`? [Reference](https://github.com/reddit/reddit/blob/c902b602933b0e02a6aa7f5364f517260617650c/r2/r2/lib/unicode.py#L24)", "created_utc": 1451700591, "gilded": 0, "name": "t1_cyivcyp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z2pgs/praw_send_message_special_characters/"}, {"author": "RemindMeBotWrangler", "body": "Thanks! It worked.", "created_utc": 1451701372, "gilded": 0, "name": "t1_cyivr2u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3z2pgs/praw_send_message_special_characters/"}, {"author": "GoldenSights", "body": "It pretty much always happens, and it's okay. See here: https://github.com/praw-dev/praw/issues/329", "created_utc": 1451598114, "gilded": 0, "name": "t1_cyhp8d5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yy8ai/error_in_praw_resourcewarning_unclosed/"}, {"author": "Albuyeh", "body": "But I already have the following in my file def fxn(): warnings.warn(\"deprecated\", DeprecationWarning) with warnings.catch_warnings(): warnings.simplefilter(\"ignore\") warnings.simplefilter(\"ignore\", ResourceWarning) fxn() Why would the error still show up?", "created_utc": 1451614410, "gilded": 0, "name": "t1_cyhxm6o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yy8ai/error_in_praw_resourcewarning_unclosed/"}, {"author": "GoldenSights", "body": "In that example, does your \"deprecated\" warning get silenced properly? If so, then the code is working how it's supposed to. The ResourceWarning always happens when the script terminates, which is after your catch_warnings context exits. According to `help(warnings.catch_warnings)`, any filters applied during the context are removed at the end, thus you have nothing blocking the RW. You can move that `simplefilter('ignore')` line to the top of your script so it is always active, then you shouldn't see the warning any more.", "created_utc": 1451645625, "gilded": 0, "name": "t1_cyi7nb9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yy8ai/error_in_praw_resourcewarning_unclosed/"}, {"author": "Albuyeh", "body": "Would adding `>> Cron.txt 2>&1` to end of my file be the reason? I know the `2>&1` means output STDERR to file as well but I am not sure if `simplefilter('ignore')` would suppress it", "created_utc": 1451670477, "gilded": 0, "name": "t1_cyieynx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yy8ai/error_in_praw_resourcewarning_unclosed/"}, {"author": "GoldenSights", "body": "Wait, what problem are we trying to solve? Like I said, the unclosed socket stuff is completely normal with PRAW, and to suppress it all you need to do is put `warnings.simplefilter('ignore')` at the top of your script. Does this stop the warnings properly? `>> Cron.txt 2>&1` is not valid Python, so when you say you put it at the end of your file, I'm not sure what you mean.", "created_utc": 1451690084, "gilded": 0, "name": "t1_cyipvy9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yy8ai/error_in_praw_resourcewarning_unclosed/"}, {"author": "not_an_aardvark", "body": "Actually, PRAW has a [lazy object](https://praw.readthedocs.org/en/stable/pages/lazy-loading.html) model, so it won't make any more requests than it needs to. With this line of code: `users = [i.author.id for i in cms]` The issue is that you're getting each author's *id*, not their username. This is why it needs to make a separate request for each author. Assuming that's actually what you want, using a different wrapper won't really help with that. However, if you're just looking for the *usernames* of the authors, you can do this: `users = [i.author.name for i in cms]` ...and it should run much faster, since it doesn't need to make a new request for each username.", "created_utc": 1451555986, "gilded": 0, "name": "t1_cyh68e6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "BearlyBreathing", "body": "Ah, I see. I did play around a bit with that, and it seemed that referencing any attribute of 'author' was triggering an API call (has_fetched: false -> true). I didn't notice .name was an exception. Thanks for pointing that out, it should help a lot. Is it documented somewhere which attribute trigger calls and which don't?", "created_utc": 1451599919, "gilded": 0, "name": "t1_cyhq6ka", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "not_an_aardvark", "body": "Basically, it depends on what information has already been sent by reddit. For example, if I load this thread, I can immediately see that your username is `BearlyBreathing`. However, I don't know some other information (other posts you've submitted, your trophies, your comment karma, etc.) just from viewing this thread -- I have to go to your user profile page to figure that out. It's sort of the same thing with the API. PRAW fetches the thread from reddit, and gets a JSON response that looks like [this](/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/.json). One of the keys in the JSON response is: `\"author\": \"BearlyBreathing\"` So it creates a `Redditor` object with `name = \"BearlyBreathing\"`. All of the other properties (e.g. `get_submitted`, `id`) will have `has_fetched = False`, which means that another api request will be required to obtain them.", "created_utc": 1451600756, "gilded": 0, "name": "t1_cyhqmt2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "13steinj", "body": "No matter what you do on a code level, you won't see many speed gains. That's because the code is still executed one by one. You'll want to use python's multiprocessing module, starting multiple processes in what *appears* to be succession (e.g. same as normal code), but the multiprocessing module will execute the different processes in consecutive order, instead of after the previous completes. Praw also has some tools for multiprocessing, but I don't know how they work nor have I ever attempted to do something like this with praw.", "created_utc": 1451536900, "gilded": 0, "name": "t1_cygzgim", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "ThisIs_MyName", "body": "This is the right long-term solution. Run multiple instances in parallel.", "created_utc": 1451584189, "gilded": 0, "name": "t1_cyhh0cq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "Snapzz_911", "body": "reroute md5 ks2?", "created_utc": 1451633921, "gilded": 0, "name": "t1_cyi519u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3yu6nj/praw_need_to_speed_up_big_data_collection_praw/"}, {"author": "dops", "body": "change `level=logging.DEBUG` to another log level. As you can see from the the [python docs](https://docs.python.org/2/library/logging.html#logging-levels) the log levels are Level | Value ---|--- CRITICAL | 50 ERROR | 40 WARNING | 30 INFO | 20 DEBUG | 10 NOTSET | 0 The lower the level (and thus, value) the more information that is going to be logged. I would suggest that you use *at least* WARNING (but I would go for DEBUG) for testing and ERROR for production but it is down to you. Edit: No such thing as a stupid noob question so my advice is ask away and learn but you could have found this out by RTFM or a simple google, try that first because it tends to stick better (at least it does for me).", "created_utc": 1451184230, "gilded": 0, "name": "t1_cyccybn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ycb4r/praw_stop_praw_from_writing_requests_to_log/"}, {"author": "bboe", "body": "PRAW doesn't use python's built-in logging mechanism (it should in a future major version). Its stdout-based logging is configured via the config variable `log_requests` as described in: http://praw.readthedocs.org/en/stable/pages/configuration_files.html One way to disable logging is by setting `r.config.log_requests = 0`. That value is `0` by default, so you probably have it explicitly enabled somewhere in your code, or in a praw.ini file.", "created_utc": 1451327005, "gilded": 0, "name": "t1_cye0dg6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ycb4r/praw_stop_praw_from_writing_requests_to_log/"}, {"author": "13steinj", "body": "1. Do not use that while loop. Off the top of my head it would fuck things up, and it's not really needed, because if there are no more comments it will do it, and it has its own \"built in\" while loop similar to that. 2. Comments are a list tree. Submission.comments is a list of comments, and each comment has a .replies attribute that is a list of comments in reply to that comment. You'll want to use `comments = praw.helpers.flatten_tree(submission.comments)` and then iterate over that new variable.", "created_utc": 1450974364, "gilded": 0, "name": "t1_cya4f2s", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "Squid__", "body": "1. I guess I misunderstood the documentation? replace_more_comments automatically makes the number of API calls needed and doesn't need to be called more than once? 2. I only wanted usernames from parent level comments so I didn't flatten them, that's working as intended. I'm trying to figure out why some parent level comments aren't showing up in the thread, even when clicked on through a users profile.", "created_utc": 1450974704, "gilded": 0, "name": "t1_cya4llq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "13steinj", "body": "1. As far as I know, yes. 2. In this sense, that's weird. I'm guessing either a timing issue? Or maybe you found a bug with replace_more_comments? If you can try to reproduce on a small scale, May be worth making an issue on the github", "created_utc": 1450974935, "gilded": 0, "name": "t1_cya4q49", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "Squid__", "body": "I'm not sure what you mean by timing issue but both of the users posted within a few hours of the original post, and I ran the script ~20 hours after the post. They 100% should have been picked up. I ran the script again and they still aren't showing up, so I'm fairly certain it isn't an issue with my code, and I'm pretty sure it isn't PRAW's fault.", "created_utc": 1450981565, "gilded": 0, "name": "t1_cya8c4c", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "Yiin", "body": "If the users can see their own comment in the thread or if the mods can, they were probably removed for whatever reason.", "created_utc": 1450985090, "gilded": 0, "name": "t1_cyaa8kz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "Squid__", "body": "The two posts do contain \"pls\", maybe the r/HHH mods have automod setup to remove posts containing that phrase. I'll have to ask them, thanks.", "created_utc": 1450985226, "gilded": 0, "name": "t1_cyaab5s", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "Yiin", "body": "If you do something like this again, I'd think it'd be worth trying to get temporary modded.", "created_utc": 1450989476, "gilded": 0, "name": "t1_cyachuy", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3y24fg/issue_with_getting_all_usernames_from_a_thread/"}, {"author": "13steinj", "body": "There's no proper way to get around the 1k limit; end of story. Search is unreliable at best unfortunately, and that will never account for comments nor saves. If you want to get a specific time slice; eg; a section of the 1k posts you retrieved within a timeframe; you could use `filter` but I don't know the syntax for that off the top of my head; so here's how to do it manually: mintime = maxtime = Things = list(r.get_*(limit=None)) #* would be user_submitted, saved, etc splicedThings = [t for t in Things if (t.created_utc >= mintime and t.created_utc", "created_utc": 1450675811, "gilded": 0, "name": "t1_cy6b48x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "washerdreier", "body": "Yeah, I'm specifically asking for slices under the 1k limit to avoid refetching posts. For example, if I wanted to fetch my saves since the last time I fetched/archived them. So a query that is bound by either a time slice or a placeholder/reference post like clockstalker did. Is this still possible with PRAW and the API and I just can't figure out the syntax, or was it removed so you can only fetch N", "created_utc": 1450702247, "gilded": 0, "name": "t1_cy6iv60", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "13steinj", "body": "You were never able to get a time based slice. This was never in praw. However you could have filled in the url parameters `after` and `before` as mentioned [here](/dev/api) via params={dictionary of url params}.", "created_utc": 1450703322, "gilded": 0, "name": "t1_cy6j7ld", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "washerdreier", "body": "There's my problem! I kept using `urldata={'after':last}` instead of `params`...... dang that was really bugging me and I must have kept reading the docs wrong... Thanks!", "created_utc": 1450748949, "gilded": 0, "name": "t1_cy7aapw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "haripriyagireesh", "body": "Could you please make it clear?. I am in a situation to collect more than 10000 post from a subreddit. Evenif I collected 100 post , I don't know how to collect another 100 post other than already collected ones inorder to remove redundancy.?", "created_utc": 1452938642, "gilded": 0, "name": "t1_cz09two", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "ClaySm1le", "body": "", "created_utc": 1450679983, "gilded": 0, "name": "t1_cy6cs7r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xnl2o/praw_how_to_get_submissions_comments_saves_etc/"}, {"author": "13steinj", "body": "> also because I like bragging about my work Ledle But on to the bug thing, it takes some time. And the nsfw one was kinda confirmed a loooong time ago.", "created_utc": 1450485220, "gilded": 0, "name": "t1_cy40g6b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xem6j/psa_ive_just_added_a_praw_helper_that_allows_you/"}, {"author": "bboe", "body": "Thanks for the awesome contribution!", "created_utc": 1450498753, "gilded": 0, "name": "t1_cy46z2x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3xem6j/psa_ive_just_added_a_praw_helper_that_allows_you/"}, {"author": "47Toast", "body": "Why do you have r.login in the while-loop?", "created_utc": 1450274627, "gilded": 0, "name": "t1_cy0zflw", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3x2sy7/keep_getting_ratelimitexceeded_when_my_bot/"}, {"author": "thatJavaNerd", "body": "That's what stood out to me too. Reddit doesn't like frequent authentication so this may be your problem", "created_utc": 1450281725, "gilded": 0, "name": "t1_cy13dj1", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3x2sy7/keep_getting_ratelimitexceeded_when_my_bot/"}, {"author": "Pokechu22", "body": "Bots are not allowed to upvote comments automatically.", "created_utc": 1450307564, "gilded": 0, "name": "t1_cy1lag2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3x2sy7/keep_getting_ratelimitexceeded_when_my_bot/"}, {"author": "13steinj", "body": "That's praw's docs. Not reddit's. Also, every object has the attributes with the same names of the json keys as provided by reddit, which is listed in reddit's documentation on github. I'd link that but I'm on mobile.", "created_utc": 1450070244, "gilded": 0, "name": "t1_cxydgor", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3wq68t/documentation_doesnt_list_attributes_of_redditor/"}, {"author": "sunbolts", "body": "Thank you. I'll check that out.", "created_utc": 1450109660, "gilded": 0, "name": "t1_cxyrcgf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3wq68t/documentation_doesnt_list_attributes_of_redditor/"}, {"author": "spookyyz", "body": "Very nicely documented, this would have been pretty useful for me about 3 weeks ago :) But, the PRAW documentation is pretty solid too, so I made do.", "created_utc": 1450217326, "gilded": 0, "name": "t1_cy09q1x", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3wpn6e/quick_buildabot_tutorial/"}, {"author": "busterroni", "body": "Nice writeup :)", "created_utc": 1450062100, "gilded": 0, "name": "t1_cxy8rfn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3wpn6e/quick_buildabot_tutorial/"}, {"author": "SyscalineGaming", "body": "Kind of irrelevant, but don't you mean /r/requestabot and not /u/requestabot?", "created_utc": 1450643971, "gilded": 0, "name": "t1_cy5t1sv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3wpn6e/quick_buildabot_tutorial/"}, {"author": "gavin19", "body": "When you installed pip (assuming you did), it only installs PRAW (or any module) for the version you specified (or whichever was the current version at the time). Easiest way that I know of is to sudo apt-get install python3-pip then you can pip3 install praw to have it install PRAW for 3. You should also be able to use python3 -m pip install praw without installing anything.", "created_utc": 1449745029, "gilded": 0, "name": "t1_cxtyv4k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3w7h97/praw_cant_get_it_to_work_with_python_3/"}, {"author": "xiongchiamiov", "body": "Did you install Python manually, outside of the package manager? Removing `/usr/local/bin/python3` and pointing it elsewhere smells very suspicious to me. As an aside, you can do that in one command: `sudo ln -snf /usr/bin/python3 /usr/local/bin/python3`", "created_utc": 1449776022, "gilded": 0, "name": "t1_cxufkis", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3w7h97/praw_cant_get_it_to_work_with_python_3/"}, {"author": "BlackFayah", "body": "Oh nice, good to know! I installed Python3 using apt-get, that's all I know..", "created_utc": 1449782950, "gilded": 0, "name": "t1_cxukjem", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3w7h97/praw_cant_get_it_to_work_with_python_3/"}, {"author": "xiongchiamiov", "body": "You probably shouldn't have `/usr/local/bin/python3` at all; if you remove it, `which python3` should point to `/usr/bin/python3`. It probably should *anyways*, since generally I think `/usr/bin` should appear before `/usr/local/bin` in your `PATH`. From what I can gather from your post, you somehow ended up with two different versions of Python installed, and were using the shell of one while praw was installed into the other; that's why I was asking about installing multiple versions of python. If you've still got multiple `pip3`s hanging around, it'd be useful to investigate and see where they go. You can also see what package (if any) a file belongs to by using `dpkg -S /path/to/file`.", "created_utc": 1449784406, "gilded": 0, "name": "t1_cxulk8f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3w7h97/praw_cant_get_it_to_work_with_python_3/"}, {"author": "joe-murray", "body": "If you are authenticated via PRAW you can get saved links via the following command (in Python): saved_links = r.user.get_saved() I'm not sure what object is returned but you should be able to tinker with it and figure out exactly how to strip out links from the correct subreddit. For example, pseudocode might look something like this: desired_links = [] saved_links = r.user.get_saved() for link in saved_links: if link.subreddit == 'news': desired_links.append(link)", "created_utc": 1449452349, "gilded": 0, "name": "t1_cxpry4q", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "Dashing_in_the_90s", "body": "Hi, Thanks for the help. This works perfectly fine but it will only return a maximum of 1000 saved posts due to a limit of reddit's api. If you have reddit gold a drop down appears at the top of your saved list that lets you select a subreddit then only show saved posts from that specific subreddit. This allows you to see up to 1000 saved posts from that subreddit and bypass the 1000 save limit from the main list basically letting you view 1000 saves per subreddit. I was asking if there is a way to get saved posts from the subreddit specific lists rather than the main list. Sorry if I wasn't clear in my main post.", "created_utc": 1449454162, "gilded": 0, "name": "t1_cxpszrc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "joe-murray", "body": "Oh, I understand now. I don't know if that is directly possible through PRAW. If it is, you might be able to just pass the name of the subreddit as an argument to the get saved function like this: saved_links = r.user.get_saved(subreddit='news') If that doesn't work, you will have to do it via data scraping. The fact that data requires authentication might make the scraping more difficult but I'm not 100% sure because I haven't tried that before.", "created_utc": 1449461225, "gilded": 0, "name": "t1_cxpwx9c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "GoldenSights", "body": "Try this: s=r.user.get_saved(params={'sr':'Askreddit'}) I have very few saved posts, but this is looking correct to me", "created_utc": 1449458635, "gilded": 0, "name": "t1_cxpvimc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "Dashing_in_the_90s", "body": "Thank you. That works perfectly turns out I wasn't using the params field properly.", "created_utc": 1449459390, "gilded": 0, "name": "t1_cxpvxm0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "13steinj", "body": "Keep in mind it only works if the account has gold.", "created_utc": 1449462003, "gilded": 0, "name": "t1_cxpxbzi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vqb21/using_praw_can_i_get_my_saved_posts_from_a/"}, {"author": "bionikspoon", "body": "That source code was not written to be python2 compatible--from here, it looks like it requires python3. In python3; the arguments for `super()` became optional. Authors not concerned with compatibility drop the arguments purely for aesthetics.", "created_utc": 1449390688, "gilded": 0, "name": "t1_cxoybe4", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "reseph", "body": "Bah. All my servers are Python2 (comes with CentOS and RedHat, and is the latest version on Yum)", "created_utc": 1449391004, "gilded": 0, "name": "t1_cxoyebm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "SmBe19", "body": "I'll take a look and try to fix it. I think the bug was introduced in the latest version, so if you use 0.3.1 you should be fine for the moment.", "created_utc": 1449404022, "gilded": 0, "name": "t1_cxp18mx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "13steinj", "body": "Now, I'm not a big fan of ever using super, especially since it confuses me a lot. But I'm 99% sure there needs to be two arguments defined. The first, the class. The second, the current object (self). Don't know why it's bugging out though. Maybe it had an update that I don't know about? /u/SmBe19", "created_utc": 1449372474, "gilded": 0, "name": "t1_cxoqtxn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "SmBe19", "body": "Thanks for notifying me. There was indeed an update two weeks ago that introduced this bug. I should test PR's better...", "created_utc": 1449407483, "gilded": 0, "name": "t1_cxp21jv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "13steinj", "body": ":P Or just add some simple test cases", "created_utc": 1449413071, "gilded": 0, "name": "t1_cxp3qqo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "micwallace", "body": "I don't know python, but the Java super() essentially executes the super-class method of the same name. Is it the same kind of thing?", "created_utc": 1449381892, "gilded": 0, "name": "t1_cxovgbr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "GoldenSights", "body": "Nope! Raymond Hettinger gave a PyCon talk called [Super Considered Super](https://www.youtube.com/watch?v=EiOglTERPEo), where he says that \"super\" should have been called \"next-in-line\", because it works very differently. Even after watching it, I still don't like using super very much.", "created_utc": 1449434704, "gilded": 0, "name": "t1_cxpg34x", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "13steinj", "body": "> Even after watching it, I still don't like using super very much. Me neither. What I do (and coincidentally, many times I've also seen this in reddit's codebase): class BaseClass(object): def __init__(self, arg, arg2): # something class BigClass(BaseClass): def __init__(self, arg, arg2, arg, arg4): BaseClass.__init__(self, arg, arg2) # do more shit Much easier for me to follow.", "created_utc": 1449465919, "gilded": 0, "name": "t1_cxpz9eo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "GoldenSights", "body": "Ooh, that is much easier! I've never worked on any really complicated inheritance problems, so when I watched Hettinger's talk maybe two weeks ago I was very surprised. For a language that stresses clarity, this sure is confusing for anybody who's used a different language before. If only there was a way to super-init without having to call the `__init__` method directly. All of the other magic methods work behind the scenes (`len(x)` using `x.__len__()`) so I always feel wrong using it up-front like that.", "created_utc": 1449468104, "gilded": 0, "name": "t1_cxq0727", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "13steinj", "body": "Is super actually considered a magic method? Regardless, I'd say the reasoning for having to manually specify `__init__()` are as follows: you aren't necessarily calling the parent (or ancestors') method of the same name that you are currently in. Theoretically, you could call `super().foo()` inside an init method, because in the BigClass you want what happens within foo to occur during initialization, but you don't want it to occur on initialization of the BaseClass, but you are *also* redefining `foo` in the BigClass. When such a way would ever be used reasonably, I don't know since I've never done it. But I guess it can be useful.", "created_utc": 1449469053, "gilded": 0, "name": "t1_cxq0kp3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "GoldenSights", "body": "Sorry, I didn't mean that Super was a magic method, I was talking about `__init__`, and having to call that manually when normally we don't call the double-underscore names manually. You're right about why we do it, I just wish there was a prettier way to run the super's constructor.", "created_utc": 1449469406, "gilded": 0, "name": "t1_cxq0pp0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "micwallace", "body": "OMG your right that is confusing.", "created_utc": 1449450382, "gilded": 0, "name": "t1_cxpqrs6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "micwallace", "body": "Thanks for filling me in and providing a con talk on the matter. I love contalks.", "created_utc": 1449449154, "gilded": 0, "name": "t1_cxpq0w4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "SmBe19", "body": "As /u/bionikspoon pointed out the problem was with python2. I fixed the problem now and the new version is now live. Sorry for the trouble.", "created_utc": 1449407411, "gilded": 0, "name": "t1_cxp20v0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "reseph", "body": "So the error is fixed, but now my code hangs when trying to run (at the OAuth2Util line). Ever seen that before? Is there a way to turn on debug msgs? I've got my key and secret in the ini file Might be stuck in an infinite loop in configparser.py on lines 1297-1299?", "created_utc": 1449442541, "gilded": 0, "name": "t1_cxplj94", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "SmBe19", "body": "You can use ` o = OAuth2Util.OAuth2Util(r, print_log=True)` to get some more information. I never saw this bug though. If you want you can post your censored oauth.ini then I can take a look.", "created_utc": 1449444301, "gilded": 0, "name": "t1_cxpmpb4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "reseph", "body": "Freezes here: Refresh Token Request new Token (REF) oauth.ini: [app] \\# These grant the bot to every scope, only use those you want it to access. scope=identity,account,edit,flair,history,livemanage,modconfig,modflair,modlog,modothers,modposts,modself,modwiki,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote,wikiedit,wikiread refreshable=True app_key=xxxxxx app_secret=xxxxx [server] server_mode=False url=127.0.0.1 port=65010 redirect_path=authorize_callback link_path=oauth \\# Will be filled automatically [token] token=None refresh_token=None valid_until=0", "created_utc": 1449447540, "gilded": 0, "name": "t1_cxpoxp2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "SmBe19", "body": "In the step it freezes it should open the browser with the confirmation dialog and waits until you confirm it. Did your browser open and did you confirm the request from reddit?", "created_utc": 1449480100, "gilded": 0, "name": "t1_cxq3x4h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "reseph", "body": "How does that work in SSH?", "created_utc": 1449497244, "gilded": 0, "name": "t1_cxq8pxm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "SmBe19", "body": "If the server is configured accordingly you can try [server mode](https://github.com/SmBe19/praw-OAuth2Util/blob/master/OAuth2Util/README.md#server-mode). Otherwise you have to start the script locally the first time and then upload the `oauth.ini` with the tokens in it to the server.", "created_utc": 1449498926, "gilded": 0, "name": "t1_cxq9igh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "reseph", "body": "Good point, thanks. I'll look at that. I assume you configured accordingly, you mean the page shouldn't be set as localhost?", "created_utc": 1449501268, "gilded": 0, "name": "t1_cxqaqwx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "SmBe19", "body": "`127.0.0.1` is the same as localhost. If you want to run it on the server using server mode, you have to change the url to the IP Address or URL of your server and adjust the port to one of your choice.", "created_utc": 1449501463, "gilded": 0, "name": "t1_cxqauwt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "reseph", "body": "Thanks!", "created_utc": 1449427885, "gilded": 0, "name": "t1_cxpbd0f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3vm3q6/oauth2util_typeerror_super_takes_at_least_1/"}, {"author": "wanderingbilby", "body": "I haven't used PRAW, but it looks like the subreddit.get_comments() call does a [GET /r/subreddit/top](https://www.reddit.com/dev/api#GET_top) call, which returns a [listing](https://www.reddit.com/dev/api#listings) which contains (among other things) the thing name. You can then use an [info call](https://www.reddit.com/dev/api#GET_api_info) to get more data... possibly. Unfortunately the API documentation falls a bit short here so you'll have to do some experimenting. Sorry it's not a complete solution, hopefully that gets you on the right path at least.", "created_utc": 1449077001, "gilded": 0, "name": "t1_cxkk9q6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lumbdi", "body": "Thanks I will look into it tomorrow.", "created_utc": 1449077117, "gilded": 0, "name": "t1_cxkkcpa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lecherous_hump", "body": "If you have the parent post's ID and the comment ID, you can generate a permalink by omitting the title portion of the URL, ala https://www.reddit.com/r/redditdev/comments/3v5muu//cxkkcpa", "created_utc": 1449082850, "gilded": 0, "name": "t1_cxkoiwb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lumbdi", "body": "**3v5muu** That's not the parent post id. That's the thread id. I don't know how to get both. I only know how to get the post id which I got through 'get_comments(limit=100)'.", "created_utc": 1449085174, "gilded": 0, "name": "t1_cxkq732", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lecherous_hump", "body": "There's no such thing as a \"thread id\". There's just the post ID. Do you mean the comment ID?", "created_utc": 1449085992, "gilded": 0, "name": "t1_cxkqtbc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lumbdi", "body": "https://www.reddit.com/r/redditdev/comments/3v5muu https://redd.it/3v5muu 3v5muu", "created_utc": 1449086416, "gilded": 0, "name": "t1_cxkr4mq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lecherous_hump", "body": "That's the post ID. It's called `link_id` by the API. People use the word \"post\" informally but technically Reddit calls them links.", "created_utc": 1449086956, "gilded": 0, "name": "t1_cxkrj1s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lumbdi", "body": "How do I retrieve the link_id?", "created_utc": 1449087209, "gilded": 0, "name": "t1_cxkrq0k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lecherous_hump", "body": "I would imagine it's part of a Comment object, but I don't use PRAW. It's part of the data sent back by the API for a comment, so it should be.", "created_utc": 1449087377, "gilded": 0, "name": "t1_cxkrurg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "lumbdi", "body": "Thanks. Got it. Editing solution into main post.", "created_utc": 1449088094, "gilded": 0, "name": "t1_cxkse8u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "ryyaa7", "body": "\u0627\u0643\u0644", "created_utc": 1449341285, "gilded": 0, "name": "t1_cxo98kg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v5muu/how_to_generate_a_link_to_the_comment/"}, {"author": "Triplanetary", "body": "Have you considered fetching the comment's JSON from the reddit server? Just do a GET request for the comment's URL + .json. It won't be a serialized PRAW object, of course, but you can get the comment's PRAW object at any later time by using get_info on the comment's name, as provided by the JSON data.", "created_utc": 1449010216, "gilded": 0, "name": "t1_cxjoqar", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "helpmewebbit", "body": "Fetching the comment's json using comment permalink URL + .json actually gives me a lot of superfluous data such as the submission's json and the children of the comment. I haven't actually tried doing this programatically yet, but so far this solution isn't looking as neat as I'd like it to be. Another reason I'm so keen on using the PRAW dict object is that it'll avoid me from having to make too many requests to the server. I mean, I already have all the data I want/need. The only problem is that I want to trim the \"replies\" field in some simple and consistent way. EDIT: For instance, here's a [permalink.json](https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/cxjoqar.json) to your comment while below is a praw.objects.Comment object's json_dict attribute: { 'parent_id' : 't3_3v1yuv', 'distinguished' : None, 'subreddit_id' : 't5_2qizd', 'removal_reason' : None, 'likes' : None, 'id' : 'cxjoqar', 'name' : 't1_cxjoqar', 'edited' : False, 'body' : \"Have you considered fetching the comment's JSON from the reddit server? Just do a GET request for the comment's URL + .json. It won't be a serialized PRAW object, of course, but you can get the comment's PRAW object at any later time by using get_info on the comment's name, as provided by the JSON data.\", 'archived' : False, 'mod_reports' : [], 'ups' : 1, 'downs' : 0, 'link_id' : 't3_3v1yuv', 'created' : 1449039016.0, 'subreddit' : 'redditdev', 'score_hidden' : False, 'user_reports' : [], 'approved_by' : None, 'report_reasons' : None, 'created_utc' : 1449010216.0, 'banned_by' : None, 'gilded' : 0, 'replies' : { 'kind' : 'Listing', 'data' : { 'before' : None, 'children' : [ ], 'modhash' : '', 'after' : None } }, 'author_flair_text' : None, 'saved' : False, 'author_flair_css_class' : None, 'body_html' : 'Have you considered fetching the comment's JSON from the reddit server? Just do a GET request for the comment's URL + .json. It won't be a serialized PRAW object, of course, but you can get the comment's PRAW object at any later time by using get_info on the comment's name, as provided by the JSON data.\\n', 'score' : 1, 'controversiality' : 0, 'num_reports' : None, 'author' : 'Triplanetary' }", "created_utc": 1449012430, "gilded": 0, "name": "t1_cxjq6xr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "Triplanetary", "body": "Wait, if you have the json_dict data, can't you just try serializing that alone?", "created_utc": 1449013525, "gilded": 0, "name": "t1_cxjqw7s", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "helpmewebbit", "body": "I could but if replies contains something like `` like above then python throws a TypeError: Unserializable. ... Having said that, I feel really stupid now because I've *just* realised I can actually remove that field programmatically with `dict.pop(\"replies\", None)`. It's kind of hacky and it will break if the field in Reddit's json data changes, but I think it's good enough for me. Thanks for all the help though!", "created_utc": 1449013754, "gilded": 0, "name": "t1_cxjr1hx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "Triplanetary", "body": "Glad you got it working!", "created_utc": 1449014091, "gilded": 0, "name": "t1_cxjr97v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "kemitche", "body": "> it will break if the field in Reddit's json data changes Well nearly every reddit app would break if that happened, so I don't think it'll change (except perhaps by accident)", "created_utc": 1449017630, "gilded": 0, "name": "t1_cxjti0j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": "shaggorama", "body": "Copy the object and remove the problematic attributes before serializing.", "created_utc": 1449065134, "gilded": 0, "name": "t1_cxkcz3b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3v1yuv/how_to_serialize_prawobjectscomment_in_a/"}, {"author": null, "body": "[deleted]", "created_utc": 1448569316, "gilded": 0, "name": "t1_cxe0oa6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "theonefoster", "body": "It does. If you hover over the submission title in the comments section, the link appears in the bottom right (on Chrome) with .jpg at the end. Right-clicking and copying the link address also copies a link with .jpg. Following the link also navigates to an image only with .jpg in the url, rather than an imgur page without. Except calling the api link from praw finds an imgur page url with no .jpg at the end - see the highlighted attribute in the image.", "created_utc": 1448569539, "gilded": 0, "name": "t1_cxe0skc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "SandyRegolith", "body": "> If you hover over the submission title in the comments section, the link appears in the bottom right (on Chrome) with .jpg at the end. Send us a screen shot. I'm hovering over it right now and all I get is http://imgur.com/KC2vjp7 -- what do you mean by \"the submission title in the comment section\"? Something other than the title at the top of the page? What specifically are you hovering over? But really, the answer to your question is \"imgur doesn't care about file extensions\". They are meaningless in that context: * http://i.imgur.com/KC2vjp7.gif * http://i.imgur.com/KC2vjp7.png * http://i.imgur.com/KC2vjp7.jpg * http://i.imgur.com/KC2vjp7.jpeg are all the same thing. While file extensions are important on your desktop computer, on a server, you're allowed to play fast and loose and map them to anything and nothing.", "created_utc": 1448570043, "gilded": 0, "name": "t1_cxe12gl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "theonefoster", "body": "As per [this comment](https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/cxe1010) and subsequent investigation, I've found that the culprit is RES. It seems to be converting indirect links into direct links at the page-source level. I'd imagine there's a setting to disable this behaviour but I've not checked yet.", "created_utc": 1448570399, "gilded": 0, "name": "t1_cxe19nc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "Walter_Bishop_PhD", "body": "Yeah, I've ran in to this before and it's made enforcing \"direct image link\" rules pretty hard to enforce with some mods thinking the link is a proper direct link. Oddly enough, the link on thumbnails is not tampered with so you can hover over that to see the \"proper\" link", "created_utc": 1448571380, "gilded": 0, "name": "t1_cxe1sc5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": null, "body": "[deleted]", "created_utc": 1448569792, "gilded": 0, "name": "t1_cxe0xj8", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "theonefoster", "body": "I only have RES. That's weird :\\ I'll try disabling it but I wouldn't have thought that was the problem. [What about this one?](https://www.reddit.com/r/gaming/comments/3udxzg/such_amazing_black_friday_deals/) For me that is also a .jpg link, but [the api claims differently and has no .jpg](https://api.reddit.com/r/gaming/comments/3udxzg/such_amazing_black_friday_deals/) --- edit: disabled RES and now I'm getting the same link as shown in the API. Weird? Thank you!", "created_utc": 1448569922, "gilded": 0, "name": "t1_cxe1010", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "Pokechu22", "body": "And **this** is why I hate RES. [RES adds JPG to the end of all imgur links](https://www.reddit.com/r/RESissues/comments/3egemb/res_changes_links_to_images_on_imgurcomid_to/?ref=search_posts), and that breaks GIFV, among other things. But the actual link is just an imgur post link, not the direct image link. If you want to imitate RES-like behavior (pls don't), here is the [original javascript](https://github.com/honestbleeps/Reddit-Enhancement-Suite/blob/v4.5.4/lib/modules/showImages.js#L2104-L2175) for it: imgur: { domains: ['imgur.com'], options: { 'prefer RES albums': { description: 'Prefer RES support for imgur albums rather than reddit\\'s built in support', value: true, type: 'boolean' } }, APIKey: 'fe266bc9466fe69aa1cf0904e7298eda', // hashRe: /^https?:\\/\\/(?:i\\.|edge\\.|www\\.)*imgur\\.com\\/(?:r\\/[\\w]+\\/)?([\\w]{5,}(?:[&,][\\w]{5,})?)(\\..+)?(?:#(\\d*))?$/i, // the modified regex below fixes detection of \"edited\" imgur images, but imgur's edited images are broken right now actually, falling into // a redirect loop. preserving the old one just in case. however it also fixes detection of the extension (.jpg, for example) which // was too greedy a search... // the hashRe below was provided directly by MrGrim (well, everything after the domain was), using that now. // in addition to the above, the album index was moved out of the first capture group. hashRe: /^https?:\\/\\/(?:i\\.|m\\.|edge\\.|www\\.)*imgur\\.com\\/(?:r\\/[\\w]+\\/)*(?!gallery)(?!removalrequest)(?!random)(?!memegen)([\\w]{5,7}(?:[&,][\\w]{5,7})*)(?:#\\d+)?[sbtmlh]?(\\.(?:jpe?g|gif|png|gifv))?(\\?.*)?$/i, albumHashRe: /^https?:\\/\\/(?:i\\.|m\\.)?imgur\\.com\\/(?:a|gallery)\\/([\\w]+)(\\..+)?(?:\\/)?(?:#?\\w*)?$/i, apiPrefix: 'https://api.imgur.com/2/', calls: {}, detect: function(href, elem) { return href.indexOf('imgur.com/') !== -1; }, handleLink: function(elem) { var siteMod = modules['showImages'].siteModules['imgur'], def = $.Deferred(), href = elem.href.split('?')[0], groups = siteMod.hashRe.exec(href), extension, albumGroups; if (!groups) { albumGroups = siteMod.albumHashRe.exec(href); } if (groups && !albumGroups) { // handling for separated list of IDs if (groups[1].search(/[&,]/) > -1) { var hashes = groups[1].split(/[&,]/); def.resolve(elem, { album: { images: hashes.map(function(hash) { return { image: { title: '', caption: '', hash: hash }, links: { original: location.protocol + '//i.imgur.com/' + hash + '.jpg' } }; }) } }); (There's more but I think this is the most relevant portion) Also, note that they have said that they'll fix the GIFV issue in a later release, but it's not in the current one.", "created_utc": 1448693582, "gilded": 0, "name": "t1_cxfj30q", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3udusd/how_do_i_get_the_full_submission_url_from_a_praw/"}, {"author": "lecherous_hump", "body": "Crashes? Are you not handling HTTP errors? You're gonna have a bad time if you don't. For this, you could look for the error code 404, which would mean that the user was in fact deleted, and it wasn't another HTTP error.", "created_utc": 1448532187, "gilded": 0, "name": "t1_cxdl9ye", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ubstm/deleted_users/"}, {"author": "cogsbox", "body": "I am not handling HTTP errors. I have read a few things about this on stack exchange, but I must be implementing it wrong. This is a little snippet of my code. gen = user.get_submitted(limit=None) for thing in gen: subreddit = thing.subreddit.display_name karma_by_subreddit[subreddit] =(karma_by_subreddit.get(subreddit, 0) + thing.score) print (karma_by_subreddit) It says that I am getting the error at **for thing in gen**. I am not sure how to implement the HTTP error handling at the beginning of a for loop. I am sure this is common sense for every one else, but I am knew and just not getting this one.", "created_utc": 1448556527, "gilded": 0, "name": "t1_cxdtja3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ubstm/deleted_users/"}, {"author": "Triplanetary", "body": "Like this: for x in y: try: do stuff except praw.errors.NotFound: continue What this does is, if the user in the current loop iteration doesn't exist, it will skip to the next loop iteration (which is what `continue` does) instead of crashing.", "created_utc": 1448559865, "gilded": 0, "name": "t1_cxdvda6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ubstm/deleted_users/"}, {"author": "cogsbox", "body": "I was able to get it working from this. Thank you so much!", "created_utc": 1448573773, "gilded": 0, "name": "t1_cxe2zv8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ubstm/deleted_users/"}, {"author": "Triplanetary", "body": "So you know it's throwing an exception. You know about try/except, right?", "created_utc": 1448547162, "gilded": 0, "name": "t1_cxdp6cm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ubstm/deleted_users/"}, {"author": "13steinj", "body": "Wasn't the latest python 2 version 2.7.10? There could just be a bug in 2.7.9's http lib. Regardless I recommend uninstalling PRAW and all dependencies, then reinstalling via `pip install praw`", "created_utc": 1448514152, "gilded": 0, "name": "t1_cxdfj4w", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3uapp8/praw_error_data_must_be_a_byte_string/"}, {"author": "thetoethumb", "body": "Thanks for the advice! I upgraded to 2.7.10 and reinstalled ``praw`` and ``requests`` using ``pip``: >>> print sys.version 2.7.10 (default, May 23 2015, 09:40:32) [MSC v.1500 32 bit (Intel)] >>> print praw.__version__ 3.3.0 >>> print requests.__version__ 2.5.0 ~~Still no dice with the MWE in the original example. Any further ideas? Any other packages I should be reinstalling/upgrading?~~ Edit: Now appears to be working. I think I just needed to re-import the requests module after upgrading (still had a console open)", "created_utc": 1448597123, "gilded": 0, "name": "t1_cxeedw0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3uapp8/praw_error_data_must_be_a_byte_string/"}, {"author": "13steinj", "body": ":}", "created_utc": 1448601642, "gilded": 0, "name": "t1_cxegkin", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3uapp8/praw_error_data_must_be_a_byte_string/"}, {"author": "thetoethumb", "body": "Traceback (most recent call last): File \"\", line 1, in runfile('[redacted path]', wdir='[redacted path]') File \"C:\\Python27\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py\", line 601, in runfile execfile(filename, namespace) File \"C:\\Python27\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\sitecustomize.py\", line 66, in execfile exec(compile(scripttext, filename, 'exec'), glob, loc) File \"[redacted path]\", line 13, in print [str(x) for x in submissions] File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\__init__.py\", line 557, in get_content page_data = self.request_json(url, params=params) File \"\", line 2, in request_json File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\__init__.py\", line 612, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\__init__.py\", line 444, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\__init__.py\", line 425, in handle_redirect verify=self.http.validate_certs, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\handlers.py\", line 148, in wrapped result = function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\handlers.py\", line 57, in wrapped return function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-3.3.0-py2.7.egg\\praw\\handlers.py\", line 103, in request allow_redirects=False, verify=verify) File \"C:\\Python27\\lib\\site-packages\\requests\\sessions.py\", line 573, in send r = adapter.send(request, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests\\adapters.py\", line 370, in send timeout=timeout File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 518, in urlopen body=body, headers=headers) File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 330, in _make_request conn.request(method, url, **httplib_request_kw) File \"C:\\Python27\\lib\\httplib.py\", line 1001, in request self._send_request(method, url, body, headers) File \"C:\\Python27\\lib\\httplib.py\", line 1035, in _send_request self.endheaders(body) File \"C:\\Python27\\lib\\httplib.py\", line 997, in endheaders self._send_output(message_body) File \"C:\\Python27\\lib\\httplib.py\", line 850, in _send_output self.send(msg) File \"C:\\Python27\\lib\\httplib.py\", line 826, in send self.sock.sendall(data) File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\contrib\\pyopenssl.py\", line 220, in sendall sent = self._send_until_done(data) File \"C:\\Python27\\lib\\site-packages\\requests\\packages\\urllib3\\contrib\\pyopenssl.py\", line 210, in _send_until_done return self.connection.send(data) File \"C:\\Python27\\lib\\site-packages\\OpenSSL\\SSL.py\", line 947, in send raise TypeError(\"data must be a byte string\") TypeError: data must be a byte string", "created_utc": 1448505301, "gilded": 0, "name": "t1_cxdb8n4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3uapp8/praw_error_data_must_be_a_byte_string/"}, {"author": "slyf", "body": "That object is a generator, not a list, generators are basically like lists which only spit things out one at a time dynamically. Once a generator runs out, it has nothing left to spit out. If you want use comments = list(comments) to store it into a list.", "created_utc": 1448484288, "gilded": 0, "name": "t1_cxczj08", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3u9f4p/why_do_comment_objects_erase_themselves_when_i/"}, {"author": "theonefoster", "body": "Thanks a lot, this clears it up. I tried comments=list(comments) and it worked fine. I just didn't understand generator objects :)", "created_utc": 1448484575, "gilded": 0, "name": "t1_cxczpf5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3u9f4p/why_do_comment_objects_erase_themselves_when_i/"}, {"author": "13steinj", "body": "Post to /r/bugs. It could be related to /u/Deimorz's voting code change but I'm not sure. But this isn't the first time I've seen a 500 internal server error from people, and IIRC reddit only returns that if something is broken server side (such as a function called but not defined, or the like. When testing on local installs I just wrap the potentially broken controller in a try except wrapper that would build a different response based on the error that raises, and that's the error that comes up the most).", "created_utc": 1448064929, "gilded": 0, "name": "t1_cx7hps9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm6r6/praw_a_script_accessing_the_banlist_is_not/"}, {"author": "GoldenSights", "body": "`cur.fetchone` should actually be a function call: `cur.fetchone()`. That way it retrieves the item from the database. You should certainly keep the `not`. You may also want to move the `INSERT INTO` line one indentation to the right. Currently it's happening for every comment you see, when it should only be happening for comments that were not yet in the database, indicated by `cur.fetchone()`. You also need to move `HypeManBot()` to the right so that it is within the while loop. >I might get a reply but I also get a traceback error What was the traceback?", "created_utc": 1448057272, "gilded": 0, "name": "t1_cx7dj90", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "AllHailTheCATS", "body": "never mind figured out what was happending and know how to handle it, thanks for the help!", "created_utc": 1448060538, "gilded": 0, "name": "t1_cx7feql", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "YodaWithASoda", "body": "how did you ever learn how to program?", "created_utc": 1448228164, "gilded": 0, "name": "t1_cx9ejnl", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "AllHailTheCATS", "body": "I started learning python this week...", "created_utc": 1448228502, "gilded": 0, "name": "t1_cx9erqa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "YodaWithASoda", "body": "how m8 give me list", "created_utc": 1448229363, "gilded": 0, "name": "t1_cx9fbst", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "AllHailTheCATS", "body": "Thought your first comment was ment to mock me, if you serious heres a good starting place: https://www.reddit.com/r/learnprogramming/wiki/faq#wiki_how_do_i_get_started_with_programming.3F Heres where learned python, I already looked at stuff like c before so picked it up quick but its a good easy language to start with: http://www.learnpython.org/", "created_utc": 1448229718, "gilded": 0, "name": "t1_cx9fk5p", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "YodaWithASoda", "body": "its ok I love you still.", "created_utc": 1448231311, "gilded": 0, "name": "t1_cx9gkib", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "AllHailTheCATS", "body": "I had the markup correct it just formatted weird on reddit, oh yea thanks for pointing that out with fetchone(). I'm getting the traceback still on the second time the bot trys comment, that is if im running the bot, he comments under me then i comment a second time I get the traceback instead of the bot commenting on my second comment. I assume this is because a bot can reply to the one account only so often? heres the traceback anyway: Traceback (most recent call last): File \"C:/Users/Conor/PycharmProjects/HypeManProject/HypeManBot.py\", line 42, in HypeManBot() File \"C:/Users/Conor/PycharmProjects/HypeManProject/HypeManBot.py\", line 33, in HypeManBot comment.reply(\"Yeaaaah!!\") File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\objects.py\", line 408, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 2569, in _add_comment retval = decorator(add_comment_helper)(self, thing_id, text) File \"\", line 2, in add_comment_helper File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\decorators.py\", line 268, in wrap return function(*args, **kwargs) File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\__init__.py\", line 2562, in add_comment_helper retry_on_error=False) File \"\", line 2, in request_json File \"C:\\Users\\Conor\\AppData\\Local\\Programs\\Python\\Python35-32\\lib\\site-packages\\praw\\decorators.py\", line 139, in raise_api_exceptions raise error_list[0] praw.errors.RateLimitExceeded: `you are doing that too much. try again in 8 minutes.` on field `ratelimit` Process finished with exit code 1", "created_utc": 1448059780, "gilded": 0, "name": "t1_cx7ezgj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tm67g/my_bot_wont_reply_when_its_ment_to/"}, {"author": "omegatheory", "body": "Erm... happy cake day?", "created_utc": 1448058230, "gilded": 0, "name": "t1_cx7e3jc", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/"}, {"author": "avinassh", "body": "ha ha thanks!", "created_utc": 1448074504, "gilded": 0, "name": "t1_cx7mbtu", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/"}, {"author": "timonsmith", "body": "Happy cake day.", "created_utc": 1448098388, "gilded": 0, "name": "t1_cx7ujng", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/"}, {"author": "avinassh", "body": "Thanks!", "created_utc": 1448098458, "gilded": 0, "name": "t1_cx7uk6x", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/"}, {"author": "Forever_Sorry", "body": "thanks for this!", "created_utc": 1448256558, "gilded": 0, "name": "t1_cx9vbhl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/"}, {"author": "13steinj", "body": "I'm late, but, actually, just to put things out there, the `requests` method is not invalid nor will you run into 429 errors. \"Using the api\" still puts requests directly on reddit's servers. You are more than well allowed to use the `www` domain without restriction. The `api` domain makes things easier and auto converts any url to json, and `oauth` would be for `oauth` only requests. Of course you'd have to go through trouble to get your access token and refresh it when just using `requests`, it's certainly feasible so long as you don't go over the api request limit (60RequestsPerMin for oauth or 30RequestsPerMin for CookieAuth / Unauth) (But 'soon' non oauth will be more heavily throttled.)", "created_utc": 1448756538, "gilded": 0, "name": "t1_cxg8kb6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tlt8o/tutorial_find_a_redditors_cake_day_using_praw_and/"}, {"author": "13steinj", "body": "Making two requests? That's strange in a sense. Can you show the log of what requests are being made?", "created_utc": 1447997752, "gilded": 0, "name": "t1_cx6mdr7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tj7qw/praw_how_to_get_user_info_a_single_request/"}, {"author": "avinassh", "body": "I think you are right. I made a mistake in my script. Now I checked again and it did not make two requests as I thought.", "created_utc": 1447998505, "gilded": 0, "name": "t1_cx6mok5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tj7qw/praw_how_to_get_user_info_a_single_request/"}, {"author": "cutety", "body": "/u/GoldenSights has a good tutorial on his subreddit for setting up your bot to work with OAuth [here](https://www.reddit.com/r/GoldTesting/comments/3cm1p8/how_to_make_your_bot_use_oauth2/). Or you can just watch his youtube video [here](https://www.youtube.com/watch?v=Uvxu2efXuiY&feature=youtu.be). There is also /u/SmBe19's [praw-OAuth2Util](https://github.com/SmBe19/praw-OAuth2Util) that makes OAuth a simple as copying and pasting a .ini file into your bots directory and adding a line to your bots .py file. [Instuctions for setting that up](https://github.com/SmBe19/praw-OAuth2Util/blob/master/OAuth2Util/README.md). [This](https://github.com/voussoir/reddit/tree/master/ReplyBot) is /r/RequestABot's replybot template. I've written several bots that respond to comments when a certain phrase is in a comment: [!d10](https://gitlab.com/EDGYALLCAPSUSERNAME/D10-Dice-Roll-Bot/tree/master). [+decodebot](https://gitlab.com/EDGYALLCAPSUSERNAME/Decode-Bot/tree/master). [Replies to various keywords](https://gitlab.com/EDGYALLCAPSUSERNAME/ReputationManager/tree/master). All those could be modified to watch for their username instead. Another way to do it is monitor the messages that come into the account and reply to any where \"Username Mention\" is the subject.", "created_utc": 1447947339, "gilded": 0, "name": "t1_cx5rnz6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "lumbdi", "body": "Is it bad if I request a new access token via the refresh token every 5 minutes? Can happen if my bot has nothing to do. If my bot has a lot to do it will ask every ~1 hour.", "created_utc": 1447968377, "gilded": 0, "name": "t1_cx65v3h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "SmBe19", "body": "The refresh method (of OAuth2Util) will check how much time passed and will only request a new token from reddit once one hour is up. So you can call refresh as often as you want.", "created_utc": 1447968474, "gilded": 0, "name": "t1_cx65xih", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "lumbdi", "body": "I'm getting a new access token each time I call the refresh_access_information(APP_REFRESH): http://i.imgur.com/UiRKJKX.png", "created_utc": 1447969201, "gilded": 0, "name": "t1_cx66flu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "lumbdi", "body": "I've seen a few source codes where people used refresh(). But I have [no such method.](http://i.imgur.com/VqB42pS.png) With refresh method you mean refresh_access_information(APP_REFRESH) ?", "created_utc": 1447968740, "gilded": 0, "name": "t1_cx6643g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "SmBe19", "body": "Refresh() is a method of OAuth2Util, so if you don't use it, my comment doesn't apply to your case. In this case you will get a new token every time you refresh but that should be no problem.", "created_utc": 1447969255, "gilded": 0, "name": "t1_cx66gym", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "lumbdi", "body": "Okay thank you! Sorry about that I should have carefully read your comment.", "created_utc": 1447969404, "gilded": 0, "name": "t1_cx66kmh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "lumbdi", "body": "Thank you so much!", "created_utc": 1447949735, "gilded": 0, "name": "t1_cx5t660", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "avinassh", "body": "[source](https://github.com/avinassh/Reddit-GoodReads-Bot) which powers /u/goodreadsbot", "created_utc": 1447997090, "gilded": 0, "name": "t1_cx6m3vl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "lumbdi", "body": "thanks :) I already got my bot up and running [see here](https://www.reddit.com/r/DotA2/comments/3tikrm/account_buying_piece_of_shit_trash_who_thinks_he/) but I still have to handle a lot of errors. The majority of them not due to PRAW. My bot is talking to Steam API and I haven't handled the potential connection errors yet.", "created_utc": 1447997467, "gilded": 0, "name": "t1_cx6m9jf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "avinassh", "body": "looks very nice! https://www.reddit.com/r/DotA2/comments/3tikrm/account_buying_piece_of_shit_trash_who_thinks_he/cx6gdav", "created_utc": 1447998590, "gilded": 0, "name": "t1_cx6mpqf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3tfqea/praw_need_example_code_for_bot_logging_in_and/"}, {"author": "13steinj", "body": "500 errors should only occur when something goes wrong on reddit's side, so you may want to get all the data you are sending / receiving, redact anything private, and submit to /r/bugs.", "created_utc": 1447910067, "gilded": 0, "name": "t1_cx5f4fy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3te2i6/im_getting_a_500_internal_server_error_when_i_try/"}, {"author": "ffranglais", "body": "I don't know how to do that. I'll even accept \"here's what you Google to get the information\".", "created_utc": 1447911420, "gilded": 0, "name": "t1_cx5fpn2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3te2i6/im_getting_a_500_internal_server_error_when_i_try/"}, {"author": "13steinj", "body": "Oh, forgot you're using PRAW. No idea if it stores information about requests / responses somewhere/how, but /u/bboe might be able to help.", "created_utc": 1447933673, "gilded": 0, "name": "t1_cx5lwl6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3te2i6/im_getting_a_500_internal_server_error_when_i_try/"}, {"author": "lecherous_hump", "body": "To answer your question about errors, 500 is the most generic HTTP error. It's a catchall for \"something went wrong\" on the server side.", "created_utc": 1447922163, "gilded": 0, "name": "t1_cx5j707", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3te2i6/im_getting_a_500_internal_server_error_when_i_try/"}, {"author": "13steinj", "body": "Huh, doesn't seem to be a way off the bat. But you can do r.get_content(url, *args, **kwargs) where url is the '/related' version of the url (you can do the submission's `permalink` attribute + '/related', and *args and **kwargs are whatever arguments / keyword arguments you would normally pass into get_content from get_new/top/hot/etc, such as `limit=None`)", "created_utc": 1447772449, "gilded": 0, "name": "t1_cx3b7sk", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3t4cax/can_you_query_praw_for_related_posts/"}, {"author": "tobiasvl", "body": "So I found the issue. Praw depends on `requests`, which has this issue currently: [2818](https://github.com/kennethreitz/requests/issues/2818) Same as [issue 719](https://github.com/shazow/urllib3/pull/719) in `urllib3`. It works again after downgrading `requests` to 2.7.0.", "created_utc": 1447782791, "gilded": 0, "name": "t1_cx3i1hh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3t1mum/error_when_logging_in_with_praw/"}, {"author": "13steinj", "body": "What python version? Also, try making a virtualenv with virtualenv and just get pip and praw.", "created_utc": 1447703975, "gilded": 0, "name": "t1_cx2eujz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3t1mum/error_when_logging_in_with_praw/"}, {"author": "tobiasvl", "body": "2.7.10 And thanks, I'll try a fresh virtualenv.", "created_utc": 1447705426, "gilded": 0, "name": "t1_cx2fuzd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3t1mum/error_when_logging_in_with_praw/"}, {"author": "_Daimon_", "body": "You need scope for OAuth to work, it just doesn't make sense otherwise. OAuth is accessing a reddit on behalf of a user, so obviously you need some permissions from a user. I would grant the `identity` scope of your personal user account and use that if you need OAuth. The code is a one-use token for a specific user. Once user, it cannot be reused. I'm not sure, but I believe the code is also tied to a specific app.", "created_utc": 1447672868, "gilded": 0, "name": "t1_cx1yo0r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "Frenchiie", "body": "yeah but how is it being checked that the code is for a specific user? what stops someone from re-using it. Can you walk me through the basics of what happens to prevent this?", "created_utc": 1447704693, "gilded": 0, "name": "t1_cx2fch2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "_Daimon_", "body": "The code is created for a specific user trying to authenticate via a specific app. Authenticating with another app will not work. Authenticating will give you permission to act on behalf of that user from their perspective. There is no user selection after the code has been given. All of that happens before. It cannot be reused. The authentication with Reddit in step 4 will fail.", "created_utc": 1447711191, "gilded": 0, "name": "t1_cx2jvkk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "Frenchiie", "body": "I'm just not understanding what information does the PRAW method get_authorize_url() send to the Reddit API which then sends to the Reddit server that basically says \"hey this an authorization permission on behalf of /r/Frenchiie\". The method that we call, get_authorize_url() returns a url that we open to get a code specific to a user. [It passes 3 arguments](http://praw.readthedocs.org/en/stable/pages/code_overview.html#praw.__init__.OAuth2Reddit.get_authorize_url), none of which gives any sort of information that validates a user. What else is going on **behind** the scene? How does the reddit API know who the user is during this step?", "created_utc": 1447721370, "gilded": 0, "name": "t1_cx2q5ql", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "_Daimon_", "body": "`get_authorize_url` says \"Hey Reddit, I would like to get authorized with x scope, y state and possibly permanently. Which user has not been specified. Based on these parameters PRAW builds a url. No direct interaction with Reddit has been neccesary. Basically PRAW just takes the arguments you've given and package them into a url, with GET parameters. There is nothing user-specific inherently about this url. You can give it to 1 user, 0 users or many many users. It doesn't matter. Let's say our FooBar website want state \"123\", scope \"identity\" and no refreshable token. We build the url and send it to /u/Frenchile and ask the user to go to this page. The user goes to this page. It is a reddit page. It says, hello \"/u/Frenchile\" app \"FooBar\" would like to temporarily get access to your \"identity\" permission. It lists what \"FooBar\" can do with the permissions you are about to give. You click ok. Now, lets say we also give this url to /u/_Daimon I too go on this page and agree to temporarily give \"identity\" permissions to \"FooBar\" app. The webserver receives a callback on the callback url with the code both times a url authorized the app. The callback is just a GET request with the code and state argument. The two code arguments are different. The state argument are identical to what we choose when we called `get_authorize_url`. Now, we take one of the codes and auhenticate on Reddit. As per step 5 in the documentation. We get a token that we can use to make requests based on this user. If we try to authenticate with the code again, it will fail. We are now authenticated as either /u/Frenchile or /u/_Daimon_. Since we have the `identity` permission, we can use `get_me` to find out who we are authenticated as. Final note. State in the above example are identical for all users. This is fine for testing and demonstration purposes. In production it should be unique for security reasons.", "created_utc": 1447722795, "gilded": 0, "name": "t1_cx2r0ht", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "kemitche", "body": "If you're making requests as a user, yes, you need scopes. The reason is that when signed in, the \"public\" feeds get filled in with personal user data - such as whether that user voted on a given submission or comment, etc. For consistency, those scopes are required even when not \"signed in\" as a user (i.e., the `client_credentials` grant). However, since there's no user, you can effectively always ask for all scopes when requesting `client_credentials` tokens.", "created_utc": 1447694172, "gilded": 0, "name": "t1_cx2893v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sv72o/oauthwith_praw_if_i_dont_actually_need_permission/"}, {"author": "Deimorz", "body": "It's not really what you're asking, but depending exactly what you're looking to do, AutoModerator may already be able to do a lot of the sort of tasks that you're thinking about. For example, it's able to respond to all 4 of the \"events\" that you listed in the post.", "created_utc": 1447354619, "gilded": 0, "name": "t1_cwxz0ew", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "Pi31415926", "body": "If AutoModerator was able to send a PM to a specified account, this would enable bots to detect any event that AM can detect. A bit like the `modmail` and `modmail_subject` directives - something like `private_message_recipient`, `private_message_subject` and `private_message_body`. Bots could check the inbox of their accounts for PMs from AutoModerator. In effect, AM would become the event listener.", "created_utc": 1447360703, "gilded": 0, "name": "t1_cwy37kx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "Deimorz", "body": "That's kind of an interesting idea. I don't think there's really anything that AutoMod listens for now that an external bot couldn't do anyway, but it would make it so that they only have to check one place, I guess.", "created_utc": 1447367240, "gilded": 0, "name": "t1_cwy7lzz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "Pi31415926", "body": "I'm guessing it will be more efficient, in terms of network traffic and processing overhead, for the bots to poll their inboxes, than it will be for them to poll various reddit listings. As you point out, AM already has the detection code done, so all that's needed is a way to send AM alerts to third-party bots, without flooding modmail. Hence, `private_message_recipient`. Also, rather than free-format PMs, AutoModerator could simply send PMs with an event code and a data field. Eg., mods would not need to specify PM body or subject in their AM rules, these could be autogenerated by AM, which would probably simplify and accelerate things a bit. There is also 'push' functionality which is mentioned occasionally, this would be even more efficient, but also more difficult to implement.", "created_utc": 1447369440, "gilded": 0, "name": "t1_cwy91oy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "IAPark", "body": "I want to have custom behavior for them though. For example in /r/crossview people often accidentally post parallel views and I'd like to detect this (I've got an algorithm that works a decent percent of the time) and then I'd like to reverse the images and provide a comment to tell the user that it was reversed and that there is a corrected version", "created_utc": 1447356117, "gilded": 0, "name": "t1_cwy016p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "IAPark", "body": "I'd also like to listen for a keyword and provide a reversed image for the submission", "created_utc": 1447356320, "gilded": 0, "name": "t1_cwy0697", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "noobit", "body": "no, there isn't; yes, it's annoying; yes, it'd be useful. Between praw's [ATDYDN](#rant \"All the detail you don't need' - I have a rant on this.\") documentation and the need to install a scheduler program (rather than just some existing bot manager script) for scheduling anything more complex than a simple cronjob, I haven't touched bot stuff in a while", "created_utc": 1447355078, "gilded": 0, "name": "t1_cwxzbmz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "lecherous_hump", "body": "That link comes back to this page.", "created_utc": 1447368368, "gilded": 0, "name": "t1_cwy8boj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "Pokechu22", "body": "Hover over it; it's intended to be used as a tooltip.", "created_utc": 1447739204, "gilded": 0, "name": "t1_cx30jah", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "bobbonew", "body": "Webhooks would be damn useful.", "created_utc": 1447355669, "gilded": 0, "name": "t1_cwxzq8a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "lecherous_hump", "body": "There's a site that lets you listen for events with Javascript. It's been linked here before, maybe someone can remember it. I'm not sure it's 100% free either; I think some limited number of requests are. But basically, you're asking for someone to hit the API as fast as possible to watch all the data coming through then alert you when it does. So it's not really about a framework, it's about a service.", "created_utc": 1447368296, "gilded": 0, "name": "t1_cwy8a1m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "xiongchiamiov", "body": "You might take a look at [these bots](https://github.com/voussoir/reddit).", "created_utc": 1448408183, "gilded": 0, "name": "t1_cxbzvh8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sk8sy/is_there_an_existing_framework_for_reddit_bots/"}, {"author": "13steinj", "body": "`created_utc` would be the timestamp attribute. The best way to check reports would be either the \"duplicates\" listing or the \"related\" listing.", "created_utc": 1447358315, "gilded": 0, "name": "t1_cwy1jqz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "CVance1", "body": "What format woudl this return? I'm assuming I would call this with, for example... r.get_hot().created_utc?", "created_utc": 1447360666, "gilded": 0, "name": "t1_cwy36mr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "13steinj", "body": "Well `get_hot` is a generator of posts, so you probably want r.get_submission(url=\"url\").created_utc", "created_utc": 1447361207, "gilded": 0, "name": "t1_cwy3k6w", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "CVance1", "body": "is it possible to get a list of, say, the top posts from the past month, and then get the created_utc for each of those by getting the url for each of those posts?", "created_utc": 1447361655, "gilded": 0, "name": "t1_cwy3vd9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "13steinj", "body": "You wouldn't need the url. r = praw.Reddit('analyser for your thing's name's) s = r.get_subreddit('redditdev') for post in s.get_top_from_month(limit=None): print(post.created_utc)", "created_utc": 1447362099, "gilded": 0, "name": "t1_cwy46bh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "CVance1", "body": "Thanks so much! How will created_utc return, in seconds since a certain time? or the actual time stamp?", "created_utc": 1447362400, "gilded": 0, "name": "t1_cwy4dwq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "13steinj", "body": "It should be in Unix timestamp format (seconds from the epoch).", "created_utc": 1447362560, "gilded": 0, "name": "t1_cwy4hu8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "CVance1", "body": "ok, thanks.", "created_utc": 1447362628, "gilded": 0, "name": "t1_cwy4jl5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3sjioe/praw_searching_posts_in_a_subreddit_by_timestamp/"}, {"author": "Ninja_Fox_", "body": "That kind of defeats the point of a spam filter. You don't want spammers knowing.", "created_utc": 1446977115, "gilded": 0, "name": "t1_cwsqvts", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3rz98x/notify_user_if_post_caught_by_spam_filter/"}, {"author": "earth-tone", "body": "There's no way to know, once a post/comment has been hidden, whether it was done by Reddit's spam filter or by a mod/automod, unless you're a mod for that sub.", "created_utc": 1446968129, "gilded": 0, "name": "t1_cwsopm7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3rz98x/notify_user_if_post_caught_by_spam_filter/"}, {"author": "13steinj", "body": "If you *are* a mod it's easy, if you aren't, it defeats the purpose of the spam filter.", "created_utc": 1447002144, "gilded": 0, "name": "t1_cwszch7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3rz98x/notify_user_if_post_caught_by_spam_filter/"}, {"author": "Albuyeh", "body": "I am a mod.", "created_utc": 1447019737, "gilded": 0, "name": "t1_cwtbcu9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3rz98x/notify_user_if_post_caught_by_spam_filter/"}, {"author": "13steinj", "body": "Then just request the modqueue, and check the `banned_by` attribute that it must be None.", "created_utc": 1447020174, "gilded": 0, "name": "t1_cwtbnot", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3rz98x/notify_user_if_post_caught_by_spam_filter/"}, {"author": "Albuyeh", "body": "Well how would I find the items that are just caught in spam filter? `banned_by` is set when a post is removed by a mod, correct?", "created_utc": 1447020473, "gilded": 0, "name": "t1_cwtbusi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3rz98x/notify_user_if_post_caught_by_spam_filter/"}, {"author": "13steinj", "body": "Yes. So, if you request the modqueue(I beleive `get_moderator_queue`), then for post in get_moderator_queue() if post.banned_by is None and post.num_reports is 0: Notify(post) where Notify is a function that notifies someone of the given argument, post.", "created_utc": 1447021441, "gilded": 0, "name": "t1_cwtcig6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3rz98x/notify_user_if_post_caught_by_spam_filter/"}, {"author": null, "body": "[deleted]", "created_utc": 1446474548, "gilded": 0, "name": "t1_cwllcpm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "hullcrush", "body": "> import praw > r = praw.Reddit('user agent for hellcrush!') > sr = r.get_subreddit('python') > for _ in sr.get_comments(limit=100): > print(_) YES! Baller. Now back to boring life.", "created_utc": 1446581226, "gilded": 0, "name": "t1_cwn9cak", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "GoldenSights", "body": "What did you do after `comments = subreddit.get_comments()`? That method returns a generator, which you can iterate over using a for loop. comments = subreddit.get_comments() for comment in comments: print(comment.author) However, it sounds like this isn't exactly what you're looking for. To get the threads and all the comments within them, you'll have to use `subreddit.get_new()` to get the submissions, and `submission.replace_more_comments(limit=None, threshold=1)` to load all the comments on each one. Am I understanding the question properly?", "created_utc": 1446411079, "gilded": 0, "name": "t1_cwks5xz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "hullcrush", "body": "You are. Thanks for answering! http://imgur.com/kUMTb3x I'd prefer the latter code but I can't get any PRAW code to work. I appended your comments code and it did not print. I'm using IDLE 2.7", "created_utc": 1446414179, "gilded": 0, "name": "t1_cwkub72", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "GoldenSights", "body": "That is pretty strange. Do you get the same results if you use python in the cmd, or when running a .py file?", "created_utc": 1446414656, "gilded": 0, "name": "t1_cwkun6l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "hullcrush", "body": "If you have a bitcoin wallet I'll shoot you some milliBTC for your help regardless, inbox me, because I have completely tapped out at this point. It was kind of a whim, I wanted to know what other people are listening to, watching, reading etc. in plain text. http://imgur.com/pyQ136N http://imgur.com/w5DXqs8 Other .py packages I believe load fine like Beautiful Soup, which might work instead here, or Scrapy.", "created_utc": 1446431046, "gilded": 0, "name": "t1_cwl4un9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "xiongchiamiov", "body": "You need to press enter again so that the Python shell knows you're finished entering that snippet. You should probably come over to r/learnpython and check out our resources for beginning programmers.", "created_utc": 1446511451, "gilded": 0, "name": "t1_cwma8s3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3r4fv9/using_praw_to_scrape_first_page_comments_of_a/"}, {"author": "GoldenSights", "body": "Out of curiosity, what happens if you put the entire thing inside a try-catch, and do that in a while loop? Looks like your request simply timed out, so it should be fine once it loops back once or twice. I would still recommend importing `traceback` and performing `traceback.print_exc()` as part of the except block, so that you can address any important issues.", "created_utc": 1446162824, "gilded": 0, "name": "t1_cwhsh1d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qroun/problem_keeping_praw_with_oauth2_running/"}, {"author": "SecretAg3nt", "body": "Thanks, this worked! I'll try playing with the traceback function aswell", "created_utc": 1446202197, "gilded": 0, "name": "t1_cwi83jp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qroun/problem_keeping_praw_with_oauth2_running/"}, {"author": "kemitche", "body": "That is the bot equivalent of \"my internet went to crap\". You just need to build some resiliency into your request logic.", "created_utc": 1446179768, "gilded": 0, "name": "t1_cwi219o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qroun/problem_keeping_praw_with_oauth2_running/"}, {"author": "13steinj", "body": "I don't know when or if it actually changed, but praw returns generator objects for listings. To cycle through the posts, use for post in submissions: # do something", "created_utc": 1446073476, "gilded": 0, "name": "t1_cwghq29", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qmign/prawinstead_of_displaying_posts_my_script_is/"}, {"author": "StormZero1", "body": "for post in submissions: print submissions did this, but now it simply prints \"\" five times, any idea what it could be?", "created_utc": 1446074152, "gilded": 0, "name": "t1_cwgi58j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qmign/prawinstead_of_displaying_posts_my_script_is/"}, {"author": "13steinj", "body": "You're suppo to do for post in submissions: print str(post) The post is the submission object. Str would grab the string representation of the object. Of course you probably still want something different than just printing that. Don't know what you want though.", "created_utc": 1446075448, "gilded": 0, "name": "t1_cwgiyje", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qmign/prawinstead_of_displaying_posts_my_script_is/"}, {"author": "xentralesque", "body": "One could also \"convert\" the generator to a list by doing `submissions = list(submissions)` but that's a waste of memory.", "created_utc": 1446077044, "gilded": 0, "name": "t1_cwgjzu9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qmign/prawinstead_of_displaying_posts_my_script_is/"}, {"author": "13steinj", "body": "There's no point in doing that in OPs case. At least from what I can tell, anyway. He'd still have to iterate through said list.", "created_utc": 1446077389, "gilded": 0, "name": "t1_cwgk7wl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qmign/prawinstead_of_displaying_posts_my_script_is/"}, {"author": "xiongchiamiov", "body": "What does the email code look like?", "created_utc": 1446511876, "gilded": 0, "name": "t1_cwmai1x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3qmign/prawinstead_of_displaying_posts_my_script_is/"}, {"author": "GoldenSights", "body": "What's happening is that `fetch=True` tries to fetch all of the information about the subreddit, such as ID and creation timestamp. /r/all doesn't have these because it isn't a real subreddit, and https://reddit.com/r/all/about.json returns 404 Not Found, exactly as you're seeing in that traceback. To override PRAW's 30 second cache, you don't need to re-request the subreddit, because that object doesn't have any effect on the /comments listing that you retrieve. All it does is toss its name into `r.get_comments` so you don't have to do it manually. You can use `r.evict('https://oauth.reddit.com/r/all/comments')` or the more general `r.handler.clear_cache()` to get a fresh listing next time.", "created_utc": 1445802246, "gilded": 0, "name": "t1_cwciehj", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3q6mhj/im_getting_a_http_error_on_rget_subredditall/"}, {"author": "theonefoster", "body": "Thank you! I added the r.handler.clear_cache before sending the request (without fetch=True) and it works great :)", "created_utc": 1445802872, "gilded": 0, "name": "t1_cwcityt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3q6mhj/im_getting_a_http_error_on_rget_subredditall/"}, {"author": "bboe", "body": "You might consider using the `comment_stream` feature of PRAW as it seems like that may be what you're actually trying to create yourself.", "created_utc": 1445813288, "gilded": 0, "name": "t1_cwcplbn", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3q6mhj/im_getting_a_http_error_on_rget_subredditall/"}, {"author": "theonefoster", "body": "I'll look into it, thanks :)", "created_utc": 1445813384, "gilded": 0, "name": "t1_cwcpnfn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3q6mhj/im_getting_a_http_error_on_rget_subredditall/"}, {"author": "mgrieger", "body": "I think I may have found what you were looking for. I believe you are looking for SubredditPaginator, which extends Paginator. https://github.com/thatJavaNerd/JRAW/blob/master/src/main/java/net/dean/jraw/paginators/SubredditPaginator.java https://github.com/thatJavaNerd/JRAW/blob/master/src/main/java/net/dean/jraw/paginators/Paginator.java You'll have to do multiple method calls but you should be able to achieve the same result.", "created_utc": 1445781218, "gilded": 0, "name": "t1_cwc5ar4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3q3q5i/jraw_get_top_submissions_from_subreddit/"}, {"author": "habnpam", "body": "Thanks. That is exactly what I wanted.", "created_utc": 1445812938, "gilded": 0, "name": "t1_cwcpdmm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3q3q5i/jraw_get_top_submissions_from_subreddit/"}, {"author": "mgrieger", "body": "No problem! Glad it works.", "created_utc": 1445813832, "gilded": 0, "name": "t1_cwcpxdo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3q3q5i/jraw_get_top_submissions_from_subreddit/"}, {"author": "GoldenSights", "body": "What's happening here is that the decorator toggles the `_use_oauth` property [on](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L250) while making the request and then [back off again](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L270) when it's done. When you made multiple threads, one of them tried to make a request while the other was already working on one, causing the error. How many streams do you plan on using? Would you consider setting up a single stream for something like /r/redditdev+botwatch+learnpython and then just doing different things based on where the submission comes from? I've literally never used the praw-multiprocess so unfortunately I can't give any help with that. You could also give each stream it's own reddit session, making sure that the total `r.config.api_request_delay` (number of seconds between requests) of all your sessions works out to being only 1 request per second since you're using oauth.", "created_utc": 1445549150, "gilded": 0, "name": "t1_cw9btud", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "bboe", "body": "If I recall correctly, I think as long as they all use the same handler, separate threaded sessions should still obey the rate limit without any modifications.", "created_utc": 1445571771, "gilded": 0, "name": "t1_cw9p4sg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "GoldenSights", "body": "Ah, okay, so something like this: handler = praw.handlers.DefaultHandler() r1 = praw.Reddit('...', handler=handler) r2 = praw.Reddit('...', handler=handler) At first I didn't think this was going to obey the ratelimiting correctly if two requests are made at the exact same time, but I guess that's what the locks are for! Seems to work just fine, that's really interesting.", "created_utc": 1445574896, "gilded": 0, "name": "t1_cw9qlt2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "bboe", "body": "Yeah that's one way to do it with the same `DefaultHandler` object. It should also work across any instance of the `DefaultHandler` class as the lock is on the class, not the instance. So the following should also work: r1 = praw.Reddit('...') r2 = praw.Reddit('...')", "created_utc": 1445576727, "gilded": 0, "name": "t1_cw9rcm2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "EndeRDIT", "body": "It looks like it's exactly as you say and it's toggling on/off. My use case seems to be something that isn't working (one oauth session -> multiple streams) and there's probably a good reason behind it that I just don't know. The plan right now is to scale to N threads (don't know ahead of time) so your suggestion is actually the best bet. It'll save API calls (collapse N > 1) if it works the way I think it'll work as well! Thanks for the help! I didn't even think to use the '+' formatting :D", "created_utc": 1445551298, "gilded": 0, "name": "t1_cw9d7dx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ptjiy/praw_submission_stream_assertionerror/"}, {"author": "GoldenSights", "body": "/r/all behaves differently from individual subreddits, because it's really a combination of other listings. Each individual listing will still cap at 1k, unfortunately.", "created_utc": 1445033745, "gilded": 0, "name": "t1_cw2d2mg", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3p1nu1/was_the_1000_limit_removed/"}, {"author": "Naurgul", "body": "Oh I see. Thanks for clearing this up!", "created_utc": 1445033996, "gilded": 0, "name": "t1_cw2d7rz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3p1nu1/was_the_1000_limit_removed/"}, {"author": "Pokechu22", "body": "Probably a case of http://shouldiblamecaching.com.", "created_utc": 1444877488, "gilded": 0, "name": "t1_cw08itd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3obe5g/are_there_restrictions_on_changing_a_subreddits/"}, {"author": "DanielGibbs", "body": "Huh. This seems to work now despite me not changing anything.", "created_utc": 1444711566, "gilded": 0, "name": "t1_cvxu4ad", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3obe5g/are_there_restrictions_on_changing_a_subreddits/"}, {"author": "13steinj", "body": "Can you try making a virtual environment via virtualenv and then install praw to that virtual environment and see if the error persists?", "created_utc": 1444521438, "gilded": 0, "name": "t1_cvva9sy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "haiguise1", "body": "This fixes the error. What could cause it though?", "created_utc": 1444558698, "gilded": 0, "name": "t1_cvvou0n", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "13steinj", "body": "Looks like an error in urllib or something else you installed.", "created_utc": 1444564056, "gilded": 0, "name": "t1_cvvq1br", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "mO4GV9eywMPMw3Xr", "body": "I have the same issue, did you figure out how to fix it without a virtual environment?", "created_utc": 1445175712, "gilded": 0, "name": "t1_cw3xrai", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "haiguise1", "body": "No, sorry :/", "created_utc": 1445183166, "gilded": 0, "name": "t1_cw41e8x", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "mO4GV9eywMPMw3Xr", "body": "Do you happen to use it on Raspbian or Debian? The error appears on my Raspberry Pi 2, but not on another computer running Arch.", "created_utc": 1445186449, "gilded": 0, "name": "t1_cw43d1y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "haiguise1", "body": "This was on Debian and Windows 7.", "created_utc": 1445188736, "gilded": 0, "name": "t1_cw44tz3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o9mt2/praw_crashes_when_trying_to_do_anything/"}, {"author": "GoldenSights", "body": "PRAW actually has an internal cache that lasts 30 seconds, which means that your first and third request will get data, but the second will not (because it happens at 20s). In terms of server-side caching, it can get pretty slow if you're using a logged out / unauthenticated session, but if you're logged in it should be pretty snappy. Can you paste the relevant part of your bot to give us a better idea?", "created_utc": 1444458185, "gilded": 0, "name": "t1_cvukxuo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "13steinj", "body": "Does the internal caching also apply to the refresh method?", "created_utc": 1444484518, "gilded": 0, "name": "t1_cvur4xi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "GoldenSights", "body": "No it does not, which is very deliberate. All of the refresh methods send in a \"uniq\" url parameter which makes the cache think it's a different request. That unique value is tied to the reddit session and increments on each refresh.", "created_utc": 1444498064, "gilded": 0, "name": "t1_cvuxmz3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "13steinj", "body": "I thought that, but then wouldn't that go against the rule where you are not allowed to make more than a request on a single page in 30 secs?", "created_utc": 1444498238, "gilded": 0, "name": "t1_cvuxqlo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "GoldenSights", "body": "I've thought about that too, but to be honest I've never known whether that's an actual rule or just a guideline. Reddit does some serverside caching so I think they're basically telling you that refreshing a page extremely often is not helpful. I'm on mobile and I cant remember where it is stated, but it's not on the [api rules list](https://github.com/reddit/reddit/wiki/API). If it is a rule somewhere else, then I guess the submission_ and comment_streams break it too.", "created_utc": 1444499310, "gilded": 0, "name": "t1_cvuycbd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "phenomist", "body": "Hmm... kinda hard since my code is kinda all over the place... It can be found here: https://github.com/phenomist/picgame-notifier/blob/master/picgame-notifier.py The gist of it is that it finds the current round (usually the most recent thread in the sub), then within the round looks for comments it hasn't seen yet.", "created_utc": 1444458802, "gilded": 0, "name": "t1_cvul3o0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "GoldenSights", "body": "To be honest I'm having a hard time following the code, the variable names are a little cryptic. I'm not sure if this will 100% work out for your case, but I would look into the possibility of using the /r/PictureGame/comments listing instead of reading each submission's comment page manually. When you find a new comment, you can decide what to do with it based on what submission it was posted on (`comment.link_id`). This should streamline a lot of processes and remove all that recursion. I'm also noticing that notify_comments should probably declare `solved` as global. This might be part of the problem because you've got other code that waits for it to be False. Lastly, FYI, you can replace `author._case_name` with just `author.name` if you want. It doesn't make any API calls if that's what you're avoiding. Also the case change isn't relevant unless you're requesting usernames based on user input with unreliable casing. The API seems to always return names in their proper case in the first place.", "created_utc": 1444459762, "gilded": 0, "name": "t1_cvulci9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o6y4x/is_there_a_delay_in_getting_commentsposts/"}, {"author": "13steinj", "body": "There's no builtin feature but you could use `filter()` / (`ifilter()`? from itertools if you are in 2.7).", "created_utc": 1444402817, "gilded": 0, "name": "t1_cvtqofl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "schmodd", "body": "oh, I am on 3.5 - sry forgot to mention it.", "created_utc": 1444403146, "gilded": 0, "name": "t1_cvtqwcd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "13steinj", "body": "Then `filter()`", "created_utc": 1444403273, "gilded": 0, "name": "t1_cvtqzd5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "schmodd", "body": "google said list-comprehension should be faster than filter() so i just replaced: submissions = current_user.get_submitted() with: submissions = [i for i in current_user.get_submitted() if i.subreddit.display_name == targetSubreddit] thanks for your hints", "created_utc": 1444404308, "gilded": 0, "name": "t1_cvtrnhj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "13steinj", "body": "Whether or not it is actually faster; a list isn't the same. You turned the generator into a list, if that actually matters to you.", "created_utc": 1444404575, "gilded": 0, "name": "t1_cvtrtqv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "schmodd", "body": "good point. I tried this: submissions = filter(lambda x: x != targetSubreddit, current_user.get_submitted()) and it was about the same speed (tested with only a few sample users). I think I am fine with the list or even a set. But good to know that filter also works :) *EDIT: I think there is an error in my filter since some posts from other subreddits were visible. How would you use filter on my example?", "created_utc": 1444405683, "gilded": 0, "name": "t1_cvtskax", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "GoldenSights", "body": "This works for me: `a=filter(lambda x: x.subreddit.display_name =='redditdev', user.get_submitted(limit=None))` You could also use the search function [like this](https://www.reddit.com/r/redditdev/search?q=author%3A%22goldensights%22&restrict_sr=on&sort=relevance&t=all) `a=r.search('author:\"GoldenSights\"', 'redditdev', sort='new', limit=None)` Maybe you can do both and see if they help fill each other's gaps. Especially if you play with different sort modes.", "created_utc": 1444422487, "gilded": 0, "name": "t1_cvu3sro", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "13steinj", "body": "IIRC for one reason or another the search method gave me some problems regarding it actually missed some posts for some reason. I mean, it obviously also gave more due to less of a restriction of 1k items. But some things were skipped too, which is probably a bug on reddits side.", "created_utc": 1444423714, "gilded": 0, "name": "t1_cvu4jlt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "GoldenSights", "body": "I've noticed some missing posts as well, but I've never researched why it actually happens. In some cases it might have to do with removals since they still appear on the user's page but not on a subreddit listing, but there's probably other reasons as well. That's why I suggest OP use both and see what gets him the most items.", "created_utc": 1444423805, "gilded": 0, "name": "t1_cvu4ltr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "lecherous_hump", "body": "Just curious, what does that handler do? Not a python person.", "created_utc": 1444425918, "gilded": 0, "name": "t1_cvu5xmf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "schmodd", "body": "speed up things hopefully :) -> https://praw.readthedocs.org/en/latest/pages/multiprocess.html", "created_utc": 1444454017, "gilded": 0, "name": "t1_cvujnjy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o3qtc/how_to_get_user_submitted_posts_filtered_by/"}, {"author": "13steinj", "body": "You used [0] which would indicate to only get the first item in an iterable. You want to do a for loop for each item in the iterable instead, and print that author.name", "created_utc": 1444351624, "gilded": 0, "name": "t1_cvt60qy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o1fu2/return_author_name_on_rborrow/"}, {"author": "cogsbox", "body": "I am a noob to praw. Would it be possible for me to see what that would look like? I tried comment = submission.comments[1] to see what other items their were and got the error: IndexError: list index out of range", "created_utc": 1444352648, "gilded": 0, "name": "t1_cvt6mr4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o1fu2/return_author_name_on_rborrow/"}, {"author": "13steinj", "body": "Under normal circumstances, yes. But that thread only has a single comment.", "created_utc": 1444352739, "gilded": 0, "name": "t1_cvt6os0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o1fu2/return_author_name_on_rborrow/"}, {"author": "cogsbox", "body": "Okay, thanks for taking time to help out.", "created_utc": 1444353330, "gilded": 0, "name": "t1_cvt71sv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3o1fu2/return_author_name_on_rborrow/"}, {"author": "GoldenSights", "body": "When you do `submission = r.submit(...)`, the `submission` variable actually becomes your submission object. This means you can then use `submission.edit()` whenever you need to, without having to worry about re-fetching it by the url. If you do want the url, you can of course do `submission.permalink`. If you've made any changes to the post, you can use `submission.refresh()` to update the properties.", "created_utc": 1444278260, "gilded": 0, "name": "t1_cvs5pyg", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "Fireislander", "body": "When I try to: print submission.permalink I get this error back: AttributeError: 'dict' object has no attribute 'permalink'", "created_utc": 1444278660, "gilded": 0, "name": "t1_cvs5wda", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "GoldenSights", "body": "Uh oh. What version of PRAW? I'm using 3.3.0 and I get [this](http://i.imgur.com/ndl06ey.png) If you `print(submission)`, what does that dict look like?", "created_utc": 1444278958, "gilded": 0, "name": "t1_cvs611d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "Fireislander", "body": "I can't remember how to check my PRAW version at the moment. I did just update it today to resolve another issue. When I printed my submission object I got this: {u'errors': []} Im assuming that means there is something strange going on with the creation of my submission object. The post creates itself correctly so the submission is working, but, if I am understanding this correctly, the submission is not properly passing itself back to the submission variable", "created_utc": 1444279434, "gilded": 0, "name": "t1_cvs68bo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "GoldenSights", "body": "You can check the praw version with `praw.__version__`, although if you did `pip install praw --upgrade` then you should be on 3.3.0. I'm not familiar with this issue at all. Can you try setting `r.log_requests=1` before submitting? You should see a POST and a subsequent GET. I would also try modifying your local PRAW install, adding some additional print statements to see where things are going. For example, printing the `result` variable in between lines 2634 and 2635 [here](https://github.com/praw-dev/praw/blob/351d408ce763cc63b93acecbf0a1bfaa7c0ed530/praw/__init__.py#L2634), and the `url` variable between 2640 and 2641 [here](https://github.com/praw-dev/praw/blob/351d408ce763cc63b93acecbf0a1bfaa7c0ed530/praw/__init__.py#L2640).", "created_utc": 1444280767, "gilded": 0, "name": "t1_cvs6rzz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nxjnq/getting_the_url_of_a_post_i_just_submitted/"}, {"author": "GoldenSights", "body": "Can you try removing the newline `\\n` from the useragent? The Python 3.5 lib\\http\\client.py source code has this line: _is_illegal_header_value = re.compile(rb'\\n(?![ \\t])|\\r(?![ \\t\\n])').search Which seems to assert that having a newline followed by a space is not allowed.", "created_utc": 1444248837, "gilded": 0, "name": "t1_cvrobw1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "rbevans", "body": "Sweet that did it. Thanks!", "created_utc": 1444249026, "gilded": 0, "name": "t1_cvrogdr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "GoldenSights", "body": "Yeah, no problem!", "created_utc": 1444249555, "gilded": 0, "name": "t1_cvrotqr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "rbevans", "body": "Is there an advantage or disadvantage of using my login information vs creating a bot account for posting?", "created_utc": 1444261686, "gilded": 0, "name": "t1_cvrwecb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "GoldenSights", "body": "In terms of the API, no. Making a separate account might be a little more safe, in the event that it gets stolen somehow, so I would recommend it. I didn't want to mention this earlier because you were still setting everything else up, but you may actually want to use OAuth instead of your plain password. I have a tutorial for converting to oauth [here](https://www.reddit.com/r/GoldTesting/comments/3cm1p8/how_to_make_your_bot_use_oauth2/), and once you do the setup a single time it behaves just like the old login. I think it's worth a try if you will be hosting the bot long-term.", "created_utc": 1444263982, "gilded": 0, "name": "t1_cvrxsar", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "rbevans", "body": "This is good to know. Being oauth is more secure I'll work on that. With oauth do you still recommend using a separate account?", "created_utc": 1444264444, "gilded": 0, "name": "t1_cvry2p3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "GoldenSights", "body": "I probably would, but it's not really necessary, since you're only using it to put together a weekly post. Personally I like being able to keep bot stuff separate from my normal account, but it's not like this bot has much of an identity anyway. Using your main account will let you see replies, which could be a plus. You're not locked into a single solution though, so you can try whatever is easiest and see how it goes.", "created_utc": 1444264860, "gilded": 0, "name": "t1_cvrycb5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "Deimorz", "body": "You're trying to set an invalid user agent, it can't have a newline (`\\n`) in it.", "created_utc": 1444248615, "gilded": 0, "name": "t1_cvro67r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "rbevans", "body": "Sweet that did it. Thanks!", "created_utc": 1444249030, "gilded": 0, "name": "t1_cvroghr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "GoldenSights", "body": "It seems that this requirement wasn't added until Python 3.5, because I'm testing it in 3.4 and it works just fine. Do you know why they would add that restriction? I'm curious what the problem was.", "created_utc": 1444249007, "gilded": 0, "name": "t1_cvrofwm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "Deimorz", "body": "I'm actually not sure why that was causing an error, the regex you posted in your other comment seems to match [the RFC](http://www.w3.org/Protocols/rfc2616/rfc2616-sec4.html), which says: > Header fields can be extended over multiple lines by preceding each extra line with at least one SP or HT. `\\n(?![ \\t])` means \"newline not followed by a tab or space\", so it should only match if it's followed by some *other* character. The UA shown in the OP does seem to have a space after the newline, so I'm not sure.", "created_utc": 1444250073, "gilded": 0, "name": "t1_cvrp6oz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "GoldenSights", "body": "Oops I must have been thinking backwards when I wrote that comment then -- the regex isn't suppsed to match for a good ua. I wonder if it's because the ua in OP's traceback is a bytes string? Does that break regex? http://stackoverflow.com/questions/5618988/python-regular-expression-parsing-binary-file", "created_utc": 1444251556, "gilded": 0, "name": "t1_cvrq7pp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "Deimorz", "body": "Definitely possible, I'm not sure why it's a bytes string.", "created_utc": 1444253185, "gilded": 0, "name": "t1_cvrrae3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "GoldenSights", "body": "I'll have to look around and see if the bytes thing is PRAW's fault. Thanks for your help!", "created_utc": 1444254886, "gilded": 0, "name": "t1_cvrsdt4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3nvjri/i_could_use_some_help_to_understand_why_im/"}, {"author": "gooeyblob", "body": "Are you trying to page through `reddits.json` or something? If so, just slow down your requests. 429 means slow down!", "created_utc": 1443471569, "gilded": 0, "name": "t1_cvha09f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3morip/trying_to_scrape_a_list_of_subreddits/"}, {"author": "sWeeX2", "body": "req = requests.get(\"https://www.reddit.com/reddits.json\") data = req.json() This is what I was trying to do. The data contains the first page of all subreddits (first 25) from what I can see. When I try to read through the .json to just get back titles I'm hitting that 429. I know it has to do with the requests coming in too fast but I'm not entirely sure how to slow this down? I'm pretty new to Python so I may just be doing something silly. I will subsequently want to be able to go through their pagination and grab a large amount of subreddit titles as well as subscriber counts. Thanks for your reply, any help is greatly appreciated.", "created_utc": 1443477518, "gilded": 0, "name": "t1_cvhdy3f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3morip/trying_to_scrape_a_list_of_subreddits/"}, {"author": "gooeyblob", "body": "You need to check out the API docs here (and understand paging): https://www.reddit.com/dev/api#GET_subreddits_{where} You also need to change your user agent to something identifiable (not the default requests user agent, which will cause you to be 429d almost immediately).", "created_utc": 1443480656, "gilded": 0, "name": "t1_cvhfsle", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3morip/trying_to_scrape_a_list_of_subreddits/"}, {"author": "sWeeX2", "body": "I have my user-agent set alright. As for the doc's I'll have a look and if I'm still some what stuck I might post back here. Thanks for the help buddy.", "created_utc": 1443481666, "gilded": 0, "name": "t1_cvhgd0a", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3morip/trying_to_scrape_a_list_of_subreddits/"}, {"author": "Agothro", "body": "I would figure they're just like comments normally removed by admins.", "created_utc": 1443406281, "gilded": 0, "name": "t1_cvggv8o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3mmt0t/how_would_i_dmca_a_comment_on_a_local_install/"}, {"author": "13steinj", "body": "I believe I tried this and it didn't work (the biggest part I see about admin mode is that the user becomes a mod of all subreddits, so removing should just be a normal remove that way), but I need to specifically test dmca removals.", "created_utc": 1443406456, "gilded": 0, "name": "t1_cvggybq", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3mmt0t/how_would_i_dmca_a_comment_on_a_local_install/"}, {"author": "Agothro", "body": "I mean, I don't think there's a different backend process afaik. The DMCA stuff might be handled differently on the human end but not the machine end. But I couldn't tell you for sure.", "created_utc": 1443406531, "gilded": 0, "name": "t1_cvggzn7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3mmt0t/how_would_i_dmca_a_comment_on_a_local_install/"}, {"author": "13steinj", "body": "No, it is different. DMCA removed things have a change in the removal_reason json key and have a special body, and the author name is still existant.", "created_utc": 1443406819, "gilded": 0, "name": "t1_cvgh4qn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3mmt0t/how_would_i_dmca_a_comment_on_a_local_install/"}, {"author": "Agothro", "body": "Hmmm ok then. Maybe trawl the source to find something? Good luck", "created_utc": 1443406869, "gilded": 0, "name": "t1_cvgh5q1", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3mmt0t/how_would_i_dmca_a_comment_on_a_local_install/"}, {"author": "GoldenSights", "body": "I'm pretty sure I know what the issue is, but I won't be able to make a pull request until I get home this evening. I'll let you know what happens -- thanks for pointing this out! edit: Here we go! https://github.com/praw-dev/praw/pull/534", "created_utc": 1443129131, "gilded": 0, "name": "t1_cvd0vwi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3m8ya2/praw_v321_crash_on_unhide_using_oauth2/"}, {"author": "boib", "body": "Thanks! I just now saw your edit.", "created_utc": 1443625935, "gilded": 0, "name": "t1_cvjcar2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3m8ya2/praw_v321_crash_on_unhide_using_oauth2/"}, {"author": "13steinj", "body": "Username mentions have have a `[\"new\"]` json dictionary key (or in PRAW, a `new` attribute) which is Boolean (True/False) If it is unread, it is true. If it is false, then it was already read. However; there are several dillemas. Do you want it to retain it's generator functionalities? To do that you'd have to make a function def strip_not_new(generator_object): for thing in generator_object: if thing.new is True: yeild thing And runt it on the `get_mentions()` generator. Or, if you want them as a list: new_messages = [thing for thing in mentions if thing.new is True] Or, if you want to turn that into a list iterator, have the previous statement and add new_messages = iter(new_messages) However be wary. I don't recall which ones if any, but I know at least one of the above will raise an index error if there is no new messages and you call the [0] index on it. You'll need to try and catch that exception.", "created_utc": 1442854044, "gilded": 0, "name": "t1_cv95vce", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "kemitche", "body": "`itertools` can also help! new_messages = itertools.ifilter(lambda x: x.new, all_messages)", "created_utc": 1442856428, "gilded": 0, "name": "t1_cv97evp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "pyskell", "body": "Haven't looked at Python lambdas yet, but that's sweet. Thanks for that. Also for anyone reading, in Python 3 the equivalent is the function filter(), same syntax, no imports needed.", "created_utc": 1442864404, "gilded": 0, "name": "t1_cv9coht", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "kemitche", "body": "`filter` is in python 2 as well, but returns a `list` instead of an iterator. I didn't realize it'd changed in 3, but it's great!", "created_utc": 1442866094, "gilded": 0, "name": "t1_cv9dszj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "pyskell", "body": "Thanks! I'll try this out. I guess I need to always check if each mention is new then? What happens if I eventually have a backlog of 1000 mentions though, will my script need to retrieve and then check each one? It seems odd to retrieve 999 mentions only to get at the 1 new one that I'm interested in. Or do you know if reddit expires mentions over time and this isn't something I'll need to worry about?", "created_utc": 1442854909, "gilded": 0, "name": "t1_cv96for", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "13steinj", "body": "You can also use itertools like /u/kemitche pointed out. ___ The thing is, it all depends on how known your account is . There can be multiple new mentions at once, theoretically. Also theoretically, there can be a lot of those. In practice, I haven't been able to get more than max of 5 new mentions in a day, so I think you're safe if you add the `limit=100` keyword argument to limit the total amount of mentions you grab.", "created_utc": 1442860773, "gilded": 0, "name": "t1_cv9a9kh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "pyskell", "body": "Yeah I suppose I'm worrying about this possible issue too early. Thanks for all your help and suggestions!", "created_utc": 1442864320, "gilded": 0, "name": "t1_cv9cmhd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lt7vv/ignoring_read_mentions/"}, {"author": "boussouira", "body": "On the command line write `pip` then press TAB to get available pip commands", "created_utc": 1442941676, "gilded": 0, "name": "t1_cvad1ay", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lpbsc/trying_to_install_praw_on_raspberry_pi/"}, {"author": "13steinj", "body": "Pip changed how it does version selective installations. It's `pip3.2`, not `pip-3.2`. Also, for the sake of consistencies and less bugs, I recommend upgrading to Python 3.4.3 (sudo apt-get something I think? Don't know how it works on Pi) and from there you could just use `pip3` as that defaults to the latest python 3 version. `pip` should default to the latest version altogether, but I have found that sometimes on windows this is not the case.", "created_utc": 1442775343, "gilded": 0, "name": "t1_cv855lq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lpbsc/trying_to_install_praw_on_raspberry_pi/"}, {"author": "tittycleavage", "body": "Judging by the error you're getting it seems that either you don't have pip installed or the version you installed isn't in the command path. If you installed pip using `apt-get install python-pip` you might not have the latest version or something borked along the way. I'd suggest just downloading the `get-pip.py` script from the makers of pip and run it. Just use wget.", "created_utc": 1442776276, "gilded": 0, "name": "t1_cv85smh", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lpbsc/trying_to_install_praw_on_raspberry_pi/"}, {"author": "charredgrass", "body": "OK, I'm a total idiot here. What's the advantage if using OAuth2 over HTTP?", "created_utc": 1442802582, "gilded": 0, "name": "t1_cv8m73g", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "body": "password based authentication is going to be removed and also PRAW4 won't have this at all. Only you have to migrate to OAuth2 anyways. If you are writing an app which is used by other users, then OAuth is better since you don't need handle their username, password at all. Reddit will do all that for you. Your app/script will have fine grained permissions. A bot using HTTP Login will have full access to account. But with OAuth2 based ones, we can restrict it. Lastly, Reddit is nice for Oauth2 users. The rate limits are less for OAuth. Admins are encouraging everyone to move to OAuth", "created_utc": 1442815955, "gilded": 0, "name": "t1_cv8s13c", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "charredgrass", "body": "I see, thanks!", "created_utc": 1442847828, "gilded": 0, "name": "t1_cv91z8n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": null, "body": "[deleted]", "created_utc": 1442794659, "gilded": 0, "name": "t1_cv8hirv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "body": "wish I could have helped you, but I don't know java :/", "created_utc": 1442815177, "gilded": 0, "name": "t1_cv8rskf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "sje46", "body": "Big help, thank you!", "created_utc": 1445485934, "gilded": 0, "name": "t1_cw8hmkv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "13steinj", "body": "While this is definitely nice; You might want to remove the underlying sense of bias. Some people such as myself would much rather use [OAuth2Util](http://www.github.com/smbe19/praw-Oauth2util), as it's much easier to go through IMO, just add your scopes, client id and app secret in a config; then in your script call o = OAuth2Util.OAuth2Util(your_reddit_client) o.refresh(force=True) and then PRAW will automatically refresh your tokens when necessary without user intervention, and at least IMO, it's much simpler.", "created_utc": 1442770421, "gilded": 0, "name": "t1_cv81t55", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "dClauzel", "body": "I agree, I personally find \u201cpraw-oauth2util\u201d far more easier to use. But eh, whatever float your goat \ud83d\udc10 See https://github.com/dClauzel/Reddit for some examples. Just this and a config file with the tokens is enough. r = praw.Reddit(user_agent=\"posix:MonScript:v0 (by /u/MonIdentifiantReddit)\") o = OAuth2Util.OAuth2Util(r, print_log=False) o.refresh()", "created_utc": 1442951524, "gilded": 0, "name": "t1_cvajrob", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "13steinj", "body": "On top of that, if you pass the `force=True` kwarg to `o.refresh()`, on PRAW >= 3.2.0, you never have to call the method again, as PRAW will handle refreshing the token from then on.", "created_utc": 1442952587, "gilded": 0, "name": "t1_cvaki4a", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "body": "> You might want to remove the underlying sense of bias. Where I have shown bias? > Some people such as myself would much rather use OAuth2Util[1] , as it's much easier to go through IMO, just add your scopes, client id and app secret in a config; then in your script call. I don't see any difference here, other than number of params in the call. Also, there is a process behind scopes, getting app key, app secret etc, hence this long tutorial.", "created_utc": 1442772418, "gilded": 0, "name": "t1_cv83671", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "13steinj", "body": "The way it was written was as if it's only possible to use oauth with your wrapper. It's more than the number of params in the function calls, don't even try to convince otherwise. With this a separate process has to be kept and maintained and used, and a refresh called when needed, whereas with OAuth2Util, You just fill in the simple config, or if wanted you can pass things through via instantiating the Oauth2Util class, call it once, if it's the first time your web browser opens ( or you connect if it's on your server; it has the ability to run on servers now), and then force refresh once, and then everything else is handled for you until the end of time, and you never have to worry whatsoever.", "created_utc": 1442773102, "gilded": 0, "name": "t1_cv83nq7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "body": "> The way it was written was as if it's only possible to use oauth with your wrapper. I don't see where it is. please highlight that part, I shall edit it from OP. > and then force refresh once, and then everything else is handled for you until the end of time if thats the case, I stand corrected. I think I missed about refresh, so I will look praw at API and try to release that feature. Without having to worry about refresh is a good thing.", "created_utc": 1442773536, "gilded": 0, "name": "t1_cv83yv3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "13steinj", "body": "The title >How to migrate your existing bots from HTTP to OAuth2 combined with >Hey folks, I am the author of `prawoauth2`, a library which makes writing Reddit bots/apps using OAuth2 super easy and simple. Lately I have been receiving private messages, seeking help in migration, so I thought I would write a tutorial and redirect them here next time. Most of the text below is copied from the documentation. Plus the given TLDR makes it seem as if this is the *only* way, but in reality, you could use this, OAuth2Util, some separate wrapper, some custom wrapper, or no wrapper at all. >>and then force refresh once, and then everything else is handled for you until the end of time >if thats the case, I stand corrected. >I think I missed about refresh, so I will look praw API and try to release that feature. Without having to worry about refresh is a good thing. Nice, I look forward to using it in some scripts in the future (I do use your wrapper in several use cases where it's more practical for me; but otherwise OAuth2Util is my way to go). Be careful though, this auto refresh functionality is only available in praw 3.2 and higher, but at this time people use versions lower, such as 3.0 and 3.1. Also, some people like to manually refresh the token at a given point in time regardless if the hour is up, so I'd suggest adding the ability as well.", "created_utc": 1442774298, "gilded": 0, "name": "t1_cv84h9g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "body": "just checked, since praw will handle the auto refresh, I don't need to make any changes in `prawoauth2` so I removed the refresh part from above tutorial.", "created_utc": 1442816398, "gilded": 0, "name": "t1_cv8s5rr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "body": "hey I just checked, but praw docs does not say anything about praw doing auto refresh: https://praw.readthedocs.org/en/stable/pages/oauth.html", "created_utc": 1442815298, "gilded": 0, "name": "t1_cv8rtw0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "GoldenSights", "body": "It's true, auto-refresh was added in 3.2.0: https://github.com/praw-dev/praw/blob/master/CHANGES.rst#praw-320 I'll update the docs, sorry about that.", "created_utc": 1442815686, "gilded": 0, "name": "t1_cv8ry5c", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "body": "edit: ninja'ed", "created_utc": 1442816049, "gilded": 0, "name": "t1_cv8s226", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "avinassh", "body": "great, thanks!", "created_utc": 1442816446, "gilded": 0, "name": "t1_cv8s6an", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/"}, {"author": "13steinj", "body": "The timestamp is the timestamp info is in the query as `timestamp:1420027200..1420070400`, not as a URL parameter.", "created_utc": 1442760332, "gilded": 0, "name": "t1_cv7wf92", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lo4gn/praw_and_timestamp_searches/"}, {"author": "lapropriu", "body": "Oh. :) Thanks!", "created_utc": 1442760867, "gilded": 0, "name": "t1_cv7wln0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3lo4gn/praw_and_timestamp_searches/"}, {"author": "47Toast", "body": "Just append .json at the end of the link and you'll get a json object of the comment. Use a json function from PHP and you have the comment. Just make sure you're using a good user-agent string and not querying more often than 30 times per minute. No need to login in, except if you want the 60 queries per minute.", "created_utc": 1442657060, "gilded": 0, "name": "t1_cv6sskz", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "jpflathead", "body": "Thanks. As an example, your comment, retrieved from an unlogged in browser. Holy shit, 8K of crapola sent for 66 bytes of your comment, and it includes the original post, ... TWICE! [ { \"kind\": \"Listing\", \"data\": { \"modhash\": \"diyerhf89v9b48746bad03491cb1e3a2bcded3644e87f7fa62\", \"children\": [ { \"kind\": \"t3\", \"data\": { \"domain\": \"self.redditdev\", \"banned_by\": null, \"media_embed\": { }, \"subreddit\": \"redditdev\", \"selftext_html\": \"<!-- SC_OFF --><div class=\\\"md\\\"><p>I want to build the world&#39;s simplest application. When someone on my website pastes in a permalink to a comment, I want my site to retrieve that comment and return it. <\\/p>\\n\\n<p>The person doing the pasting does not even need to log in. <strong>I just want to retrieve a comment.<\\/strong><\\/p>\\n\\n<p>Yet, it seems like I&#39;m being forced to jump through 1000 hoops of fire to get everything to work. Why is Reddit&#39;s API so complicated to use compared to Twitter, AWS, or any other API?<\\/p>\\n\\n<p>Based off what I read, it seemed like I needed an API wrapper, so I went and downloaded <a href=\\\"https:\\/\\/github.com\\/jcleblanc\\/reddit-php-sdk\\\">https:\\/\\/github.com\\/jcleblanc\\/reddit-php-sdk<\\/a>. <\\/p>\\n\\n<p>And so, the problems began. <\\/p>\\n\\n<ol>\\n<li><p>Oh, my project uses composer. This API wrapper doesn&#39;t. I tried for hours to try and get class autoloading set up. It didn&#39;t work. I then had to fork it, reorganize the project, add PSR-0 support, and then add it to packagist. That took up all my time last night.<\\/p><\\/li>\\n<li><p>Cool, now I&#39;m using an semi-abandoned API wrapper to try and interact with a poorly documented API. <\\/p><\\/li>\\n<li><p>Now I can&#39;t get OAuth2 to work correctly. Why do I need to even use OAuth2 for this?! <\\/p><\\/li>\\n<li><p>Spend some more hours wasting my time looking at other API wrappers, PRAW, Guzzle, OAuth-2 repositories.<\\/p><\\/li>\\n<\\/ol>\\n\\n<p>I&#39;ve spend the last 2 days of my time doing this, with no results to show for it. The simplest task ever, and there&#39;s roadblocks everywhere.<\\/p>\\n\\n<p>Sorry for the rant, but I feel like I&#39;m doing something fundamentally wrong here. Is it really this difficult to retrieve a comment from Reddit? I feel like I&#39;m not even using the correct architecture here, should I just be using a bot? How does that work and how does it differ from an &quot;app&quot;?<\\/p>\\n<\\/div><!-- SC_ON -->\", \"selftext\": \"I want to build the world's simplest application. When someone on my website pastes in a permalink to a comment, I want my site to retrieve that comment and return it. \\n\\nThe person doing the pasting does not even need to log in. **I just want to retrieve a comment.**\\n\\nYet, it seems like I'm being forced to jump through 1000 hoops of fire to get everything to work. Why is Reddit's API so complicated to use compared to Twitter, AWS, or any other API?\\n\\nBased off what I read, it seemed like I needed an API wrapper, so I went and downloaded https:\\/\\/github.com\\/jcleblanc\\/reddit-php-sdk. \\n\\nAnd so, the problems began. \\n\\n1. Oh, my project uses composer. This API wrapper doesn't. I tried for hours to try and get class autoloading set up. It didn't work. I then had to fork it, reorganize the project, add PSR-0 support, and then add it to packagist. That took up all my time last night.\\n\\n2. Cool, now I'm using an semi-abandoned API wrapper to try and interact with a poorly documented API. \\n\\n3. Now I can't get OAuth2 to work correctly. Why do I need to even use OAuth2 for this?! \\n\\n4. Spend some more hours wasting my time looking at other API wrappers, PRAW, Guzzle, OAuth-2 repositories.\\n\\nI've spend the last 2 days of my time doing this, with no results to show for it. The simplest task ever, and there's roadblocks everywhere.\\n\\nSorry for the rant, but I feel like I'm doing something fundamentally wrong here. Is it really this difficult to retrieve a comment from Reddit? I feel like I'm not even using the correct architecture here, should I just be using a bot? How does that work and how does it differ from an \\\"app\\\"?\\n\", \"likes\": null, \"suggested_sort\": null, \"user_reports\": [ ], \"secure_media\": null, \"link_flair_text\": null, \"id\": \"3ligwq\", \"from_kind\": null, \"gilded\": 0, \"archived\": false, \"clicked\": false, \"report_reasons\": null, \"author\": \"EchoLogic\", \"media\": null, \"name\": \"t3_3ligwq\", \"score\": 3, \"approved_by\": null, \"over_18\": false, \"hidden\": false, \"thumbnail\": \"\", \"subreddit_id\": \"t5_2qizd\", \"edited\": false, \"link_flair_css_class\": null, \"author_flair_css_class\": null, \"downs\": 0, \"mod_reports\": [ ], \"secure_media_embed\": { }, \"saved\": false, \"removal_reason\": null, \"stickied\": false, \"from\": null, \"is_self\": true, \"from_id\": null, \"permalink\": \"\\/r\\/redditdev\\/comments\\/3ligwq\\/how_does_a_reddit_bot_differ_from_an_application\\/\", \"hide_score\": false, \"created\": 1442659476, \"url\": \"http:\\/\\/www.reddit.com\\/r\\/redditdev\\/comments\\/3ligwq\\/how_does_a_reddit_bot_differ_from_an_application\\/\", \"author_flair_text\": null, \"quarantine\": false, \"title\": \"How does a reddit bot differ from an application? Are they the same thing? When do I need OAuth2?\", \"created_utc\": 1442630676, \"ups\": 3, \"upvote_ratio\": 1, \"num_comments\": 17, \"visited\": false, \"num_reports\": null, \"distinguished\": null } } ], \"after\": null, \"before\": null } }, { \"kind\": \"Listing\", \"data\": { \"modhash\": \"diyerhf89v9b48746bad03491cb1e3a2bcded3644e87f7fa62\", \"children\": [ { \"kind\": \"t1\", \"data\": { \"subreddit_id\": \"t5_2qizd\", \"banned_by\": null, \"removal_reason\": null, \"link_id\": \"t3_3ligwq\", \"likes\": true, \"replies\": \"\", \"user_reports\": [ ], \"saved\": false, \"id\": \"cv6sskz\", \"gilded\": 0, \"archived\": false, \"report_reasons\": null, \"author\": \"47Toast\", \"parent_id\": \"t3_3ligwq\", \"score\": 2, \"approved_by\": null, \"controversiality\": 0, \"body\": \"Just append .json at the end of the link and you'll get a json object of the comment. Use a json function from PHP and you have the comment. Just make sure you're using a good user-agent string and not querying more often than 30 times per minute.\\n\\nNo need to login in, except if you want the 60 queries per minute.\", \"edited\": false, \"author_flair_css_class\": null, \"downs\": 0, \"body_html\": \"<div class=\\\"md\\\"><p>Just append .json at the end of the link and you&#39;ll get a json object of the comment. Use a json function from PHP and you have the comment. Just make sure you&#39;re using a good user-agent string and not querying more often than 30 times per minute.<\\/p>\\n\\n<p>No need to login in, except if you want the 60 queries per minute.<\\/p>\\n<\\/div>\", \"subreddit\": \"redditdev\", \"score_hidden\": false, \"name\": \"t1_cv6sskz\", \"created\": 1442685860, \"author_flair_text\": null, \"created_utc\": 1442657060, \"distinguished\": null, \"mod_reports\": [ ], \"num_reports\": null, \"ups\": 2 } } ], \"after\": null, \"before\": null } } ]", "created_utc": 1442671892, "gilded": 0, "name": "t1_cv6wq0y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "earth-tone", "body": "You should use Oauth all the time. Aside from getting twice the number of requests and not having to deal with cookie/session/modhash management anymore, non-Oauth access is unpredictable at best. It is for me, anyway. Most of my scripts are Oauth, and the one that wasn't started getting 403s constantly (unpredictably, but constantly). So much so that it became basically useless and I went full Oauth. Other people say this hasn't happened to them, so your mileage may vary. I don't know about that API wrapper, but I know it's very simple in the [Perl wrapper](http://search.cpan.org/~earthtone/Reddit-Client/lib/Reddit/Client.pm). my $reddit = new Reddit::Client(username, password, client id, secret, arbitrary user agent string); my $info = $reddit->info(the comment's fullname, aka t1_foobar); Most people around here will probably be using the Python wrapper, though (PRAW). But basically, in any functioning wrapper, you should be able to plug in your 4 pieces of information (username, password, client id, token) and make a request.", "created_utc": 1442633577, "gilded": 0, "name": "t1_cv6m5om", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "5225225", "body": "My problem with oauth is the complexity. Yeah, I'm sure it's so much better than just being able to pass a username/password and you get logged in, but is there any way around needing to run a *webserver* on the client's machine just to log in?", "created_utc": 1443475143, "gilded": 0, "name": "t1_cvhcg1i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "earth-tone", "body": "Webserver? You just put in the username, password, oauth ID and secret. If you're running a personal script (i.e. it won't need permission to use other people's accounts), that's all you need. The example I linked is just those two lines of code, literally (in that case it's just getting info about a post).", "created_utc": 1443475648, "gilded": 0, "name": "t1_cvhcrnv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "5225225", "body": "But if you want to make a proper client that can authenticate as people other than you, you do have to set up something to get the redirect. To me, it feels way too over engineered for something that could just be \"here's my username, here's my password\". You still have to trust the application not to fuck with you no matter what you're doing.", "created_utc": 1443475954, "gilded": 0, "name": "t1_cvhcyoy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "13steinj", "body": "I know some endpoints are supposedly oauth only, so that could be a cause for the 403s. I've had no problem so far (I only use cookie auth for one time things though, since setting up tokens for something that I'll only run once is stupid).", "created_utc": 1442634235, "gilded": 0, "name": "t1_cv6mfq8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "earth-tone", "body": "No, this was intermittent 403s-- sometimes it would work, sometimes 403. But I make a lot of requests and it's possible I was going over the 30 limit but staying under the 60 limit. Either was it was simpler for me just to go full Oauth (in fact my non-Oauth script was a halfhearted attempt to support non-Oauth for other people, not for myself).", "created_utc": 1442634407, "gilded": 0, "name": "t1_cv6miat", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "EchoLogic", "body": "Thanks. That code looks so beautifully simple. > in any functioning wrapper Sadly, it doesn't seem like there's a quality PHP wrapper. The recommended one either doesn't work or I'm doing something stupid with it that constantly gives me 403 requests. So now, I'm having to look into manual OAuth2 authentication and using an HTTP client like Guzzle to do all the requests manually.", "created_utc": 1442634230, "gilded": 0, "name": "t1_cv6mfnk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "earth-tone", "body": "It won't be bad, just a bunch of typing. I'm not familiar with Guzzle (or even composer-- for someone that got paid to write PHP for years, I'm behind the times). You could do it with CURL requests. The only thing differentiating it from a normal request is that it needs a special header that looks like \"Authorization: (token type) (token)\". For a script the token type would be bearer.", "created_utc": 1442634803, "gilded": 0, "name": "t1_cv6moaz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "EchoLogic", "body": "Well, I got this far: $client = new GuzzleHttp\\Client(); $response = $client->request('POST', 'https://www.reddit.com/api/v1/access_token', array( 'query' => [ 'user' => 'myClientId', 'password' => 'myClientSecret' ], 'form-params' => [ 'grant_type' => 'client_credentials' ] )); Still not working. I don't really understand this.", "created_utc": 1442636807, "gilded": 0, "name": "t1_cv6ngxm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "earth-tone", "body": "* Grant type should be password. * grant type, username, and password should all be in the POST data * client id and secret go in the URL (I think there's another way to do this, but this is how I did it), like so: `$url = \"https://$client_id:$secret\\@www.reddit.com/api/v1/access_token\";` * don't forget the user agent * don't forget the form header (or maybe Guzzle is doing that for you): 'content-type' => 'application/x-www-form-urlencoded'", "created_utc": 1442637938, "gilded": 0, "name": "t1_cv6nwhw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "EchoLogic", "body": "Why should grant type be password? Don't I want [Application-only auth](https://github.com/reddit/reddit/wiki/OAuth2#application-only-oauth) since I'm not interacting with users at all?", "created_utc": 1442638308, "gilded": 0, "name": "t1_cv6o1fh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "earth-tone", "body": "You want script type, if it's only you (or your site) using it. Other grant types are for when you don't have a password (i.e. other people using an app you wrote to access their accounts or something).", "created_utc": 1442638591, "gilded": 0, "name": "t1_cv6o50e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "EchoLogic", "body": "Okay, so do I have this right? In the URI, I should have the following query parameters: * client_id * response_type * state * code * redirect_uri * duration * scope In the HTTP basic Auth header, I should have: * My account (not script) username, my account (not scropt) password In the x-www-form-urlencoded POST data, I should have: * the grant type of password Right now, I'm getting a 401.", "created_utc": 1442639774, "gilded": 0, "name": "t1_cv6ojoj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "earth-tone", "body": "Just to get the token, the post data only needs username, password, and grant type. If this helps, this is the code that gets a token in Perl's Reddit::Client. Obviously it's not the same language but the syntax is closer to PHP than anything else out there so it's not terribly different: $url = \"https://$client_id:$secret\\@www.reddit.com/api/v1/access_token\"; $ua = LWP::UserAgent->new(user_agent => $useragent); $req = HTTP::Request->new(POST => $url); $req->header('content-type' => 'application/x-www-form-urlencoded'); $postdata = \"grant_type=password&username=$username&password=$password\"; $req->content($postdata); Then it makes the request $res = $ua->request($req); So the \"browser\" is LWP::UserAgent, and the request itself is an HTTP::Request object, which basically just formats the header and body nicely. Whatever the equivalent for those would be in Guzzle.", "created_utc": 1442641567, "gilded": 0, "name": "t1_cv6p4c1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "EchoLogic", "body": "Thanks! I did get it working, but it ended up requiring the client id and client secret of my application, in addition to my personal username, password, plus the grant type, code below: $client = new GuzzleHttp\\Client(); $response = $client->post('https://www.reddit.com/api/v1/access_token', array( 'query' => [ ['client_id' => Credential::RedditID, 'response_type' => 'code', 'state' => '4939fjkek3', 'redirect_uri' => 'http://localhost/reddit/test.php', 'duration' => 'permanent', 'scope' => 'save,modposts,identity,edit,flair,history,modconfig,modflair,modlog,modposts,modwiki,mysubreddits,privatemessages,read,report,submit,subscribe,vote,wikiedit,wikiread'] ], 'auth' => [Credential::RedditId, Credential::RedditSecret], 'form_params' => [ 'grant_type' => 'password', 'username' => Credential::MyRedditAccount, 'password' => Credential::MyRedditPassword ] )); I can't believe it's taken me just that long to retrieve a token...", "created_utc": 1442642461, "gilded": 0, "name": "t1_cv6pdzg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "earth-tone", "body": "Yeah that's what this line was doing in another way, you can see the client ID and secret are there, just interpolated into the URL: $url = \"https://**$client_id**:**$secret**\\@www.reddit.com/api/v1/access_token\";", "created_utc": 1442642642, "gilded": 0, "name": "t1_cv6pfvn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "EchoLogic", "body": "Ah, I see now. Thanks for your help! I still haven't set a user agent. I should probably do that...", "created_utc": 1442642819, "gilded": 0, "name": "t1_cv6phqc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "damontoo", "body": "I find the API to be a pretty simple and typical REST service. Especially if you compare it to some Google API's which are just horrendous.", "created_utc": 1442647186, "gilded": 0, "name": "t1_cv6qnzf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "EchoLogic", "body": "I don't think it's so much the API anymore, it's just the particular wrapper I was trying to use. The documentation could be a bit better in some places, but you could chalk that up to me being useless I guess.", "created_utc": 1442647299, "gilded": 0, "name": "t1_cv6qp0j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ligwq/how_does_a_reddit_bot_differ_from_an_application/"}, {"author": "13steinj", "body": "~~Define \"download\".~~ ~~Python stores objects (including praw comment objects ) in RAM, not on the hard disk. With the pickle or cpickle libraries you can natively store and retrieve the object from a file on the hard disk (if that's what you would mean by download).~~ ~~If you don't need the full data, get what you need and putting it in a file of your choice, whether it be json, sql, etc for later retrieval.~~ What happens when you only read the title? You answer the wrong question. Speedwise you can't do much. If you literally need to get all those comments there's not much you can do. The number discrepancy is due to mod removed and deleted comments still being in the counter.", "created_utc": 1442429226, "gilded": 0, "name": "t1_cv3sbey", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "sWeeX2", "body": "Ahh okay thanks for the help. Download was a bad word my apologies, what I'm really looking to do is just scrape them, turn it into a file whether it be CSV or throw it into an SQL DB not sure yet. I need the volume of comments because I want to try conduct some sort of sentimental analysis on posts across comments/subreddits and then preform some sort of visualisation of the whole thing. I guess I'll just scrape a lot of data just the once and store it all instead of continually scraping (will have to perform demo possibly so wouldn't want to keep people waiting for all the data to come in). Thanks for the help.", "created_utc": 1442430405, "gilded": 0, "name": "t1_cv3t45y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "mjgcfb", "body": "Have you tried the comment stream helper that PRAW offers? https://praw.readthedocs.org/en/stable/pages/code_overview.html#praw.helpers.comment_stream", "created_utc": 1442439454, "gilded": 0, "name": "t1_cv3zf9c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "frescani", "body": "Did you ever solve this?", "created_utc": 1447962076, "gilded": 0, "name": "t1_cx61iel", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "sWeeX2", "body": "I'm afraid not, I've been taking a break with that project to concentrate on other commitments right now, but I'll be going back to it soon. Are you looking to do something similar?", "created_utc": 1447966981, "gilded": 0, "name": "t1_cx64vw3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "frescani", "body": "Yeah, trying to get all the comments on a recent AMA. It looks like the script we're using only goes to the 10th child comment, which is giving a similar truncation to what you mentioned, something like 15% of comments not being returned.", "created_utc": 1447968362, "gilded": 0, "name": "t1_cx65uop", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "sWeeX2", "body": "I remembered that I posted on another thread about this and someone responded to me about it. What he told me is that the discrepancy in the amount of comments you get back could be due to comments that have been deleted by mods, so if it tell you that there is 500 comments and you get 480, the 20 that are missing may have been deleted but are still in the counter, hope that helps a little. I'll be getting back to my project now towards Christmas and I can PM you or even drop a response on here if/when I make progress.", "created_utc": 1448057096, "gilded": 0, "name": "t1_cx7dflx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l7as5/praw_downloading_comments_from_threads/"}, {"author": "zimm3rmann", "body": "Use the account for a bit and get it some karna.", "created_utc": 1442329828, "gilded": 0, "name": "t1_cv2e0nu", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l15u0/praw_no_captcha_posting_and_approving_posts_on/"}, {"author": "badmonkey0001", "body": "Have the bot post a few times in /r/freekarma.", "created_utc": 1442335611, "gilded": 0, "name": "t1_cv2ho1c", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l15u0/praw_no_captcha_posting_and_approving_posts_on/"}, {"author": "13steinj", "body": "1. Go on /r/freekarma and get like 5 karma. Captchas should be gone. 2. Make the bot an approved submitter to the subreddit.", "created_utc": 1442345514, "gilded": 0, "name": "t1_cv2o8un", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3l15u0/praw_no_captcha_posting_and_approving_posts_on/"}, {"author": "zimm3rmann", "body": "If connecting with OAuth2 you can make up to 60 requests a minute, so as long as you aren't doing anything other than requesting messages you're good. I'm curious why you need that kind of speed though vs every 5 seconds even.", "created_utc": 1442260350, "gilded": 0, "name": "t1_cv1hx2a", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ky4l4/how_often_can_i_request_my_inbox/"}, {"author": "-SuicideThrowaway-", "body": "Just like to get my messages right away. Especially when I have lots coming in fast sometimes.", "created_utc": 1442260462, "gilded": 0, "name": "t1_cv1hzq2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ky4l4/how_often_can_i_request_my_inbox/"}, {"author": "kemitche", "body": "This might help you understand things: print redd.get_subreddit print redd.get_subreddit('subreddit') print redd.get_subreddit('subreddit').get_new print redd.get_subreddit('subreddit').get_new(limit=10) With that in mind, now try print redd.user.get_upvoted print redd.user.get_upvoted() ;)", "created_utc": 1442255825, "gilded": 0, "name": "t1_cv1exqi", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kxsbb/praw_get_upvoted_returning_a_bound_method_rather/"}, {"author": "atlusio", "body": "I thought the parentheses weren't necessary so I didn't specifically forget them, but when I added them that did the trick. Why it is that I receive a bound method when I omit the parentheses?", "created_utc": 1442289103, "gilded": 0, "name": "t1_cv1zm8j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kxsbb/praw_get_upvoted_returning_a_bound_method_rather/"}, {"author": "kemitche", "body": "`get_upvoted` is the function, so \"print\"ing it tells you that it's a function. \"Adding\" the parentheses means you're \"calling\" the function, and getting something out of it. Think of it instead like this: upvoted_things = redd.user.get_upvoted() print upvoted_things The call to `get_upvoted()` results in \"stuff\" happening, and you get `upvoted_things` back, which is what you want.", "created_utc": 1442295085, "gilded": 0, "name": "t1_cv22cw8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kxsbb/praw_get_upvoted_returning_a_bound_method_rather/"}, {"author": "zurtex", "body": "Try this: def foo(): return range(10) print(foo) print(foo()) The parenthesis will instantiate a method / function etc... Otherwise you just are just assigning the method / function to some other variable name. So it seems that get_upvoted is a method to return a get_content generator and not one itself. This could of been done differently by PRAW, just gotta understand the objects you're working with.", "created_utc": 1442294108, "gilded": 0, "name": "t1_cv21yx7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kxsbb/praw_get_upvoted_returning_a_bound_method_rather/"}, {"author": "GoldenSights", "body": "Youre forgetting the parentheses after get_upvoted :)", "created_utc": 1442255850, "gilded": 0, "name": "t1_cv1eycm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kxsbb/praw_get_upvoted_returning_a_bound_method_rather/"}, {"author": "13steinj", "body": "Firstly, you shouldn't be using `r.login()`. You should witch to OAuth, use OAuth2Util if you have to. Secondly, to be honest I have no idea. The root of the problem seems to be your login. Maybe try removing the `.encode('utf-8')` bit? I've never seen that, even in 2.7 scripts.", "created_utc": 1442206294, "gilded": 0, "name": "t1_cv0urpm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kuj2j/typeerror_buf_must_be_a_byte_string_in_praw/"}, {"author": "NotoriousHakk0r4chan", "body": "the .encode('utf-8') bit came at the recommendation of Google, but it didn't work, and neither does removing it. I will try to use OAuth though.", "created_utc": 1442260244, "gilded": 0, "name": "t1_cv1hujk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kuj2j/typeerror_buf_must_be_a_byte_string_in_praw/"}, {"author": "13steinj", "body": "If that doesn't work, we probably won't be able to help without looking at your source.", "created_utc": 1442260490, "gilded": 0, "name": "t1_cv1i0go", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kuj2j/typeerror_buf_must_be_a_byte_string_in_praw/"}, {"author": "NotoriousHakk0r4chan", "body": "Well, I got the same error when using OAuth on the reddit.get_access_information() part, any idea?", "created_utc": 1442262681, "gilded": 0, "name": "t1_cv1jgpl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kuj2j/typeerror_buf_must_be_a_byte_string_in_praw/"}, {"author": "13steinj", "body": "Try [this](http://www.reddit.com/r/redditdev/comments/3aybk8/login_error/cskfbx2), If it doesn't work I'd need a list of your currently installed packages and versions of them via `pip list` in your terminal.", "created_utc": 1442263594, "gilded": 0, "name": "t1_cv1k2fa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kuj2j/typeerror_buf_must_be_a_byte_string_in_praw/"}, {"author": "NotoriousHakk0r4chan", "body": "All my packages are as up-to-date as they can be. [Pip list here](http://pastebin.com/t2esactD). I tried hand installing pyopenssl from git because I read that might help. Python version is 2.9.7, but apt-get says its up to date.", "created_utc": 1442265115, "gilded": 0, "name": "t1_cv1l1ol", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kuj2j/typeerror_buf_must_be_a_byte_string_in_praw/"}, {"author": "13steinj", "body": "While not all, but enough of your packages are out of date. Try installing virtualenv and only include the packages that you need for your script to run. Then run with that virtual environment.", "created_utc": 1442265552, "gilded": 0, "name": "t1_cv1lbmy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kuj2j/typeerror_buf_must_be_a_byte_string_in_praw/"}, {"author": "GoldenSights", "body": "Each of the Comment and Submission objects will have a `subreddit` attribute, which is a Subreddit object. So you can use `comment.subreddit.display_name` on each of the comments you get to see where the user has been posting. Reddit doesn't have any shortcut that would tell you this immediately.", "created_utc": 1442015228, "gilded": 0, "name": "t1_cuyhvt0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3klvrj/can_the_api_get_a_list_of_subreddits_where_a/"}, {"author": "Frenchiie", "body": "thanks, this helps!", "created_utc": 1442016760, "gilded": 0, "name": "t1_cuyin52", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3klvrj/can_the_api_get_a_list_of_subreddits_where_a/"}, {"author": "kemitche", "body": "You can check the [user's karma](https://www.reddit.com/dev/api#GET_api_v1_me_karma) if you're acting on behalf of that user; that is effectively a list of places they've posted or commented.", "created_utc": 1442029929, "gilded": 0, "name": "t1_cuyoqpz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3klvrj/can_the_api_get_a_list_of_subreddits_where_a/"}, {"author": "13steinj", "body": "Firstly; may I say I like the \"hacked open\" handler, and if you would like to add it to praw modify it slightly to work via some form of keyword argument; then submit a pull request. Secondly; it **does** limit the rate; at least from what I know (try checking without a steam helper function). The stream helper functions are bugged in the latest stable release (3.2.1) and I believe lower. In the dev version, they are similarly bugged(I don't think /u/bboe had the chance to fix it since the issue i still open). [Disucssion here](https://github.com/praw-dev/praw/issues/501)", "created_utc": 1441857519, "gilded": 0, "name": "t1_cuwfavl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kci9s/how_does_tze_defaulthandler_work_in_praw/"}, {"author": "DarkMio", "body": "Actually, it didn't. After a day working on several tasks it struck me: https://github.com/praw-dev/praw/blob/master/praw/handlers.py#L104 The regular handler gets monkey patched to call class.rate_limit(request()) instead of class.request(). This is a use-case of a decorator (which is even written as one) and then patched up. Changing my PRAW Handler to this works then fine: @RateLimitHandler.rate_limit def request(self, request, proxies, timeout, verify, **_): self.logger.debug('{:4} {}'.format(request.method, request.url)) return self.http.send(request, proxies=proxies, timeout=timeout, allow_redirects=False, verify=verify) Looking deeper into the source, I believe [issue #319](https://github.com/praw-dev/praw/issues/319) is easily fixable by rewriting a chunk of the basic RateLimitHandler (which I probably will do today)", "created_utc": 1441859525, "gilded": 0, "name": "t1_cuwg7qb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kci9s/how_does_tze_defaulthandler_work_in_praw/"}, {"author": "bboe", "body": "The function is being decorated not monkey patched. It's necessary to decorate the old-school way because you can not use a decorator via `@class.function` within the same class the decorator is defined (as far as I recall). Overall I don't think the default handlers are thread safe so you'll need to add locks around the access and modification to the class variables. I'm not sure how any other changes would be of benefit. Edit: nevermind the rate_limit function should be thread safe so I'm not sure what the issue is.", "created_utc": 1441866222, "gilded": 0, "name": "t1_cuwimgn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kci9s/how_does_tze_defaulthandler_work_in_praw/"}, {"author": "DarkMio", "body": "Yeah, I worked a ton on it and threw together a completely new Handler: https://github.com/DarkMio/RedditRover/blob/master/core/PRAWHandler.py#L8 This one is right now like RateLimitHandler and supports OAuth, Cookie-Auth and No-Auth independently: OAuths can send a request every second, Cookie/No-Auths can send every two seconds. This should also fix this issue: https://github.com/praw-dev/praw/issues/319 Editedit: I ment issue #319 all the time, not 501. If you want, I can refactor this handler, add an optional logger for it and build the other handlers on top of it. A log of the current requests can be found here: http://hastebin.com/jumekicija.avrasm I am not sure how API Limitations are ment: Do I have to limit API-Calls to 30 calls per minute per IP when I am using no-auth and this applies to OAuth users aswell or are OAuth users completely independent?", "created_utc": 1441868373, "gilded": 0, "name": "t1_cuwj8je", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3kci9s/how_does_tze_defaulthandler_work_in_praw/"}, {"author": "13steinj", "body": "The `search` function returns a `get_content` generator object. What exactly are you attempting to print? The title? Normally one would do `for post in requests: print(post)`", "created_utc": 1441466975, "gilded": 0, "name": "t1_curk3ka", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "metaranha", "body": "Ok, this makes sense to me. I am trying to print the title, but i'll re-write things so that it doesn't use the generator object. Thanks so much for your reply!", "created_utc": 1441467231, "gilded": 0, "name": "t1_curk7yl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "body": "Also, what you are doing searches for something with no reason to do so. print('grabbing subreddit') subreddit = r.get_subreddit('IAMA') count = 0 while (count", "created_utc": 1441467311, "gilded": 0, "name": "t1_curk9b9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "metaranha", "body": "This script actually does most of what I need, but I'm still getting an error when I run it. Here's the output after it runs: grabbing subreddit 1885 :: [AmA Request] Someone from the Rowan County Clerk's Office Traceback (most recent call last): File \"./test1.py\", line 14, in if submission.link_flair_text.lower() == \"request\": AttributeError: 'NoneType' object has no attribute 'lower' Am I missing an import that has the lower function or something? It seems to choke the script and it won't pull any of the other top hits.", "created_utc": 1441468933, "gilded": 0, "name": "t1_curl16u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "body": "Well read my other comment just in case. It turns out /r/iama doesn't force flair text, so you'll have to incase the conditional block in a `try` and `except AttributeError: pass`.", "created_utc": 1441469539, "gilded": 0, "name": "t1_curlc4z", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "metaranha", "body": "sorry for being such a novice, but can you explain what you mean by \"IAMA doesn't force flair text\"?", "created_utc": 1441470401, "gilded": 0, "name": "t1_curls2p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "body": "The /r/Iama mods don't make flairs required/ some flairs have no text. This text is tested to be the request flair; but if a post has no flair the attribute is `null` or `NoneType` in Python. `lower()` can't be called on non-strings.", "created_utc": 1441471115, "gilded": 0, "name": "t1_curm5e1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "metaranha", "body": "oh, I understand. So how can you iterate over null flair text then? Edit: can I use an else statement?", "created_utc": 1441471778, "gilded": 0, "name": "t1_curmhs7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "body": "Like I said, use `try` ... try: except AtrributeError: pass", "created_utc": 1441472821, "gilded": 0, "name": "t1_curn1sk", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "metaranha", "body": "What i'm trying to do is build a bot that will search through posts for \"requested\" flair, and then output to a file the vote count and title. As usual though, I'm getting wrapped around the axle on overcomplicated ways of doing that, so I'm trying to write one part of the script at a time to simplify things.", "created_utc": 1441467660, "gilded": 0, "name": "t1_curkf7k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "body": "Well there's more than one way to do that, but I would do something like this for the sake of ease (though, this would create a shit ton more requests than needed since /r/Iama is slow) DESIRED_AMOUNT = 10 file = open('output.txt', 'a') r = praw.Reddit('USERAGENT') while count", "created_utc": 1441469166, "gilded": 0, "name": "t1_curl5ds", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "metaranha", "body": "I have another question after playing with the code a little more. Out of curiosity, why would you choose to use the .submission_stream function? After reading about it, my understanding is that it never actually stops trying to parse as it watches for new submissions coming in. Is that just to make the bot persistent? Also, there weren't any results getting dumped to the output file even after adding the try envelope. Any ideas why that might be?", "created_utc": 1441552158, "gilded": 0, "name": "t1_cusi453", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "body": "1\\. Can you post a [gist](http://gist.github.com) or your code? 2\\. Yeah, but there are other ways to stop the submission_stream, such as forcing an error and catching it after a certain amount of time, like here's an unconventional use: class SubmissionStreamBreaker(Exception): pass counter = 0 try: while True: # By Default the session is always True for post in praw.helpers.submission_stream(r, 'IAMA', limit=None): count +=1 # do something to posts if count >= 37: # we did something for 37 new posts # it's time to do something else now # raise the exception that was made so that it is # caught and then continue raise SubmissionStreamBreaker('Break It') except SubmissionStreamBreaker: pass #do something else now.", "created_utc": 1441553400, "gilded": 0, "name": "t1_cusiol0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "metaranha", "body": "Functionally, it's basically the same code from yesterday, but I made a few changes (removed the stream call and added the try envelope) print(\"creating output file\") file = open('output.txt', 'a') r = praw.Reddit('IAMA') print(\"grabbing subreddit\") subreddit = r.get_subreddit('IAMA') while count", "created_utc": 1441555117, "gilded": 0, "name": "t1_cusji1a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "body": "Hm, don't know. It shouldn't acutally happen. Try this: while count", "created_utc": 1441558934, "gilded": 0, "name": "t1_cuslg7m", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "metaranha", "body": "yeah, very strange. It still hits an infinite loop. I don't see why it would be doing that, it should die after 5 attempts to get the hot list, right?", "created_utc": 1441559214, "gilded": 0, "name": "t1_cusllfx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "13steinj", "body": "Well, you could just not be hitting the wanted posts. Change `limit = 5` to something like `limit=25`", "created_utc": 1441560376, "gilded": 0, "name": "t1_cusm7bu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jr2qc/printing_search_results_generator_object_search/"}, {"author": "peoplma", "body": "Thanks, was wondering why it wasn't working. Need to figure out iauth lol", "created_utc": 1441327846, "gilded": 0, "name": "t1_cupx1jn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/"}, {"author": "13steinj", "body": "I recommend using [OAuth2Util](https://www.reddit.com/r/botwatch/comments/38k30h/oauth2util_a_wrapper_around_praws_oauth2/) as it makes the process much easier (the post is outdated; but [here's the latest update](https://www.reddit.com/r/botwatch/comments/3hwzkh/oauth2util_update/)).", "created_utc": 1441329375, "gilded": 0, "name": "t1_cupxx23", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/"}, {"author": "peoplma", "body": "Nice, thank you, I'll look into it :)", "created_utc": 1441330619, "gilded": 0, "name": "t1_cupyne8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/"}, {"author": "13steinj", "body": "Most people were already on https by now already; and /u/bboe released a supported version when the announcement post was made. I doubt anyone is actually going to have an issue", "created_utc": 1441326267, "gilded": 0, "name": "t1_cupw56n", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/"}, {"author": "green_flash", "body": "well, I did have an issue. Procrastination is a thing unfortunately.", "created_utc": 1441326423, "gilded": 0, "name": "t1_cupw8ij", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/"}, {"author": "13steinj", "body": "\u00af\\\\\\_(\u30c4)_/\u00af", "created_utc": 1441326690, "gilded": 0, "name": "t1_cupwe1x", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3jjqlq/reddit_login_is_https_only_now_it_seems_be_sure/"}, {"author": "GoldenSights", "body": "By using `comment.submission.id`, you're actually consuming API calls by fetching the submission object. Comments have a `link_id` attribute which will give you the t3 fullname immediately. I honestly think this is the most efficient solution, and the easiest, unless the post is a bit older and the 1,000 comment cache doesn't contain them all.", "created_utc": 1440913608, "gilded": 0, "name": "t1_cuki28m", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ixgq2/comment_stream_of_submission/"}, {"author": "codsane", "body": "I was noticing that when using comment.submission.id that things were running a bit slower, maybe that was the issue, thank you. I have been thinking for other solutions, and this looks like what I will have to go with. The 1,000 item cache definitely isn't enough, and a simple database of comments is no issue, just going to have to keep a script running through a post's lifetime, which isn't too big of a deal.", "created_utc": 1440913776, "gilded": 0, "name": "t1_cuki4hj", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ixgq2/comment_stream_of_submission/"}, {"author": "GoldenSights", "body": "Wait, now I'm wondering if you have two separate questions. The title is about streaming from a submission, but at the top of the post you also have \"get all comments of a submission\". Wouldn't `replace_more_comments` work for filling out the comment tree? Unless you're expecting the thread to blow up so much that watching the comment stream is easier than dealing with all of the morecomments loading later on (that was the case for /r/millionairemakers at one point).", "created_utc": 1440914033, "gilded": 0, "name": "t1_cuki7vu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ixgq2/comment_stream_of_submission/"}, {"author": "codsane", "body": "Well, I looked into grabbing all the comments, and for threads that may reach ~10k comments (rare, but possible), it seemed like it wouldn't be an easy task due to API limiting. I resorted to using a comment stream, but if there is a different way then I am totally open to switching.", "created_utc": 1440914297, "gilded": 0, "name": "t1_cukibav", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ixgq2/comment_stream_of_submission/"}, {"author": "GoldenSights", "body": "Yeah, in that case I would probably prefer to use the stream method. Doing the `replace_more_comments` on such a large post is a real pain. To cut down on API calls though, I would replace the `comment_stream` with the plain old `get_comments`. I don't think there's any subreddit that moves too quickly for that to work, and if you just use it once per 30 seconds it shouldn't interfere too much with other scripts you might be running.", "created_utc": 1440914453, "gilded": 0, "name": "t1_cukid98", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ixgq2/comment_stream_of_submission/"}, {"author": "GoldenSights", "body": "While method names are typically lowercased and underscore_separated, class names are typically upper and camel-cased in Python style guides. It's `praw.Reddit`.", "created_utc": 1440887999, "gilded": 0, "name": "t1_cuk5miy", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3iw7ij/module_object_has_no_attribute_reddit/"}, {"author": "natos20", "body": "Thanks!", "created_utc": 1440889044, "gilded": 0, "name": "t1_cuk660i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3iw7ij/module_object_has_no_attribute_reddit/"}, {"author": "kemitche", "body": "> Am I supposed to ask all the users to register their application? Yes, that would be the current best approach.", "created_utc": 1440780374, "gilded": 0, "name": "t1_cuisbkw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ipjs6/oauth2_with_open_source_apps/"}, {"author": "GoldenSights", "body": "Try this: s = r.get_subreddit('subreddit') userflairs = list(s.get_flair_list(limit=None)) The largest subreddit I mod returns 1,380 entries so presumably you will be able to get all of them.", "created_utc": 1440555428, "gilded": 0, "name": "t1_cuftkt5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3iey8r/praw_how_do_i_get_a_list_of_all_the_users_in_my/"}, {"author": "boib", "body": "Thanks. I have 5.5mil, but I'm sure that not all users have set. ... I got 20,622 :) Thanks for the help!", "created_utc": 1440555718, "gilded": 0, "name": "t1_cuftqqr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3iey8r/praw_how_do_i_get_a_list_of_all_the_users_in_my/"}, {"author": "13steinj", "body": "You need to change `get_saved(sort=\"new\", time='all')` to `get_saved(sort=\"new\", time=\"all\", limit=None)` Also, this won't get all saved links, but it will get all of the saved links visually and API-lly available. You can't go over 1k, if you do, the older ones simply can not be viewed both in browser and API", "created_utc": 1440470721, "gilded": 0, "name": "t1_cueo080", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ia8m4/getting_all_saved_links/"}, {"author": "PoodleWorkout", "body": "ahh, I had a feeling I'd have to specify a limit :P Thanks for your help!", "created_utc": 1440471573, "gilded": 0, "name": "t1_cueohcf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ia8m4/getting_all_saved_links/"}, {"author": "13steinj", "body": "Np.", "created_utc": 1440471697, "gilded": 0, "name": "t1_cueojv0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ia8m4/getting_all_saved_links/"}, {"author": "TomSparkLabs", "body": "Never mind. I found https://github.com/avinassh/prawoauth2 while browsing the subreddit. Made it a whole lot easier.", "created_utc": 1440427928, "gilded": 0, "name": "t1_cudxbwi", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3i7dcc/how_do_i_upgrade_to_oauth2_authentication_with/"}, {"author": "13steinj", "body": "It's not an error, it's a warning. Sooner or later CookieAuth will be heavily throttled, and when that time comes PRAW will be OAuth/Unauthenticated only while removing the login method. That said, I use /u/smbe19's praw-OAuth2Util.", "created_utc": 1440438313, "gilded": 0, "name": "t1_cue3v9k", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3i7dcc/how_do_i_upgrade_to_oauth2_authentication_with/"}, {"author": "TomSparkLabs", "body": "I'm currently using /u/avinassh's prawoath2. https://github.com/avinassh/prawoauth2 Pretty easy fix. Plus, doesn't require me to start a Django/Flask server or anything.", "created_utc": 1440700004, "gilded": 0, "name": "t1_cuhqhfj", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3i7dcc/how_do_i_upgrade_to_oauth2_authentication_with/"}, {"author": "13steinj", "body": "It depends on the use. I don't know what the major difference is thoug. Probably one had a server - esque mode whereas previosuly before the recent update oauth2util did not", "created_utc": 1440702214, "gilded": 0, "name": "t1_cuhrzwj", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3i7dcc/how_do_i_upgrade_to_oauth2_authentication_with/"}, {"author": "GoldenSights", "body": "The praw search should return Submission objects. You can then get the id with `submission.id`. The votes and title I think is the str() representation of the object. I'm not quite sure what you're doing to get that though.", "created_utc": 1440275328, "gilded": 0, "name": "t1_cuc46n6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hzg8r/get_submission_id_from_the_results_of_a_praw/"}, {"author": "13steinj", "body": "Stupid question: Why are you using the `--allow-external` parameter in the first place?", "created_utc": 1440183159, "gilded": 0, "name": "t1_cuaz03a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "cartoonii", "body": "Well, even without it, it throws the same error message..", "created_utc": 1440187084, "gilded": 0, "name": "t1_cub1ju6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "13steinj", "body": "That's extremely weird. Are you running on a server or a local machine? In anycase try to run as an admin, if that still doesn't work, install Python 3.4.3 side by side it, and go to that scripts folder and run \"pip34 install praw-oauth2util\".", "created_utc": 1440187233, "gilded": 0, "name": "t1_cub1nej", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "cartoonii", "body": "Local and I tried running CMD as an administrator but no luck and for the record, I have Python33 and Python34 folders on my C:\\", "created_utc": 1440187448, "gilded": 0, "name": "t1_cub1sig", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "13steinj", "body": "Which version do you want to install to? If you want to install to 3.3 replace `pip` with `pip3.3`, if you want to install to 3.4 replace `pip` with `pip3.4`, if that still doesn't work try adding the `--user` argument.", "created_utc": 1440187753, "gilded": 0, "name": "t1_cub1ziz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "cartoonii", "body": "Well, in my Python33 Scripts folder I have 'pip.py'..", "created_utc": 1440187850, "gilded": 0, "name": "t1_cub21r0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "13steinj", "body": "...That doesn't make any sense whatsoever. In the python scripts folder, on Windows, there should be `pip.exe`, and if you have more than one installation, `pip3.exe` and `pip3.X.exe` (X being the installation sub version). If you don't have these, you have a bigger issue. My recommendation is to completely reinstall Python unfortunately, as that's what I had to do in a similar weird situation.", "created_utc": 1440188564, "gilded": 0, "name": "t1_cub2i85", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "cartoonii", "body": "Ahh.. What I did was change my PATH system variable to 3.4 and it installed! God, I'm stupid ;3", "created_utc": 1440188784, "gilded": 0, "name": "t1_cub2nbu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "DeonCode", "body": "Consider updating your post with your solution to the error?", "created_utc": 1440285469, "gilded": 0, "name": "t1_cuc9fv4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "cartoonii", "body": "Alright, done", "created_utc": 1440324405, "gilded": 0, "name": "t1_cucotrt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "cartoonii", "body": "But now, how do you actually USE OAuth2Util in your file?", "created_utc": 1440188859, "gilded": 0, "name": "t1_cub2p0j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hvli0/installing_praw_oauth2util/"}, {"author": "GoldenSights", "body": "/u/13steinj is working on a pull request this very moment. It will be part of the praw development build by the end of the day! *https://github.com/praw-dev/praw/commit/f6b073c6716c55561f5bd78199a609259619d117", "created_utc": 1440010754, "gilded": 0, "name": "t1_cu8ksam", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "D0cR3d", "body": "Awesome! thanks for letting me know.", "created_utc": 1440010880, "gilded": 0, "name": "t1_cu8kvml", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "13steinj", "body": "It's been merged. Use pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip To upgrade to the dev build", "created_utc": 1440013590, "gilded": 0, "name": "t1_cu8mue8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "D0cR3d", "body": "Wow! thank you.", "created_utc": 1440013881, "gilded": 0, "name": "t1_cu8n1yz", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "13steinj", "body": "Np.", "created_utc": 1440014124, "gilded": 0, "name": "t1_cu8n86y", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "13steinj", "body": "It's done; just needs to be merged :P.", "created_utc": 1440011647, "gilded": 0, "name": "t1_cu8lfw3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "GoldenSights", "body": "bam!", "created_utc": 1440013107, "gilded": 0, "name": "t1_cu8mhu9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hlmtc/is_there_a_get_edited_function_in_praw_to_be_able/"}, {"author": "GoldenSights", "body": "My understanding is that you can't just change one setting at a time. The endpoint literally requires you to provide ALL of the subreddit settings when submitting the change. It's a pretty colossal inconvenience, but that's beside the point. If you go to your subreddit's settings and try to enter it with a blank title, you'll get a \"we need something here\" error. Given that the title is a mandatory field, and you're required to explicitly provide all the settings, I suppose that's why it's a required parameter in praw. You need to fetch your subreddit's current settings, and pass the ones you want to keep back into set_settings while making the desired changes. Or keep a template of all the settings so you don't lose them with a single click. Maybe there should be a method in praw that does a \"smart settings update\" where it will check the previous data for you and only apply the new changes. That'd be nice. https://www.reddit.com/dev/api/oauth#POST_api_site_admin", "created_utc": 1439982384, "gilded": 0, "name": "t1_cu84gqa", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hkgou/praw_why_is_title_a_required_argument_for_set/"}, {"author": "wlhlm", "body": "Thanks a lot! Yeah, makes sense that it's imposed by the Reddit API. And I guess it doesn't make a difference, because you have to pass all the settings when you don't want them reset to their defaults. I think editing the respective wiki page at `wiki/config/sidebar` is safer here and I can drop `modconfig` oauth scope.", "created_utc": 1439983927, "gilded": 0, "name": "t1_cu84x6h", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hkgou/praw_why_is_title_a_required_argument_for_set/"}, {"author": "bboe", "body": "See update_settings \u263a", "created_utc": 1439997321, "gilded": 0, "name": "t1_cu8bct5", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hkgou/praw_why_is_title_a_required_argument_for_set/"}, {"author": "GoldenSights", "body": "Hey, I must be psychic!", "created_utc": 1440012569, "gilded": 0, "name": "t1_cu8m45b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hkgou/praw_why_is_title_a_required_argument_for_set/"}, {"author": "wlhlm", "body": "Wow, nice! I think I still go with the wiki approach, that way I can also add a Revision note.", "created_utc": 1440010272, "gilded": 0, "name": "t1_cu8kft8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hkgou/praw_why_is_title_a_required_argument_for_set/"}, {"author": "RemindMeBotWrangler", "body": "Unless a bot is called by a specific command, messaging users without their explicit permission is spam. For example in your code, you're grabbing all the users that commented in that thread, even ones who didn't \"ask\".", "created_utc": 1439983296, "gilded": 0, "name": "t1_cu84qcv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "Bromskloss", "body": "I'm grabbing all the comments (or that was what I tried to do, anyway) that were replies to the comment that said something like \"comment here to get reminded\". It's true that some of those replies were just along the lines of \"I don't need a reminder, but I think this is cool\", but would it be bad to remind those people too? In any case, supposed I send only to the ones who explicitly asked for it. Will it not be stopped by a spam filter anyway?", "created_utc": 1439985075, "gilded": 0, "name": "t1_cu85abs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "RemindMeBotWrangler", "body": "Well they should be PMs at the very least. And it's not so much that you would trigger the spam filter. One time for /u/RemindMeBot was down for a few weeks and I had 10k messages to send out and it was fine. It's more that you can be reported and if they investigate they have reasons to see that you are in fact spamming.", "created_utc": 1439986142, "gilded": 0, "name": "t1_cu85nko", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "Bromskloss", "body": "> I had 10k messages to send out and it was fine Not until now do I realise that you are the one behind RemindMeBot. :-) Is it possible that your bot was able to to this because it had gained some \"reputation points\" by having been active without issue for some time already? Or will it work fine for a new script as well? > It's more that you can be reported and if they investigate they have reasons to see that you are in fact spamming. I see. I'm not too worried about that. This is just a one-off thing to save the poor guy from having to do it manually.", "created_utc": 1439986602, "gilded": 0, "name": "t1_cu85tnh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "RemindMeBotWrangler", "body": "I'm sure once you past the new account limit all accounts are equal under God's eyes. I mean if that's the case worse case scenario do it under a different IP but even then I don't think Reddit bans on IP.", "created_utc": 1439986927, "gilded": 0, "name": "t1_cu85y6h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "Bromskloss", "body": "> I'm sure once you past the new account limit When does that happen? Do I need to use the account for some time first, before I send out the messages? (Or maybe I should just use an existing account.)", "created_utc": 1439987837, "gilded": 0, "name": "t1_cu86b2s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "RemindMeBotWrangler", "body": "You need I think 5 link and/or comment karma.", "created_utc": 1439988059, "gilded": 0, "name": "t1_cu86eel", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "Bromskloss", "body": "Where can I read about these restrictions?", "created_utc": 1439989237, "gilded": 0, "name": "t1_cu86wmi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "picflute", "body": "They aren't publicly written. The basic idea is that you get your bot some karma and you're good to go.", "created_utc": 1440127939, "gilded": 0, "name": "t1_cuaafwj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "Bromskloss", "body": "Aww, how annoying.", "created_utc": 1440144245, "gilded": 0, "name": "t1_cuag54x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hiwod/how_to_send_a_message_to_a_few_hundred_users/"}, {"author": "GoldenSights", "body": "Each of the new PRAW custom exceptions have an attribute called `_raw` which contains the original exception from the requests module. You can do: try: stuff() except praw.errors.HTTPException as e: if e._raw.status_code == 503: print('reddit is busy!') else: raise You might also want to look out for other 500 errors, such as 502 bad gateway, 504 gateway timeout, and 520 origin error. There might be even more variants but that's a subject I need to learn more about.", "created_utc": 1439943323, "gilded": 0, "name": "t1_cu7oq53", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hil7j/how_to_add_reddit_overload_support_to_my_script/"}, {"author": "solaceinsleep", "body": "Thank you. Exactly what I'm looking for. I have several except statements targeting specific things, and one general except to catch any other ones I missed. So far I haven't encountered any of those except the 503 (one time). So I will try to add them if they become an issue. By the way the else statement reraises the exception right? If so how would that be handled? Can it be caught by a next except? Kind of like here: try: stuff() except praw.errors.HTTPException as e: if e._raw.status_code == 503: print('reddit is busy!') else: raise except Exception as e: print(traceback.format_exc())", "created_utc": 1439944460, "gilded": 0, "name": "t1_cu7pdtj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hil7j/how_to_add_reddit_overload_support_to_my_script/"}, {"author": "GoldenSights", "body": "No, the re-raised exception from the else statement will not be caught by the next except block. You'll probably have to do something like this: try: stuff() except Exception as e: if isinstance(e, praw.errors.HTTPException): if e._raw.status_code == 503: print('reddit is busy') else: # It's some other httpexc raise else: traceback.print_exc() At least, that's the first thing that comes to mind. I wouldn't be surprised if there's a smarter, more pythonic way of doing this, but I'm blanking at the moment. Maybe someone else can make a suggestion.", "created_utc": 1439945007, "gilded": 0, "name": "t1_cu7pp8k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3hil7j/how_to_add_reddit_overload_support_to_my_script/"}, {"author": "GoldenSights", "body": "Hi OP, when I do this: s = r.get_subreddit('StevenUniverse') flairs = s.get_flair_choices() print(len(flairs['choices'])) I get 234. I'm using PRAW 3.2.0 on Python 3.4.3, with \"flair\" and \"identity\" as my only active OAuth scopes. It also works when using `r.get_flair_choices('stevenuniverse')`. Are you still having problems with this method? Any other info you can give maybe?", "created_utc": 1439943042, "gilded": 0, "name": "t1_cu7okbm", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "13steinj", "body": "I can't reproduce. Mainly because I don't have a sub with more than 20 flairs. What subreddit and are you trying for link or user flairs?", "created_utc": 1439865823, "gilded": 0, "name": "t1_cu6mvvs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "EliteMasterEric", "body": "/r/StevenUniverse. It has ~~like 200 or something~~ 242. And my flair statistics bot was working fine a while ago.", "created_utc": 1439871810, "gilded": 0, "name": "t1_cu6pwxp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "13steinj", "body": "Well I'm on mobile now, so I couldn't tell you at this moment.", "created_utc": 1439872624, "gilded": 0, "name": "t1_cu6q9m6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "bboe", "body": "Could you try downgrading PRAW to an older version to confirm that it in fact due to the PRAW version?", "created_utc": 1439873534, "gilded": 0, "name": "t1_cu6qnld", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "EliteMasterEric", "body": "Nope, doesn't seem to have worked. In addition, just after it, it gives me a Forbidden error when attempting to edit a wiki page even though I have wiki perimission (modwiki, wikiedit, wikiread)", "created_utc": 1439909963, "gilded": 0, "name": "t1_cu72yz9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "powderblock", "body": "Odd about the wiki thing. But I think this would be a problem with reddit. I read in the [praw Git repo in objects.py at line 1168](https://github.com/praw-dev/praw/blob/242190fe7e8fd029c4c79612a9c1fa10bddfb764/praw/objects.py#L1168): \"def get_flair_choices(self, *args, **kwargs): \"\"\"Return available link flair choices and current flair. ... :returns: The json response from the server.\" So I think reddit probably limited the number of responses in the json to 20? Does this make sense to anyone else? I am a novice developer. Edit: I tried to find a changelog or something that would support the fact that reddit doesn't give more than 20 flair objects but I can't find anything to back this up. Hm. All the code I've read though seems like reddit really does return all the flairs.", "created_utc": 1439911124, "gilded": 0, "name": "t1_cu73p34", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "EliteMasterEric", "body": "Okay, I'll try that tomorrow and report on how it worked.", "created_utc": 1439875635, "gilded": 0, "name": "t1_cu6rh93", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3he8ev/praw_not_getting_all_flair_choices_after_praw_320/"}, {"author": "lecherous_hump", "body": "This sounds like the Reddit limit for new accounts, not an API limit. How much karma does the bot account have? I don't know when the limits lift, but 1k post or comment karma is a safe bet. I simply posted a bunch of porn, got to 1k karma, then deleted the posts. Heh. Edit: I assume the bot is already a moderator of the sub, too? Once the new account limit is gone, you still may have a posting limit sometimes, unless you're a moderator of the sub.", "created_utc": 1439769777, "gilded": 0, "name": "t1_cu5ctzq", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "KidKrule", "body": "The bot is this account. Fairly new, and no karma. I'm a Reddit noob. Thanks for the trick though, now I HAVE TO get some porn !", "created_utc": 1439770945, "gilded": 0, "name": "t1_cu5deki", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "picflute", "body": "It's like 5 link or comment karma dude chill", "created_utc": 1439774390, "gilded": 0, "name": "t1_cu5f41n", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "D0cR3d", "body": "What you are running into is the Subreddit based Post Timer for Non Mod and Non Contributors for a subreddit. Since you are only a user posting a subreddit you aren't a contributor on, nor a mod on, you fall to the standard anti spam/post limit that all users fall into. Try creating your own subreddit, or become a contributor on a sub and you shouldn't run into that specific rate limit.", "created_utc": 1439771486, "gilded": 0, "name": "t1_cu5dnzr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "KidKrule", "body": "Oh, I wasn't aware of this limit depending on whether you're a mod or not. I'll try this. Thanks =) EDIT : I tried to create my own sub reddit, but my account is new (2 months) so it won't let me. What I'll do is get someone to make one for me and add me as a mod. Think that should do the trick.", "created_utc": 1439772573, "gilded": 0, "name": "t1_cu5e7fl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "Stuck_In_the_Matrix", "body": "Why are you creating new threads for each tweet? Couldn't you just create a thread and post a comment into that thread for each tweet? There has to be some throttling involved in thread creation or else we'd get 5+ million new threads a day instead of a sane ~ 200,000.", "created_utc": 1439767517, "gilded": 0, "name": "t1_cu5bocd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "KidKrule", "body": "Well each of those tweets is a commit message from developers of a game. The sub I want to post to already does that using IFTTT and I wanted to give the creator of the sub a better bot than this. We are talking 5 tweets a day, max. I find it sane. As for why a thread per tweet, it's because we'd like to allow people to discuss a specific commit (new addition to the game, a fix, etc...) and threads are perfect for that.", "created_utc": 1439768084, "gilded": 0, "name": "t1_cu5bzgg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "Stuck_In_the_Matrix", "body": "5 threads a day isn't much. If you create a new one every hour, I don't see why you'd run into a limitation. Maybe one of the admins can chime in.", "created_utc": 1439768265, "gilded": 0, "name": "t1_cu5c2uf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "KidKrule", "body": "Well right now I was testing. So I ran into the limitation posting 5 in 45 minutes. The thing, this can happen in real life : 5 commits in a row, then nothing for 24 hours. I hope an admin can give me some advice on how to get around this. Thank you =)", "created_utc": 1439768438, "gilded": 0, "name": "t1_cu5c60p", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "picflute", "body": "Can't you just run it as a sidebar bot?", "created_utc": 1439774420, "gilded": 0, "name": "t1_cu5f4jy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h91fe/trying_to_make_a_bot_but_running_into_drastic_api/"}, {"author": "13steinj", "body": "No idea. You could probably do it with html scraping if you also use CookieLib to \"log in\", but I'm not sure if that kind of cookieauth will also be disabled soon. Sidenote: YAY Some features that used to have to be done manually are now directly in a stable version of PRAW :P", "created_utc": 1439703229, "gilded": 0, "name": "t1_cu4lyco", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h40nx/praw_getting_the_expiration_time_of_a_ban_and_the/"}, {"author": "13steinj", "body": "Looking at the json from a test sub: {\"kind\": \"Listing\", \"data\": {\"modhash\": \"yilm2j681w62622384365fc27be3fd3eb79dbcc1191579a678\", \"children\": [{\"date\": 1439703553.0, \"note\": \"test\", \"name\": \"Username\", \"id\": \"t2_lx7th\"}], \"after\": null, \"before\": null}} So...it's not in the API as of yet, and as such not in PRAW. You could use html scraping like I mentioned in the previous comment (which would be done with BeautifulSoup4, requests, and as it's a log in only page CookieLib, but I'm not sure if that form of cookie auth will be removed soon or not).", "created_utc": 1439703808, "gilded": 0, "name": "t1_cu4m5oz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h40nx/praw_getting_the_expiration_time_of_a_ban_and_the/"}, {"author": "dClauzel", "body": "So yes, it is such I understood: not available in Reddit\u2019s API, so no way go collect these data \ud83d\ude12 Thanks for your time!", "created_utc": 1439715166, "gilded": 0, "name": "t1_cu4patq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h40nx/praw_getting_the_expiration_time_of_a_ban_and_the/"}, {"author": "13steinj", "body": "Well just because it's not in the API doesn't mean it's impossible. Like I said, you can use html scraping.", "created_utc": 1439745175, "gilded": 0, "name": "t1_cu4zjsy", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h40nx/praw_getting_the_expiration_time_of_a_ban_and_the/"}, {"author": "roxven", "body": "Are you using '*' scope? I just opened an [issue](https://github.com/reddit/reddit/issues/1391) for this.", "created_utc": 1440192637, "gilded": 0, "name": "t1_cub50xi", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "kaisermagnus", "body": "How much karma does the bot have? I found in the past (this may not linger be the case) PMs are prevented via API unless a certain karma threshold is passed.", "created_utc": 1439673629, "gilded": 0, "name": "t1_cu48cnj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "spawnofyanni", "body": "It's sitting at 1159/1865 currently, I've submitted a couple of posts on its behalf before in the hope that it gets past most karma thresholds.", "created_utc": 1439673882, "gilded": 0, "name": "t1_cu48h59", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "13steinj", "body": "How old is the bot? Thresholds count karma per time period as well. If it's one year old and semi rarely logged in it's not enough. Try running it off of your account. --- Even with this, wouldn't this be a place where a captcha is requested? Tags: /u/bboe, /u/GoldenSights", "created_utc": 1439877932, "gilded": 0, "name": "t1_cu6sa71", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "bboe", "body": "It's possible that a private message reply currently doesn't work in PRAW. We've yet to update all the tests to their OAuth equivalent. I unfortunately don't currently have the time to verify.", "created_utc": 1439962178, "gilded": 0, "name": "t1_cu7ysw6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "kaisermagnus", "body": "That should be more than enough, so I'm afraid I can't help you.", "created_utc": 1439674503, "gilded": 0, "name": "t1_cu48rul", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "gooeyblob", "body": "Can you try to get a token with *just* privatemessages and attempt to reply with that, and then one with *just* submit? Either should work at this point, but just want to be sure.", "created_utc": 1440191508, "gilded": 0, "name": "t1_cub4cji", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "spawnofyanni", "body": "I'm getting the same error in both cases.", "created_utc": 1440202506, "gilded": 0, "name": "t1_cubaji6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "gooeyblob", "body": "Someone has identified the issue [here](https://github.com/reddit/reddit/issues/1391) and I have a fix for it ready that will go out on Monday. For now, you can work around it by requesting a scope that specifically has the privatemessages scope instead of the * scope. Thanks for the report!", "created_utc": 1440206520, "gilded": 0, "name": "t1_cubcnfi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "gooeyblob", "body": "This has been fixed [here](https://github.com/reddit/reddit/commit/7d32950cbc0132a375cfcf54d277ab871b752c27). Thanks for reporting and thanks u/roxven!", "created_utc": 1440444526, "gilded": 0, "name": "t1_cue7xtm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3s34/bot_gets_forbidden_error_when_trying_to_reply_to/"}, {"author": "GoldenSights", "body": "A little while ago, PRAW started wrapping all of its exceptions under custom names. In this case, PRAW caught the 503 requests.exceptions.HTTPError, then reraised a new praw.errors.HTTPException, which you can see at the bottom of your traceback. This is the exception you should be catching. I think the system could use some improvement, but that's where we're at currently. As for your second question, it seems that the praw exceptions return a blank string for str(). Personally I would raise an issue on http://github.com/praw-dev/praw/issues, that seems like a pretty big deal to me.", "created_utc": 1439650150, "gilded": 0, "name": "t1_cu3wu8f", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3hmm/how_do_you_catch_a_specific_exception/"}, {"author": "soymilkbot", "body": "I see, that makes sense. However that raises another problem. Is there an equivalent of requests' response.status_code for praw.errors.HTTPException? I looked through the documentation and I can't find praw.errors.HTTPException's attributes. I need the status_code to identify the kind of HTTPError. About the blank string, I'll report it right now. Thank you so much for the help! Edit: I was wondering, is this kind of question more suited to this subreddit or /r/botwatch? I don't want to post to the wrong place. Edit2: After some more searching I figured out that the function that wraps the original `requests.exceptions.HTTPError` raises the `HTTPException` with `requests.exceptions.HTTPError.response` as its `_raw` argument, so I should be able to get the `status_code` from `_raw`. Here is the relevant part of the function: try: response.raise_for_status() # These should all be directly mapped except exceptions.HTTPError as exc: raise HTTPException(_raw=exc.response) I changed my code to look like this: try: # some code except KeyboardInterrupt: print(\"Shutting down.\") break except praw.errors.HTTPException as e: exc = e._raw print(\"Some thing bad happened! HTTPError\", exc.status_code) if exc.status_code == 503: print(\"Let's wait til reddit comes back! Sleeping 60 seconds.\") time.sleep(60) except Exception as e: print(\"Some thing bad happened!\", e) traceback.print_exc() I hope this works...", "created_utc": 1439651251, "gilded": 0, "name": "t1_cu3xbuy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3hmm/how_do_you_catch_a_specific_exception/"}, {"author": "GoldenSights", "body": "Sorry for the late response. You are correct in using the _raw attribute to see the original requests exception. This subreddit is perfectly fine for this kind of question, but we wouldn't mind answering it in bofwatch either.", "created_utc": 1439680034, "gilded": 0, "name": "t1_cu4bhi7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3hmm/how_do_you_catch_a_specific_exception/"}, {"author": "soymilkbot", "body": "No worries, it works now. Thank you for the help!", "created_utc": 1439680182, "gilded": 0, "name": "t1_cu4bjy7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3h3hmm/how_do_you_catch_a_specific_exception/"}, {"author": "rtheunissen", "body": "I helped fix /api/info when I was building rockets :)", "created_utc": 1439551634, "gilded": 0, "name": "t1_cu2npc2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "Stuck_In_the_Matrix", "body": "You are a hero my friend :)", "created_utc": 1439552010, "gilded": 0, "name": "t1_cu2ntf6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "ELFAHBEHT_SOOP", "body": "This is the coolest thing ever.", "created_utc": 1439535773, "gilded": 0, "name": "t1_cu2jo9c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "Stuck_In_the_Matrix", "body": "Thanks!", "created_utc": 1439536179, "gilded": 0, "name": "t1_cu2js6b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "bboe", "body": "This tool looks very awesome. Can you describe your ingest methods and your comparison to PRAW? It'd be awesome to know in what cases and how your tool can receive more comments and submissions. Also were you able to benchmark your SSE server for the number of concurrent connections it can support? If so what tools did you use for that? Thanks for all the hard work.", "created_utc": 1439564415, "gilded": 0, "name": "t1_cu2tml7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "Stuck_In_the_Matrix", "body": "I hit the /api/info endpoint and ask for the id's directly and crawl upwards. PRAW may have updated their code since the last time when they were hitting /r/all/comments. The SSE Server is handling surprisingly well. With 15 connections, it's only using 15% of one CPU. Redis seems to be very efficient and I use pipelining mode to reduce the amount of back and forth between the Redis server. It looks like this VPS could support at least 100 streams. Bandwidth would eventually become an issue at that rate.", "created_utc": 1439593947, "gilded": 0, "name": "t1_cu3ckl2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": null, "body": "[deleted]", "created_utc": 1439570636, "gilded": 0, "name": "t1_cu2xo2c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "Stuck_In_the_Matrix", "body": "You can make one call per second with oath which is more than enough (requesting up to 100 objects each time)", "created_utc": 1439571329, "gilded": 0, "name": "t1_cu2y4ot", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": null, "body": "[deleted]", "created_utc": 1439571902, "gilded": 0, "name": "t1_cu2yioi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "Stuck_In_the_Matrix", "body": "The most I've ever seen comments spike up in one second is around 50. You can see the average here: https://pushshift.io/", "created_utc": 1439583103, "gilded": 0, "name": "t1_cu362vz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "dClauzel", "body": "This is completely fantastic \u2764", "created_utc": 1439579583, "gilded": 0, "name": "t1_cu33qbf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "thristhart", "body": "Super cool. Can't wait to dig in and mess around.", "created_utc": 1439582893, "gilded": 0, "name": "t1_cu35xtb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "RubyPinch", "body": "some domain options would be nice for bots like /u/MassDropBot, /u/untouchedURL, etc etc [fnmatch](https://docs.python.org/3/library/fnmatch.html#fnmatch.fnmatch) or the equivilant for whatever language you are using, would be nice to allow hitting an entire domain (`domain=example.com,*.example.com`)", "created_utc": 1441658910, "gilded": 0, "name": "t1_cutrsag", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gy4zd/supercharging_the_sse_stream_now_supports_a_ton/"}, {"author": "GoldenSights", "body": "The first thing I notice is that the request for /about/banned doesn't have the \"Substituting api.reddit.com for oauth.reddit.com\" message above it. This would lead me to believe that the oauth headers are not being included in the request, giving you the 403. I would download the .zip of the current praw repo [here](https://github.com/praw-dev/praw) and overwrite your installation, because this issue may have already been fixed. I'm guessing it has something to do with the restrict_access decorators, which I know have had a few changes since 3.1.0.", "created_utc": 1439371601, "gilded": 0, "name": "t1_cu079nz", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "body": "No idea for the URL substitutions. I did not specified anything for this.", "created_utc": 1439444525, "gilded": 0, "name": "t1_cu1axhn", "num_comments": null, "score": -5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "body": "Yeah, I understand that you didn't purposely change anything. I'm suggesting you download the GitHub version of PRAW because this problem may have been fixed since the 3.1.0 release.", "created_utc": 1439444608, "gilded": 0, "name": "t1_cu1ayjj", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "body": "I have to look at this, I am not sure how to replace pip\u2019s version with the github one.", "created_utc": 1439444664, "gilded": 0, "name": "t1_cu1az9a", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "body": "If you download the zip and open it up, you will see a folder called \"praw-master\", with a subfolder named \"praw\". This folder has \"\\_\\_init\\_\\_.py\", \"decorators.py\", and a few others. Extract these files to /usr/local/lib/python3.4/dist-packages/praw/ (I copied that path from your traceback), overwriting the ones that came from pip. Then you can re-run your script and see what happens.", "created_utc": 1439444853, "gilded": 0, "name": "t1_cu1b1nc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "body": "With this modification, it works perfectly $ ./demo.py substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json connexion\u2026 connect\u00e9. substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json Je suis dClauzel et j\u2019ai un karma de 15981 pour mes commentaires. Top 5 substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/Europe/about/.json substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/europe/.json - Immigration Megathread - Part VI \u2014 ModeratorsOfEurope \u2014 http://www.reddit.com/r/europe/comments/3frno2/immigration_megathread_part_vi/ - Access to the entire Reddit.com domain has been reportedly blocked in Russia \u2014 zurfer75 \u2014 https://meduza.io/news/2015/08/12/reddit-vnesli-v-reestr-zapreschennyh-saytov - Hey Europe, what are your country's top conspiracy theories? \u2014 herr_wildow \u2014 http://www.reddit.com/r/europe/comments/3gpgyx/hey_europe_what_are_your_countrys_top_conspiracy/ - EU rejects Eastern states' call to outlaw denial of crimes by communist regimes: Eastern European states wanted Soviet crimes 'treated according to the same standards' as those of Nazi regimes \u2014 nastratin \u2014 http://www.theguardian.com/world/2010/dec/21/european-commission-communist-crimes-nazism - Putin is actually in serious trouble \u2014 giggster \u2014 http://www.businessinsider.com/putin-is-actually-in-serious-trouble-2015-8 Liste des bannis substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/r/europe/about/banned/.json [Redditor(user_name='redacted'), Redditor(user_name='redacted'), Redditor(user_name='redacted'), Redditor(user_name='redacted'), Redditor(user_name='redacted'), Redditor(user_name='redacted')] fin sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty Many thanks for your time, you solved my problem. Do you know when this new version of praw will be available in pip?", "created_utc": 1439447054, "gilded": 0, "name": "t1_cu1bu8r", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "body": "Awesome, just as I expected. The 3.2.0 release isn't scheduled yet, but next time I talk to the project owner I will ask if there are any more features he wants to add. Since the mandatory-oauth thing got delayed by kemitche's departure, there isn't really a solid schedule at the moment.", "created_utc": 1439447457, "gilded": 0, "name": "t1_cu1bz0n", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "bboe", "body": "Just FYI, I absolutely recommend using the pip install from master approach over manual package updates: pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip It's a nice simple one-liner that's easy to undo.", "created_utc": 1439451512, "gilded": 0, "name": "t1_cu1d7df", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "body": "Thank you! (This would be pip3 for me \u2190 python 3.4)", "created_utc": 1439451860, "gilded": 0, "name": "t1_cu1daur", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "body": "Whenever I do that, I get this output: Collecting https://github.com/praw-dev/praw/archive/master.zip Downloading https://github.com/praw-dev/praw/archive/master.zip (1.8MB) 100% |################################| 1.8MB 161kB/s Collecting decorator>=3.4.2 (from praw==3.1.0) Using cached decorator-4.0.2-py2.py3-none-any.whl Requirement already up-to-date: requests>=2.3.0 in c:\\python34\\lib\\site-packages (from praw==3.1.0) Requirement already up-to-date: six>=1.4 in c:\\python34\\lib\\site-packages (from praw==3.1.0) Requirement already up-to-date: update-checker>=0.11 in c:\\python34\\lib\\site-packages (from praw==3.1.0) Installing collected packages: decorator, praw Found existing installation: decorator 3.4.2 Cannot remove entries from nonexistent file c:\\python34\\lib\\site-packages\\easy-install.pth and my installation remains unchanged. Google hasn't been very helpful with that error message, but to be fair I only looked at the first 2 pages of results since the manual install is good enough for me. Are you familiar with this error? And, since OP was wondering, when did you want to release the next version of PRAW? Is there anything on the table besides more test ports? I still need to make that next pr.", "created_utc": 1439452339, "gilded": 0, "name": "t1_cu1dfft", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "body": "I had to install decorator myself: `pip3 install decorator`. No problem after that. # pip3 install --upgrade https://github.com/praw-dev/praw/archive/master.zip Downloading/unpacking https://github.com/praw-dev/praw/archive/master.zip Downloading master.zip (unknown size): 1.8MB downloaded Running setup.py (path:/tmp/pip-ykthgbwi-build/setup.py) egg_info for package from https://github.com/praw-dev/praw/archive/master.zip Requirement already up-to-date: decorator>=3.4.2 in /usr/local/lib/python3.4/dist-packages (from praw==3.1.0) Requirement already up-to-date: requests>=2.3.0 in /usr/lib/python3/dist-packages (from praw==3.1.0) Requirement already up-to-date: six>=1.4 in /usr/lib/python3/dist-packages (from praw==3.1.0) Requirement already up-to-date: update-checker>=0.11 in /usr/local/lib/python3.4/dist-packages (from praw==3.1.0) Installing collected packages: praw Found existing installation: praw 3.1.0 Uninstalling praw: Successfully uninstalled praw Running setup.py install for praw Installing praw-multiprocess script to /usr/local/bin Successfully installed praw Cleaning up...", "created_utc": 1439452862, "gilded": 0, "name": "t1_cu1dk9i", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "body": "Hmm, I'm not sure what's going on C:\\>pip install decorator --upgrade Collecting decorator Using cached decorator-4.0.2-py2.py3-none-any.whl Installing collected packages: decorator Found existing installation: decorator 3.4.2 Cannot remove entries from nonexistent file c:\\python34\\lib\\site-packages\\easy-install.pth Might be a Windows thing. I'm not too worried about solving it for myself, but if I'm working with another Linux user I'll have to share this method with them and see what happens.", "created_utc": 1439453150, "gilded": 0, "name": "t1_cu1dmwq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "bboe", "body": "Well that's interesting. I personally use virtual environments to avoid mucking around with the site packages thus whenever something like this happens I simply delete the virtual environment and start over. In your case you may simply be at a point where a virtual environment is a necessity as different packages certainly will require conflicting versions of the same dependency. As for releasing PRAW. There was a point where I would release a minor version every day. So feel free to do that as often as you'd like as long as it's not a backwards breaking change.", "created_utc": 1439476256, "gilded": 0, "name": "t1_cu1lgsf", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "body": "I was able to create a a blank \"easy-install.pth\" file in the location it was looking for, and that satisfied the error message. Good enough for me I guess. Let's see how long I can go without using virtual envs. 3.2.0 is now up! Let me know if I did something wrong, but I think it went much more smoothly than last time.", "created_utc": 1439585869, "gilded": 0, "name": "t1_cu37wee", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "body": "Upgrading: # pip3 install --upgrade praw Downloading/unpacking praw from https://pypi.python.org/packages/3.4/p/praw/praw-3.2.0-py2.py3-none-any.whl#md5=6902294428b656c8535df5a636f1ad02 Downloading praw-3.2.0-py2.py3-none-any.whl (68kB): 68kB downloaded Requirement already up-to-date: decorator>=3.4.2 in /usr/local/lib/python3.4/dist-packages (from praw) Requirement already up-to-date: requests>=2.3.0 in /usr/lib/python3/dist-packages (from praw) Requirement already up-to-date: six>=1.4 in /usr/lib/python3/dist-packages (from praw) Requirement already up-to-date: update-checker>=0.11 in /usr/local/lib/python3.4/dist-packages (from praw) Installing collected packages: praw Found existing installation: praw 3.1.0 Uninstalling praw: Successfully uninstalled praw Successfully installed praw Cleaning up... No problem to report, the scripts run correctly.", "created_utc": 1439586955, "gilded": 0, "name": "t1_cu38knz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "body": "Woohoo!", "created_utc": 1439588211, "gilded": 0, "name": "t1_cu39ce1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "gooeyblob", "body": "It's a little hard to piece things apart given the formatting - can you try a gist or paste bin?", "created_utc": 1439371385, "gilded": 0, "name": "t1_cu077uo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "body": "Sure: http://sebsauvage.net/paste/?4b68b2979df48cc7#rYIgM+pPveDv8a1LdHhvlgJINx7vx2JGZxqHEcSu4IE=", "created_utc": 1439371804, "gilded": 0, "name": "t1_cu07be2", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": null, "body": "Real men don't need PRAW: https://www.reddit.com/r/europe/about/banned.json I highly recommend https://snoocore.readme.io Good luck with your censorship.", "created_utc": 1439438278, "gilded": 0, "name": "t1_cu189wj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "dClauzel", "body": "Not helping. This is to be integrated with other command line tools.", "created_utc": 1439444484, "gilded": 0, "name": "t1_cu1awxx", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": null, "body": "Snoocore works great with node at the command line. Seems like praw is broken unfortunately.", "created_utc": 1439445190, "gilded": 0, "name": "t1_cu1b60o", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gpbiu/prawoauth2util_problem_using_get_banned_raise/"}, {"author": "GoldenSights", "body": "When you make a comment, you should receive the comment object back as a response from Reddit. If you're using PRAW, it goes like this: comment = submission.add_comment('first message') reply = comment.reply('second message') You could then come up with a method that chains replies until the entire text is submitted.", "created_utc": 1439261025, "gilded": 0, "name": "t1_ctyqdh2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gjm58/making_a_comment_chain_to_overcome_reddit_comment/"}, {"author": "sufficiency", "body": "Ah of course! Thank you...", "created_utc": 1439261235, "gilded": 0, "name": "t1_ctyqhrh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gjm58/making_a_comment_chain_to_overcome_reddit_comment/"}, {"author": "gooeyblob", "body": "Is this for something in particular? I'd think in general circumventing the comment limit to post the contents of a website would be frowned upon, unless there was a very specific use case.", "created_utc": 1439371481, "gilded": 0, "name": "t1_cu078ls", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gjm58/making_a_comment_chain_to_overcome_reddit_comment/"}, {"author": "sufficiency", "body": "It is like I have said in OP. Imagine I am reposting something from a website (like the Twitter bot), but the content to be reposted gets long SOMETIMES. I want to be consistent and make sure the bot can post something. As far as spamming is concerned, I do not think it is a big problem.", "created_utc": 1439381733, "gilded": 0, "name": "t1_cu0a1qj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3gjm58/making_a_comment_chain_to_overcome_reddit_comment/"}, {"author": "GoldenSights", "body": "This issue has been fixed, but not yet released. You can download the current praw source from [this page](https://github.com/praw-dev/praw) with the button on the right side. I'm not sure when the next praw release is going to happen. edit: Here's a little bit more [info](https://github.com/praw-dev/praw/issues/489#issuecomment-127091365)", "created_utc": 1438767312, "gilded": 0, "name": "t1_cts53uo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "Aton_Five", "body": "Thank you. It seems my frustration interfered with my ability to google.", "created_utc": 1438774687, "gilded": 0, "name": "t1_cts70lt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "dClauzel", "body": "# Solved App lacked the right scope (Reddit added it recently), so a simple solution was to create a new app (as script). ----- Is this the same problem? With PRAW v3.2.1, when trying to ban a user I get this error: raise OAuthInsufficientScope('insufficient_scope', response.url) praw.errors.OAuthInsufficientScope: insufficient_scope on url https://oauth.reddit.com/api/friend/.json (ping /u/GoldenSights) ---- D\u00e9moBan.py #!/usr/bin/env python3 # -*- coding: utf-8 -*- import praw import OAuth2Util r = praw.Reddit(user_agent=\"posix:MonScript:v0 (by /u/MonIdentifiantReddit)\", site_name=\"Reddit\") o = OAuth2Util.OAuth2Util(r, print_log=False) o.refresh() sousjlailu = r.get_subreddit(\"Banquise\") utilisateur = r.get_redditor(\"TheUserToBan\") sousjlailu.add_ban(utilisateur, **{\"duration\": 30, \"note\": \"A sassy private comment\", \"ban_message\": \"A nice message telling to GTFO\"}) oauth.txt # Config scope=identity,account,edit,flair,history,livemanage,modcontributors,modconfig,modflair,modlog,modothers,modposts,modself,modwiki,mysubreddits,privatemessages,read,report,save,submit,subscribe,vote,wikiedit,wikiread refreshable=True # Appinfo app_key=redacted app_secret=redacted # Token token=redacted refresh_token=redacted valid_until=redacted Running D\u00e9moBan.py $ ./D\u00e9moBan.py Traceback (most recent call last): File \"./D\u00e9moBan.py\", line 16, in sousjlailu.add_ban(utilisateur, **{\"duration\": 30, \"note\": \"A sassy private comment\", \"ban_message\": \"A nice message telling to GTFO\"}) File \"\", line 2, in do_relationship File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 268, in wrap return function(*args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 113, in do_relationship return session.request_json(url, data=data) File \"\", line 2, in request_json File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 113, in raise_api_exceptions return_value = function(*args, **kwargs) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 605, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.4/dist-packages/praw/__init__.py\", line 438, in _request _raise_response_exceptions(response) File \"/usr/local/lib/python3.4/dist-packages/praw/internal.py\", line 193, in _raise_response_exceptions raise OAuthInsufficientScope('insufficient_scope', response.url) praw.errors.OAuthInsufficientScope: insufficient_scope on url https://oauth.reddit.com/api/friend/.json sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty sys:1: ResourceWarning: unclosed", "created_utc": 1440616023, "gilded": 0, "name": "t1_cugkyo4", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "GoldenSights", "body": "Can you try doing this using a PRAW-only implementation of oauth? That is, r = praw.Reddit('posix:MonScript:v0 (by /u/MonIdentifiantReddit)') r.set_oauth_app_info(app_id, app_secret, app_uri) r.refresh_access_information(refresh_token) print(r._authentication, r.access_token) sousjlailu = r.get_subreddit(\"Banquise\") utilisateur = r.get_redditor(\"TheUserToBan\") sousjlailu.add_ban(utilisateur, **{\"duration\": 30, \"note\": \"A sassy private comment\", \"ban_message\": \"A nice message telling to GTFO\"}) Because I'm not having any issues with this: http://i.imgur.com/BtNKTGT.png", "created_utc": 1440627200, "gilded": 0, "name": "t1_cugsgam", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "dClauzel", "body": "{'modposts', 'edit', 'subscribe', 'wikiread', 'privatemessages', 'modlog', 'history', 'modself', 'mysubreddits', 'vote', 'modwiki', 'livemanage', 'account', 'identity', 'modothers', 'save', 'wikiedit', 'modconfig', 'submit', 'modflair', 'read', 'flair', 'report'} redacted Traceback (most recent call last): File \"./D\u00e9moBanPur.py\", line 26, in sousjlailu.add_ban(utilisateur, **{\"duration\": 30, \"note\": \"A sassy private comment\", \"ban_message\": \"A nice message telling to GTFO\"}) File \"\", line 2, in do_relationship File \"/usr/local/lib/python3.4/dist-packages/praw/decorators.py\", line 265, in wrap raise errors.LoginOrScopeRequired(function.__name__, scope) praw.errors.LoginOrScopeRequired: ``do_relationship` requires a logged in session or the OAuth2 scope `modcontributors`` requires a logged in session sys:1: ResourceWarning: unclosed /usr/lib/python3.4/importlib/_bootstrap.py:2150: ImportWarning: sys.meta_path is empty sys:1: ResourceWarning: unclosed Looks like the scope `modcontributors` is not present. I created the app as a \u201cscript\u201d, so all rights should be available; it was\u2026 3 weeks ago? Here is what I have for it in Reddit\u2019s pref: * Moderate Subreddit Configuration * Edit My Subscriptions * Vote * Wiki Editing * My Subreddits * Submit Content * Moderation Log * Moderate Posts * Moderate Flair * Save Content * Invite or remove other moderators * Read Content * Private Messages * Report content * My Identity * Manage live threads * Update account information * Read Wiki Pages * Edit Posts * Moderate Wiki * Make changes to your subreddit moderator and contributor status * History * Manage My Flair Obviously, as I am the creator of r/Banquise, I have full access on it.", "created_utc": 1440628497, "gilded": 0, "name": "t1_cugt8cg", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "GoldenSights", "body": "Yeah if you had the modcontributors scope then that list would also say \"Approve submitters and ban users\" so that's what's missing. You'll have to go through the refresh-token-generation process again including that scope with the rest of them. Just because you made the app a \"script\" type doesn't immediately give you all permissions automatically.", "created_utc": 1440628806, "gilded": 0, "name": "t1_cugtex0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "dClauzel", "body": "Easier to create a new app. I have these rights on this one: * Approve submitters and ban users * Moderate Subreddit Configuration * Edit My Subscriptions * Vote * Wiki Editing * My Subreddits * Submit Content * Moderation Log * Moderate Posts * Moderate Flair * Save Content * Invite or remove other moderators * Read Content * Private Messages * Report content * My Identity * Manage live threads * Update account information * Read Wiki Pages * Edit Posts * Moderate Wiki * Make changes to your subreddit moderator and contributor status * History * Manage My Flair Looks like Reddit is quietly doing changes on the oauth API, because last time I didn\u2019t get the `Approve submitters and ban users` scope\u2026 And now of course with this app, it works pretty fine; with and without OAuth2Util. Thanks again for your help.", "created_utc": 1440629798, "gilded": 0, "name": "t1_cugu05e", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "GoldenSights", "body": "Great, glad to hear it. reddit definitely has been making some changes recently, but the `modcontributors` scope is not new, it was just previously pretty hidden. The only place it appeared on the /api page was in the /friend endpoint, and it didn't even have a description. I don't blame you for missing it.", "created_utc": 1440629883, "gilded": 0, "name": "t1_cugu1wp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fuv52/praw_issues_banning_with_local_script_using_oauth2/"}, {"author": "GoldenSights", "body": "The best way to get around the 1,000 item cache, and collect all submissions to a subreddit, is to use the Cloudsearch timestamp query. I wrote a bit of a tutorial on how to use it [here](https://www.reddit.com/r/reddittips/comments/2ix73n/use_cloudsearch_to_search_for_posts_on_reddit/), and I have a praw script that collects submissions into an sqlite3 database, [here](https://github.com/voussoir/reddit/blob/master/Prawtimestamps/timesearch.py). You don't have to use it, but the main idea behind the get_all_posts function is to create a sliding window, performing the ts search from the subreddit's creation date to now.", "created_utc": 1438758201, "gilded": 0, "name": "t1_cts2qwl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "EmperorSofa", "body": "Neat i'll look into it then since that sounds exactly like what I need. Is it possible to filter those results on whether or not they are self posts?", "created_utc": 1438758870, "gilded": 0, "name": "t1_cts2y1d", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "GoldenSights", "body": "Yes, it appears you can. Here's the query I used. query = '(and is_self:1 (and timestamp:%d..%d))' % (lower, upper) Or switch to 0 for linkpost only.", "created_utc": 1438759277, "gilded": 0, "name": "t1_cts32an", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "EmperorSofa", "body": "I had a question about: searchresults = list(r.search(query, subreddit=subreddit, sort='new', limit=100, syntax='cloudsearch')) In your code it seems like you do a single search, break out of the while loop , reverse the results, and then based on how many items you find you change the interval of the search. You keep doing this until lower >= maxupper. I'm kind of confused as to the behavior of r.search. If I call that function once it just fetches the latest 100 posts of either a subreddit or a user that I specify. If I call it twice how does it not simply repeat the first 100 results again? Does it somehow know that what i'm asking for is the next 100 posts made after I already recieved the first 100 posts?", "created_utc": 1439102921, "gilded": 0, "name": "t1_ctwo90k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "GoldenSights", "body": "If you were to call the same function with the exact same parameters multiple times, you will get the same 100 results again. As a matter of fact, PRAW keeps a 30 second cache of the responses, so you'll get those items back without even making a request. Reddit doesn't \"know\" that I want the next 100 items, but since I'm specifying the lower and upper timestamps in the cloudsearch query, I will get what I was looking for. Does that help, or did I miss the point of your question?", "created_utc": 1439106975, "gilded": 0, "name": "t1_ctwpg4p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "EmperorSofa", "body": "No I think you got it. I didn't realize that after the while loop you make changes to lower and upper based on the number or results found. One thing though. Why do you have interval set to 86400? Also in: else: #Intentionally not elif lower = upper upper = lower + interval toomany_inarow = max(0, toomany_inarow-1) smartinsert(sql, cur, searchresults) It looks like you're saying that in between the upper and lower dates if you only find 75 or less items you need to move the search window up a little bit. However if you're making changes to the upper and lower bounds that the user specifies aren't you also making changes to the dates that the user originally specified for the search? It looks like you're using that else statement as your exit from the while lower", "created_utc": 1439107827, "gilded": 0, "name": "t1_ctwpo3v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "GoldenSights", "body": "The default interval of 86,400 seconds (1 day) just seemed like a good starting point. The interval expands and contracts depending on how many items are found on each scan, so that was just a comfortable default. > It looks like you're using that else statement as your exit from the while lower 99:` above it. This is probably an indication that I should go add some better comments. Here was my thought process if itemsfound 99: # Since we got >= 100 items, there is PROBABLY # a second page of results that we missed out on. # The lower bound stays the same, while the upper # bound gets shrunk, so that the results will all fit # on one page next time. # Since we don't slide the lower bound, we will see # some duplicate results until this block stops triggering. else: # As long as we got less than 100 items, we can # slide the window forward. At first glance, it would seem like these three should be an if-elif-else chain, but if it was then I would have to include a copy of the window-sliding code inside the However if you're making changes to the upper and lower bounds that the user specifies aren't you also making changes to the dates that the user originally specified for the search? I'll be honest, I haven't actually tested this a whole lot using specific end-dates. I did test it a few times and I remember it working, but there's a chance that you're right. However, this loop changes the `upper` variable, but not the `maxupper` one, which is controlled by the user input. The maxupper should still be in charge. The lower bound can be modified as much as we want because that doesn't matter after the first cycle.", "created_utc": 1439109513, "gilded": 0, "name": "t1_ctwq38u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "tblahosh", "body": "I'm not sure if this will help, but there is a way to get rid of set intervals completely. Basically, you just leave the lower bound off. So the query looks like (and timestamp:..%d) This will return the last 100 posts, then you just have to update %d to be the unix timestamp of the 100th post for the next iteration.", "created_utc": 1441598670, "gilded": 0, "name": "t1_cut53p7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "GoldenSights", "body": "I'll have to check that out! That would make it really easy to do the process in reverse, starting with the top post on /new and working backwards. Thanks for the tip.", "created_utc": 1441600810, "gilded": 0, "name": "t1_cut5zhk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fujpg/praw_iterating_through_subreddit_posts_based_on/"}, {"author": "GoldenSights", "body": "I just tried the submission you mentioned, and I also got a 403 until I changed it to https. Does that fix any of the other problematic URLs?", "created_utc": 1438664876, "gilded": 0, "name": "t1_ctqsmi6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fpmx8/praw_get_submission_works_only_sometimes_with/"}, {"author": "RemindMeBotWrangler", "body": "That seems to be exactly the reason! I guess the issue now is that the `permalink` returned in the `comment` object returns as http. I'll just have to query through to clean my database and change it to https when saving.", "created_utc": 1438665527, "gilded": 0, "name": "t1_ctqsw3b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fpmx8/praw_get_submission_works_only_sometimes_with/"}, {"author": "GoldenSights", "body": "Hmm, for me the `permalink` attribute is https. I'm using the github version rather than the pypi version of PRAW, so I don't know if that was a change introduced between 3.1.0 and now. By next release if you're still getting http then there is probably a config issue.", "created_utc": 1438667403, "gilded": 0, "name": "t1_ctqtln3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fpmx8/praw_get_submission_works_only_sometimes_with/"}, {"author": "RemindMeBotWrangler", "body": "Yeah, you're correct! I was just looking at old code it seems. Thanks for the help. Does oAuth2 only work with https which is why this all happened?", "created_utc": 1438667715, "gilded": 0, "name": "t1_ctqtpqy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fpmx8/praw_get_submission_works_only_sometimes_with/"}, {"author": "GoldenSights", "body": ">does OAuth2 only work with https which is why all this happened I don't know for a fact, but I would assume you're correct. Reddit is supposed to be site-wide https soon, and oauth is all about improving account security, so I'd say they go together. Glad it worked out!", "created_utc": 1438667893, "gilded": 0, "name": "t1_ctqts1i", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fpmx8/praw_get_submission_works_only_sometimes_with/"}, {"author": "GoldenSights", "body": "Woah, that's really bizarre. I've just tested this and found that the privatemessages scope allows the user to start new PM threads via api/compose, but not continue old ones via api/comment/. It appears that reddit is just [checking the endpoint itself](https://github.com/reddit/reddit/blob/c942cef02c71d3228cdaee234d3195aeec37e70e/r2/r2/controllers/api.py#L1785), and not whether the object you're replying to is a Message. I can change these praw docs for you, but I'd be more interested in seeing if the admins will consider reworking this. It's a very silly quirk. /u/gooeyblob?", "created_utc": 1438597509, "gilded": 0, "name": "t1_ctprern", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "gooeyblob", "body": "It does indeed seem like a strange quirk. I think it mostly has to do with how things are abstracted in the backend. I'm not sure this is easily fixed unfortunately without either creating a new endpoint or doing some gnarly checks inside the POST_comment function. Let me check with some other people here.", "created_utc": 1438637913, "gilded": 0, "name": "t1_ctqdki5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "gooeyblob", "body": "Thanks for pointing this out! It's been [fixed](https://github.com/reddit/reddit/commit/9ccda2c49fe17321dad97e20bb6b2799e2e006d1). I've left the `submit` scope in there for backwards compatibility, but I'd love to get PRAW updated to use the new scope. u/bboe what would be the best way to do that? Thanks again!", "created_utc": 1438905509, "gilded": 0, "name": "t1_ctu84pk", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "GoldenSights", "body": "Awesome! I'll go ahead and make a PR with PRAW to get that patched up. Thank you very much!", "created_utc": 1438905587, "gilded": 0, "name": "t1_ctu86b0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "gooeyblob", "body": "Great, thanks!", "created_utc": 1438907126, "gilded": 0, "name": "t1_ctu918m", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "bboe", "body": "/u/GoldenSights is on it. He's awesome.", "created_utc": 1438910162, "gilded": 0, "name": "t1_ctuaq8m", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3flqqh/praw_incorrect_scopes/"}, {"author": "avinassh", "body": "probably your ouath tokens expired?", "created_utc": 1438521310, "gilded": 0, "name": "t1_ctotqjy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "Phteven_j", "body": "Then how could I be removing posts and distinguishing comments under the expired token?", "created_utc": 1438535002, "gilded": 0, "name": "t1_ctozrgt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "avinassh", "body": "hmm you are right can you check OAuth permissions?", "created_utc": 1438535335, "gilded": 0, "name": "t1_ctozxi4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "Phteven_j", "body": "In what way? I listed my _authentication variable but otherwise I'm not sure what to check.", "created_utc": 1438535403, "gilded": 0, "name": "t1_ctozypi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "SmBe19", "body": "It looks like praw expects a logged in session (i.e. login via user/pw) and the oauth scope isn't correctly set in praw (i.e. praw doesn't know which scope is required). If that's the case, you can't do much except maybe telling /u/bboe.", "created_utc": 1438531585, "gilded": 0, "name": "t1_ctoy0sv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "Phteven_j", "body": "/u/bboe pls", "created_utc": 1438535014, "gilded": 0, "name": "t1_ctozroh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "bboe", "body": "Can you file an issue please? https://github.com/praw-dev/praw/issues", "created_utc": 1438535663, "gilded": 0, "name": "t1_ctp03dh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "Phteven_j", "body": "Yiss", "created_utc": 1438540722, "gilded": 0, "name": "t1_ctp2sln", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fgo6t/praw_getting_loginrequired_error_when_attempting/"}, {"author": "GoldenSights", "body": "When you get the `access_information` variable, print it to the console and you will see a key called `refresh_token`. Copy this token and save it to a text file / py module. Then, when you restart the bot: r.set_oauth_app_info(client_id, client_secret, redirect_uri) r.refresh_access_information(refresh_token) And you will be authorized. You shouldn't be going through the web browser process more than once (for personal scripts with permanent tokens).", "created_utc": 1438371244, "gilded": 1, "name": "t1_ctn5jsh", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "body": "Thanks man, that did the trick. Have a gold for your trouble.", "created_utc": 1438375156, "gilded": 0, "name": "t1_ctn86hw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "GoldenSights", "body": "No trouble at all, thank you!", "created_utc": 1438375325, "gilded": 0, "name": "t1_ctn8ak4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "body": "One question if you don't mind. I added a bunch of scopes and redid the authorization code, but I can't seem to do mod stuff. I tried to initiate a ban with the script and got this back: `do_relationship` requires a logged in session These are my scopes: \"identity modposts edit flair modconfig modflair modwiki read save submit vote wikiedit wikiread\"", "created_utc": 1438376792, "gilded": 0, "name": "t1_ctn982k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "GoldenSights", "body": "Hmm, the first thing that comes to mind is that you aren't using the new token correctly. When you do the browser thing and get the \"code\" and plug it into PRAW, that will give you a new refresh token. This new token is completely separate from the older one, it doesn't update the older one with new scopes or anything like that. If you're sure that you're doing it correctly, maybe you can try updating praw with the version on github to see if something has been changed that would fix it.", "created_utc": 1438377276, "gilded": 0, "name": "t1_ctn9iqk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "body": "I just did a --upgrade, so I know I'm current. Is the refresh token tied to the access token between runs? Meaning can I get the code from the URL, shut down the script, and relaunch it to execute the access_information=r.get_access_information(\"\") print access_information bit? Or is there a way to tell whether my authentication works and what the resulting scope is? I assumed it was successful since it didn't throw any errors when I fed it the refresh token.", "created_utc": 1438377568, "gilded": 0, "name": "t1_ctn9pa3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "GoldenSights", "body": "> Is the refresh token tied to the access token between runs? The \"\" (I'm still not sure if that code has an actual name, I just call it the Access Code) should work perfectly fine even if you restart python before using `get_access_information`. At this point in the process, you do not have an access token or refresh token -- that's what `get_access_information` gives you. All you have is the access code. I guess I'm not sure how to answer your question, because you're asking about refresh tokens, but you're demonstrating something else. FWIW, you get a new access_token every time you refresh, even if the old one has not yet expired, if that's what you mean. In that sense, access tokens and refresh tokens aren't really \"tied together\" because access tokens are disposable. > Is there any way to tell whether my authentication works and what the resulting scope is? You can `print(r._authentication)` to see your current scopes. The scopes are also included in the dictionary that's returned by `get_access_information` and `refresh_access_information`", "created_utc": 1438378217, "gilded": 0, "name": "t1_ctna3t1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "body": "Cool, so printing _auth gives me set([u'wikiedit', u'save', u'wikiread', u'edit', u'submit', u'modconfig', u'read', u'modposts', u'modflair', u'vote', u'modwiki', u'identity', u'flair']) So that's fairly definitive proof I am a) logged in and b) able to do mod stuff, right? In this case, leaving a comment, distinguishing the comment, and banning the user.", "created_utc": 1438378439, "gilded": 0, "name": "t1_ctna8rm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "GoldenSights", "body": "Okay, I see why you had trouble with the ban (do_relationship) earlier. Banning falls under the `modcontributors` scope, which is not listed on the side of [this page](https://www.reddit.com/dev/api/oauth). The only way to know it exists is to notice it in the docs for [api/friend](https://www.reddit.com/dev/api/oauth#POST_api_friend) which is the endpoint they use for controlling relationships of many types, not just friends. Sorry for that inconvenience, I think it's very poor doc design to have a single scope not listed on the side with the rest of the scopes.", "created_utc": 1438378713, "gilded": 0, "name": "t1_ctnaeq8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "body": "Ha, that's weird as hell! Thanks for your help, bro. How the hell is anyone supposed to know that? And PRAW doesn't error on it except to tell you you aren't logged in (it should OAuth error IMO). /u/deimorz, might want to look into this.", "created_utc": 1438378819, "gilded": 0, "name": "t1_ctnah16", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "GoldenSights", "body": "I'll check out the praw exceptions and see if that can be fixed. The oauth scope restriction decorators are supposed to get overhauled for PRAW4 and I think those are involved in the \"requires logged in session\" message.", "created_utc": 1438378899, "gilded": 0, "name": "t1_ctnairc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "body": "Are you sure it's \"modcontributors\"? Still erroring for me with that.", "created_utc": 1438379149, "gilded": 0, "name": "t1_ctnao3m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "GoldenSights", "body": "Yeah I'm sure http://i.imgur.com/tQ81miV.png", "created_utc": 1438379322, "gilded": 0, "name": "t1_ctnaryp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "body": "OK, so is it possible I'm not logged in even if r._authentication shows my correct authentication? I commented out the access code generation lines, so this is what I have at the start of my script: refresh_token=\"string here\" r = praw.Reddit(\"gunnit mod agent\") r.set_oauth_app_info(client_id,client_secret,redirect_uri) r.refresh_access_information(refresh_token) print r._authentication", "created_utc": 1438379743, "gilded": 0, "name": "t1_ctnb15u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "GoldenSights", "body": "Mmm not really, the code you have there is correct and you should have access at that point as long as you're using the new refresh_token which includes the modcontributors scope. I'm really not the best at remote troubleshooting. If I were in your position, I might try starting the process from scratch now that I know what I did right and wrong along the way. Other than that I would say just keep playing around with it in the interpreter and see if there's something you missed.", "created_utc": 1438380438, "gilded": 0, "name": "t1_ctnbg1q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "Phteven_j", "body": "Well, I was able to change a user's flair, so that's at least one mod-task that is working, but still can't ban. Really bizarre.", "created_utc": 1438381088, "gilded": 0, "name": "t1_ctnbtoz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "SmBe19", "body": "If you do not want to handle oauth (the whole token retrieval) yourself there is a [small library](https://github.com/SmBe19/praw-OAuth2Util) available. Regarding your problem: it seems like you didn't fully understand how oauth works. Short summary: 1. Script wants to access reddit on behalf of a user. It sends the user to reddit to ask him whether he does want to grant access. (`r.get_authorize_url(...)`, `webbrowser.open(url)`) 1. The user clicks accept and reddit generates a code which it sends to the given callback url. Normally there would be a webserver listening to grab the code, but for small bots we just copy the code from the url ourselves. 1. This code is not to access reddit, it is only a \"one time use ticket\". You can trade it in and you receive a token and a refresh token. (`r.get_access_information(...)`) 1. The token is your \"ticket\" to access reddit, but it is only valid for one hour. The refresh token allows you to get a new ticket when the old one is expired. As you see you only need the code the first time you access reddit. Afterwards you have to provide the refresh token to retrieve a new token without the need to ask the user again. So you don't have to write the code permanently in your script, but you should save the refresh token and use this on a restart.", "created_utc": 1438373575, "gilded": 0, "name": "t1_ctn751p", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "13steinj", "body": "Question: Does your library automatically handle token refreshing on the hour mark or does one still have to sideload a clock for that?", "created_utc": 1438390922, "gilded": 0, "name": "t1_ctnh36w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "SmBe19", "body": "Everytime you call `o.refresh` it checks how old the token is. If the token is too old, it will request a new one. So if you call this method everytime before a big chunk of praw things (e.g. at the beginning of a main loop), you don't need to keep track of the time yourself.", "created_utc": 1438415586, "gilded": 0, "name": "t1_ctnr20i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "FanfictionBot", "body": "RemindMe! 5 days", "created_utc": 1439153918, "gilded": 0, "name": "t1_ctx8bdy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3fbqfw/oauth_help_for_simple_python_bots_getting_invalid/"}, {"author": "erktheerk", "body": ">UserWarning: The keyword `bot` in your user_agent may be problematic. C:\\Python34\\lib\\site-packages\\praw\\decorators.py:88: DeprecationWarning: reddit intends to disable password-based authentication of API clients sometime in the near future. As a result this method will be removed in a future major version of PRAW. > For more information please see: > * Original reddit deprecation notice: https://www.reddit.com/comments/2ujhkr/ > * Updated delayed deprecation notice: https://www.reddit.com/comments/37e2mv/ So as soon as they update it most likely.", "created_utc": 1438275208, "gilded": 0, "name": "t1_ctlpl87", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "avinassh", "body": "well, if you want a library which can do all the OAuth for you, then you can try using my library I wrote: https://github.com/avinassh/prawoauth2 it is almost similar as one written by /u/SmBe19", "created_utc": 1438521505, "gilded": 0, "name": "t1_ctotsqo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "DontKillTheMedic", "body": "Would this allow my web app to bypass the OAuth initialization/set up every time someone wants to use it? Including the refreshing of tokens and such?", "created_utc": 1438527916, "gilded": 0, "name": "t1_ctow9hu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "avinassh", "body": "yes. thing is, it doesn't matter if you use the library or not. while getting authorisation, you have to pass `permanent=True` i.e. Reddit will grant the permission for a user permanently, till he revokes them manually. and you get refresh tokens without asking permission from user.", "created_utc": 1438528189, "gilded": 0, "name": "t1_ctowdsd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "DontKillTheMedic", "body": "Okay but what if I want to use another reddit account that isn't a user's? They wouldn't know the account's pw/login.", "created_utc": 1438528616, "gilded": 0, "name": "t1_ctowkr0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "avinassh", "body": "I don't understand, can you elaborate", "created_utc": 1438530227, "gilded": 0, "name": "t1_ctoxby4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "DontKillTheMedic", "body": "Of course. So let's say a web application has a surrogate account that would be responsible for all comment traversal and analysis. It seems that the way OAuth is set up, every time I want to authenticate an account, I have to grab a code from a generated URL/web page. I then would have to paste that code into my .py file and only then may I use the account. Right now I just use .login(\"account\", \"accountpw\"), but that is leaving us very soon. Does this help?", "created_utc": 1438531983, "gilded": 0, "name": "t1_ctoy831", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "avinassh", "body": "yes. you don't need oauth token everytime. like I said earlier, when you pass `permanent` parameter, you get access permanently. you don't need to grab the code from URL or anything. it's one time process. check if this [readme](https://github.com/avinassh/prawoauth2#how-what-and-why) helps", "created_utc": 1438532873, "gilded": 0, "name": "t1_ctoyob1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "powderblock", "body": "August 3rd if I remember correctly.", "created_utc": 1438277920, "gilded": 0, "name": "t1_ctlrkx0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "powderblock", "body": "Yep. >DeprecationWarning: Password-based authentication will stop working on 2015/08/03 and as a result will be removed in PRAW4.", "created_utc": 1438277973, "gilded": 0, "name": "t1_ctlrmby", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "SmBe19", "body": "No, they will change later: https://www.reddit.com/comments/37e2mv/ It was planed for August 3rd but they moved it.", "created_utc": 1438279684, "gilded": 0, "name": "t1_ctlsvfz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "gooeyblob", "body": "Yes - please move to OAuth as soon as you can. OAuth should soon be faster (if not already), and eventually we're going to turn off cookie authentication to the API.", "created_utc": 1438299689, "gilded": 0, "name": "t1_ctm69uj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "DontKillTheMedic", "body": "Faster? As in 'the process of logging into an account is faster', or that 'the account can run through threads/comments/submissions faster'?", "created_utc": 1438302250, "gilded": 0, "name": "t1_ctm7rgv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "gooeyblob", "body": "Every part of OAuth should be faster than cookie auth soon, if not already. Logging in, getting comments, making posts, any part of the API.", "created_utc": 1438302617, "gilded": 0, "name": "t1_ctm7z0n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "DontKillTheMedic", "body": "Holy crap, if that is true, then this could be my solution to my web-application's gateway timeout issue", "created_utc": 1438302703, "gilded": 0, "name": "t1_ctm80qm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "gooeyblob", "body": "Where is your web app hosted?", "created_utc": 1438302804, "gilded": 0, "name": "t1_ctm82sj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "DontKillTheMedic", "body": "I'll pm you", "created_utc": 1438303502, "gilded": 0, "name": "t1_ctm8h34", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "zzpza", "body": "Is there a simple OAuth option for single user scripts? I have a couple of scripts I've written that help me moderate my subs. They only run under one account and will never have a web interface or multiple users.", "created_utc": 1438323904, "gilded": 0, "name": "t1_ctmisuv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "gooeyblob", "body": "Are you using an API wrapper?", "created_utc": 1438332511, "gilded": 0, "name": "t1_ctmlc3m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "zzpza", "body": "I'm using PRAW if that's what you're asking (sorry - n00b). I've read the section in the PRAW documentation about OAuth and it seemed very complex for my needs, especially as my scripts would need multiple scopes. It wasn't clear how I would handle that either - would I need to have a different token for each scope and swap between them before I did anything that requires a different scope?", "created_utc": 1438344607, "gilded": 0, "name": "t1_ctmopgz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "SmBe19", "body": "I made a python module to help with OAuth for praw. Use `pip install praw-oauth2util` to install it. https://github.com/SmBe19/praw-OAuth2Util", "created_utc": 1438350890, "gilded": 0, "name": "t1_ctmrode", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "spaceysbot", "body": "I tried to use this, and it just hangs on: >>> o = OAuth2Util.OAuth2Util(r) I don't get a new prompt or anything. Are the keys meant to be in quotes in the oauth.txt?", "created_utc": 1438936113, "gilded": 0, "name": "t1_ctulpvw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "SmBe19", "body": "Keys don't have to be in quotes. No idea why it would hang. Are you sure it's this line it hangs and not something else in your script?", "created_utc": 1439027358, "gilded": 0, "name": "t1_ctvs7gf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "zzpza", "body": "Awesome, thank you :)", "created_utc": 1438374722, "gilded": 0, "name": "t1_ctn7wc8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "gooeyblob", "body": "It's not really as complex as it seems! You create an app [here](https://www.reddit.com/prefs/apps/), then use that client ID, client secret, and redirect URI in your script (you can just use localhost for a redirect URI if it's just a local script). Set the code in PRAW, let it do the token exchange for you, and then you can use that access token and refresh token to make requests going forward. No - you can request all the scopes you need the first time and use those forever. If you request 4 different scopes, they all apply to that token, you don't get 4 different tokens.", "created_utc": 1438345008, "gilded": 0, "name": "t1_ctmov0o", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "Spacedementia87", "body": "How do you get the bot to do the token exchange? I was following the tutorial on the PRAW documentation but I don't have a webbrowser vis SSH so i got stuck", "created_utc": 1438934480, "gilded": 0, "name": "t1_ctulayk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "gooeyblob", "body": "You don't need to open the authorize page in the same session that you're running PRAW, you just need to get the `code` query parameter after you open the URL and hit approve.", "created_utc": 1438970673, "gilded": 0, "name": "t1_ctv1j5o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "zzpza", "body": "Ok, thanks for the info, it is appreciated. I'll give it a try, but may go with the other module that was linked.", "created_utc": 1438374802, "gilded": 0, "name": "t1_ctn7y7y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f5rzi/praw_redditlogin_when_is_it_leaving/"}, {"author": "GoldenSights", "body": "As far as I know, this is not possible. The closest you can get is performing an 'author_fullname' search, but that relies on the account having made a submission somewhere.", "created_utc": 1438222331, "gilded": 0, "name": "t1_ctl2pvg", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f3ydz/how_to_get_the_latest_registered_users/"}, {"author": "jian142857", "body": "Yes, the only possible idea I have now, is to search the latest submissions and comments and check if the redditors are new registered. Thank you all the same!", "created_utc": 1438225855, "gilded": 0, "name": "t1_ctl4s7h", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f3ydz/how_to_get_the_latest_registered_users/"}, {"author": "gooeyblob", "body": "I don't think we have any support for this. What type of research are you trying to do? It may not help to get users that have never posted or commented anyway.", "created_utc": 1438234084, "gilded": 0, "name": "t1_ctl8pcg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f3ydz/how_to_get_the_latest_registered_users/"}, {"author": "GoldenSights", "body": "This is probably dumb, but do you mind if I ask why /api/info isn't supported for accounts? I assume it's for user privacy but I've never actually asked an admin about it.", "created_utc": 1438238658, "gilded": 0, "name": "t1_ctlacd0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f3ydz/how_to_get_the_latest_registered_users/"}, {"author": "gooeyblob", "body": "Not sure - I'll check with some people here see if anyone has an idea of why. edit: After conferring with some people the sentiment is we don't want to make it any easier to scrape for user accounts. If there is some real legitimate use case you can let us know, but we sort of want to discourage that type of usage.", "created_utc": 1438243114, "gilded": 0, "name": "t1_ctlbnsa", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f3ydz/how_to_get_the_latest_registered_users/"}, {"author": "GoldenSights", "body": "That's what I had figured. Thanks for taking the time to do that!", "created_utc": 1438288722, "gilded": 0, "name": "t1_ctlzbdd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3f3ydz/how_to_get_the_latest_registered_users/"}, {"author": "GoldenSights", "body": "This feature hasn't been released, so it is not included in the package that comes from pypi (and therefore pip). To download it, [go here](https://github.com/praw-dev/praw) and click \"Download Zip\", then extract the contents to overwrite your praw installation at C:\\python34\\lib\\site-packages\\praw (on linux I think it's /usr/local/lib/python3.4/lib or something like that).", "created_utc": 1438122992, "gilded": 0, "name": "t1_ctjkbi0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eyfz9/praw_sticky_bottomtrue_function_not_being/"}, {"author": "TheEnigmaBlade", "body": "Alternatively, you could use the command provided in the PRAW readme. Same thing, but pip does it for you: pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip", "created_utc": 1438140988, "gilded": 0, "name": "t1_ctjv2b8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eyfz9/praw_sticky_bottomtrue_function_not_being/"}, {"author": "13steinj", "body": "Also [OP](/u/OtakuSRL) The dev build may be buggy but also could have more features, such as the ability to pass params into the search function.", "created_utc": 1438302826, "gilded": 0, "name": "t1_ctm8383", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eyfz9/praw_sticky_bottomtrue_function_not_being/"}, {"author": "rtheunissen", "body": "Is this for a submission that was created a while ago or a new one?", "created_utc": 1438070492, "gilded": 0, "name": "t1_ctirub1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "iceph03nix", "body": "A new one. I ended up just letting it run and it took about 30 minutes but I got it", "created_utc": 1438083907, "gilded": 0, "name": "t1_ctiv9ab", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "rtheunissen", "body": "Take a look if [Rockets](https://github.com/rtheunissen/rockets) could help you out? I realise it's not PRAW-related but it might suit your task. You can subscribe to `comments`, and use filters for `post` and `root`.", "created_utc": 1438088570, "gilded": 0, "name": "t1_ctix2ir", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "iceph03nix", "body": "Ahh, I guess I misunderstood when you asked about a new thread vs an old. All the content I'm looking for is already there, and anything new doesn't count. (The mods set up a contest, and didn't plan ahead how to judge it)", "created_utc": 1438091429, "gilded": 0, "name": "t1_ctiyjin", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "rtheunissen", "body": "Ah right, makes sense. Not sure how to do that with PRAW I'm afraid.", "created_utc": 1438094613, "gilded": 0, "name": "t1_ctj0g3q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "iceph03nix", "body": "yeah, I can see how to do it AFTER I get the records, but that doesn't really save me any download/access time. Oh well, it works as it is, and it's only a short term projects so I won't worry about it.", "created_utc": 1438095922, "gilded": 0, "name": "t1_ctj1b1g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "rtheunissen", "body": "You could try setting the delay to 1s per request to mitigate the long running time. Should be fine if you're OAuth.", "created_utc": 1438096243, "gilded": 0, "name": "t1_ctj1ip1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "iceph03nix", "body": "I wasn't doing any login type stuff. I don't need to have it auto post so I was just grabbing the publicly available info.", "created_utc": 1438097077, "gilded": 0, "name": "t1_ctj22wc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eu705/loading_all_top_level_comments_from_a_specific/"}, {"author": "GoldenSights", "body": "[PRAW will not use the 1 second ratelimit until password auth is removed.](https://github.com/praw-dev/praw/issues/319#issuecomment-120555369) In the meantime, you can do `r.config.api_request_delay = 1` to force it. Based on what I know, I'm willing to bet that multiprocess currently does not distinguish between multiple users' oauth sessions, but that it will in PRAW4. I've never so much as used multiprocess so I could be wrong.", "created_utc": 1437953638, "gilded": 0, "name": "t1_cth656a", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3epjgl/praw_and_oauth_rate_limit/"}, {"author": "GoldenSights", "body": "The attribute is named display_name. You can see the other properties by doing dir(object) or checking the json page itself: http://reddit.com/r/redditdev/about.json", "created_utc": 1437887244, "gilded": 0, "name": "t1_ctgetvy", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eld2o/praw_how_to_get_a_subreddit_objects_name/"}, {"author": "avinassh", "body": "`s.display_name`", "created_utc": 1438020970, "gilded": 0, "name": "t1_cti0ojc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eld2o/praw_how_to_get_a_subreddit_objects_name/"}, {"author": "_inu", "body": "You mean the name of the subreddit? Wrap it in a str(). Yeah, it threw me first time. str(s.subreddit) == 'worldnews'", "created_utc": 1437869666, "gilded": 0, "name": "t1_ctg6prn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3eld2o/praw_how_to_get_a_subreddit_objects_name/"}, {"author": "GoldenSights", "body": "What happens when you use a different useragent?", "created_utc": 1437802132, "gilded": 0, "name": "t1_ctfg7kn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ej576/getting_forbidden_error_not_sure_why/"}, {"author": "codsane", "body": "Tried that last time and didn't seem to work, tried it again with \"Different Useragent\" and it worked. Not sure if I was passing a bad character or what happened. Appreciate the help very much!", "created_utc": 1437804442, "gilded": 0, "name": "t1_ctfh0gs", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ej576/getting_forbidden_error_not_sure_why/"}, {"author": "GoldenSights", "body": "Automoderator should be able to do this pretty easily. Just look for regex matches in the title and make it assign the flair as the matched text. I'm not great with AM, and I'm on mobile anyway, but /r/automoderator and the wiki on github should have plenty of examples of this.", "created_utc": 1437777024, "gilded": 0, "name": "t1_ctf49ao", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ehx0t/praw_automaticallly_assign_flair_to_posts_in_sub/"}, {"author": "MycTyson", "body": "You know, I have automoderator doing so much it hadn't occured to me to lean on that sucker to do this lifting for me. I just really wanted an excuse to learn both Python and PRAW haha. Thanks for pointing me in the direction of automod, though! I'll likely end up going that route, however if anyone has any suggestions relevant to Python and PRAW I am still listening :) Thanks /u/GoldenSights", "created_utc": 1437777466, "gilded": 0, "name": "t1_ctf4i7p", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ehx0t/praw_automaticallly_assign_flair_to_posts_in_sub/"}, {"author": "GoldenSights", "body": "Haha, I know what you mean about finding an excuse to use python. If you want to do it that way, it's going to be the same process -- read the new queue, regex the titles, set the flair if it isn't already correct. I don't have any code examples for that but it should be quite short.", "created_utc": 1437777902, "gilded": 0, "name": "t1_ctf4qxw", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ehx0t/praw_automaticallly_assign_flair_to_posts_in_sub/"}, {"author": "MrRogersbot", "body": "/u/deimorz, how did you get automoderator to do it?", "created_utc": 1437586962, "gilded": 0, "name": "t1_ctcccc2", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3e80jx/praw_setting_action_reason_for_removal_in_mod_log/"}, {"author": "Deimorz", "body": "AutoModerator is built into the site now, it doesn't use the external API any more. Setting that reason is something that you can't do via the external API yet.", "created_utc": 1437587054, "gilded": 0, "name": "t1_ctccesp", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3e80jx/praw_setting_action_reason_for_removal_in_mod_log/"}, {"author": "MrRogersbot", "body": "sad Thanks for the superfast reply though", "created_utc": 1437587237, "gilded": 0, "name": "t1_ctccjkz", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3e80jx/praw_setting_action_reason_for_removal_in_mod_log/"}, {"author": "SmBe19", "body": "Expanding on what /u/xfile345 said: If you create an app for OAuth and choose `script` as app type, you have to add all accounts that the bot has to be able to log in with as developers. If you retrieve the access token from reddit, you have to be logged in with the user with which the bot should perform the actions. If you want to use several accounts, you have to specify different config files in OAuth2Util (with `o = OAuth2Util.OAuth2Util(r, configfile=\"config_user2.txt\")`) and create an OAuth2Util instance for each user you would like to use. You can then call `o.set_access_credentials()` on the OAuth2Util instance of the user you would like to perform the next action with. An access token is bound to a user, so if you have multiple tokens (retrieved with different accounts), the token you use will determine which account will perform the action.", "created_utc": 1437420486, "gilded": 0, "name": "t1_cta0znb", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "AchillesDev", "body": "Thank you for the additional information. To make sure I understand this correctly, if each user will use two different accounts (that do not need to be logged in concurrently), I should set up two config files for OAuth2Util, correct? Thank you again!", "created_utc": 1437427904, "gilded": 0, "name": "t1_cta5yd4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "SmBe19", "body": "Exactly. Each config file can store one token. So you need one config file per account.", "created_utc": 1437456314, "gilded": 0, "name": "t1_ctaletz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "AchillesDev", "body": "Perfect. Thanks!", "created_utc": 1437495325, "gilded": 0, "name": "t1_ctb068y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "xfile345", "body": "In order for a bot to perform any action on behalf of another user, they will have to allow your bot to do so. Instead of supplying your bot with a username and password, they will simply be given an \"Allow\" button to push with the details of your bot and what your bot intends to do with their account and for how long it is permitted to do so. An access token will then be supplied to your bot, which it will need to use to tell reddit it is acting on that user's behalf.", "created_utc": 1437357448, "gilded": 0, "name": "t1_ct98p9k", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "AchillesDev", "body": "So the account I add the script to in order to get the app secret and key doesn't matter in this case?", "created_utc": 1437357927, "gilded": 0, "name": "t1_ct98yrs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "xfile345", "body": "Correct. Your client ID and secret is only what identifies your bot and the user who created it. In order ton act on behalf of any user, including the user that created the bot, it must first be authorized via Reddit to obtain an access_token. That access_token is what gives your bot \"access\" to perform duties via an account.", "created_utc": 1437358182, "gilded": 0, "name": "t1_ct9940x", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "AchillesDev", "body": "Ah I see, that makes more sense now. Thank you!", "created_utc": 1437385932, "gilded": 0, "name": "t1_ct9jg9m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3dw2y8/prawoauth_using_oauth_to_automate_user_actions_on/"}, {"author": "n1c", "body": "Is your bot crawling backwards through the pagination? I ran into a similar issue the other day, only to find out that you just can't page that far back through history.", "created_utc": 1436966165, "gilded": 0, "name": "t1_ct43g6b", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ddlcv/crawling_issue/"}, {"author": "Ouiski", "body": "I got around that by using the cloudsearch syntax for searching by time stamp. So I have 40000 submissions from a subreddit going back 6 years. I'm using these submission ids then to generate urls to collect data from. I think it could be my token expiring but I thought the way my bot called it got a new token regularly. I'll have to check up on that when I get home from work.", "created_utc": 1436967525, "gilded": 0, "name": "t1_ct443y8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ddlcv/crawling_issue/"}, {"author": "bboe", "body": "Sounds like it may be a bug. Care to file on github: https://github.com/praw-dev/praw/issues There is no ability to provide different cache lengths for different end points. However, you could change the default cache value to five minutes for all objects, and then add a unique parameter to bypass the cache for the listing requests. That might accomplish your task.", "created_utc": 1436930630, "gilded": 0, "name": "t1_ct3rlwf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3db07x/praw_crashing_on_nonstandard_characters/"}, {"author": "pcjonathan", "body": "Done :)", "created_utc": 1436933316, "gilded": 0, "name": "t1_ct3t0k7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3db07x/praw_crashing_on_nonstandard_characters/"}, {"author": "bboe", "body": "Thanks.", "created_utc": 1436971276, "gilded": 0, "name": "t1_ct465vi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3db07x/praw_crashing_on_nonstandard_characters/"}, {"author": "GoldenSights", "body": "/u/teaearlgraycold is working on a [pull request](https://github.com/praw-dev/praw/pull/467). There was something funny going on with the test suite but I'm sure it'll be resolved pretty quickly.", "created_utc": 1436862609, "gilded": 0, "name": "t1_ct2q1u4", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "zzpza", "body": "Awesome, thanks for the head's up. :)", "created_utc": 1436862719, "gilded": 0, "name": "t1_ct2q2uc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "teaearlgraycold", "body": "Not sure if this will upstage /u/bboe and /u/GoldenSights, but you can download from my repository and run setup.py to install a version that supports two stickies. https://github.com/teaearlgraycold/praw Syntax is sticky(1) or sticky(2). If you don't select either it chooses the second position.", "created_utc": 1436886967, "gilded": 0, "name": "t1_ct2z95m", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "Rapptz", "body": "Shouldn't it select the first one so it isn't a breaking change?", "created_utc": 1436907811, "gilded": 0, "name": "t1_ct3dxvo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "teaearlgraycold", "body": "https://www.reddit.com/dev/api#POST_api_set_subreddit_sticky Slot #2 is the default", "created_utc": 1436908018, "gilded": 0, "name": "t1_ct3e2ti", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "bboe", "body": "While I absolutely appreciate /u/teaearlgraycold for the pull request, I don't recommend updating the package via this approach. The changes are merged in master so the suggested way to run get the development version is: pip install --upgrade https://github.com/praw-dev/praw/archive/master.zip", "created_utc": 1436927616, "gilded": 0, "name": "t1_ct3px5p", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "teaearlgraycold", "body": "Of course, it wasn't in the original github when I commented that though so please everyone use the official.", "created_utc": 1436928076, "gilded": 0, "name": "t1_ct3q6fx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d8csx/praw_support_for_two_stickies/"}, {"author": "GoldenSights", "body": "Perhaps you're banned from the subreddit it's trying to post to? You may be able to get slightly more information with: try: comment.reply('dongers') except Exception as e: print(e._raw.text) I wouldn't be surprised if such a dongerbot got banned somewhere.", "created_utc": 1436829347, "gilded": 0, "name": "t1_ct2achj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d6p0y/my_bot_code_is_returning_a_rather_lengthy/"}, {"author": "austin_18", "body": "Posting to a sub that I'm banned from is something that crossed my mind. What does the exception exactly do, if you don't mind quickly explaining.", "created_utc": 1436829565, "gilded": 0, "name": "t1_ct2ah19", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d6p0y/my_bot_code_is_returning_a_rather_lengthy/"}, {"author": "GoldenSights", "body": "\"Forbidden\" should be pretty self explanatory -- you're not allowed to do whatever you're trying to do. PRAW gives you that exception any time you get a 403 error, which happens when you try to post in somewhere you're banned, or trying to access the config of somebody else's subreddit, etc.", "created_utc": 1436829655, "gilded": 0, "name": "t1_ct2aj06", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d6p0y/my_bot_code_is_returning_a_rather_lengthy/"}, {"author": "austin_18", "body": "That seemed to work! I posted a key word in a sub I know that my bot is banned from, and it returned {\"error\": 403} in the python shell and wasn't interrupted. Thanks for your help!", "created_utc": 1436830465, "gilded": 0, "name": "t1_ct2azns", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d6p0y/my_bot_code_is_returning_a_rather_lengthy/"}, {"author": "austin_18", "body": "Alright, thanks. I'll run it and might post to a sub that I know the bot is banned from to see if anything happens.", "created_utc": 1436829878, "gilded": 0, "name": "t1_ct2anm8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3d6p0y/my_bot_code_is_returning_a_rather_lengthy/"}, {"author": "GoldenSights", "body": "Can you paste the code that led up to this error?", "created_utc": 1436581461, "gilded": 0, "name": "t1_cszby89", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "teaearlgraycold", "body": "Here's the traceback: Traceback (most recent call last): File \"/home/pi/bots/TeaBot-OAuth/teaBot.py\", line 193, in message_commands resp = self.do_shadowban(subreddit, message, arguments) File \"/home/pi/bots/TeaBot-OAuth/teaBot.py\", line 282, in do_shadowban subreddit.un.add_note(n) File \"/home/pi/bots/TeaBot-OAuth/modules/puni.py\", line 206, in add_note self.set_json(notes, '\"create new note on user ' + note.username + '\" via puni') File \"/home/pi/bots/TeaBot-OAuth/modules/puni.py\", line 152, in set_json self.r.edit_wiki_page(self.subreddit, self.page_name, json.dumps(notes), reason) File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 1237, in edit_wiki_page return self.request_json(self.config['wiki_edit'], data=data) File \"/usr/local/lib/python3.2/dist-packages/praw/decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 423, in _request response = handle_redirect() File \"/usr/local/lib/python3.2/dist-packages/praw/__init__.py\", line 397, in handle_redirect url = _raise_redirect_exceptions(response) File \"/usr/local/lib/python3.2/dist-packages/praw/internal.py\", line 180, in _raise_redirect_exceptions raise RedirectException(response.url, new_url) The code: https://github.com/teaearlgraycold/TeaBot/blob/master/modules/puni.py#L152", "created_utc": 1436581672, "gilded": 0, "name": "t1_cszc1vt", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "GoldenSights", "body": "Okay, looks like PRAW isn't sending the oauth headers for that request. I'm not quite sure why yet, but I'll let you know when I've got a fix.", "created_utc": 1436583116, "gilded": 0, "name": "t1_cszcql8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "teaearlgraycold", "body": "Thanks!", "created_utc": 1436583437, "gilded": 0, "name": "t1_cszcvyo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "GoldenSights", "body": "Welp, I just went on a ridiculous goose chase only to find it was missing a single line :| Go to your python Lib\\site-packages\\praw\\\\\\_\\_init\\_\\_.py https://github.com/praw-dev/praw/blob/3d0be7a1697c90f715d4dd4bb73c63318810b14a/praw/__init__.py#L1225 Change that so it says: @decorators.restrict_access(scope='wikiedit') def edit_wiki_page(self, subreddit, page, content, reason=''): And that should do it for you. There's probably still a few functions missing their decorators.", "created_utc": 1436586852, "gilded": 0, "name": "t1_cszeg3c", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "teaearlgraycold", "body": "Since I can't see any pull request I made my own. Sorry if I'm just dumb and missed yours.", "created_utc": 1436631108, "gilded": 0, "name": "t1_cszs53j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "GoldenSights", "body": "You're not dumb, I just had it stashed away with a bunch of other stuff. I should have mentioned it, sorry!", "created_utc": 1436642736, "gilded": 0, "name": "t1_cszxwob", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "teaearlgraycold", "body": "Thanks! I hope you made a pull request.", "created_utc": 1436588133, "gilded": 0, "name": "t1_cszf05j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "avinassh", "body": "Just a shameless plug, do check out [prawoauth2](https://github.com/avinassh/prawoauth2) for OAuth things. it will make lot of things easier.", "created_utc": 1436625958, "gilded": 0, "name": "t1_cszpvfp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "teaearlgraycold", "body": "I'm using OAuth2Util. Is yours any better?", "created_utc": 1436630747, "gilded": 0, "name": "t1_cszryx5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "avinassh", "body": "functionality wise it's same. just the way configs are handled is different. Also, for first time, the operations are different and mine is bit beginner friendly.", "created_utc": 1436632272, "gilded": 0, "name": "t1_cszsou8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "teaearlgraycold", "body": "Maybe I'll convert!", "created_utc": 1436637292, "gilded": 0, "name": "t1_cszv5z3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "avinassh", "body": "cool check the example here: https://github.com/avinassh/prawoauth2/tree/master/examples/halflife3-bot", "created_utc": 1436639380, "gilded": 0, "name": "t1_cszw85r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cvdof/string_error_after_implementing_oauth/"}, {"author": "TrackReddit", "body": "There's about ~80 new comments made throughout all of Reddit (all public subreddits, that is) every ~2 seconds. Obviously it pretty heavily fluctuates on time of day, etc etc. (Warning: Very rough estimate) So yeah, as someone else mentioned OAUTH currently allows you to retrieve all of them from the /r/all stream. Well, atleast the large majority if you do it right. Not too much you can do if Reddit is bogged down in general, too. Main thing is, for the sake of Reddit and their already generous usage limits; Do your best to automate the process of slowing down your API hit rate when you can, For example on Reddit's off hours you can afford to slow down to maybe once every 2-4 seconds and still get all new comments. Also, I'd reccommend giving like 1+ minute timeouts if you receive certain error messages from Reddit. Like if Reddit is down or showing sporadic 503's, You hitting it every second is only going to worsten the problem. So be aware of the common error codes and implement a sort of safety net so you/your server plays nice with Reddit even on it's bad days.", "created_utc": 1436570598, "gilded": 0, "name": "t1_csz67ix", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "merreborn", "body": "https://www.reddit.com/r/all/comments.json should have the raw comment data.", "created_utc": 1436578781, "gilded": 0, "name": "t1_cszam2h", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "jpfau", "body": "I'm pretty sure it doesn't read them all, but it does read them pretty fast. It'll fetch a batch of comments per API call, and then once you have the comments you want, reading them is as fast as your computer can do it. If you want to constantly read the newest comments in a subreddit, there is the [comment stream](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.helpers.comment_stream) helper function, and there is the [submission stream](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.helpers.submission_stream) for fetching posts. I would imagine many bots just loop through these in whatever subreddits they're interested in. Edit: I just noticed you aren't interested in using Python. Sorry! I don't know about any C# API, but the answer to your 'reading all comments' question still applies, I think.", "created_utc": 1436546736, "gilded": 0, "name": "t1_csyqhuq", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "IAMA_YOU_AMA", "body": "reddit bots should be able to read every single comment posted, unless there is an unusually high amount of comments per second. I don't have the API handy, but I think you are allowed 1 request every 2 seconds and each request can generate 50 comments (100 with gold, last time I checked). So there would have to be more than 1500 comments per minute for you to miss any. I really don't think that happens. Also keep in mind that PRAW has built in rate limiting, so you don't need to worry about upsetting reddit. If you write your bot outside of PRAW, then you need to make sure you don't exceed 30 requests/minute or you may have the account blocked.", "created_utc": 1436548576, "gilded": 0, "name": "t1_csyrqca", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "tappify", "body": "PRAW stands for Python Reddit API Wrapper! (Ignore this, just testing my bot)", "created_utc": 1436792114, "gilded": 0, "name": "t1_ct1ncu1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "xfile345", "body": "Just to clarify a few things in your response. You are allowed 30 requests in 60 seconds (an average of 1 every 2 seconds) via the API, unless you use OAuth, in which case that's 60 requests in 60 seconds (1 per second). Each single request for comments can fetch up to 100 comments, regardless of your gold subscription. But you are only able to get 1,000 comments at a time using `after` in each request (so that's 10 seconds time to retrieve 1,000 comments). Reddit is a *very* busy place, so it's easy for bots to be overwhelmed if there's been more than 1,000 comments in whatever interval the bot is set to run. If Reddit is going at 1,500 comments per minute (easy on some days/occasions) and your bot runs once per minute, that's 500 lost comments. If it's busier, that's more that's lost.", "created_utc": 1436550239, "gilded": 0, "name": "t1_csysud8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "joe-murray", "body": "And it's not just 500 lost, it's 500 lost *per minute*.", "created_utc": 1436554767, "gilded": 0, "name": "t1_csyvxbz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": null, "body": "[deleted]", "created_utc": 1436591414, "gilded": 0, "name": "t1_cszgc8e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ct9m8/how_do_reddit_bots_read_every_single_comment/"}, {"author": "bboe", "body": "reddit doesn't provide any replies along with comments on the user's comment page so the only way to get them is to make one request (at least) per comment to get its replies.", "created_utc": 1436412835, "gilded": 0, "name": "t1_csx14ea", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cl8oq/load_replies_to_a_comment_fetched_from_user_page/"}, {"author": "Bitani", "body": "Thanks for the response. I figured as much, but that's the problem - I'm not seeing a way to load a single comment's replies.", "created_utc": 1436416379, "gilded": 0, "name": "t1_csx2xoo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cl8oq/load_replies_to_a_comment_fetched_from_user_page/"}, {"author": "bboe", "body": "r.get_submission(comment.permalink).comments[0].replies", "created_utc": 1436417458, "gilded": 0, "name": "t1_csx3fxj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cl8oq/load_replies_to_a_comment_fetched_from_user_page/"}, {"author": "joe-murray", "body": "I gotta check this out! Thanks for making this!", "created_utc": 1436398319, "gilded": 0, "name": "t1_cswsp6x", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "Thank you :) Let me know if you have any questions.", "created_utc": 1436412987, "gilded": 0, "name": "t1_csx17bu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "sallurocks", "body": "The project you forked this from worked only on python 3 when I tried to run it, is this the same? Looks pretty cool, was needed very much.", "created_utc": 1436426644, "gilded": 0, "name": "t1_csx6r10", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "actually I was about to contact you to send a PR, ha ha. yes, it will work on both versions.", "created_utc": 1436445316, "gilded": 0, "name": "t1_csxbt82", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "body": "This is very cool, and it was working great last night, but I'm suddenly having problems - praw is throwing HTTP 400 errors when connecting. I have the latest version of your module and of PRAW. [Here is the traceback.](http://pastie.org/private/o7usngbgmhjaolhqphiakg) Code: app_key = \"J...\" app_secret = \"X...\" access_token = \"3...\" r = praw.Reddit(user_agent=\"Thingy by /u/diagonalfish\") try: oauth_helper = PrawOAuth2Mini(r, app_key=app_key, app_secret=app_secret, access_token=access_token, scopes=[\"read\", \"mysubreddits\", \"submit\", \"edit\", \"modlog\", \"modposts\", \"identity\"]) except praw.errors.HTTPException as err: print err._raw exit()", "created_utc": 1436547755, "gilded": 0, "name": "t1_csyr6gy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "Hey, I have pushed the update and latest v0.1.5 should fix this issues. I have just made the `refresh_token` mandatory, thats all was required. Please try and let me know if you come across any new issues. Thank you!", "created_utc": 1436625787, "gilded": 0, "name": "t1_cszpsys", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "body": "I tried including the refresh_token in the constructor last night and it started working. So yep, that was it. Thanks!", "created_utc": 1436626196, "gilded": 0, "name": "t1_cszpyyn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "Great :)", "created_utc": 1436626692, "gilded": 0, "name": "t1_cszq67x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "Hey, that traceback is not that helpful. However I suggest you to update the module and try again. if you stilll get error, then let me know. Also, tell me exact python and praw version. with exact name of exception. pip install --upgrade prawoauth2 Most probably your tokens are not correct", "created_utc": 1436549536, "gilded": 0, "name": "t1_csysdlt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "body": "All righty. Yeah, I had a feeling it wouldn't be, but I figured it was worth a try. :) Python 2.7.6 on Linux, PRAW 3.1.0. I turned on request logging in PRAW and I see the following (app key and secret subsituted - I checked them and they seem to be correct): substituting https://oauth.reddit.com for https://api.reddit.com in url GET: https://oauth.reddit.com/api/v1/me.json /usr/local/lib/python2.7/dist-packages/requests/packages/urllib3/contrib/pyopenssl.py:198: DeprecationWarning: unicode for buf is no longer accepted, use bytes return self.connection.send(data) status: 401 POST: https://api.reddit.com/api/v1/access_token/ data: {u'grant_type': u'refresh_token', u'refresh_token': '', u'redirect_uri': 'http://127.0.0.1:9999/authorize_callback'} auth: ('', '') status: 400 I emphasize that this was working last night, with no changes, same tokens, everything. I successfully posted a thing with this script using the auth token I got from using a slightly hacked version example onetime.py script. Not sure what's up with that pyopenssl deprecation warning, but I don't think it is relevant since I was getting that before I switched this script over to using OAuth last night. ETA: I'm thinking the problem may be the blank refresh token?", "created_utc": 1436550650, "gilded": 0, "name": "t1_csyt4ec", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "yes, you are sending blank refresh token and also make sure the redirect uri is correct", "created_utc": 1436551041, "gilded": 0, "name": "t1_csytd54", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "body": "Hmm. Your [example bot](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/bot.py) doesn't use the refresh token, though. Doesn't seem to be passed into PrawOAuth2Mini anywhere. Did I miss something here? Is the refresh token passed to that API supposed to be the one you get from the one-time setup call?", "created_utc": 1436551849, "gilded": 0, "name": "t1_csytwjn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "You can pass the refresh token, but it isn't mandatory. only access_token is enough. I do refresh thing here: https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/bot.py#L26", "created_utc": 1436552112, "gilded": 0, "name": "t1_csyu30m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "body": "Right, but you don't pass the refresh token into PrawOAuth2Mini anywhere, leaving it at the default empty string, which is why it's an empty string on the initial /access_token call in my log output there. I'm calling refresh later on but this error is happening during the initial setup (the constructor for the Praw2OAuthMini object). It seems like maybe passing the refresh token to it *is* necessary? (I don't have access to the original refresh token right now so I can't test this theory until later today.)", "created_utc": 1436552459, "gilded": 0, "name": "t1_csyubob", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "Gotcha! I figured out where is the issue and it is from me only. I really apologize. In my timezone, its almost midnight, so I will push the changes to library by tomorrow. however here's what happened. You are using access token and refresh token from yesterday. Now the access token is already over 60 minutes, it is expired and it requires a new access token. In order to get new access token, you need to send refresh token. So, it is my mistake on my part that, making refresh token non-mandatory. I was able to reproduce the [same error](http://dpaste.com/3837BPN). Refresh token needs to be sent always. And in fact, it is access token which is not mandatory. (cos, with valid refresh token, you can always get a access token). I will make the changes. So for now, please send refresh token as a parameter.", "created_utc": 1436553004, "gilded": 0, "name": "t1_csyupfy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "body": "Hey, it's totally fine, man. Don't sweat it, and I really appreciate the help figuring this out. Your library is really helpful and I'm going to continue using it for all my reddit API needs :)", "created_utc": 1436553091, "gilded": 0, "name": "t1_csyurmn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "Thank :) Please send a PR and add your bot to [the list](https://github.com/avinassh/prawoauth2#bots-using-prawoauth2)!", "created_utc": 1436553205, "gilded": 0, "name": "t1_csyuuju", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "diagonalfish", "body": "I'll have to update the github version first, but then, sure :)", "created_utc": 1436553280, "gilded": 0, "name": "t1_csyuwfk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "(:", "created_utc": 1436553303, "gilded": 0, "name": "t1_csyux16", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "MatchThreader", "body": "I'm trying to login via OAuth2. I am fairly certain I have followed the instructions correctly and am running Python 2.7.10 and just upgraded PrawOAuth2. I have successfully used PrawOAuth2 to get the access token and refresh token and setup the server. user_agent = 'Testing OAuth access for me' r = reddit_client = praw.Reddit(user_agent) app_key = \"blah\" app_secret = \"blah blah\" access_token = '41036844-randomnumbers' refresh_token = '41036844-randomletters' scopes = 'flair history modconfig modflair modlog privatemessages read save submit wikiread' oauth_helper = PrawOAuth2Mini(reddit_client, app_key=app_key, app_secret=app_secret, access_token=access_token, refresh_token = \"\", scopes=scopes) sub = \"ussoccer\" oauth_helper.refresh() settings = r.get_settings(sub) oauth_helper.refresh() sidebar_contents = settings['description'] print sidebar_contents I get the error: Traceback (most recent call last): line 39, in settings = r.get_settings(sub) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py\", line 345, in wrapped raise errors.LoginOrScopeRequired(function.__name__, scope) LoginOrScopeRequired: 'get_settings' requires a logged in session or the OAuth2 scope 'modconfig' requires a logged in session The account I am using has mod powers and I'm pretty sure I have passed it the correct scopes --- Edit: I just tried changing the scopes to ['flair', 'history', 'modconfig', 'modflair', 'modlog', 'privatemessages', 'read', 'save', 'submit', 'wikiread'] and got this series of errors: Traceback (most recent call last): , line 40, in settings = r.get_settings(sub) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py\", line 348, in wrapped return function(cls, *args, **kwargs) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/__init__.py\", line 1487, in get_settings params=params)['data'] File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/decorators.py\", line 173, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/__init__.py\", line 579, in request_json retry_on_error=retry_on_error) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/__init__.py\", line 424, in _request _raise_response_exceptions(response) File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/praw/internal.py\", line 189, in _raise_response_exceptions raise OAuthInsufficientScope('insufficient_scope', response.url) OAuthInsufficientScope: insufficient_scope on url https://oauth.reddit.com/r/ussoccer/about/edit/.json", "created_utc": 1437400451, "gilded": 0, "name": "t1_ct9o8yi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "> OAuthInsufficientScope hey there. This exception happens when you have provided insufficient scope. However for accessing sidebar things you should not get this error. I am at work now, I will try to reproduce the error and see whats going wrong. Give me 2-3 hours. However I am pretty sure it's not due to my library. Meanwhile, start a separate thread and I am sure someone will be able to help you", "created_utc": 1437401506, "gilded": 0, "name": "t1_ct9orwj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "MatchThreader", "body": "Ok, I just wanted to make sure that I was using your library correctly. It will definitely save me headaches in the future!", "created_utc": 1437403657, "gilded": 0, "name": "t1_ct9pyqo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "cool. when in doubt, run the example Half Life 3 bot.", "created_utc": 1437405856, "gilded": 0, "name": "t1_ct9ra61", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "MatchThreader", "body": "I think my issue is with scope and I'm about to pull out my hair. I understand what scope is but I don't understand how I am supposed to pass it to the 2Mini. If I create a `scopes` variable to pass as scopes in the PrawOAuth2Mini method, what form is `scopes` supposed to take? Do I need `scopes` = ['identity','privatemessages'] or 'identity privatemessages' or ['identity privatemessages']??", "created_utc": 1437539775, "gilded": 0, "name": "t1_ctbs0rg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "> `scopes = ['identity','privatemessages']` this is correct. I am sorry if this is not clear in docs. I will improve it. EDIT: The `readme` does not mention it clearly. I will make changes soon. however here's how it is done in [example](https://github.com/avinassh/prawoauth2/blob/master/examples/halflife3-bot/settings.py#L1)", "created_utc": 1437540253, "gilded": 0, "name": "t1_ctbs8hf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "MatchThreader", "body": "Ok I appreciate it. I'm still having `insufficient scope` issues but I will address them when reddit makes us have to use OAuth. This is compatible with 2.7 right?", "created_utc": 1437574475, "gilded": 0, "name": "t1_ctc3ua0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "avinassh", "body": "yup, works for both python versions", "created_utc": 1437577454, "gilded": 0, "name": "t1_ctc5p8l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ckq5a/hi_i_released_a_helper_module_for_praw_which/"}, {"author": "GoldenSights", "body": "Are you using OAuth? If so, do you have the identity scope? Without it, `r.user` does not exist, which would explain this bug. I'll try making a PR to solve that, but in the meantime I think you'll need identity. Edit: I'm not sure if this scope thing is actually the reason you're having trouble, but if so, give this a try: Go to /usr/local/lib/python2.7/site-packages/praw/objects.py, Find [this line](https://github.com/praw-dev/praw/blob/master/praw/objects.py#L382) change to return self.reddit_session._mark_as_read([self.fullname]) so it bypasses the user stuff.", "created_utc": 1436315173, "gilded": 0, "name": "t1_csvn08h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3chntm/praw_messagemark_as_read_and_mark_as_unread_broken/"}, {"author": "umop_aplsdn", "body": "Yes, I'm using OAuth. I'm not sure what you mean by identity scope. I'll give it a try. Yep, that fixed it! Thanks.", "created_utc": 1436316255, "gilded": 0, "name": "t1_csvnmik", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3chntm/praw_messagemark_as_read_and_mark_as_unread_broken/"}, {"author": "GoldenSights", "body": "I should note that you need to give objects.Inboxable.mark_as_unread a similar treatment. I'll see to it that this is included in the next praw release.", "created_utc": 1436318031, "gilded": 0, "name": "t1_csvolp2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3chntm/praw_messagemark_as_read_and_mark_as_unread_broken/"}, {"author": "GoldenSights", "body": "When you were registering the OAuth app, and you got to the page that says >OauthApp requests to connect with your reddit account. > Allow OAuthApp to: > - Access my inbox and send private messages to other users. If it had a bullet point that said \"Access my reddit username and signup date.\", then the app should have the Identity scope. You can check what scopes your praw session has with `print(r._authentication)`. [Here's the praw docs](https://github.com/praw-dev/praw/blob/master/docs/pages/oauth.rst#oauth-scopes) for Oauth scopes, though the table is currently incomplete.", "created_utc": 1436316680, "gilded": 0, "name": "t1_csvnuye", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3chntm/praw_messagemark_as_read_and_mark_as_unread_broken/"}, {"author": "GoldenSights", "body": "The `comment_stream` is going to loop forever unless you break / return, so just make sure you don't have any code outside and below the for-loop. The `time.sleep(2)` is also unnecessary because that function probably isn't going to be ending naturally. You shouldn't be breaking any API rules with this (the stream manages itself), though it's possible that it will be considered spam assuming it's some kind of grammar or spelling bot. On another note, at some point you'll probably want to make the cache something other than a list, or having a way of cleaning it out so it doesn't just grow forever. Probably not a big deal at this point, but I think it's worth mentioning.", "created_utc": 1436237965, "gilded": 0, "name": "t1_csuko70", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cdo5a/praw_am_i_breaking_any_api_rules_with_this/"}, {"author": "bboe", "body": "I don't recommend `limit=None` for comment stream as that's how many comments it should go back in history when first starting up, and when there happens to be more than 100 comments since the last check. I suggest either setting limit to either `100` or `200`.", "created_utc": 1436245103, "gilded": 0, "name": "t1_csuo6gn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3cdo5a/praw_am_i_breaking_any_api_rules_with_this/"}, {"author": "gschizas", "body": "I have tested this and it works, but I may have some logic errors in my thinking of how OAuth2 works. I appreciate any criticism or any other comment on how to improve this (preferably on the side of making it smaller).", "created_utc": 1436017879, "gilded": 0, "name": "t1_csrxqdr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "avinassh", "body": "Existing solutions, have a look at them: - /u/SmBe19's [praw-OAuth2Util](https://github.com/SmBe19/praw-OAuth2Util) and [my fork](https://github.com/avinassh/praw-OAuth2Util). The way configs are handled is bit different in my fork - /u/KissTheBlade_ 's [script](https://github.com/x89/Shreddit/blob/master/get_secret.py)", "created_utc": 1436019575, "gilded": 0, "name": "t1_csrycbw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "gschizas", "body": "First of all, thank you for the recommendations :) /u/KissTheBlade_'s script is using tornado as a webserver. I tried to make do with as less external dependencies (especially ones that won't work in Python 3 - although it seems tornado is now supporting it) as possible. praw-OAuth2Util is very close to what I've done, but it seems it's about twice the size. Of course my own script isn't very configurable, but I was aiming for simplicity rather than configurability. Furthermore, OAuth2Util needs a manual refresh, while my own script handles the refresh automatically.", "created_utc": 1436108008, "gilded": 0, "name": "t1_cssulgv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "SmBe19", "body": "How do you handle the expiration after one hour without calling a method from your script? Or are you supposed to create a new Reddit object for each operation?", "created_utc": 1436287664, "gilded": 0, "name": "t1_csv4pl1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "gschizas", "body": "The concept of a script is that it runs ~~once~~, and then it exits. It's not supposed to be constantly running, so this probably isn't a problem. You don't create a new `RedditAgent` object for each operation, you are supposed to run this once at the start of each script. Now, if you want to embed it in a long-running operation (for example, a web site), this module will not do, as it assumes there is no web server, and it starts a very simple one of its own. I have made another version that is supposed to be used from a web server, but it doesn't work properly (mostly because I think I've only properly understood how OAuth2 works with *this* module). I will probably be updating it after I can move the logic from a single object initiation (as it is now, and is very convenient for scripts) to a more detached process, where the object is initiated and the state is kept in the http session (session cookie) instead of an .ini file. A bad version (i.e. it can't really do refresh) of this exists on https://github.com/gschizas/reddit-mod-helper/ (this is a Flask and Bootstrap based web site). My problem at the moment is how to make `RedditAgent` trigger a redirect to reddit.com (for OAuth2 authentication), but without mingling the `RedditAgent` class with issuing the actual redirection (which is a job for the web server). EDIT: Perhaps a custom exception is the way to go, but I'd rather avoid exceptions in the default flow", "created_utc": 1436288287, "gilded": 0, "name": "t1_csv54xo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "SmBe19", "body": "OAuth2Util doesn't need a manual refresh if you use it like you described (that is a script that runs once and then terminates). The refresh is only needed for a constantly running script (i.e. a bot).", "created_utc": 1436289553, "gilded": 0, "name": "t1_csv611f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "gschizas", "body": "Hold on, I don't think we have our terminology right. What I mean by a \"constantly running script\" is something like a service (Windows) or a daemon (Unix), where there is one long-running process. You can have a script that runs and exits and put it on Task Scheduler or cron, to be run periodically and unattended. The usage is that you run the script manually the first time (to open the browser, accept the permissions etc.), and *then* you put it on the Task Scheduler/cron so that it can run automatically e.g. every 5 minutes. This will run and exit in a very small period of time (so there is no need to do a new object per operation, or at least I don't think so), but it **will** have to do a refresh, preferably without waiting for the user to enter their credentials or press \"Accept\" on the browser. And I don't see any handling for *that* scenario in praw-OAuth2Util. You do it manually, by calling the `refresh` method of `OAuth2Util` class. I have the whole `refresh` logic inside the reddit_agent function call (I've since converted this to an object, that's why I mentioned the `RedditAgent` class).", "created_utc": 1436290927, "gilded": 0, "name": "t1_csv70bn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "SmBe19", "body": "I see. I run most bots in an endless loop with a few `sleep(x)` in between. That's what I was referring to as \"constantly running script\". OAuth2Util calls refresh when you call `o = OAuth2Util(r)`, so the script runs only once and will start again from the start, no `refresh()` call is necessary.", "created_utc": 1436292427, "gilded": 0, "name": "t1_csv8232", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": null, "body": "> /u/KissTheBlade_'s script is using tornado as a webserver. I tried to make do with as less external dependencies (especially ones that won't work in Python 3 Works in Python 3! I don't use Python 2 on my own machine unless I'm making sure it's backwards compatible. I was going to make it use asyncio (the import is still dangling there actually) but it was easier to just use tornado.", "created_utc": 1436108918, "gilded": 0, "name": "t1_cssuynp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "avinassh", "body": "I agree about dependencies, however Tornado is very small and I don't think it really matters. Without Tornado or Flask, the code will be bit messy as you have to handle requests and status codes etc. I am aware of issues with Praw OAuth2Util thats why I stopped my fork and started working on something better: https://github.com/avinassh/prawoauth2 it's not complete yet, but it works. IMO, it is simple. And is also configurable.", "created_utc": 1436108560, "gilded": 0, "name": "t1_cssutlw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c3phr/praw_the_simplest_bot_i_could_make_i_hope_someone/"}, {"author": "GoldenSights", "body": "There may be other issues, but the first thing I'm noticing is that you do `comment.body.lower()`, but you do not lower the words_to_match phrase, so they will never match up.", "created_utc": 1435982734, "gilded": 0, "name": "t1_csrob95", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c2jeu/simple_noob_question_bot_not_finding_comments/"}, {"author": "swim1929", "body": "It worked, thanks!", "created_utc": 1435983031, "gilded": 0, "name": "t1_csrofg2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c2jeu/simple_noob_question_bot_not_finding_comments/"}, {"author": "SandyRegolith", "body": "I don't know python/praw but you're lowercasing the comment body while searching for `'I could care less'` which has an uppercase \"i\"...", "created_utc": 1435983191, "gilded": 0, "name": "t1_csrohq7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c2jeu/simple_noob_question_bot_not_finding_comments/"}, {"author": "SleepyHarry", "body": "Not having a lowercase 'I' was your issue, but I have a different suggestion. Instead of `cache` being a `list` (`[] `), I suggest making it a `set`. Membership tests (`x in collection`) are O(1) rather than O(n). This simply means it's faster to use a `set` as the cache grows. If you do this, don't forget to change `cache.append` to `cache.add`.", "created_utc": 1435992190, "gilded": 0, "name": "t1_csrrikc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c2jeu/simple_noob_question_bot_not_finding_comments/"}, {"author": "green_flash", "body": "Maybe I'm missing something but why not just define words_to_match as a string? words_to_match = 'i could care less' isMatch = words_to_match in comment.body.lower()", "created_utc": 1436001251, "gilded": 0, "name": "t1_csrto4q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c2jeu/simple_noob_question_bot_not_finding_comments/"}, {"author": "SleepyHarry", "body": "What if you want to also reply when someone says `'You could care less'`? Although this just goes to show how important using correctly descriptive names is. Wouldn't it be clearer if instead of `words_to_match` it was `phrases_to_match`?", "created_utc": 1436007032, "gilded": 0, "name": "t1_csruyae", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3c2jeu/simple_noob_question_bot_not_finding_comments/"}, {"author": "nemec", "body": "Some features graduate from Gold-only to regular features. [This must be one of them.](https://www.reddit.com/r/changelog/comments/19f94p/reddit_change_gold_beta_allow_users_to_optionally/)", "created_utc": 1435842659, "gilded": 0, "name": "t1_cspt345", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3buzu2/praw_send_replies_a_gold_only_feature/"}, {"author": "armandg", "body": "Huh. Hopefully someone sees this and fixes the issue. Will also submit an issue on github. Edit: I am an idiot. I was not using a correct Boolean statement for triggering the function. By \"None\" I was telling PRAW that there were no trigger to send, so use default. Changing from \"None\" to \"False\" worked like a charm.", "created_utc": 1435847346, "gilded": 0, "name": "t1_cspvg1x", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3buzu2/praw_send_replies_a_gold_only_feature/"}, {"author": "Herschel_Frisch", "body": "This post has been archived. If you would like to view this post please request it from user /u/Herschel_Frisch. The reference ID of the post Comment: csoemmm.", "created_utc": 1435728310, "gilded": 0, "name": "t1_csoemmm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bky0m/search_for_links_in_comments/"}, {"author": "thomasbomb45", "body": "Yes, that looks like what I'll end up having to do. (Your first suggestion, not the second)", "created_utc": 1435739933, "gilded": 0, "name": "t1_csoi888", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bky0m/search_for_links_in_comments/"}, {"author": "joe-murray", "body": "Wow, I had no idea password-based authentication was going to be deprecated...that sucks. I don't really understand Oauth, but if it can't be scripted then my bots are fucked.", "created_utc": 1435600513, "gilded": 0, "name": "t1_csmlqun", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "wanderingbilby", "body": "The level of difficulty depends on what your bots are doing. If they're only communicating with reddit via their own reddit user account, it's a somewhat annoying process you need to do once. If your bot is acting on behalf of other reddit users you'll need a couple of pages and a bit of code to make that work, but it's pretty easy and maintenance-free once it's running. See my top-level comment for more information.", "created_utc": 1435615551, "gilded": 0, "name": "t1_csmvfl1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": null, "body": "https://github.com/x89/Shreddit/blob/master/get_secret.py If you like you can just steal that. I threw it together to use myself. It's actually quite easy using PRAW. You fill in some details from the Reddit API in your praw.ini, open the given URL and voila!", "created_utc": 1435621149, "gilded": 0, "name": "t1_csmymz7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "avinassh", "body": "So, access token is needed only once? So, if I understand correctly: 1. get the access token (call it as refresh token) 2. make requests 3. get refresh tokens every 60 mins?", "created_utc": 1435935457, "gilded": 0, "name": "t1_csr1baq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": null, "body": "The one PRAW gets for you is permanent so you don't have to re-auth every 60 minutes. That part is just added to confuse! You only have to visit the Reddit API one time.", "created_utc": 1435940811, "gilded": 0, "name": "t1_csr40es", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "avinassh", "body": "but [docs](https://praw.readthedocs.org/en/v3.0.0/pages/oauth.html#step-6-refreshing-the-access-token) say otherwise: > An access token lasts for 60 minutes. To get access after that period, we\u2019ll need to refresh the access token.", "created_utc": 1435941244, "gilded": 0, "name": "t1_csr48n3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "YaManicKill", "body": "They mean that the refresh token is permanent. You get the access token and refresh token back from reddit. The refresh token will never expire, but can **only** be used for getting a new access token. The access token can be used for 60 minutes. After that, you can use the refresh token to get a new access token. This new access token will also be active for only 60 minutes, and after that you need to get a new one (using the refresh token) etc...", "created_utc": 1436266566, "gilded": 0, "name": "t1_csuucpn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "avinassh", "body": "thanks for the info!", "created_utc": 1436268554, "gilded": 0, "name": "t1_csuuvri", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "EliteMasterEric", "body": "Oh okay. So you NEED to access the page through a browser to authorize it on your account, but you only need to do that once. That makes more sense. I'll definitely try your script though! Thanks for the help!", "created_utc": 1435625678, "gilded": 0, "name": "t1_csn16hw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "avinassh", "body": "If anyone come here searching for how, I wrote a tutorial on HTTP to OAuth migration: https://www.reddit.com/r/redditdev/comments/3lotxj/tutorial_how_to_migrate_your_existing_bots_from/", "created_utc": 1442815451, "gilded": 0, "name": "t1_cv8rvj1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "wanderingbilby", "body": "OAuth is a little funky to wrap your head around, but once you get it things will be more smooth. [This tutorial](http://tsenior.com/2014-01-23-authenticating-with-reddit-oauth/) is pretty good and covers the concepts that PRAW is talking you through. Your initial Authorization string is a static URL, so if you don't have hosting anywhere you can link directly to it. What DOES need hosting is the callback URL which must accept and process a GET call. Once the user is redirected, you read the params from the GET call and use them to make a POST call back to reddit to get the token and appproved scopes. If your bot needs permanent (>1 hour) access to authentication, it will need to process that second part each time it needs to access data on the user's behalf to get a new token. You'll need a way to store and update both the current bearer token and the refresh token. Does your bot need users to authenticate to it, or do you just need to authenticate the bot to its reddit account? If you just need to authenticate the bot, you can throw a temporary page up somewhere, but if you need users to authenticate you'll need a page hosted to handle it.", "created_utc": 1435615392, "gilded": 0, "name": "t1_csmvc79", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "avinassh", "body": "So, access token is needed only once? So, if I understand correctly: 1. get the access token (call it as refresh token) 2. make requests 3. get refresh tokens every 60 mins?", "created_utc": 1435935578, "gilded": 0, "name": "t1_csr1dea", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "wanderingbilby", "body": "exactly. if you request and are granted permanent access, you don't need to re-authenticate through the interface unless the user account revokes that access. The access token is like a wristband at the fair. You have to beg your mom for one. Once she says yes, you can use your wristband for as many rides as you want- but for only one hour. The refresh token is a coupon your mom buys that can be exchanged for a new wristband and a new coupon when the current wristband expires. ... aaaaand now I want to go ride fair rides.", "created_utc": 1435941854, "gilded": 0, "name": "t1_csr4k0s", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "YaManicKill", "body": "> The refresh token is a coupon your mom buys that can be exchanged for a new wristband and a new coupon when the current wristband expires. I just wanted to pick up on 1 issue with this analogy. The refresh token is permanent. So, it would actually be: \"The refresh token is a coupon your mom buys that can be *infinitely exchanged for a new writsband** when the current wristband expires.\" Your refresh token doesn't ever need to be changed, it can stay the same forever.", "created_utc": 1436266757, "gilded": 0, "name": "t1_csuuefg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "wanderingbilby", "body": "Mea culpa, you are correct. It's been a while since I have had to implement this, and on skimming the oAuth documentation it looked like you got a new refresh token each time you requested a new bearer token. On further reading it looks like the JSON just contains the same refresh token, possibly as a parity / anti-attack measure.", "created_utc": 1436269546, "gilded": 0, "name": "t1_csuv625", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "YaManicKill", "body": "It's cool, your analogy was pretty good, I just can't help but be pedantic.", "created_utc": 1436269645, "gilded": 0, "name": "t1_csuv73j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "wanderingbilby", "body": "If I'd read the authentication procedure correctly I would have written the analogy correctly (or maybe not, haha)", "created_utc": 1436270007, "gilded": 0, "name": "t1_csuvb30", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "avinassh", "body": "thanks for the great analogy!", "created_utc": 1435942269, "gilded": 0, "name": "t1_csr4ry2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bit3y/prawoauth_how_do_i_make_an_automated_bot/"}, {"author": "joe-murray", "body": "There's a good post here: https://www.reddit.com/r/redditdev/comments/2ujhkr/important_api_licensing_terms_clarified/coch3em If you're very inexperienced it might be hard to follow, but it's not too bad. The PRAW docs are kind of unbearable though, so don't feel bad for being confused by that one.", "created_utc": 1435603132, "gilded": 0, "name": "t1_csmngb4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "bboe", "body": "> The PRAW docs are kind of unbearable though, so don't feel bad for being confused by that one. We'd love to see any improvements you have to offer.", "created_utc": 1435638734, "gilded": 0, "name": "t1_csn80oi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "Liudvikam", "body": "Well, in the case of the OAuth guide I mentioned, I can think of three: * In step 3, the \"This takes 3 parameters. `state`, which is a unique key that represent this client\" bit makes it a bit unclear as to what the `state` parameter should be set to. Someone *might* think that you need to retreive that unique string from somewhere, rather than just generating a random one or thinking up of one. * The \"Example webserver\" bit can be a bit unclear as to why it might be needed for for people that don't know anything about OAuth. * In step 3, it should be made more clear as to how to set several OAuth scopes. I think that the guide should be clear even to beginners, because OAuth will soon be pretty much the only way to get authentication. And then there are a few grammatical mistakes which don't really make the guide any less clear, but could be fixed anyway: * In Step 3: \"This takes 3 parameters. `state`, which is a unique key that represent this client\" should be \"This takes 3 parameters. `state`, which is a unique key that represent**s** this client\" * In Step 6: \"Neither `scope` or `refresh_token` will have changed\" should be \"Neither `scope` **n**or `refresh_token` will have changed\"", "created_utc": 1435655010, "gilded": 0, "name": "t1_csnd5a0", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "avinassh", "body": "can I use `user_agent` string as `state`?", "created_utc": 1436091403, "gilded": 0, "name": "t1_cssq1ap", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "Liudvikam", "body": "I don't see why not, though I might be wrong. I just use a random string.", "created_utc": 1436091472, "gilded": 0, "name": "t1_cssq1rm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "avinassh", "body": "great, thanks", "created_utc": 1436109387, "gilded": 0, "name": "t1_cssv5oz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "bboe", "body": "Do you have any interest in opening a pull request for the changes? PRAW now more than ever relies on the community to help make the improvements necessary to access new and updates features, and to make improvements that will benefit newcomers. Here's a direct edit link to the page in question :) https://github.com/praw-dev/praw/edit/master/docs/pages/oauth.rst", "created_utc": 1435677109, "gilded": 0, "name": "t1_csnl8h5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "Liudvikam", "body": "Sure, here it is: https://github.com/praw-dev/praw/pull/437", "created_utc": 1435679900, "gilded": 0, "name": "t1_csnn0nc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "bboe", "body": "Fantastic. Thanks.", "created_utc": 1435714698, "gilded": 0, "name": "t1_cso805b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "Liudvikam", "body": "Thanks a bunch! That link led me to eventually understand how to make it work.", "created_utc": 1435675918, "gilded": 0, "name": "t1_csnki29", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bipzt/help_with_oauth/"}, {"author": "GoldenSights", "body": "I believe this feature is currently not implemented in praw. In the meantime, I may have a workaround for you. `multireddit.subreddits` should give you a list containing all the subreddits in the multi. Now, this feature appears to be slightly broken as well, but I'll try to fix that pretty soon here. It's supposed to return a bunch of Subreddit objects, but instead I'm getting a dict with {\"name\": \"redditdev\"}. So what you can do is: subs = multireddit.subreddits subs = [x['name'] for x in subs] subs = '+'.join(subs) r.search(query, subreddit=subs) Give that a try, and I'll see if I can help to fix whatever is currently broken.", "created_utc": 1435550063, "gilded": 0, "name": "t1_csm2ca7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "TerminalCase", "body": "I've just had the chance to try this. Unfortunately, the search API returns a 503 error if it is passed a query that involves more than four subreddits. Looks like the only way to search large multis is to break the search into groups of four subs and merge the results afterwards.", "created_utc": 1435795257, "gilded": 0, "name": "t1_cspb74u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "GoldenSights", "body": "I think you just got unlucky. I just did a search using this method on a multireddit of 93 subs, so it should certainly work. 503 is just the unavailable code.", "created_utc": 1435795752, "gilded": 0, "name": "t1_cspbgvr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "TerminalCase", "body": "What query are you sending? For the past few hours I've got nothing but 503s for this: query = \"timestamp:0..1435829421\" url = \"NamibiaPics+SouthKoreaPics+NorthKoreaPics+NationalPhotoSubs+PortugalPics\" r.search(query, subreddit=url, sort='new', limit=100, syntax='cloudsearch', period = 'year') If I cut the subreddit list to four, the query usually succeeds. If I ask for any five subreddits, it always fails.", "created_utc": 1435800880, "gilded": 0, "name": "t1_cspe8i7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "GoldenSights", "body": "Well, in that case, it's no longer really a PRAW question, I can't even get this search to run in the browser: https://www.reddit.com/r/NamibiaPics+SouthKoreaPics+NorthKoreaPics+NationalPhotoSubs+PortugalPics/search?q=timestamp:0..1435829421&syntax=cloudsearch&restrict_sr=on&sort=new&period=year Using proper multireddits doesn't seem to solve the problem either: https://www.reddit.com/u/goldensights/m/test/search?q=timestamp:0..1435829421&syntax=cloudsearch&restrict_sr=on&sort=new&period=year I guess it's just too much for the server to do at once, you might have to cut this up into a couple simpler requests. To answer your question, the query I used was something simple like \"test\", I thought you meant it wasn't working at all.", "created_utc": 1435801377, "gilded": 0, "name": "t1_cspei2k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "th0rnz", "body": "You can use get_multireddit('user','multireddit') So for your example would be get_multireddit('I_AM_STILL_A_IDIOT','nationalphotosubs')", "created_utc": 1435550105, "gilded": 0, "name": "t1_csm2cx3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bgtlp/how_can_i_search_a_multi_with_praw/"}, {"author": "GoldenSights", "body": "Sorry, submissions don't store a history of past flairs. You would have to keep track of them in real time.", "created_utc": 1435521072, "gilded": 0, "name": "t1_cslnxc0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bf2qz/getting_history_of_link_flair/"}, {"author": "dado3212", "body": "Damn. Well, thank you!", "created_utc": 1435522390, "gilded": 0, "name": "t1_cslolbg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bf2qz/getting_history_of_link_flair/"}, {"author": "gavin19", "body": "If you mean the text in a self/text post then submission.selftext", "created_utc": 1435507773, "gilded": 0, "name": "t1_cslhf52", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3betip/praw_python_reading_keywords_in_submission_name/"}, {"author": "_thelichking_", "body": "No, I meant in any image submission, like if I put 'cat' in the input, it should check for the word 'cat' in the title of the submission.", "created_utc": 1435508061, "gilded": 0, "name": "t1_cslhjxk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3betip/praw_python_reading_keywords_in_submission_name/"}, {"author": "gavin19", "body": "Then you want `submission.title`.", "created_utc": 1435508277, "gilded": 0, "name": "t1_cslhnll", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3betip/praw_python_reading_keywords_in_submission_name/"}, {"author": "_thelichking_", "body": "Thanks! I'll check it out :D! Is there any list of such objects?", "created_utc": 1435508451, "gilded": 0, "name": "t1_cslhqg8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3betip/praw_python_reading_keywords_in_submission_name/"}, {"author": "gavin19", "body": "You can use `dir` and pass in a submission, like dir(submission) and it'll spit out something like ['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_comment_sort', '_comments', '_comments_by_id', '_extract_more_comments', '_get_json_dict', '_info_url', '_insert_comment', '_orphaned', '_params', '_populate', '_replaced_more', '_underscore_names', '_update_comments', 'add_comment', 'approve','approved_by', 'archived', 'author', 'author_flair_css_class', 'author_flair_text', 'banned_by', 'clear_vote', 'clicked', 'comments', 'created', 'created_utc', 'delete', 'distinguish', 'distinguished', 'domain', 'downs', 'downvote', 'edit', 'edited', 'from', 'from_api_response', 'from_id', 'from_kind', 'from_url', 'fullname', 'get_duplicates', 'get_flair_choices', 'gild', 'gilded', 'has_fetched', 'hidden', 'hide', 'id', 'ignore_reports', 'is_self', 'json_dict', 'likes', 'link_flair_css_class', 'link_flair_text', 'mark_as_nsfw', 'media', 'media_embed', 'mod_reports', 'name', 'num_comments', 'num_reports', 'over_18', 'permalink', 'reddit_session', 'refresh', 'removal_reason', 'remove', 'replace_more_comments', 'report', 'report_reasons', 'save', 'saved', 'score', 'secure_media', 'secure_media_embed', 'selftext', 'selftext_html','set_contest_mode', 'set_flair', 'short_link', 'stickied', 'sticky', 'subreddit', 'subreddit_id', 'suggested_sort', 'thumbnail', 'title', 'undistinguish', 'unhide', 'unignore_reports', 'unmark_as_nsfw', 'unsave', 'unset_contest_mode', 'unsticky', 'ups', 'upvote', 'url', 'user_reports', 'visited', 'vote'] so you can see all the attached properties/methods.", "created_utc": 1435508820, "gilded": 0, "name": "t1_cslhwgh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3betip/praw_python_reading_keywords_in_submission_name/"}, {"author": "_thelichking_", "body": "Thanks a lot!", "created_utc": 1435511395, "gilded": 0, "name": "t1_cslj49r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3betip/praw_python_reading_keywords_in_submission_name/"}, {"author": "xfile345", "body": "I'm not familiar with praw or that specific API wrapper, but instead of requesting a temporary access token, request a permanent one by changing the `duration=permanent` during your authorization request. You will not only receive an `access_token`, but a `refresh_token` that you can save and use to obtain a new `access_token` each time without the need to re-authorize the app. When you have a `refresh_token`, provide it using the `grant_type=refresh_token` to `https://www.reddit.com/api/v1/access_token` and you will receive your `access_token` with that single request ([instructions here](https://github.com/reddit/reddit/wiki/OAuth2#refreshing-the-token)).", "created_utc": 1435423046, "gilded": 0, "name": "t1_cskld95", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "great, thanks!!", "created_utc": 1435423805, "gilded": 0, "name": "t1_csklr5i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "irrational_function", "body": "You report to the author that their wrapper is defective. The problem is that it supports refresh_tokens, but not correctly on startup. It saves your access_token and your refresh_token, but if your access_token is expired when you next create a wrapper object, it obtains completely new tokens instead of using the refresh_token. If you have unexpired tokens on startup or you obtain new tokens on startup, *then* it lets you refresh the tokens using the wrapper's `refresh` method. Here's the [documentation](https://praw.readthedocs.org/en/v3.0.0/pages/oauth.html) for using OAuth in PRAW. You can follow that directly, or you can use a wrapper without this problem. In particular, when using PRAW, you almost never ought to be bypassing it to access the reddit API directly. You use a refresh token in PRAW via the `refresh_access_information` method.", "created_utc": 1435458657, "gilded": 0, "name": "t1_csl2cls", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "I will do it or I will work on it and send a PR. Thanks for the guidance!", "created_utc": 1435462345, "gilded": 0, "name": "t1_csl3wck", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "irrational_function", "body": "You're welcome! I can be a bit more explicit, actually. In the `_read_access_credentials` method, if you get an `OAuthInvalidToken` exception and have a refreshable token, you need to call `refresh_access_information`. PRAW doesn't use the refresh token automatically. Even passing it to `set_access_credentials`, it just remembers the refresh token so you can call `refresh_access_information` with no argument later.", "created_utc": 1435504117, "gilded": 0, "name": "t1_cslfu88", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "great :D", "created_utc": 1435508545, "gilded": 0, "name": "t1_cslhrzb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "SmBe19", "body": "Thank you for pointing this out, should be fixed now.", "created_utc": 1435523474, "gilded": 0, "name": "t1_cslp5vw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": null, "body": "[deleted]", "created_utc": 1435507459, "gilded": 0, "name": "t1_cslha3s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "yes, it contains the tokens. I will check again.", "created_utc": 1435507904, "gilded": 0, "name": "t1_cslhhap", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": null, "body": "[deleted]", "created_utc": 1435507953, "gilded": 0, "name": "t1_cslhi3h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "no, not terminal based browser. the issue happens only at startup. check [this](https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/csl2cls)", "created_utc": 1435508594, "gilded": 0, "name": "t1_cslhsrk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": null, "body": "[deleted]", "created_utc": 1435508695, "gilded": 0, "name": "t1_cslhudc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "at startup?", "created_utc": 1435508777, "gilded": 0, "name": "t1_cslhvr0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": null, "body": "[deleted]", "created_utc": 1435509067, "gilded": 0, "name": "t1_csli0j1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "this is my script: http://dpaste.com/053276V", "created_utc": 1435509344, "gilded": 0, "name": "t1_csli514", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "SmBe19", "body": "I can't see anything wrong with your script. The fact that you are asked for access every time indicates that the token retrieval doesn't work correctly. Make sure you use the correct app key and app secret. If they are correct, try to delete `oauthtoken.txt` and run the script again. Check if the tokens were written. If this all didn't help tell me so I can take a closer look.", "created_utc": 1435510575, "gilded": 0, "name": "t1_csliq4u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "sent PR, it should work now :)", "created_utc": 1435511973, "gilded": 0, "name": "t1_csljedf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "shit, I introduced this bug :/ anyways, sending a PR to fix this.", "created_utc": 1435511301, "gilded": 0, "name": "t1_cslj2n5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": null, "body": "[deleted]", "created_utc": 1435509439, "gilded": 0, "name": "t1_csli6ng", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "I don't see how you can call `o.refresh()` before creating `o`! also, the authentication happens at the creation of `o`. anyways, can you share your script? I will try.. may be I did something wrong? and also, scope I have set is `Identity` and refreshable is set to true.", "created_utc": 1435510113, "gilded": 0, "name": "t1_cslii5h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": null, "body": "[deleted]", "created_utc": 1435510424, "gilded": 0, "name": "t1_csling9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "thats not a complete example. can you give me complete example? sorry, if I am asking too much", "created_utc": 1435510521, "gilded": 0, "name": "t1_cslip5n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "SmBe19", "body": "import praw import OAuth2Util r = praw.Reddit(\"OAuth2Util Demo by /u/SmBe19\") o = OAuth2Util.OAuth2Util(r, print_log=True) o.refresh() print(\"Hi, {0}, you have {1} comment karma!\".format( r.get_me().name, r.get_me().comment_karma))", "created_utc": 1435511080, "gilded": 0, "name": "t1_csliyy1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": null, "body": "[deleted]", "created_utc": 1435510636, "gilded": 0, "name": "t1_cslir76", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "avinassh", "body": "no sir, it isn't.", "created_utc": 1435511338, "gilded": 0, "name": "t1_cslj3a6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3bb8tw/how_do_i_give_access_of_my_bot_account_to_my/"}, {"author": "SandyRegolith", "body": "[Here is an exhaustively commented JSFiddle showing how to do it via javascript](http://jsfiddle.net/twentythreenineteen/0Lpb2bbo/1/) and display the results in a nice table. So you could probably just cut out 99% of that and just go with fetching `\"http://www.reddit.com/api/info.json?url=\" + theURL` then check the json's `[data][children].length()` in the language of your choice.", "created_utc": 1435213194, "gilded": 0, "name": "t1_cshzlof", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3b1jry/check_if_link_already_submitted_to_a_subreddit/"}, {"author": "NamesNotCrindy", "body": "Could you use get_submission(url=...) and if it exists, check if it's in a particular subreddit?", "created_utc": 1435221622, "gilded": 0, "name": "t1_csi1wgb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3b1jry/check_if_link_already_submitted_to_a_subreddit/"}, {"author": "bboe", "body": "Unfortunately, there are so many things that could be going wrong here. (1) Can you provide the simplest bit of python code that reproduces the issue (e.g, the whole file with only necessary lines). (2) Can provide the version of PRAW and requests you have installed? import praw, requests print(praw.__version__) print(requests.__version__) (3) What is the output of the following? import requests requests.get('https://reddit.com/.json?limit=1').json()", "created_utc": 1435205564, "gilded": 0, "name": "t1_cshwkwr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "nandhp", "body": "I am having this problem, too. So I went looking. First, my versions: Python 2.7.9 (default, Mar 1 2015, 12:57:24) [GCC 4.9.2] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import praw, requests >>> print(praw.__version__) 3.0.0 >>> print(requests.__version__) 2.4.3 And a minimal test case: import praw r = praw.Reddit(user_agent=\"Command-line test\") l = r.get_top(limit=5) print [str(x) for x in l] The exception occurs because `requests` (and/or the underlying `httplib`) is trying to write the HTTP request as a Unicode string. (Although it only complains when using OpenSSL, it's obviously a bug somewhere in there.) The request is in Unicode because of Unicode-ness propagated by `\"\\r\\n\".join(self._buffer)` -- well, it comes from the `method` provided by PRAW: # praw/internal.py from __future__ import print_function, unicode_literals def _prepare_request(..., method=None): # ... if method: pass elif data or files: method = 'POST' else: method = 'GET' # ... request = Request(method=method, ...) The method is Unicode because of the use of `unicode_literals`, and for some reason it's not being converted to a byte string by `requests`. The problem goes away if I use `method = b'GET'` or `Request(method=str(method), ...)`, though I've only tried it on Python 2.", "created_utc": 1435373383, "gilded": 0, "name": "t1_csk5ct0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "bboe", "body": "Can you try upgrading your requests version? I'm running 2.7.0.", "created_utc": 1435379682, "gilded": 0, "name": "t1_csk84gu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "nandhp", "body": "Updating requests to 2.7.0 does not help. **Edit:** Upgrading pyOpenSSL from 0.14 to 0.15.1 helps, though I do get a warning: /usr/lib/python2.7/dist-packages/urllib3/contrib/pyopenssl.py:208: DeprecationWarning: unicode for buf is no longer accepted, use bytes return self.connection.sendall(data) I traced this to [pyOpenSSL bug #15](https://github.com/pyca/pyopenssl/issues/15): some APIs fail to accept Unicode strings in 0.14. Apparently the resolution is that some APIs accept byte strings and some accept Unicode strings and the application developer should be paying attention (see also some of the application bugs that reference this one).", "created_utc": 1435407353, "gilded": 0, "name": "t1_cskfbx2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "QAFY", "body": "Same problem. Did you ever find a solution? My log files are filling up with this warning :(", "created_utc": 1442177343, "gilded": 0, "name": "t1_cv0eqje", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "nandhp", "body": "No, but it only shows up once when I start my bot, so it doesn't bother me. Maybe /u/bboe has looked into it some more, though?", "created_utc": 1442196333, "gilded": 0, "name": "t1_cv0ppf0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "QAFY", "body": "I fixed it by updating to python 2.7.10. I was using 2.7.3 for some reason.", "created_utc": 1442210464, "gilded": 0, "name": "t1_cv0w9c5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "bboe", "body": "Looks like it may be an issue with pyopenssl + requests. If so, I think the recommended course of action is to use a virtualenv that does not have pyopenssl installed or updated to a version where it may not be a problem.", "created_utc": 1435205736, "gilded": 0, "name": "t1_cshwnv6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "DailMail_Bot", "body": "check your pyopenssl version: pip show pyopenssl, it needs to be > 0.14 for this to go away", "created_utc": 1435577525, "gilded": 0, "name": "t1_csma6ke", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3aybk8/login_error/"}, {"author": "GoldenSights", "body": "Check out [OAuth2Util, by /u/smbe19](http://www.reddit.com/r/botwatch/comments/38k30h/oauth2util_a_wrapper_around_praws_oauth2/). All it takes is a couple of text files and you're on your way", "created_utc": 1435151006, "gilded": 0, "name": "t1_csgzttx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ay1c9/praw_oauth_login/"}, {"author": "xoru", "body": "I made a small one-time script for easily setting up OAuth with praw. You should be able to easily modify the example at the bottom of the usage instructions to fit your code. Check it out [here](https://github.com/xoru/easy-oauth) and let me know if you need any more help.", "created_utc": 1436268722, "gilded": 0, "name": "t1_csuuxhz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ay1c9/praw_oauth_login/"}, {"author": "ipitythefoobar", "body": "I was not able to get the code from that page to work. I had to change the code below... access_information = { 'access_token': 'YourUpdatingAccessToken', 'refresh_token': 'YourPermanentRefreshToken', 'scope': {'identity', 'submit', 'read'} } to this instead... access_information = { 'access_token': 'YourUpdatingAccessToken', 'refresh_token': 'YourPermanentRefreshToken', 'scope': {'identity, submit, read'} } The scope needed to have all items comma delimited within one quote instead of each item independently quoted.", "created_utc": 1448981789, "gilded": 0, "name": "t1_cxj4xzm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ay1c9/praw_oauth_login/"}, {"author": null, "body": "You get the oauth refresh token after you allow access through the reddit apps page. You're returned to (in your case) localhost:65010/authorize_callback with a GET param containing ?code=foo, you enter foo back into your script and that'll give you the access_information you need to sec_access_credentials. The refresh_token will be in your access_information dict. Once you have the refresh token you'll not have to go through authorizing the script via webpages again (assuming you got a permanent token).", "created_utc": 1435153001, "gilded": 0, "name": "t1_csh0qnd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ay1c9/praw_oauth_login/"}, {"author": "BnMcGi", "body": "I tried my code again in IDLE in interactive shell mode and it worked fine, so I copied and pasted from there and it seems to be working now... How strange!", "created_utc": 1434918706, "gilded": 0, "name": "t1_cse3g7s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3amxbp/praw_and_refresh_tokens/"}, {"author": null, "body": "It's `oauth_refresh_token` you need in your script (I put it in praw.ini) as well as your client_id and secret, then you just call `r.refresh_access_information()` and you're logged in. I did this yesterday https://github.com/x89/Shreddit/blob/master/get_secret.py and it works fine.", "created_utc": 1435137000, "gilded": 0, "name": "t1_csgvwjy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3amxbp/praw_and_refresh_tokens/"}, {"author": "xfile345", "body": "What is the error that you get when it \"errors out\"? Are you reaching your ratelimit for API requests, or is there some other error? Is your server timing out because it's taking so long? A CSV file can contain no more than 100 entries, so you're looking at 800+ requests. If you aren't using OAuth, you can only make 30 requests in 60 seconds, 60 requests with OAuth. So that's a bare minimum of just over 13 minutes for your code to be sending that many requests following proper API rules (nearly a half hour without OAuth). That's the most I can guess without knowing what error messages you are getting.", "created_utc": 1434754394, "gilded": 0, "name": "t1_cscdkd0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3afuui/limit_on_rset_flair_csv/"}, {"author": "TeroTheTerror", "body": "That's 80,000 entries not 800, and I'm not directly in front of a comeputer at the moment so I can't tell you what the error said", "created_utc": 1434768441, "gilded": 0, "name": "t1_csck72v", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3afuui/limit_on_rset_flair_csv/"}, {"author": "xfile345", "body": "If you want to update 80,000 flairs, that's 800 CSV requests (100 entries each).", "created_utc": 1434768512, "gilded": 0, "name": "t1_csck85l", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3afuui/limit_on_rset_flair_csv/"}, {"author": "TeroTheTerror", "body": "Ah I see what you were saying", "created_utc": 1434768950, "gilded": 0, "name": "t1_csckf36", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3afuui/limit_on_rset_flair_csv/"}, {"author": "largenocream", "body": ">This tells me that the limit is based on the total size of the HTTP request It's not, it's just that [the content limit is based on the size of the _rendered markdown_](https://github.com/reddit/reddit/blob/c6f959504466333c0d7d51c131240473aaf78b04/r2/r2/controllers/wiki.py#L372-L402), ostensibly to prevent people posting markdown bombs that are several orders of magnitudes large in size when rendered to HTML. _Everything_ in the HTML generated from the markdown counts against that limit, and every wiki page automatically includes a header with the page name, explaining the limit difference with longer page names. >The special_length_restriction_bytes dict doesn't have a number for this, so it falls back on pylons.g.wiki_max_page_length_bytes but I don't know what pylons is or where it comes from. `pylons.g` contains a bunch of globals and settings loaded from the INI, `wiki_max_page_length_bytes` is `524288` in production. > And perhaps an error message when editing over the limit in the browser (Currently the saving wheel just spins around, then stops)? I'll defer to someone else here, it looks like [there's supposed to be](https://github.com/reddit/reddit/blob/c6f959504466333c0d7d51c131240473aaf78b04/r2/r2/controllers/wiki.py#L372-L402), but overly-large requests are also handled in a few places above this check, and they might not be returning the error in the format that the JS expects.", "created_utc": 1434711821, "gilded": 0, "name": "t1_csbqh5e", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "GoldenSights", "body": "Thanks a lot for the reply. The only part I'm not clear about yet is why Praw allowed me to submit more characters than chrome did, using nothing but the letter \"o\" five hundred thousand times. In fact, at one point I submitted the page via praw, using 511,891 characters, then I opened the page in chrome and clicked Save without making any changes at all, and it failed. What would explain this?", "created_utc": 1434733781, "gilded": 0, "name": "t1_csc19fa", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "xfile345", "body": "> The only part I'm not clear about yet is why Praw allowed me to submit more characters than chrome did I have no idea what the answer is to this, but I know I've had a similar situation where I was editing my subreddit's CSS and it wouldn't update from the wiki editor because of file size, but if I copy/pasted the entire thing into the stylesheet editor (the config page which includes the image upload area), it would accept it just fine. There was roughly 1 KB of leeway between the two pages if I recall correctly, so for some reason, it *does* matter which method you're using to submit something.", "created_utc": 1434735551, "gilded": 0, "name": "t1_csc2eva", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "largenocream", "body": "> The only part I'm not clear about yet is why Praw allowed me to submit more characters than chrome did, using nothing but the letter \"o\" five hundred thousand times. Ahhh yeah, [there's also a global limit on the size of POST bodies](https://github.com/reddit/reddit/blob/master/r2/r2/config/middleware.py#L323,) (which is lower than the wiki's rendered size limit, `512000`) so PRAW _could_ be sending a smaller body than the browser. If you're using OAuth with praw, you don't have to send the modhash. [www.reddit.com](https://www.reddit.com/)'s js always puts `&r=` on the end of the POST body, that can be avoided if you use URLs like `/r//api/` as PRAW might. It makes sense that you'd hit the POST data size limit instead of the wiki's size limit if you're sending something that doesn't expand in size much when rendered as markdown. If the HTTP response body for the \"too big\" error is HTML with a single script tag in it rather than JSON, you're hitting the global POST data limit rather than the wiki's page limit, and it looks like the wiki's JS doesn't know how to handle hitting that limit.", "created_utc": 1434735793, "gilded": 0, "name": "t1_csc2khe", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "GoldenSights", "body": "Hey, so I was right! At least, when it comes to a page full of the same letter. This also explains why PRAW gave me 500 errors instead of 403-Too-Big. Are comments and submissions also subject to this post-markdown-render size that you mentioned? I always thought they were capped at 10k and 40k input characters, but wiki pages seem to be different in that way. Anyways, thank you. I'll be able to make this work. Happy cake day!", "created_utc": 1434737825, "gilded": 0, "name": "t1_csc3vi7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "largenocream", "body": "You're getting 500 errors? That shouldn't be happening... Just wrote up a script that repros the 500 error, I'll try poking around. You should always get 413 if the request body is too large.", "created_utc": 1434742038, "gilded": 0, "name": "t1_csc6ini", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "GoldenSights", "body": "Only in PRAW: >>> s = r.get_subreddit('goldtesting') >>> s.edit_wiki_page('75000', 'o'*511891) {} >>> try: ... s.edit_wiki_page('75000', 'o'*511892) ... except Exception as e: ... a=e ... >>> a._raw.status_code 500 >>> a._raw.reason 'Internal Server Error' >>> Chrome does 413 as expected: http://i.imgur.com/1ZFhJyO.png", "created_utc": 1434750671, "gilded": 0, "name": "t1_cscbm6n", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "largenocream", "body": "This was a bug that only showed up in production for some reason. [Should be fixed now](https://github.com/reddit/reddit/commit/ec607e73754956de385e4f7d5c0367bd967a0299), thanks for the heads-up!", "created_utc": 1435181842, "gilded": 0, "name": "t1_cshjja0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "GoldenSights", "body": "It works, thanks!", "created_utc": 1435181976, "gilded": 0, "name": "t1_cshjmac", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "picflute", "body": "This is something that I brought up on in /r/Bugs since Wiki Pages aren't working correctly.", "created_utc": 1434897069, "gilded": 0, "name": "t1_csdt27z", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3ad6j0/why_do_wiki_pages_have_an_arbitrary_character/"}, {"author": "thorarakis", "body": "Getting the scope is part of requesting the oauth token, not a separate step afterwards. (fyi, you can request multiple scopes in the same request) When you get an oauth token back it will have the requested scopes associated with the token itself, so any request that you use with that token will be made with the originally requested scopes. So in regards to comments you'll be needing the 'submit' scope. As seen [here](https://www.reddit.com/dev/api/oauth#POST_api_comment)", "created_utc": 1434573920, "gilded": 0, "name": "t1_cs9yyea", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3a5v11/oauth2_authorization_for_python/"}, {"author": "ErrorX", "body": "These two lines seemed to be what I was missing: reddit.set_oauth_app_info(client_id=clientId, client_secret=clientSecret, redirect_uri=redirectUri) reddit.set_access_credentials({'identity','read', 'submit', 'privatemessages'},token,refresh_token=None,update_user=True)", "created_utc": 1434717123, "gilded": 0, "name": "t1_csbs24n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3a5v11/oauth2_authorization_for_python/"}, {"author": "chicken_bridges", "body": "If you are not breaking your schools T&C then you can download [miniconda](http://conda.pydata.org/miniconda). It is a self contained python distribution, meaning you can install whatever you want in your user directory. I think it includes 'pip' but if not then you can do 'conda install pip'. After that just use pip to install whatever you like.", "created_utc": 1434359042, "gilded": 0, "name": "t1_cs7151y", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39vzmt/how_to_install_praw_on_my_personal_webspace_on_my/"}, {"author": "gradiuscypher", "body": "You can use setup.py along with the --user argument to install to your home directory, rather than installing to the system. See this link for more info about that: http://stackoverflow.com/questions/14179941/how-to-install-python-packages-without-root-privileges", "created_utc": 1434379018, "gilded": 0, "name": "t1_cs77ehl", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39vzmt/how_to_install_praw_on_my_personal_webspace_on_my/"}, {"author": "bboe", "body": "http://praw.readthedocs.org/en/latest/pages/oauth.html", "created_utc": 1434235113, "gilded": 0, "name": "t1_cs5ncnf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "MrJohz", "body": "That doesn't work for script-type apps, which don't use the auth token flow. I'm specifically looking for a way to use the [script app type](https://github.com/reddit/reddit/wiki/oauth2-app-types#script).", "created_utc": 1434236269, "gilded": 0, "name": "t1_cs5nvva", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "bboe", "body": "That's not currently supported, but you can use the web-based flow without using the \"login api\".", "created_utc": 1434267254, "gilded": 0, "name": "t1_cs5zzrb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "MrJohz", "body": "How? By my reading, if I want access to a particular user's data, that user needs to manually click to give the script access. How can I run an automated bot of the user needs to click to allow it to run each time it runs?", "created_utc": 1434268747, "gilded": 0, "name": "t1_cs60cxq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "bboe", "body": "Regardless of how you choose to accomplish things, if you're trying to run the script on behalf of another user, you need them to grant you permission.", "created_utc": 1434268932, "gilded": 0, "name": "t1_cs60ehe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "MrJohz", "body": "Well that's the point, I am the user (or at least the account the script will use is owned by me), so there shouldn't be a need to ask for permission each time. Hence why I wanted to use script-based oauth - given that I have the login details and that the script's account is a registered developer account, that seems most appropriate to me. Tbh, I've been working against PRAW for a few things, it'll probably be easier at this point to put together my own module, but thanks nonetheless.", "created_utc": 1434294336, "gilded": 0, "name": "t1_cs671kd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "bboe", "body": "Obtain a permanent, refreshable token, and then you never need to ask for permission again so long as you save that token, and don't revoke access from your account.", "created_utc": 1434299471, "gilded": 0, "name": "t1_cs69g6b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "armandg", "body": "Help a noob out, how do I do this? Today I have several scripts running x number of times throughout the day, and only through crontab. No loops to keep the script going constantly. Only need the script to be able to authenticate once per run of script.", "created_utc": 1434572490, "gilded": 0, "name": "t1_cs9y0me", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "bboe", "body": "You might find some help in this thread where the documentation link is, and a little tool I wrote to help obtain refresh tokens: https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/", "created_utc": 1434599052, "gilded": 0, "name": "t1_csaci6a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "armandg", "body": "Thanks. I've read about this already, but got stomped because I don't understand yet how I can keep using the refresh-token permanently in my script. I'll keep reading up on the docs to learn more about the details on how OAuth works.", "created_utc": 1434617876, "gilded": 0, "name": "t1_csaiuez", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "bboe", "body": "Once you have a refresh token you can (re)authenticate via: self.r.refresh_access_information(REFRESH_TOKEN) Your PRAW instance will likely need to have the right OAuth parameters configured. You are probably already doing that, but for completeness: r.set_oauth_app_info(CLIENT_ID, CLIENT_SECRET, REDIRECT_URI)", "created_utc": 1434641479, "gilded": 0, "name": "t1_csarwll", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "armandg", "body": "Seems I was trying to make this harder than it actually was =) Got it working now, thanks! :D", "created_utc": 1434659300, "gilded": 0, "name": "t1_csb4596", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "ShivWeaselMD", "body": "https://www.reddit.com/r/redditdev/comments/36zbd0/can_we_get_nonexpiring_tokens_for_script_apps/crk2hq6", "created_utc": 1434318207, "gilded": 0, "name": "t1_cs6j031", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/39pmbl/praw_guide_to_scriptbase_oauth/"}, {"author": "xfile345", "body": "I'm not sure if there were any changes in the API, but now that AutoModerator is built into Reddit, [this can easily be done](https://www.reddit.com/r/AutoModerator/wiki/library#wiki_user_whitelist) without the need to code a bot.", "created_utc": 1433883490, "gilded": 0, "name": "t1_cs10s6c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/397dng/approving_submissions_of_shadowbanned_users/"}, {"author": "totallyknowyou", "body": "Is it possible? Yeah. You'd need PRAW and also an API to parse webpages (I use Requests to get the page and BeautifulSoup4 to parse them). However, having it so that it constantly posts a comment when a story is posted on your site could possibly be considered spam, but I think that would be more of a subreddit issue than anything else. I'd ask the community of that sub and private message the mods if it would be okay to do that. I'll help if you have any more questions. I'm currently working on my own bot(a bit complicated heh heh...) And already see a couple methods we could get yours rolling with.", "created_utc": 1433789155, "gilded": 0, "name": "t1_crzphnv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "wanderingbilby", "body": "Yes, you can use PRAW or any other tool to access the API. You need to do an OAUTH request with the correct scopes, then you can post as the user. TOS-wise, what's the spirit of the tool? If you're helping people submit to /r/writingprompts (formatting helper, something like that) you should be fine - you aren't causing any more or less traffic to the sub than there would be anyway. If your site is a platform itself and offers a \"cross-post to /r/writingprompts\" option, you might want to check with the mods there before putting it in place. Hypothetically it could cause unwanted traffic. An example of a similar application: I made a page for /r/randomactsofvroom that allowed users to fill out a series of questions and submit to the sub. This created a pre-formatted self post or image post, made by the user. An associated bot also used the data to set flair on the post and update an index in a stickied post. It also handled marking posts filled or removed as needed.", "created_utc": 1433798866, "gilded": 0, "name": "t1_crzvmeh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "raymestalez", "body": "Hi! Sorry for replying so late, just wanted to thank you, your post really helped!!", "created_utc": 1434087593, "gilded": 0, "name": "t1_cs3yjyk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "wanderingbilby", "body": "You're welcome :)", "created_utc": 1434112887, "gilded": 0, "name": "t1_cs45iae", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "ketralnis", "body": "> And is it okay Look at it this way: how would you like reddit if the front page were all 0-comment autoposted threads from people wanting to promote their websites?", "created_utc": 1433793947, "gilded": 0, "name": "t1_crzskj8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "zoetry", "body": "Technically, OP didn't say anything about self-promotion. There are tons of sites that offer a 'share to reddit' button.", "created_utc": 1433839068, "gilded": 0, "name": "t1_cs0ebls", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/391vlc/can_i_automatically_submit_comments_to_reddit/"}, {"author": "GoldenSights", "body": "Yep, it's one of the Subreddit attributes: s = r.get_subreddit('redditdev') print(s.description)", "created_utc": 1433801310, "gilded": 0, "name": "t1_crzx2hl", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/390uf4/praw_retrieve_sidebar_contents_as_nonmoderator/"}, {"author": "Xeran_", "body": "It worked, thanks!", "created_utc": 1433835654, "gilded": 0, "name": "t1_cs0dhul", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/390uf4/praw_retrieve_sidebar_contents_as_nonmoderator/"}, {"author": "xfile345", "body": "You said you are able to get an `access_token`. To get a `refresh_token`, you make the same request but set `duration=permanent` during your authorize request. This will include a `refresh_token` in the response. As for being able to authorize an account, I'm not sure that's even possible without a browser page since that's the entire purpose of OAuth--to make users only supply secured information on a Reddit.com page. The only way to get a `code` is from a response of a user clicking \"Allow\" from the reddit preferences page. But if you're doing this to get a `refresh_token`, it only has to be done once. A `refresh_token` can be stored so the user won't have to ever click \"Allow\" again unless they (or you) revoke access, deleting the `refresh_token`.", "created_utc": 1433467273, "gilded": 0, "name": "t1_crvz3by", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/"}, {"author": "haiguise1", "body": "Thanks, I guess I will have to authorize the script myself and then get the `code`.", "created_utc": 1433507815, "gilded": 0, "name": "t1_crwdikv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/"}, {"author": "________QUAY________", "body": "What I had to do for one of my apps was request an 'access_token' and 'refresh_token' once outputting the url to console. After I got that I put in the code reddit gave me and set it up to only use the refresh token without needing the site again.", "created_utc": 1433474310, "gilded": 0, "name": "t1_crw2v8c", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/"}, {"author": "bboe", "body": "I wrote a script to help me generate refresh tokens for various scopes. You can find that script here: https://gist.github.com/bboe/670d139037c3eec554eb The client id, secret, and url must match exactly those in your app. I use localhost as the redirect url and then just copy and paste the code from the address bar when the redirect fails. An example run would look like: reddit_oauth.py myapikey mysecret-key https://127.0.0.1:65010/authorize_callback identity,wikiread", "created_utc": 1433483623, "gilded": 0, "name": "t1_crw74ex", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/"}, {"author": "haiguise1", "body": "Thanks.", "created_utc": 1433507852, "gilded": 0, "name": "t1_crwdj3p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38lo61/praw_and_oauth/"}, {"author": "xfile345", "body": "I don't use PRAW, but the API will return `after` with any list of items. You'll have to make a second request (and 3rd, etc) sending the previous `after` parameter to get a new list starting where the first list left off. Once `after` contains `null`, you've exhausted everything modmail will show you. To my knowledge there is no limit on how many modmail pages you can view as opposed to normal listing pages which typically maxes out at 1,000.", "created_utc": 1433432145, "gilded": 0, "name": "t1_crvdym6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "picflute", "body": "By any chance could you assist me in the code portion of this? I'm very very new to using the API and don't know which GET to use for ModMail", "created_utc": 1433438967, "gilded": 0, "name": "t1_crvic6u", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "russellvt", "body": "I believe you're looking for the parameters to [get_content()](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit.get_content), which are passed as additional arguments via get_mod_mail().", "created_utc": 1433444589, "gilded": 0, "name": "t1_crvm02m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "picflute", "body": "Where do I get the after_field content? What can I call to get it exactly?", "created_utc": 1433450862, "gilded": 0, "name": "t1_crvq3kc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "bboe", "body": "There's a very strong likeliness that if PRAW returns 6668 items, that your web browser would also only show you 6668 items. I am not aware of any item listings in PRAW that return less data than obtainable via the browser.", "created_utc": 1433402798, "gilded": 0, "name": "t1_crv3lhb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "picflute", "body": "Hey, by any chance what json page does get_mod_mail() check?", "created_utc": 1433833912, "gilded": 0, "name": "t1_cs0d0sd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "xfile345", "body": "***UPDATE:*** I recently decided to archive the modmail of one of my subreddits and I found out what the limit is: 15,000 replies. You're only getting 6,668 \"links\" because that's when the total messages + replies totals 15,000. I did the same as you and looped Modmail until it finished to get all the message and came up with an odd number (3438) and still a long way to go until even the start of when I was a moderator in that subreddit. Then I looked deeper at the replies within in message and counted 11561 replies. Adding the number replies to the number of messages and I got **14,999**. Hope this helps in some way. I have no idea how to go about any other messages older than that, though.", "created_utc": 1433905868, "gilded": 0, "name": "t1_cs1drus", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "picflute", "body": "Well I guess I'm fucked and the limit is 15,000. Hopefully Deimorz offers a chance for mod teams to get an archive dump of modmail messages via text file or something", "created_utc": 1433907132, "gilded": 0, "name": "t1_cs1ehyw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "xfile345", "body": "The only other way I can think of to get older messages would be to check every single URL (/message/messages/`idcode`) for messages that don't produce a 403 and match the target subreddit. But that's about a quarter of a *BILLION* messages to check through, which at the API's limit of 60 requests per 60 seconds (with OAuth), would take somewhere in the neighborhood of 7\u00bd years to go through. So that's not exactly a viable option. And all that work just to find a few thousand more messages is just insane, and likely to get you banned from Reddit. lol So yeah. You'd have to have special access to the database, most likely. Good luck!", "created_utc": 1433907441, "gilded": 0, "name": "t1_cs1eooo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38hgnu/how_to_deep_dive_into_modmail_archives/"}, {"author": "GoldenSights", "body": "It appears that in the [unreleased version of PRAW](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L2391) this has been updated to if text is None and bool(text) == bool(url): So `text=''` will make an empty post.", "created_utc": 1433384439, "gilded": 0, "name": "t1_cruwa76", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "i-am-you", "body": "Ah, thanks, but that would introduce another bug where bool(text) and bool(url) are both true but continues without error. Right?", "created_utc": 1433384796, "gilded": 0, "name": "t1_cruwh19", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "GoldenSights", "body": "Yeah, I suppose you're right. For some reason I'm struggling to find the best one-liner solution to this that also accounts for non-string inputs. What do you think it should be?", "created_utc": 1433386862, "gilded": 0, "name": "t1_cruxllw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "i-am-you", "body": "Hmm, not very experienced programmer but here is what I came up with: if (text and url) or (url is None and text is None) :", "created_utc": 1433387895, "gilded": 0, "name": "t1_cruy4ba", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "GoldenSights", "body": "This is making me feel very silly. Apparently boolean logic is my weakness. Here's what I came up with that passes all the tests I feel are appropriate: def check(url=None, text=None): print(repr(url), repr(text), end=' ') if (text != None and url != None) or (url == None and text == None) or (url != None and not isinstance(url, str)): print('Bad') else: print('Good') check(None, None) # Bad check('hi', None) # Good check(None, 'hi') # Good check(None, '') # Good check(None, 0) # Good check(0, None) # Bad check('hi', '') # Bad check('', 'hi') # Bad The problem with your test is it passes `check('hi', '')` which should fail because praw will interpret `text=''` as making a blank selfpost when you might also be intending to submit a url string.", "created_utc": 1433388737, "gilded": 0, "name": "t1_cruyiwq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "i-am-you", "body": "Ha, well that line certainly isn't pretty", "created_utc": 1433421128, "gilded": 0, "name": "t1_crv88ue", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "bboe", "body": "`isinstance(text, six.string_types) == bool(url)` should do the trick. This line doesn't permit binary strings to be passed in python 3, but that really shouldn't be passed anyway.", "created_utc": 1433397597, "gilded": 0, "name": "t1_crv22ff", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "GoldenSights", "body": "I knew it was going to be something simple like this. Thanks!", "created_utc": 1433397925, "gilded": 0, "name": "t1_crv26ap", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/38gokh/praw_how_to_submit_empty_self_post/"}, {"author": "MrQamboy", "body": "he got banned via /u/botwatchman which essentially means he's been banned from a lot of subreddits (ones that use the botwatchman)", "created_utc": 1433102811, "gilded": 0, "name": "t1_crr428m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37z2ql/obamabot_not_responding_to_all_posts/"}, {"author": "obamabot9000", "body": "I know but I remade him on him on another account (this one). This bot has no correlation to the other one.", "created_utc": 1433108524, "gilded": 0, "name": "t1_crr75pc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37z2ql/obamabot_not_responding_to_all_posts/"}, {"author": "MrQamboy", "body": "http://redd.it/37rter hm", "created_utc": 1433109019, "gilded": 0, "name": "t1_crr7f0n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37z2ql/obamabot_not_responding_to_all_posts/"}, {"author": "santa_mana", "body": "I see 2 problems: 1.) Your bot qualifies as a respond-to-keyword(s) type of bot, and those kinds of bot are very shunned upon, even though there are a few like it out there (I don't know why some get special treatments while others are banned.....looking at you /u/slothfactsbot). But generally, a lot of the more popular subreddits won't allow your bot to post. 2.) /u/obamabot and /u/obamabot9000 are new accounts, so they can only post every 10 minutes. So if there are multiple \"Thanks obama\" in a thread, only the first one will be responded to. After that, the 10 minutes will kick in, and when the bot tries to respond a 2nd time, the bot will break. To get rid of the 10 minute limit, you need to get some **link** karma (idk the exact requirements, but aim for 100+). Maybe verify your email. Some comment karma couldn't hurt. And maybe wait a week (or more) so your account is a bit older. --- But don't get discouraged. It's awesome that you made a bot. Bot-making is always welcomed and encouraged. One of the big reason why those bots are shunned is because they are easy karma, and if they were allowed Reddit would be full of them. And most of the time they don't add anything to the conversation.", "created_utc": 1433123924, "gilded": 0, "name": "t1_crrf413", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37z2ql/obamabot_not_responding_to_all_posts/"}, {"author": "obamabot9000", "body": "/u/obamabot is not an account in my jurisdiction, I have nothing to do with it. This account is the only one, and I have only received one \"you are doing that too much,\" and that was during the testing phase. At the moment, ObamaBot looks for comments every 60s. He has no trouble looking for comments in one subreddit, (at the moment he looks in /r/botwatch) but for some reason he can't handle /r/all or more than one subreddit (which might be a noob formatting error). I have also been very careful to use four spaces and not a tab. Also, /u/slothfactsbot works as mine does, as does the /u/cahbot, which looks (I'm assuming) for comments containing \"/u/cahbot\" somewhere in there. I like your karma suggestion, I will try /r/freekarma. Thanks!", "created_utc": 1433164977, "gilded": 0, "name": "t1_crrsr0f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37z2ql/obamabot_not_responding_to_all_posts/"}, {"author": "thorarakis", "body": "Looks like praw isn't handling wiki read oauth access correctly. I have [opened a PR](https://github.com/praw-dev/praw/pull/421) to resolve this.", "created_utc": 1433362762, "gilded": 0, "name": "t1_crujpez", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "radd_it", "body": "Is your callback URL really ``http://127.0.0.1``? I don't see how that'd work.", "created_utc": 1432946158, "gilded": 0, "name": "t1_crpg7lk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "habnpam", "body": "Yes. It's the one used in the tutorial. So what should it be instead? Or, what is a typical redirect_uri look like?", "created_utc": 1432949309, "gilded": 0, "name": "t1_crphq9d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "thorarakis", "body": "If you are using a web app then you'll need a redirect uri that points back to your application. If you are running the web app locally then 127.0.0.1 is totally fine, although you'll need to change that if you move your app to a production environment. If you are using one of the other app types then the uri doesn't really matter.", "created_utc": 1433352223, "gilded": 0, "name": "t1_cruclnl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "boib", "body": "Any news on this? I'm also getting a Forbidden exception on wiki edit.", "created_utc": 1435353341, "gilded": 0, "name": "t1_csjuw08", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "habnpam", "body": "/u/thorarakis [opened a PR](https://github.com/praw-dev/praw/pull/421) to fix it. It still doesn't work for me, though. Although the 403 client error doesn't show up any more. Now I get a \"praw.errors.Forbidden\" error ( from line 194 internal.py). It seems the PR added a \"wikiread\" refresh token, but we still can't edit because there's no \"wikiedit\" refresh token yet. But that's just a guess. I don't really know how it all works.", "created_utc": 1435525253, "gilded": 0, "name": "t1_cslq2i8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "boib", "body": "The PR fixed get_wiki_page() but not edit_wiki_page(). I've never done a PR but I did edit my local copy of PRAW to add a decorator to edit_wiki_page(). 1) Find the file `__init__.py` under PRAW 2) Search for `def edit_wiki_page` and on the line just above it, add `@decorators.restrict_access(scope='wikiedit', login=False)` 3) Voila! Maybe /u/thorarakis could submit a PR for this as well :)", "created_utc": 1435526842, "gilded": 0, "name": "t1_cslqx7v", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "thorarakis", "body": "My bad, probably should have caught that too. But sounds like you've got the fix already written, which means you've done the hard part and might as well get the credit for the PR yourself :) You just need to fork a copy of [PRAW in github](https://github.com/praw-dev/praw), commit then push the changes in your fork and [create the PR](https://help.github.com/articles/creating-a-pull-request/) from your fork to PRAW. Shouldn't take much more then 10 minutes if you are familiar with git. And if you aren't I highly recommend that you start! If you have any questions I'd be happy to help. But the community really appreciates having more people contributing, and small fixes like these make a big difference.", "created_utc": 1435632504, "gilded": 0, "name": "t1_csn4y2f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37s4or/how_can_i_edit_wiki_while_using_oauth/"}, {"author": "SandyRegolith", "body": "I downloaded a bunch of reddit comments a while ago, about 1.4 million. You're welcome to them if it would help. There are some duplicates, because the person I did it for wanted to see karma over time.", "created_utc": 1432961869, "gilded": 0, "name": "t1_crpnaob", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37riu0/scraping_comment_karma_data_in_an_efficient_way/"}, {"author": "gimunu", "body": "Well first thanks for the offer! Did you store the user comment karma from the comments? If so, how did you retrieve the information? I am asking because I could not think of a faster way than checking the user page, but if I could get users' name and comment karma from comment pages, I could gather information much quicker.", "created_utc": 1432992075, "gilded": 0, "name": "t1_crpuvzm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37riu0/scraping_comment_karma_data_in_an_efficient_way/"}, {"author": "SandyRegolith", "body": "I'll PM you a link to the comments data, just in case ten thousand people try to download it. My method for getting the comments with scores was to get the comments in one pass, then in a second pass, get the individual comment details using `http://reddit.com/api/info/.json?id=` plus a comma-separated list of the comments, as in http://www.reddit.com/api/info/.json?id=t1_crpnaob is the API link to the details of one comment, but http://www.reddit.com/api/info/.json?id=t1_crpnaob,t1_crpuvzm is the link to get the details of two comments. You can do 100 at a time.", "created_utc": 1433038402, "gilded": 0, "name": "t1_crqg9wd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37riu0/scraping_comment_karma_data_in_an_efficient_way/"}, {"author": "psdtwk", "body": "https://www.reddit.com/r/redditdev/comments/dtg4j/want_to_help_reddit_build_a_recommender_a_public/ Does that help?", "created_utc": 1432943387, "gilded": 0, "name": "t1_crpeuu9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37riu0/scraping_comment_karma_data_in_an_efficient_way/"}, {"author": "thorarakis", "body": "First, pip installing from a github repository with that syntax works for me. It looks like you may not have git installed on your local system, I suggest doing some googling for 'git on windows' and make sure you can clone projects locally before trying to install from git using pip. If `git --version` doesn't return a nice version string then the pip command will probably not work either.", "created_utc": 1432922559, "gilded": 0, "name": "t1_crp28l7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37psu6/new_to_using_praw_and_pip/"}, {"author": "Day_Old_Pizza", "body": "Thanks. It worked after I installed git.", "created_utc": 1433034876, "gilded": 0, "name": "t1_crqekvi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37psu6/new_to_using_praw_and_pip/"}, {"author": "GoldenSights", "body": "The function appears to be called `r.get_domain_listing`, defined [here](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L747). But you don't stack the get_hot() function on top of it, it has a parameter for sort.", "created_utc": 1432880600, "gilded": 0, "name": "t1_crok6wa", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ow7j/how_to_get_domain_instead_of_get_subreddit/"}, {"author": "1ama", "body": "Thank you!", "created_utc": 1432880760, "gilded": 0, "name": "t1_crok8px", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ow7j/how_to_get_domain_instead_of_get_subreddit/"}, {"author": "radd_it", "body": "imgur.com is a special case tho-- reddit sees so many of those links that [/domain/imgur.com](/domain/imgur.com) is no longer updated. You'll be looking at the same (outdated) results every time. You'll need to pull /r/all/new and find the imgur.com results yourself.", "created_utc": 1432885358, "gilded": 0, "name": "t1_croljh2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ow7j/how_to_get_domain_instead_of_get_subreddit/"}, {"author": "1ama", "body": "Yes, but I'll use other domain. I just gave an example. Thanks for the heads up.", "created_utc": 1432885842, "gilded": 0, "name": "t1_crolnzy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ow7j/how_to_get_domain_instead_of_get_subreddit/"}, {"author": "thorarakis", "body": "I'm pretty sure the intention was that `display_name` would be how the multi reddit appeared on the tabs in the left of the UI, whereas `name` should be the canonical identifier since it ends up being part of the path to the multi `/user/bboe/m/awesome_multi_name` That being said, last I checked the `name` is used in the display. This should probably be fixed.", "created_utc": 1432741865, "gilded": 0, "name": "t1_crml0cf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37foul/why_do_multireddits_have_a_separate_display_name/"}, {"author": "bboe", "body": "Why not just set the initial limit to 100? That's one request worth of history, then you get everything going forward. If you really want the history, manually fetch the comments before turning on the stream.", "created_utc": 1432709579, "gilded": 0, "name": "t1_crm9g1f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37eyjb/praw_commentsubmission_streams/"}, {"author": "thomasbomb45", "body": "That's what I'm doing now. I just think an option should exist. \"ordering = None\" or something.", "created_utc": 1432759346, "gilded": 0, "name": "t1_crmw5hc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37eyjb/praw_commentsubmission_streams/"}, {"author": "GoldenSights", "body": "The PRAW Comment object includes the subreddit_id, but also the subreddit name. Try `comment.subreddit.display_name`. If you *had* to get the name for a given ID, you'd want to use `r.get_info(thing_id='t5_2r8lo')` to get the Subreddit, and use its display_name", "created_utc": 1432616125, "gilded": 0, "name": "t1_crl1l76", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "shaggorama", "body": "Ah, I glossed over the fact that this was coming from comment objects already. Yeah, OP: use this version. It'll save you a GET request (to hit the /info/ api endpoint, since the subreddit name is in the comment data already. I think.).", "created_utc": 1432619181, "gilded": 0, "name": "t1_crl2pwm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "jbw976", "body": "nice, using comment.subreddit.display_name worked great. i'm new to python, so i was having a hard time inspecting objects and reading the PRAW docs for useful properties...thanks for the help!", "created_utc": 1432618042, "gilded": 0, "name": "t1_crl2bgt", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "fragmede", "body": "`dir(thing)` is your friend. As in, run python, then paste in import praw r = praw.Reddit(user_agent='/u/fragmede') thing = r.get_info(thing_id='t5_2r8lo') dir(thing) # prints out big long list of attributes on thing. If dir(thing) is too long, I'll type out a quick list comprehension to print a shorter list. Something like: [x for x in dir(thing) if 'name' in x.lower()] That'll print out any attribute with the word 'name' on thing; in this case, display_name.", "created_utc": 1432651469, "gilded": 0, "name": "t1_crlcnq1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "bboe", "body": "`vars(thing)` is also super useful as it'll show you the value. It however, misses things like functions that `dir` does not.", "created_utc": 1432655752, "gilded": 0, "name": "t1_crlf3q1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "shaggorama", "body": "`r.get_info(thing_id=u't5_2r8lo').display_name`", "created_utc": 1432616252, "gilded": 0, "name": "t1_crl1n0q", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "jbw976", "body": "that exactly answers it, thanks!", "created_utc": 1432618092, "gilded": 0, "name": "t1_crl2c4h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/37ah9n/convert_subreddit_id_to_subreddit_name/"}, {"author": "phil_s_stein", "body": "Why use php? Take a look at the first comment on your link. Just use PRAW (python). Much more simple for a first project. Don't use Karmalytics. You can write a simple bot that responds to user mentions, grabs the text pf the post to scan it for keywords. If found respond to the post. [PRAW documentation on writing a simple bot.](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html)", "created_utc": 1432591431, "gilded": 0, "name": "t1_crkop0g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/378zz9/reply_to_parent_comment/"}, {"author": "AnneBancroftsGhost", "body": "I was playing around with that, too. I just thought the php/karmalytics route would be better because I don't need to run it locally. But if python/praw is the way to go then I'll happily do that. Can you offer any advice though for my original question? Even using python, I still haven't been able to figure out a way to reply to the parent of the comment that calls the bot...", "created_utc": 1432592014, "gilded": 0, "name": "t1_crkozqu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/378zz9/reply_to_parent_comment/"}, {"author": "phil_s_stein", "body": "> I was playing around with that, too. I just thought the php/karmalytics route would be better because I don't need to run it locally. This is a good point. I've got a few remote machines, so I forget that not everyone has that. I know you can get a virtual machine for 5 bucks or so a month. Anyway... Reply to the parent? So someone calls the bot and you want to respond to the comment above that one? You want something like this: for comment in reddit.get_mentions(): parent = comment.reddit_session.get_info(thing_id=comment.parent_id) parent.reply('Hi Mom!') edit: yeah [Digital Ocean will get you a VM for $5 a month](https://www.digitalocean.com/pricing/). Might be useful if you're going to be doing bot development.", "created_utc": 1432593438, "gilded": 0, "name": "t1_crkppya", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/378zz9/reply_to_parent_comment/"}, {"author": "AnneBancroftsGhost", "body": "This is exactly what I needed. Thank you!", "created_utc": 1432655543, "gilded": 0, "name": "t1_crlez4q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/378zz9/reply_to_parent_comment/"}, {"author": "cris9696", "body": "Did you find a fix for this? I am having the same problem on my bot (https://github.com/crisbal/PlayStoreLinks_Bot)", "created_utc": 1435158419, "gilded": 0, "name": "t1_csh3tvk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36yk6c/commentreplies_returns_empty_list/"}, {"author": "BananaGranola", "body": "I didn't. I ended up rewriting my bot to use mentions instead.", "created_utc": 1435159705, "gilded": 0, "name": "t1_csh4noe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36yk6c/commentreplies_returns_empty_list/"}, {"author": "cris9696", "body": "Ok, thanks for the answer. This is very strange and no one seems to have this problem.", "created_utc": 1435159785, "gilded": 0, "name": "t1_csh4pn3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36yk6c/commentreplies_returns_empty_list/"}, {"author": "Deimorz", "body": "You just call `.distinguish()` on the object returned when you submit: post = sub.submit(\"Title of post\", text='''Long winded body of the post''') post.distinguish()", "created_utc": 1432337396, "gilded": 0, "name": "t1_crhy5bs", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36xioo/praw_how_can_my_weekly_autopost_script/"}, {"author": "Fi8_CoC", "body": "Thank you! Worked perfectly", "created_utc": 1432360896, "gilded": 0, "name": "t1_cri86kl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36xioo/praw_how_can_my_weekly_autopost_script/"}, {"author": "Fi8_CoC", "body": "In: (as_made_by=u'mod') do I have to replace mod with the username of the bot?", "created_utc": 1432335650, "gilded": 0, "name": "t1_crhxa9i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36xioo/praw_how_can_my_weekly_autopost_script/"}, {"author": "kemitche", "body": "Looks like it's trying to pickle the generator from get_single_submission. I don't think *any* generator pickles well/at all - try converting to a list first.", "created_utc": 1432607579, "gilded": 0, "name": "t1_crkxavt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36unvz/unable_to_pickle_single_submissions_comment_with/"}, {"author": "DarkMio", "body": "It won't work either, as a single submission object is already breaking pickle.", "created_utc": 1432614494, "gilded": 0, "name": "t1_crl0wip", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36unvz/unable_to_pickle_single_submissions_comment_with/"}, {"author": "bboe", "body": "Thanks for asking. There really should be an example on the page you linked to. The following should explain it: https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html?highlight=site#praw.__init__.BaseReddit And here is an example to use the 'bboe' site. r = praw.Reddit('USER AGENT', site_name='bboe')", "created_utc": 1432174737, "gilded": 0, "name": "t1_crfxy3k", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36pfcm/praw_how_is_the_correct_site_selected_in_prawini/"}, {"author": null, "body": "[deleted]", "created_utc": 1432175789, "gilded": 0, "name": "t1_crfyjoz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36pfcm/praw_how_is_the_correct_site_selected_in_prawini/"}, {"author": "bboe", "body": "Yes it has an affect. However, reddit also has a cache that you're likely hitting. There are some tricks you can do to bypass the cache. You should check out [comment_stream](https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html?highlight=comment_stream#praw.helpers.comment_stream) if you're trying to get new comments for a subreddit regularly.", "created_utc": 1432176700, "gilded": 0, "name": "t1_crfz2nk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36pfcm/praw_how_is_the_correct_site_selected_in_prawini/"}, {"author": "GoldenSights", "body": "praw.Reddit is a class, representing a session with reddit. It is defined [here](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L2269). It looks pretty barren, but that's because all of its methods come from the different mixins that it inherits from, each of which inherit from AuthenticatedReddit, which inherits from UnauthenticatedReddit, which inherits from BaseReddit, which is where your useragent ends up when you instantiate it. You can consider pretty much the entire \\_\\_init__.py file as the definition of the Reddit object, as far as I understand it.", "created_utc": 1432093053, "gilded": 0, "name": "t1_creuuww", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36kwbq/praw_where_is_the_prawreddit_method_defined/"}, {"author": "Agent_HK-47", "body": "Thank you very much for the insight.", "created_utc": 1432093248, "gilded": 0, "name": "t1_creuynj", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36kwbq/praw_where_is_the_prawreddit_method_defined/"}, {"author": "grimpunch", "body": "Here, code from my bot might help you. I had to work out the same thing for comment in subreddit.get_comments(limit=None): cid = str(comment.id) match = re.search('TL;?DR please', comment.body, re.IGNORECASE) if match and cid not in comments_already_done: if username in str(comment.author): # Don't reply to the bot itself. logging.info('Found comment from %s , so ignore it' % username) return https://github.com/grimpunch/TLDRifyBot/blob/master/__init__.py", "created_utc": 1432024664, "gilded": 0, "name": "t1_crdvg47", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36ggaz/getting_child_comments_from_parent_comment_object/"}, {"author": "ibbignerd", "body": "Almost. This is so it doesn't reply to itself. A comment will have a keyword that will make it a flagged comment. Some code needs to be executed after seeing this flagged comment. The first time the bot runs through, it sees the comment, flags it (adds it to an sql db) and executes the code. The next time it runs, I need to check if the bot has already replied, and in essence already executed the code. Therefore, it needs to check the children of the flagged comment for it's own username. --- Does that make more sense?", "created_utc": 1432044836, "gilded": 0, "name": "t1_cre1w69", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36ggaz/getting_child_comments_from_parent_comment_object/"}, {"author": "grimpunch", "body": "Ah, if you read the rest of the file linked. I handle that too! Except I go about it differently. I keep a log of the post ids and add them to a list of already done comments. Then check against those before replying", "created_utc": 1432046454, "gilded": 0, "name": "t1_cre2rtt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/36ggaz/getting_child_comments_from_parent_comment_object/"}, {"author": "TheEnigmaBlade", "body": "You can add extra parameters using the `params` keyword argument, and more information on the available parameters can be found on the [reddit API page](https://www.reddit.com/dev/api#POST_api_friend). Something like this (although I've never banned anyone using PRAW, so it's untested): sub.ban_user(user, params={\"duration\": 7, \"note\": mod_reason, \"ban_message\": user_reason})", "created_utc": 1431833369, "gilded": 0, "name": "t1_crbmo7z", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/367w2j/praw_ban_reason_and_length/"}, {"author": "lizardsrock4", "body": "Thank you so much!", "created_utc": 1431833795, "gilded": 0, "name": "t1_crbmumy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/367w2j/praw_ban_reason_and_length/"}, {"author": "Eathed", "body": "\"thing\" is a comment object. It doesn't just contain the message, it contains the time posted, author, points, all of that type of stuff. In your case, use thing.body for the text of a comment. [Here](https://praw.readthedocs.org/en/v2.1.21/pages/comment_parsing.html) is a guide on comment parsing from the docs.", "created_utc": 1431519454, "gilded": 0, "name": "t1_cr7nm5s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35tfe9/praw_noob_issue_while_retrieving_comments_from_a/"}, {"author": "Arnoyo12", "body": "Thanks, it indeed works now that I modified it. So should I think of the comment object as a \"dict\" that contains multiple attributes of the comment itself? In that case is there a way to observe the comment object and to know how to access its components?", "created_utc": 1431534051, "gilded": 0, "name": "t1_cr7v7qh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35tfe9/praw_noob_issue_while_retrieving_comments_from_a/"}, {"author": "exoendo", "body": "one easy way you can do it is to do: for thing in submission.comments: pprint.pprint(vars(thing)) break This will print out one entry of a comment objects various attributes. It will look like this: {'_info_url': 'http://www.reddit.com/api/info/', '_replies': [, , , , , , , , ], '_submission': , '_underscore_names': ['replies'], 'approved_by': None, 'archived': False, 'author': Redditor(user_name='pacfcqlkcj4'), 'author_flair_css_class': None, 'author_flair_text': None, 'banned_by': None, 'body': u'>PLUM LINE: You say this isn\\u2019t about Clinton. But don\\u2019t Democratic primary voters deserve to hear you challenge the front runner on these issues?\\n\\n>SANDERS: I am prepared to do that when I find out what her position is going to be.\\n\\nOh shit. Calling her out on being cagey and not having any positions to stand on. I like it.', 'body_html': u'<div class=\"md\"><blockquote>\\n<p>PLUM LINE: You say this isn\\u2019t about Clinton. But don\\u2019t Democratic primary voters deserve to hear you challenge the front runner on these issues?</p>\\n\\n<p>SANDERS: I am prepared to do that when I find out what her position is going to be.</p>\\n</blockquote>\\n\\n<p>Oh shit. Calling her out on being cagey and not having any positions to stand on. I like it.</p>\\n</div>', 'controversiality': 0, 'created': 1430668919.0, 'created_utc': 1430665319.0, 'distinguished': None, 'downs': 0, 'edited': False, 'gilded': 0, 'has_fetched': True, 'id': u'cqwymdw', 'json_dict': None, 'likes': None, 'link_id': u't3_34pp8j', 'mod_reports': [], 'name': u't1_cqwymdw', 'num_reports': None, 'parent_id': u't3_34pp8j', 'reddit_session': , 'removal_reason': None, 'report_reasons': None, 'saved': False, 'score': 2184, 'score_hidden': False, 'subreddit': Subreddit(display_name='politics'), 'subreddit_id': u't5_2cneq', 'ups': 2184, 'user_reports': []} If you wanted to access the karma of the comment, you would just do: print thing.score etc..", "created_utc": 1431669940, "gilded": 0, "name": "t1_cr9rii1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35tfe9/praw_noob_issue_while_retrieving_comments_from_a/"}, {"author": "Arnoyo12", "body": "thank you!", "created_utc": 1431676131, "gilded": 0, "name": "t1_cr9t7pd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35tfe9/praw_noob_issue_while_retrieving_comments_from_a/"}, {"author": "jermany755", "body": "Unfortunately the docs aren't very complete when it comes to object attributes, but you can use dir() to see them.", "created_utc": 1431664824, "gilded": 0, "name": "t1_cr9pjs7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35tfe9/praw_noob_issue_while_retrieving_comments_from_a/"}, {"author": "GoldenSights", "body": "Comments have a `parent_id` attribute that contains the fullname of whatever they replied to, whether it's a parent comment or the submission. You'll have to use `r.get_info` to actually get the object from this fullname though.", "created_utc": 1431453715, "gilded": 0, "name": "t1_cr6t8fd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35qi47/praw_how_to_reference_a_comment_above/"}, {"author": "GoldenSights", "body": "You'll want to use comment_stream or get_comments, and save the items with the correct thread_id attribute. get_comments behaves on a 30 second cache, so if speed is absolutely critical then use the stream. Otherwise I prefer to use get_comments.", "created_utc": 1431412486, "gilded": 0, "name": "t1_cr6cb7p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35okwm/praw_whats_the_best_way_to_get_comments_as_fast/"}, {"author": "Toofifty", "body": "Alright, thanks. Speed is pretty critical in this case so I'll read more into comment_stream.", "created_utc": 1431414078, "gilded": 0, "name": "t1_cr6cqxf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35okwm/praw_whats_the_best_way_to_get_comments_as_fast/"}, {"author": "GoldenSights", "body": "MoreComments objects have a method called 'comments()' that will return a list of the replies behind it. Of course, there may be another mc in that list so you might have to get a little recursive. Does that do what you're looking for?", "created_utc": 1431232315, "gilded": 0, "name": "t1_cr48kv9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "opsoyo", "body": "That's got me a bit farther. I'm confused, though, as to why the comment being accessed has 600+ replies, but only 92 replies and 1 MoreComments object is return in the initial request. And, once I run 'comments()' on that MoreComments object it only gives me another 20 replies. All of the replies I want (600+) are first level. I don't care for anything below first level replies, but I'm not seeing another MoreComments object after that first one.", "created_utc": 1431233189, "gilded": 0, "name": "t1_cr48xcy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "GoldenSights", "body": "I'm actually not at my computer, so I can't test any of this unfortunately. When you say 600 comments, are you referring to the \"load 600 more\" button? I'm pretty sure that counts all of the comment chains, not just first-level replies, right? I would compare the numbers and comment authors to what you are able to load in the browser and see where the discrepancies are.", "created_utc": 1431233730, "gilded": 0, "name": "t1_cr4954f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "opsoyo", "body": "I appreciate the help nonetheless. But, actually, you're right. The 92, which are in the initial request, are above the 'load X more' (590 in this case) button. The 92 + 590 add up to the total value of first-level replies. So, that part is correct. Okay, so it appears that the MoreComments object is holding/showing the first 20 of the 'load more' section. Now, how do I get the rest? Lol.", "created_utc": 1431234459, "gilded": 0, "name": "t1_cr49evs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "GoldenSights", "body": "In my experience, the final item in the list returned by morecomments.comments() will be another mc object if there are more to load. You're getting 20 more comments and nothing else? I've never seen a comment that received over 600 direct replies, would you mind linking to it?", "created_utc": 1431235269, "gilded": 0, "name": "t1_cr49pdt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "opsoyo", "body": "My playground: /r/opsoyo/ That's correct. 20 replies in a list and no other MoreComment object.", "created_utc": 1431236035, "gilded": 0, "name": "t1_cr49yjp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "GoldenSights", "body": "Hot damn. I'm gonna go ahead and say that this is not a praw issue. When I view the thread normally I get a \"You broke reddit\" screen, but the page actually loads if I use .compact. When I click the button at the bottom, only 1 set of comments comes up, exactly as you describe. So it's not PRAW's fault, reddit is just not serving that content. Luckily, all of those comments should fit in your subreddit's cache. If you do: comments = subreddit.get_comments(limit=None) keep = [] for comment in comments: if comment.parent_id == 't1_cr3etpa': keep.append(comment) keep.sort(key= lambda x: x.created_utc) Now the list `keep` *should* have all of the comments on the thread, in order of creation.", "created_utc": 1431237185, "gilded": 0, "name": "t1_cr4abb1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "opsoyo", "body": "That worked! Albeit, it's slow, but it's definitely better than *not* working at all. Thank you very much!", "created_utc": 1431240393, "gilded": 0, "name": "t1_cr4b7is", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/35got2/how_to_use_replace_more_comments_with_replies_in/"}, {"author": "GoldenSights", "body": "Most likely, the command \"pip\" is bound to the pip for python 2. [Here's a simple way](http://stackoverflow.com/a/25123329) to tell it which version of python you want to use.", "created_utc": 1431053097, "gilded": 0, "name": "t1_cr25a7z", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358zpt/having_trouble_getting_praw_to_work_with_python_3/"}, {"author": "hizinfiz", "body": "Thanks for the response :) I actually did some searching of my own after posting here and [followed this](http://stackoverflow.com/a/27957073) pip3 install praw worked flawlessly for me.", "created_utc": 1431053934, "gilded": 0, "name": "t1_cr25q3a", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358zpt/having_trouble_getting_praw_to_work_with_python_3/"}, {"author": "redalastor", "body": "What you should do is put everything related to one project in one virtualenv. To create a python 3 env, do `virtualenv -p python3 venv`. Once you activate it, python, pip, and everything else will always refer to the correct interpreter.", "created_utc": 1431095394, "gilded": 0, "name": "t1_cr2js7w", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358zpt/having_trouble_getting_praw_to_work_with_python_3/"}, {"author": "mgrieger", "body": "Just for future reference, I've found the following project very helpful for managing separate Python versions and virtual environments: https://github.com/yyuu/pyenv.", "created_utc": 1431066582, "gilded": 0, "name": "t1_cr2azom", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358zpt/having_trouble_getting_praw_to_work_with_python_3/"}, {"author": "bboe", "body": "> I'm wondering if there's a reason the functionality isn't in there to begin with. No one has added it yet. You should make a PR with your changes. :)", "created_utc": 1431059677, "gilded": 0, "name": "t1_cr28hmx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358osw/refresh_token_and_prawini/"}, {"author": "SpotiList", "body": "Ok, done! Thanks. =)", "created_utc": 1431096475, "gilded": 0, "name": "t1_cr2ke27", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358osw/refresh_token_and_prawini/"}, {"author": "bboe", "body": "Thank you.", "created_utc": 1431099240, "gilded": 0, "name": "t1_cr2m25u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358osw/refresh_token_and_prawini/"}, {"author": "GoldenSights", "body": "Look for phrases like \"origin error\", \"origin down\" or \"service unavailable\". The codes that I see most are 502, 503, and 520, though there may be another. You can also check out the notes about Exceptions in [praw's documentation](https://praw.readthedocs.org/en/v2.1.21/pages/exceptions.html?highlight=exceptions)", "created_utc": 1431045714, "gilded": 0, "name": "t1_cr21bv1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358lu6/what_exception_may_i_expect_when_reddit_is_busy/"}, {"author": "redalastor", "body": "Damn... I was on an older praw's doc page... Thanks.", "created_utc": 1431046073, "gilded": 0, "name": "t1_cr21isr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358lu6/what_exception_may_i_expect_when_reddit_is_busy/"}, {"author": "kemitche", "body": "Generally, an error code from 501-599 indicates a server-side reddit error that you can safely wait and retry again later. As the praw docs that GoldenSights linked mention, 500 is often transient, but it can be persistent and may indicate a bug on the reddit server side. If you notice a persistent 500 error that lasts more than a few minutes (and reddit is [not currently experiencing elevated error rates](https://www.redditstatus.com)), then you should post here with as much info as possible.", "created_utc": 1431375729, "gilded": 0, "name": "t1_cr5uxh1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358lu6/what_exception_may_i_expect_when_reddit_is_busy/"}, {"author": "GoldenSights", "body": "Hi, looks like this is based off one of my templates. Firstly, what's your traceback? What line does the error point to? I do notice that you passed two parameters into r.get_subreddit, which only needs one, but that doesn't explain a 403. Sometimes 403 means you've been banned from the subreddit, but since the bot isn't commenting out in public yet, that probably isn't the case.", "created_utc": 1431041358, "gilded": 0, "name": "t1_cr1yzdy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "WheelsOnPavement", "body": "I wouldn't be surprised if I did base it off of one of your templates- if so, thanks, big help. EDIT: I guess it's also worth mentioning that if I log in as the bot in a browser I can post fine in any sub, albeit with the new account restrictions. Here's my traceback: Traceback (most recent call last): File \"C:\\Users\\Sam Hosmer\\Desktop\\Bot 2\\bot3export.py\", line 60, in run_bot() File \"C:\\Users\\Sam Hosmer\\Desktop\\Bot 2\\bot3export.py\", line 47, in run_bot comment.reply('You have summoned the bagel gods! Bask in their glory!') File \"C:\\Python34\\lib\\site-packages\\praw\\objects.py\", line 384, in reply response = self.reddit_session._add_comment(self.fullname, text) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 338, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 2173, in _add_comment retry_on_error=False) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 561, in request_json retry_on_error=retry_on_error) File \"C:\\Python34\\lib\\site-packages\\praw\\__init__.py\", line 403, in _request _raise_response_exceptions(response) File \"C:\\Python34\\lib\\site-packages\\praw\\internal.py\", line 178, in _raise_response_exceptions response.raise_for_status() File \"C:\\Python34\\lib\\site-packages\\requests\\models.py\", line 851, in raise_for_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 403 Client Error: Forbidden", "created_utc": 1431041635, "gilded": 0, "name": "t1_cr1z50b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "GoldenSights", "body": "Huh, I wonder why the try-catch for HTTPError didn't get that one. Well, 403 when creating a comment usually means that you're banned in that subreddit. Did this happen in /r/test or somewhere else?", "created_utc": 1431042165, "gilded": 0, "name": "t1_cr1zfln", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "WheelsOnPavement", "body": "Though I guess that's a possibility, not only have I tried using multiple accounts and subs with the bot, but the error also occurs when I try replying in my own subreddit that I created for the specific purpose of testing it. EDIT: the try-catch does work, I just disabled it for the time being so that I could get the traceback.", "created_utc": 1431042331, "gilded": 0, "name": "t1_cr1zix7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "GoldenSights", "body": "Can you try adding more detail to your useragent? In the past I've gotten 429s when using an insufficient useragent, but 403 would make sense too.", "created_utc": 1431042488, "gilded": 0, "name": "t1_cr1zm0v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "WheelsOnPavement", "body": "I lengthened the useragent by quite a bit and got the same error. r = praw.Reddit(user_agent=\"BB by /u/WheelsOnPavement- created out of boredom for no real purpose. Made in Python using Reddit's PRAW API.\")", "created_utc": 1431042833, "gilded": 0, "name": "t1_cr1zssv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "WheelsOnPavement", "body": "Whup, figured it out. The commend.id was set to comment.author.name for some reason.. after renaming comment.id to comment.author it works perfectly. Thanks for your help, GoldenSights!", "created_utc": 1431095237, "gilded": 0, "name": "t1_cr2jp5a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/358amz/suddenly_getting_a_403_when_attempting_to_use/"}, {"author": "bboe", "body": "As I recall, all that parameter does is disable the \"orangered\" icon that's red when you have a new message. That \"read\" state of each message is unaffected.", "created_utc": 1430881247, "gilded": 0, "name": "t1_cqzu3wz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3502mg/messageunread_does_not_mark_my_mail_as_read_even/"}, {"author": "GoldenSights", "body": "Oh, I just realized there is a [read_all_messages](http://www.reddit.com/dev/api#POST_api_read_all_messages) endpoint. I feel like \"mark\" should trigger this automatically, but making a separate request isn't too bad.", "created_utc": 1430881634, "gilded": 0, "name": "t1_cqzub63", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/3502mg/messageunread_does_not_mark_my_mail_as_read_even/"}, {"author": "GoldenSights", "body": "This is definitely a Python question over a praw question. Essentially the goal is to take a large string, locate \"@Search:\", and take the next word. I think every person's solution to this would look slightly different, and some would probably go the regex route, but here's the first thing that comes to my mind: def findterm(body): # If the user does \"@search:heyo\" then the following split won't separate the two words # So just replace the colon with the space which will be split. # If this causes problems you'll have to find an alternative body = body.replace(':', ' ') words = body.split(' ') # This assumes that body is already .lower() but you could move that here instead. index = words.index('@search') try: term = words[index+1] # Remove any \\n if this was at the end of a line. term = term.strip() return term except IndexError: return None cterm = findterm(cbody) if cterm: Do your stuff If you want to allow the user to search multiple words by encasing them in quotes, you'll have to make it a little more fancy, but it shouldn't be too bad. This was off the top of my head so I'm sure there are some critiques to be made here, but I hope this gets you on the right track.", "created_utc": 1430814001, "gilded": 0, "name": "t1_cqywffv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34x4ri/parsing_comments/"}, {"author": "jordanzzz", "body": "Yeah, I did end up using the split function there. Here is what I have that is currently working: cbody = comment.body if SETPHRASES in cbody: userInput = cbody mylist=userInput.split(\"@LoLFinder: \") summonerexcess = mylist[1] mylist = summonerexcess.split(\" \") summoner = mylist[0] print (mylist[0]) get_summoner(summoner) That way it splits it up before the @Search part (now @LoLFinder) and reads til the first space afterwards. Still got to do a lot of error checking and what not of course, but it is a start.", "created_utc": 1430815748, "gilded": 0, "name": "t1_cqywsj0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34x4ri/parsing_comments/"}, {"author": "GoldenSights", "body": "Yeah this is what I mean by every solution looking different. I'm sure you'll find things to change, but what you've got is great.", "created_utc": 1430816164, "gilded": 0, "name": "t1_cqywvhl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34x4ri/parsing_comments/"}, {"author": "kpurdon", "body": "Using sudo should be done with caution, you might want to get yourself checked. =) It seems your installing things to your system python site-packages (permission denied tips this off), you should look into and start using [virtualenv](http://docs.python-guide.org/en/latest/dev/virtualenvs/) ASAP. Also pip was bundled with python for the first time in Python 3.4 (you get pip when you get python). What OS are you on? In addition /r/learnpython is a much better place for this question next time.", "created_utc": 1430797339, "gilded": 0, "name": "t1_cqyr5u3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "demonlordghirahim", "body": "this is really the only time I've used sudo because it was confirmed okay on stackoverflow. Should I be okay this time? I'm really overly paranoid when it comes to these things Also I'm on a mac (OS X yosemite). Anyway I got it to work- thanks!", "created_utc": 1430797633, "gilded": 0, "name": "t1_cqyrb3l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "redalastor", "body": "> Should I be okay this time? Nope, listen to what he said about learning virtualenv. You'll get yourself into quite a mess if you don't. If you uses PyCharm as your IDE (which I suggest, it's a great IDE), it can even manage your virtualenv for you.", "created_utc": 1430797976, "gilded": 0, "name": "t1_cqyrh3t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "demonlordghirahim", "body": "what possible damage could I have done? like i have no idea what to look for", "created_utc": 1430798459, "gilded": 0, "name": "t1_cqyrpnh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "redalastor", "body": "To your system, little. To your projects? Your dependencies will be fucked up and you'll have a hard time replicating them on a different machine. It'll work fine as long as you have just one project on one machine. Then all bets are off. virtualenv isolates your dependencies and pins them to a precise version. Once you activate a given virtualenv, you're still using pip as before (minus the sudo) but it install just in that virtualenv, not globally.", "created_utc": 1430798830, "gilded": 0, "name": "t1_cqyrvzo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "demonlordghirahim", "body": "is there a way to reset my system and fix everything? I'm never listening to stackexchange again also how can I get my computer checked?", "created_utc": 1430798905, "gilded": 0, "name": "t1_cqyrx8n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "redalastor", "body": "Your system's fine. If you follow python's best practices you always use a virtualenv so it doesn't matter what packages your system has or doesn't have because you always use the self-contained ones instead.", "created_utc": 1430799031, "gilded": 0, "name": "t1_cqyrzfc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "demonlordghirahim", "body": "okay thanks. I was scared I accidentally downloaded something horrible or totally fucked up my computer. I'll learn more about virtualenv before I do anything more on my computer like that. Thank you for everything!", "created_utc": 1430799181, "gilded": 0, "name": "t1_cqys1vl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "redalastor", "body": "You're welcome. A short primer on how to use it (after you install it, I don't have a mac and no clue how to install stuff on it) : to create the folder where your packages are : virtualenv venv The `venv` is the folder's name, you can call it whatever you want but by convention it's called venv. Put it somewhere close to your project. To activate your virtualenv (your prompt will change once activated): venv/bin/activate To see install packages, just use pip. To see what packages you have : pip freeze To pin down versions: pip freeze > requirements.txt To install all of those dependencies at once (after you move your projects to a different machine): pip install -r requirements.txt To uninstall something: Remove it from the requirements.txt file, delete your `venv` folder, recreate it (with the virtualenv command) use the above \"install everything command\". You can see right there why it's so convenient to have it isolated, uninstalling globally installed stuff is dangerous. Don't forget to always run pip after you activated your virtualenv.", "created_utc": 1430799723, "gilded": 0, "name": "t1_cqysav0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "demonlordghirahim", "body": "you rule I really appreciate everything. One last question - should I uninstall PRAW (using pip) and only have it in a venv?", "created_utc": 1430799872, "gilded": 0, "name": "t1_cqysdgq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "redalastor", "body": "Once you activate a virtualenv, the rest of your system doesn't matter. At its creation, the python executable itself will be copied into the venv folder. Don't touch the system wide libraries. Don't install or uninstall. It's safer this way.", "created_utc": 1430800063, "gilded": 0, "name": "t1_cqysgmz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "demonlordghirahim", "body": "perfect. thanks again!", "created_utc": 1430800426, "gilded": 0, "name": "t1_cqysmbn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "redalastor", "body": "Try [PyCharm](https://www.jetbrains.com/pycharm/), you'll like it. As I said before it can manage your virtualenv (it's in the \"interpreter\" section of your project's setting). Using a virtualenv is second nature to any experienced Pythonista and we rarely notice that there's actually no sign post to guide the new users there.", "created_utc": 1430800686, "gilded": 0, "name": "t1_cqysqc8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "demonlordghirahim", "body": "I'm really annoyed my classes didn't cover this (currently a computer science student). It seems pretty essential! I downloaded PyCharm but I havent had time to really look into starting to use it yet. Honestly you've taught my more than my classes have so thanks for helping me.", "created_utc": 1430810539, "gilded": 0, "name": "t1_cqyvn5d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "redalastor", "body": "You should subscribe to [PyCoders](http://pycoders.com/). It's one email every Friday with interesting articles, useful libraries and the like that came up during the week. If something important came up during the week, you should learn about it there.", "created_utc": 1430828682, "gilded": 0, "name": "t1_cqyzz8l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "kpurdon", "body": "Your totally fine, sudo is totally fine to use with when you're aware of what your doing. Again check out virtualenv (it will let you not use sudo). Good luck.", "created_utc": 1430804543, "gilded": 0, "name": "t1_cqytwzl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "demonlordghirahim", "body": "def checking it out! thanks so much for your help you are a wonderful human being! :)", "created_utc": 1430810442, "gilded": 0, "name": "t1_cqyvm8e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34wfsa/trouble_installing_praw_via_pip/"}, {"author": "ELFAHBEHT_SOOP", "body": "It looks like it's: https://www.reddit.com/api/register/[username] With form data > op: reg >user: [username] > passwd: [password] > passwd2: [password] > email: > api_type: json email looks like it's optional, but it still has to be there. You just don't fill it out if there isn't an email I guess.", "created_utc": 1430746903, "gilded": 0, "name": "t1_cqxxgha", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "BullpenInc", "body": "Ah great, but captcha is required too right?", "created_utc": 1430749344, "gilded": 0, "name": "t1_cqxyn6t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "ELFAHBEHT_SOOP", "body": "I did that request and it worked just fine.", "created_utc": 1430755545, "gilded": 0, "name": "t1_cqy24uc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "xiongchiamiov", "body": "We removed the registration captcha [last year](https://www.reddit.com/r/changelog/comments/2mlr1g/reddit_change_redesign_of_loginaccount_creation/cm5e78f).", "created_utc": 1430784681, "gilded": 0, "name": "t1_cqyjte9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "kemitche", "body": "> I couldn't find anything in the API documentation The registration endpoint is deprecated for API consumers. This goes alongside [asking all API clients to convert to OAuth](https://www.reddit.com/r/redditdev/comments/2ujhkr/important_api_licensing_terms_clarified/); we're moving to an ecosystem wherein *3rd party clients aren't directly asking users to input passwords*. The expected path is that, during your OAuth sign in flow, a new user will get directed to the standard sign in / registration page on reddit.com.", "created_utc": 1430784371, "gilded": 0, "name": "t1_cqyjmmh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "BullpenInc", "body": "Yup I gathered as much, and I rather not have passwords passing through my app... so registration will be supported in OAuth, anytime soon? or will registration through API as a whole be deprecated? Should I use the old end point in the meantime or skip it and simply let them use the registration page.", "created_utc": 1430806091, "gilded": 0, "name": "t1_cqyuell", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "kemitche", "body": "To be clear: You don't need to do anything. Just use OAuth \"normally\". For someone using your app/site, when they start the OAuth flow, they won't have a reddit account, but the normal OAuth flow will send them to a \"sign-in or register\" page, where they will (theoretically) register. Basically, just assume everyone already has a reddit account.", "created_utc": 1430807985, "gilded": 0, "name": "t1_cqyuynx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "BullpenInc", "body": "Ahh, of course, thanks!", "created_utc": 1430817815, "gilded": 0, "name": "t1_cqyx76n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "thatJavaNerd", "body": "Account creation is not supported with OAuth2. All non-OAuth methods were removed from the official documentation, so that's why you couldn't find it.", "created_utc": 1430741943, "gilded": 0, "name": "t1_cqxvi54", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/34t0df/creating_account_through_api/"}, {"author": "GoldenSights", "body": "It's because either you didn't log in to the PRAW session, or you did log in but your account does not have the Over 18 checkbox set. PRAW is expecting to be redirected to a submission, but instead it's getting the \"are you over 18\" page. You can either do r.login with an account that has the checkbox set, or you can do r.http.cookies.set('over18', '1') before asking for a random submission.", "created_utc": 1430301055, "gilded": 0, "name": "t1_cqsgt85", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/349c0d/praw_cant_get_nsfw_submission/"}, {"author": "Mackan90095", "body": "Ah! I'll try that ! Thanks EDIT! That worked! Thanks!", "created_utc": 1430301116, "gilded": 0, "name": "t1_cqsgtpj", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/349c0d/praw_cant_get_nsfw_submission/"}, {"author": "bboe", "body": "You are looking at the docs for an old version of PRAW. This is the current release version doc: https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html?highlight=get_comments#praw.objects.Subreddit.get_comments There is not an explicit \"sort\" parameter to `get_comments` because the `subreddit/comments` endpoint does not provide different sort methods. You may be looking for the comment sort order on a single submission's comments. You can do that via `comment_sort` on the `get_submission` method: https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html?highlight=get_comments#praw.__init__.UnauthenticatedReddit.get_submission", "created_utc": 1430251434, "gilded": 0, "name": "t1_cqrtcbm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/345x51/praw_subredditget_comments_does_not_take_sort/"}, {"author": "ychaouche", "body": "It looks like a bug in in urllib3. Can you tell us what version of requests are you using ? Also, make sure the version you're using is compatible with python3.4 (becasue in request's source code I can see that the line that is giving you errors was meant for python 2.7, and you are running python 3.4)", "created_utc": 1430771336, "gilded": 0, "name": "t1_cqybyhv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33xwym/praw_post_submit_problem/"}, {"author": "ychaouche", "body": "This might also be useful to you : https://github.com/kennethreitz/requests/issues/1915", "created_utc": 1430858103, "gilded": 0, "name": "t1_cqzh48f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33xwym/praw_post_submit_problem/"}, {"author": "Taph", "body": "Good catch. It looks like I'm using requests 2.6.2 though there's a 2.7.0 version available on their site. Doing a bit of Googling shows me that there's apparently a Python 3 compatible version that I can download with apt-get so I'll give that a shot. I figured the version that I had was the latest and was likely Python 3 compatible since pip told me I had the latest version when I tried to upgrade requests. Thanks for the reply.", "created_utc": 1430772140, "gilded": 0, "name": "t1_cqych4l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33xwym/praw_post_submit_problem/"}, {"author": "hargikas", "body": "I get the same error when I am trying to do: subreddit.submit(title, url=link)", "created_utc": 1430604878, "gilded": 0, "name": "t1_cqweymx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33xwym/praw_post_submit_problem/"}, {"author": "Taph", "body": "I never found an actual solution so I just wrapped my function (make_post()) in a try/except statement to catch the errors and keep the script from crashing. Not an ideal solution since I'd like to know what's causing the problems, but everything seems to work so far.", "created_utc": 1430605348, "gilded": 0, "name": "t1_cqwf6fi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33xwym/praw_post_submit_problem/"}, {"author": "GoldenSights", "body": "As long as you're only running a single praw bot, you don't have to worry because praw handles the timer for you. If you were writing your own wrapper, you would have to use those ratelimit headers that are returned by reddit.", "created_utc": 1430017557, "gilded": 0, "name": "t1_cqoujos", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "sunbolts", "body": "So just to confirm, I can just run my PRAW bot and it'll automatically stop sending requests if I've hit the limit until the timer resets?", "created_utc": 1430017645, "gilded": 0, "name": "t1_cqoul5z", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "GoldenSights", "body": "It doesn't stop sending the requests, it just waits 2 seconds in between each one to make sure you're never over the limit. They'll all get done, though. Now, if you're running a bot on a new account and you start to get \"You're doing that too much\" errors, that's *completely different*. That's the limit that stops accounts from commenting too frequently until they start to gain some karma.", "created_utc": 1430017796, "gilded": 0, "name": "t1_cqounni", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "bboe", "body": "Almost correct. PRAW waits up to two seconds before making the next request. It's actually 2 - (current time - last request time). So if your script does some other busy work between requests it may not need to wait at all. For a long time I had been meaning to write something that would support bursty-requests, for long-lived processes (wouldn't work for scripts that are started every few seconds via cron), but I never followed through. With the rate limit changes for OAuth, and the OAuth requirement, PRAW4 will probably provide a burst-mode and utilize the rate-limit headers.", "created_utc": 1430069975, "gilded": 0, "name": "t1_cqpccls", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "sunbolts", "body": "Ah I see, thank you very much for that clarification and for your help. In any case, do you happen to know if there is a way to see the headers anyways, for observational purposes?", "created_utc": 1430017942, "gilded": 0, "name": "t1_cqoupyo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "GoldenSights", "body": "Hmm, that's a good question. Praw puts the requests behind a lot of layers, but as far as I can tell, the actual requests happen [here](https://github.com/voussoir/praw/blob/master/praw/handlers.py#L101). (C:\\python34\\lib\\site-packages\\praw\\handlers.py) So if you were to intercept it with a= self.http.send(request, proxies=proxies, timeout=timeout, allow_redirects=False) print(a) print(a.headers) print(a.cookies) return a You still don't get to play with the actual response, but you can at least read it. Do a couple of requests and you'll see all their headers.", "created_utc": 1430018495, "gilded": 0, "name": "t1_cqouz5b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "sunbolts", "body": "Awesome! Thanks a lot for this info. and for the prompt replies.", "created_utc": 1430018921, "gilded": 0, "name": "t1_cqov5hv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "_Guinness", "body": "Question, if I run multiple bots for multiple subreddits on different accounts but for the same IP, does this come from the pool of same requests? So if I write an MLB game thread bot, and then all the baseball team's subreddits want me to run their bot for them. And I have say...20 bots running for 20 different subreddits on 20 different accounts. Am I limited to 20 bots doing 30 requests per minute, or each bot only doing a request every 90 seconds? I guess what I am asking is this, what happens when your bot becomes really popular? I don't want to violate the rules, but I feel as if this scenario is legitimate. Because whether I run all 20 bots, or each subreddit has someone else running their bot, the total API calls would be the same no matter what. So...letter of the law vs spirit of the law.", "created_utc": 1430231404, "gilded": 0, "name": "t1_cqrghpd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "GoldenSights", "body": "As far as I know, multiple bots hosted in the same place need to follow the 2 second rule all together. I know what you mean about that last sentence though, I think maybe you should ask the admins directly, rather than me speculating. It's definitely a good question.", "created_utc": 1430244254, "gilded": 0, "name": "t1_cqrolk1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33vuok/how_do_you_check_the_ratelimit_usage/"}, {"author": "seriouslulz", "body": "if comment_author is not YourBot: // do your thing", "created_utc": 1429962044, "gilded": 0, "name": "t1_cqo5hzl", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33t71g/praw_how_to_make_a_bot_ignore_certain_users/"}, {"author": "JustLTU", "body": "Thank you very much!", "created_utc": 1429963698, "gilded": 0, "name": "t1_cqo5w9a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33t71g/praw_how_to_make_a_bot_ignore_certain_users/"}, {"author": "JustAnotherUser_1", "body": "Not a python/praw dev - But is it possible to obtain the username? I know there are bots out there that don't reply to themselves, so either they check the username they're replying to, or only reply per 1 parent. If it's possible to obtain the username, then maybe do a bit of POC on a private subreddit? * create own subreddit * install bot * post under your username with keyword * bot replies with \"debug mode\" on\\* * configure your bot, to stop after x comments (to prevent the bot spamming itself) *` keywork matched: . Username is bot? true/false` Hope this helps. ###Edit [Stackoverflow thread might help](http://stackoverflow.com/questions/20822808/praw-comment-submitters-username) >Anyway, a Comment has a author attribute, which is a Redditor instance of the author.", "created_utc": 1429962366, "gilded": 0, "name": "t1_cqo5knh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33t71g/praw_how_to_make_a_bot_ignore_certain_users/"}, {"author": "JustLTU", "body": "Thank you very much, this is just what I needed!", "created_utc": 1429963724, "gilded": 0, "name": "t1_cqo5wh3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33t71g/praw_how_to_make_a_bot_ignore_certain_users/"}, {"author": null, "body": "[deleted]", "created_utc": 1429936436, "gilded": 0, "name": "t1_cqnz6ba", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33rz9o/all_delete_requests_are_returning_403_please_sign/"}, {"author": "GoldenSights", "body": "Yeah, I suppose so. But why would deleting a subreddit be oauth-only when adding is not? The docs page doesn't make any special note of this as far as I can tell. The [source code](https://github.com/reddit/reddit/blob/master/r2/r2/controllers/multi.py#L437) for this doesn't have anything out of the ordinary either, the validations are very similar to the other multi editing tools.", "created_utc": 1429937007, "gilded": 0, "name": "t1_cqnze7l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33rz9o/all_delete_requests_are_returning_403_please_sign/"}, {"author": null, "body": "[deleted]", "created_utc": 1429937373, "gilded": 0, "name": "t1_cqnzj7u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33rz9o/all_delete_requests_are_returning_403_please_sign/"}, {"author": "GoldenSights", "body": "I'm not unsubscribing, I'm removing a subreddit from a multireddit [as per this](http://www.reddit.com/dev/api/oauth#DELETE_api_multi_%7Bmultipath%7D_r_%7Bsrname%7D). I noted in my post that \"Multireddits use a Delete method while everything else uses a Post method\"", "created_utc": 1429937465, "gilded": 0, "name": "t1_cqnzkh8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33rz9o/all_delete_requests_are_returning_403_please_sign/"}, {"author": "radd_it", "body": "This has less to do with PRAW and more to do with reddit's requested 2-second delay between calls. Can you speed it up? Sure, you can: * Be smarter about what you're loading. Do you really need *all* the comments? Many of those \"load more\"s are just a handful of downvoted comments. Maybe you only load one \"more\" for each top-level comment. * Bypass the 2-second delay. If you're just getting *all the comments* from a single post, it's ok to bypass the rate limit in short bursts. Careful, too many pings and reddit will cut you off and/ or ban your IP.", "created_utc": 1429834904, "gilded": 0, "name": "t1_cqmmhqx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33nfe0/prawreplace_more_comments_is_bottlenecking_my/"}, {"author": "bboe", "body": "I suppose the `replace_more_comments` [documentation](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.replace_more_comments) could be a bit more clear. If you set `limit=0`, then all `MoreComments` objects will be removed without making a single additional request.", "created_utc": 1429843761, "gilded": 0, "name": "t1_cqmrl54", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33nfe0/prawreplace_more_comments_is_bottlenecking_my/"}, {"author": "PantlessKitten", "body": "For what's worth, I'm having the exact same issue. Was kinda waiting to see if what /u/bboe ^(ping) suggested worked for you. Tried different logins (with different passwords), no dice. Python 2.7.6 & praw 3.0a1, testing with: import praw r = praw.Reddit(user_agent=\"Quick Login Test\") r.login(\"username\", \"password\", disable_warning=True)", "created_utc": 1429798378, "gilded": 0, "name": "t1_cqlzo49", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Nylese", "body": "Did you ever get this to work?", "created_utc": 1442096715, "gilded": 0, "name": "t1_cuzf857", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "body": "Try removing \"bot\" from the user agent string.", "created_utc": 1429795142, "gilded": 0, "name": "t1_cqly0yq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "I did that and I'm still getting this error.", "created_utc": 1429798111, "gilded": 0, "name": "t1_cqlzist", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "PantlessKitten", "body": "We're noobs! :D Nevermind. Different error. ~~Try adding ````api_domain=\"https://api.reddit.com\"````to praw.Reddit. It should get you past the login. Like so:~~ ~~````r = praw.Reddit(\"Your user agent here\", api_domain=\"https://api.reddit.com\")````~~", "created_utc": 1429799602, "gilded": 0, "name": "t1_cqm0cv7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "Well that certainly got rid of the JSON error.", "created_utc": 1429800083, "gilded": 0, "name": "t1_cqm0n27", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "body": "Unfortunately I am unable to reproduce. You could try outputting the raw response by adding some debugging lines around: https://github.com/praw-dev/praw/blob/master/praw/handlers.py#L147 There might be some interesting information in the response headers: from pprint import pprint pprint(vars(result.headers)) or in the response body: print(result.text) Edit: `result` not `response` Edit2: `text` not `body`", "created_utc": 1429799247, "gilded": 0, "name": "t1_cqm05nj", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "How do I go about putting those in? I just tried and it's telling me headers and response are undefined. I'm a complete noob at this. :(", "created_utc": 1429799451, "gilded": 0, "name": "t1_cqm09tz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "body": ":-/ You have to modify the line after the one I linked to in your local copy of the PRAW source files. Unfortunately, I presently don't have the time to walk you through it but maybe someone else can. Good luck!", "created_utc": 1429799693, "gilded": 0, "name": "t1_cqm0eqy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "http://i.imgur.com/eHpKkXB.png Here's what it says after I modified the lines under the code that I have.", "created_utc": 1429800717, "gilded": 0, "name": "t1_cqm10p2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "PantlessKitten", "body": "Wrong file. Out of curiosity I printed the ````result```` in the line he mentioned and we get an ````OK```` status code. I've never touched python in my life but I'm poking around to see if I can figure it out (I'll let you know if I do).", "created_utc": 1429800849, "gilded": 0, "name": "t1_cqm13lp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "I put the lines of code he wanted directly under the one that he linked. I got zero errors when compiling the handlers.py file to reload it.", "created_utc": 1429801523, "gilded": 0, "name": "t1_cqm1i63", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "PantlessKitten", "body": "Sorry, I'm not sure why I thought it was the wrong one. Too sleepy I guess. Any luck?", "created_utc": 1429804041, "gilded": 0, "name": "t1_cqm32xq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "So far, none. I'm going to sleep soon, though. I sent bboe a screenshot of the full results text. Hopefully that'll be enough to diagnose the problem. I wonder if there's a way to just... ignore the JSON error?", "created_utc": 1429804385, "gilded": 0, "name": "t1_cqm3awp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "PantlessKitten", "body": "> I wonder if there's a way to just... ignore the JSON error? Now I laughed. I did try it, but it fails with a similar error later on (as soon as it tries to deal another JSON response. To be honest I'm just trying to figure out the responses, I was expecting pretty JSON, as a response, but nope. And I'm too noob and sleepy to figure it out. No worries though, we'll get it working tomorrow. ;)", "created_utc": 1429805233, "gilded": 0, "name": "t1_cqm3u65", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "Okay, so apparently there's a Unicode error from the full-text that bboe had me print out from the handler. It's trying to decode something that doesn't exist.", "created_utc": 1429838500, "gilded": 0, "name": "t1_cqmoj6e", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "PantlessKitten", "body": "Ha, good morning! Well that explains why I couldn't figure out the response nor decode it. Thought I was just too dumb to find out how to de-gzip the json response. Do you know how to fix it?", "created_utc": 1429839662, "gilded": 0, "name": "t1_cqmp7w2", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "I'm trying to look through the code and see if I can do anything to maybe change the encode to UTF-8 or ignore the error outright. I don't know why JSON decoding is needed here if there's no JSON file to decode? Or maybe we can't connect to the JSON file or something. I'm brainstorming, but I'm new to Python, so I have no idea what I should be looking for here.", "created_utc": 1429840364, "gilded": 0, "name": "t1_cqmpmpw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "PantlessKitten", "body": "It makes two of us, never touched python before. (And I'm really not that good when it comes to JSON). I just wanted a \"bot\" to go through posts of an old account and wipe them out. Anyway, I'm going to bed (again) in a bit so I guess it's your turn! :P After you went to bed I tried to follow the code (starting with ``__init__.py``). So, it starts in ``__init__``, then do do ``login``. From there it jumps to ``request_json`` with the user and pwd. Right [here](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L551-L552) it sends the login info to the server that replies back with a response. So I sent a json request with ``curl`` to the server and, as expected, got a json response: {\"json\": {\"errors\": [], \"data\": {\"need_https\": true, \"hsts_redir\": \"https://reddit.com/modify_hsts_grant?dest=\", \"modhash\": \"ihzmt8xcpj37498d7ad6c74ae848b80a899fb826d7db82eac8\", \"cookie\": \"16367057,2015-04-23T19:02:44,5ec90f403d58ef96b0cc805e8af0d885082b3a3c\"}}} I tried to print the ````response```` right after ([this here](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L551-L552)) (``self._request(yada yada`` ) and here's what comes back: u'\\x1f\\ufffd\\x08\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x1c\\ufffd\\ufffdN\\ufffd \\x14\\x05\\x7f\\ufffd\\ufffdu\\ufffd\\x05J\\ufffd\\ufffd\\x18\\x7f\\x9d1\\ufffd-\\ufffd\\nO[\\x0c`\\ufffdy\\u9fdbv{\\ufffd\\ufffd", "created_utc": 1429842656, "gilded": 0, "name": "t1_cqmqymt", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "body": "I'm sorry it should be `result` not `response`. Great job finding the file to edit!", "created_utc": 1429803446, "gilded": 0, "name": "t1_cqm2p6j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "Here's what it says now: http://i.imgur.com/BX7avw6.png Hopefully there's nothing secret in that picture.", "created_utc": 1429803765, "gilded": 0, "name": "t1_cqm2wj2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "body": "Well your session was not created so there is nothing secret. Also please replace `body` with `text`. I seldom guess that one right.", "created_utc": 1429807492, "gilded": 0, "name": "t1_cqm5awm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "Here's the error I'm getting now: http://i.imgur.com/dawkGoa.png Sorry that took me a while, I took a very long nap. Edit: So, would just attaching something to make the unicode UTF-8 or something work? Or even just ignoring the error? I'm messing around with the code trying to see if I can catch the unicode exception and make it pass it.", "created_utc": 1429830763, "gilded": 0, "name": "t1_cqmk6e6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "body": "Yeah you can try converting the unicode. I'm only asking you to print it so we can see what it contains. It _should_ be valid json, but it seems in this case it is not. Maybe try: print(result.text.encode('utf-8')) If that doesn't work perhaps you can simply iterate over the characters of the text and output their values. I'll let you figure that out, I just somehow need to see what is being returned to help you any further. If you ignore the error you'll bypass the error but you won't be logged in.", "created_utc": 1429842504, "gilded": 0, "name": "t1_cqmqvk4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "http://i.imgur.com/O7gPwtM.png here's what i get now.", "created_utc": 1429842843, "gilded": 0, "name": "t1_cqmr2fe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "body": "Well that certainly does not look like JSON. My guess is something is not installed properly on your machine. How did you install PRAW?", "created_utc": 1429842958, "gilded": 0, "name": "t1_cqmr4vu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": "pip install praw like the instructions told me to. How exactly should I do this? I can uninstall everything and retry.", "created_utc": 1429843569, "gilded": 0, "name": "t1_cqmrh8m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "body": ":-/ Yeah that seems right. I've never seen that kind of output. I really don't know how to help you further. Sorry.", "created_utc": 1429844358, "gilded": 0, "name": "t1_cqmrx4i", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "Lark_vi_Britannia", "body": ":(", "created_utc": 1429847615, "gilded": 0, "name": "t1_cqmto31", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "bboe", "body": ":(", "created_utc": 1429798645, "gilded": 0, "name": "t1_cqlztf7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33l4jt/using_praw_and_getting_a_json_error_no_json/"}, {"author": "GoldenSights", "body": "To see the attributes of a submission or comment, I would recommend visiting the /api/info link for that item, like this: http://www.reddit.com/api/info.json?id=t3_33eill http://www.reddit.com/api/info.json?id=t1_cqk8l3c On the thread, you're looking for \"selftext\", \"created_utc\", and \"score\". On the comments it's called \"body\" instead of selftext. Upvote ratio is not available for comments. upvote_ratio is available for submissions, but you have to go to it's specific json page instead of api/info for some reason. http://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/.json The 1,000 item limit only applies to item listings, not comment trees. If you're getting comments by /r/redditdev/comments then yes, you're limited to 1,000, but if you're getting the comments off a specific submission you aren't. However, any time there is a \"load 7 more\" button in the browser, that's a spot where PRAW has to make a separate request to fetch those items. This means that large threads can take a very long time to fill in all the comments. To do this, you'll want to call `submission.replace_more_comments`. [Docs](https://praw.readthedocs.org/en/v2.1.21/pages/code_overview.html#praw.objects.Submission.replace_more_comments)", "created_utc": 1429660665, "gilded": 0, "name": "t1_cqk8l3c", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/"}, {"author": "pandoraparadox", "body": "I am really thankful! I will start working with the JSON and the API. Thank you very much!", "created_utc": 1429727520, "gilded": 0, "name": "t1_cql2xzt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/"}, {"author": "Stuck_In_the_Matrix", "body": "Solid advice, but the API won't work for his project. Down votes are no longer shown, so getting a ratio of up / down isn't possible.", "created_utc": 1429686234, "gilded": 0, "name": "t1_cqklkyg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/"}, {"author": "GoldenSights", "body": "You're forgetting about the upvote_ratio attribute [here](http://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/.json). It's still fuzzed, and you can't extrapolate to get true ups-downs, but it's better than nothing. For comments, there is only \"controversiality\", which is not very useful.", "created_utc": 1429686373, "gilded": 0, "name": "t1_cqklmbh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/33eill/praw_need_comments_text_time_upvotes_for_topics/"}, {"author": "GoldenSights", "body": "Hmm, at first I thought I was getting the same error as you, but now I can't reproduce it. Here's what I wrote: def print_comments(comment): print(comment.body) for child in comment.replies: try: print_comments(child) except: pass a=r.get_submission(submission_id='333jjs') print_comments(a.comments[0]) original = praw.helpers.flatten_tree(a.comments) print(len(original)) print([x.body for x in original[:-1]]) print([x.replies for x in original[:-1]]) a.replace_more_comments(limit=None, threshold=0) print_comments(a.comments[0]) now = praw.helpers.flatten_tree(a.comments) print(len(now)) print([x.body for x in now]) print([x.replies for x in now]) results: one two three four five six seven eight nine ten 11 ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten'] [[], [], [], [], [], [], [], [], [], []] one two three four five six seven eight nine ten eleven twelve thirteen 13 ['one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine', 'ten', 'eleven', 'twelve', 'thirteen'] [[], [], [], [], [], [], [], [], [], [], [], [], []] What happens if you run this snippet?", "created_utc": 1429420267, "gilded": 0, "name": "t1_cqh7t3u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "Amablue", "body": "When I run that snippet I get the following output: one two three four five six seven eight nine ten 11 [u'one', u'two', u'three', u'four', u'five', u'six', u'seven', u'eight', u'nine', u'ten'] [[], [], [], [], [], [], [], [], [], []] one two three four five six seven eight nine ten 10 [u'one', u'two', u'three', u'four', u'five', u'six', u'seven', u'eight', u'nine', u'ten'] [[], [], [], [], [], [], [], [], [], []] [Finished in 3.5s]", "created_utc": 1429425454, "gilded": 0, "name": "t1_cqh9jb2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "GoldenSights", "body": "I wonder if it's a version thing? I'm using Python 3.4.2, PRAW 2.1.21. I can see that you're using Py2, so if that's not the issue then praw will need some changes.", "created_utc": 1429426331, "gilded": 0, "name": "t1_cqh9sjn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "scandinavian-", "body": "You code works fine here, Python 2.7.8 and Praw 2.1.21. So it must be Praw version or some weird sort of caching issue with reddit?", "created_utc": 1429434657, "gilded": 0, "name": "t1_cqhbqqy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "GoldenSights", "body": "That's a good point about caching. /u/AmaBlue, what happens if you do r.login() before the scan?", "created_utc": 1429434758, "gilded": 0, "name": "t1_cqhbrhf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "Amablue", "body": "Logging does not seem to make a difference. I get the same output when logged in. However, I did try updating my version of praw like /u/scandinavian- suggested and now I seem to be getting correct results. Yay! Thanks /u/scandinavian-.", "created_utc": 1429471001, "gilded": 0, "name": "t1_cqhpdrc", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/333k7r/replace_more_comments_doesnt_seem_to_do_anything/"}, {"author": "bboe", "body": "The problem is that the comment represented by `\"id\": \"cqewoy7\"` is appearing twice consecutively ([see here](https://api.reddit.com/r/HelpMeFind/comments.json?limit=50)). The `stream_generator` in PRAW decides when to fetch stop yielding 'new' comments based on encountering duplicate IDs, and unfortunately will not respond well to this invalid data.", "created_utc": 1429280529, "gilded": 0, "name": "t1_cqfh5kz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "Icnoyotl", "body": "The bot has been having this problem for awhile, there have been thousands of new comments since it last worked properly, so it seems like this must be something recurring? Could it have something to do with our AutoModerator?", "created_utc": 1429281245, "gilded": 0, "name": "t1_cqfhkcb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "habnpam", "body": "That id points to [this comment made by the Automoderator](http://www.reddit.com/r/HelpMeFind/comments/32v4sj/hmf_whatever_this_cube_thing_is_info_in_comments/). If the problem is that the id appears twice, then can you try to delete one of them?", "created_utc": 1429289357, "gilded": 0, "name": "t1_cqfmmpp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "Icnoyotl", "body": "I just removed it, from my end as a moderator. But, it might still show up to the bot since the bot is a moderator? Not sure about that, but anyway, I ran the bot afterwards and it grabbed 42 and then stopped.", "created_utc": 1429289715, "gilded": 0, "name": "t1_cqfmv4n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "habnpam", "body": "So I ran the actual code that you sent me. The comment with the id \"cqewoy7\" still shows up. When I ran my tests, that comment doesn't show up for me. I am not too familiar with being a mod, but maybe mods can still see moderator-deleted comments. And your bot logs in as a mod, so it sees the comments. I would try to get /u/autoModerator to manually remove his comments. I don't know who maintains that bot, but you could maybe message him with the comment id, and the post that the comment was posted in, and request he remove the comment...? Or you could remove /u/hmf_bot from mod status, but then it wouldn't be able to post the loggings.", "created_utc": 1429292783, "gilded": 0, "name": "t1_cqfotn8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "Icnoyotl", "body": "It looks like it will be too hard to get all the automoderator comments off, I am guessing it has been doing this for awhile and there is no function with automoderator to remove its own comments. So, as of right now, it looks like we will have to wait until the past 500 or whatever don't have any automoderator comments, unless there is a better way to write the points bot. Once this is actually fixed, it will only need to catch...maybe 3 days at maximum, when I can't run the bot, so 500-1000 of the most recent comments would work fine.", "created_utc": 1429305630, "gilded": 0, "name": "t1_cqfwmkt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "bboe", "body": "I think you may be using `comment_stream` for something it's not intended for. The purpose of the comment stream is to get all comments going forward in time. It's really just a bonus that we _can_ fetch some of the history. If you really want accurate comments in the past you could first iterate over all comments on the sub, and then start the stream with a 100 single-iteration limit and handle the duplicates yourself.", "created_utc": 1429298174, "gilded": 0, "name": "t1_cqfs8kx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "habnpam", "body": "It works for me now. Try again. I believe it takes like 30 seconds or so for Reddit to really update.", "created_utc": 1429289834, "gilded": 0, "name": "t1_cqfmxvg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "Icnoyotl", "body": "It still isn't working for me; I noticed that I still needed to update praw so I did that, (it didn't work before or after the update) and now I get a new error: :0: UserWarning: The keyword `bot` in your user_agent may be problematic. C:\\Users\\MyPC\\Anaconda\\lib\\site-packages\\requests\\packages\\urllib3\\util\\ssl_.py:79: InsecurePlatformWarning: A true SSLContext ob ject is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. For m ore information, see https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning. InsecurePlatformWarning But, it does still run after that, it just only loads in 43. Is yours successfully pulling in the 1,000?", "created_utc": 1429290207, "gilded": 0, "name": "t1_cqfn689", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "habnpam", "body": "Yeah, Reddit doesn't want people using the word \"bot\" anymore. Go to line 48. It should look like this: self.reddit = praw.Reddit(SUBREDDIT_NAME + ' bot') The convention that I use to name my bots is: self.reddit = praw.Reddit(\"/u/my_username short-description-of-what-my-bot-does-goes-here\") d --- This is the line I used: for c in comment_stream(r, \"HelpMeFind\", limit=10000): The names of the variables are a little different,but it's all the same. And yes I do get the 1000 comments.", "created_utc": 1429290934, "gilded": 0, "name": "t1_cqfnmn0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32wnhw/praw_comment_stream_messes_up_when_getting/"}, {"author": "GoldenSights", "body": "You'll want to use submission.comments, but on large threads you'll also have to fill in the comments that are behind \"load x more\" buttons. To get all the comments on a thread would be like this: submission = r.get_submission(submission_id='32rc86') submission.load_more_comments(limit=None, threshold=1) And now `submission.comments` contains all the comments in a tree structure. Trees aren't as easy to iterate through, so if you want a list of all comments, commentlist = praw.helpers.flatten_tree(submission.comments)", "created_utc": 1429150973, "gilded": 0, "name": "t1_cqdwglj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32rc86/praw_how_do_i_get_the_comments_to_a_particular/"}, {"author": "scarface1993", "body": "So, if i want the comments i do submission= r.get_submission(submission_id=\"\") submission.load_more_comments(limit=None,threshold=1) commentList=praw.helpers.flatten_tree(submission.comments) Will that give me all the comments in the post ?", "created_utc": 1429151555, "gilded": 0, "name": "t1_cqdwuoh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32rc86/praw_how_do_i_get_the_comments_to_a_particular/"}, {"author": "GoldenSights", "body": "Yeah, but on large threads it will be extremely slow.", "created_utc": 1429152310, "gilded": 0, "name": "t1_cqdxc66", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/32rc86/praw_how_do_i_get_the_comments_to_a_particular/"}, {"author": "gschizas", "body": "By further investigation, it seems that it might be intentional (or simply obsolete), although I don't understand why. Since the standard login is being deprecated, a scope should exist for all API methods. I've found the following methods that actively don't use any kind of scope: @decorators.restrict_access(scope=None, login=True) def accept_moderator_invite(self, subreddit): @decorators.restrict_access(scope=None, login=True) def select_flair(self, item, flair_template_id='', flair_text=''): @decorators.restrict_access(scope=None, mod=True) def get_banned(self, subreddit, user_only=True, *args, **kwargs): @decorators.restrict_access(scope=None, mod=True) def get_mod_queue(self, subreddit='mod', *args, **kwargs): @decorators.restrict_access(scope=None, mod=True) def get_reports(self, subreddit='mod', *args, **kwargs): @decorators.restrict_access(scope=None, mod=True) def get_spam(self, subreddit='mod', *args, **kwargs): @decorators.restrict_access(scope=None, mod=True) def get_unmoderated(self, subreddit='mod', *args, **kwargs): @decorators.restrict_access(scope=None, mod=True) def get_wiki_banned(self, subreddit, *args, **kwargs): @decorators.restrict_access(scope=None, mod=True) def get_wiki_contributors(self, subreddit, *args, **kwargs): According to the API, they should have the following OAuth scope: method|API call|scope ---|---|--- accept_moderator_invite|api/accept_moderator_invite|modself get_banned|r/*subreddit*/about/banned/|read get_mod_queue|r/*subreddit*/about/modqueue/|read get_reports|r/*subreddit*/about/reports/|read get_spam|r/*subreddit*/about/spam/|read get_unmoderated|r/*subreddit*/about/unmoderated/|read get_wiki_banned|r/*subreddit*/about/wikibanned/|read get_wiki_contributors|r/*subreddit*/about/wikicontributors/|read select_flair|api/selectflair/|flair I know now what I have to tamper with, but I'd appreciate a sanity check :)", "created_utc": 1428588794, "gilded": 0, "name": "t1_cq6j509", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31zs3a/praw_get_mod_queue_doesnt_work_with_oauth2/"}, {"author": "kemitche", "body": "Looks about right. You'll know pretty quick if you get it wrong, too ;)", "created_utc": 1428598529, "gilded": 0, "name": "t1_cq6pdf7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31zs3a/praw_get_mod_queue_doesnt_work_with_oauth2/"}, {"author": "gschizas", "body": "I've already tried it and now it works. I've only tested **get_mod_queue** (and it's going to be easy to test all \"read\"), but I don't expect to have anything wrong. UPDATE: I get HTTP 403 in get_banned, get_wiki_banned and get_wiki_contributors. I *should* make some kind of test, but in order to understand what's really going on, I need to convince Python that [fiddler](http://getfiddler.com/)'s certificates really should be trusted... :)", "created_utc": 1428598987, "gilded": 0, "name": "t1_cq6poy9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31zs3a/praw_get_mod_queue_doesnt_work_with_oauth2/"}, {"author": "gschizas", "body": "1. You can just print the url, but I'm not sure what to do with regard to refreshing. 2. You can use multiple scopes, by putting them in a list.", "created_utc": 1428583421, "gilded": 0, "name": "t1_cq6gl8g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31z8g6/questions_regarding_converting_from_userpass_to/"}, {"author": "zzpza", "body": "Cool, thanks! :)", "created_utc": 1428601798, "gilded": 0, "name": "t1_cq6rm5y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31z8g6/questions_regarding_converting_from_userpass_to/"}, {"author": "kemitche", "body": "For now, keep doing what you're doing. I'm hoping to work with PRAW to seamlessly support username/password auth for simple scripts/bots by pulling the stuff from https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example into PRAW", "created_utc": 1428598451, "gilded": 0, "name": "t1_cq6pbi4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31z8g6/questions_regarding_converting_from_userpass_to/"}, {"author": "zzpza", "body": "Ah, that makes sense! It did seem a little complex for simple scripts. Thank you :D", "created_utc": 1428601907, "gilded": 0, "name": "t1_cq6roy0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31z8g6/questions_regarding_converting_from_userpass_to/"}, {"author": "rram", "body": "You were blocked because your crawler was hitting us on aggregate well in excess of our [API ratelimits](https://github.com/reddit/reddit/wiki/API#rules). Please do not crawl our site in excess of our ratelimits. I've removed your block.", "created_utc": 1428606089, "gilded": 0, "name": "t1_cq6uj1m", "num_comments": null, "score": 8, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/"}, {"author": "MrFanzyPanz", "body": "First of all, thank you for removing the block! > on aggregate Can you elaborate a bit more on what this means? My team and I have more than one server pulling data, but we tried not to make it excessive. Also, I didn't see anything in the rules against using multiple servers to pull data, so long as the 30 hits per minute rule applied to each individual server. We do not want to violate the API rules, we just need a lot of data.", "created_utc": 1428611400, "gilded": 0, "name": "t1_cq6y4ur", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/"}, {"author": "Stuck_In_the_Matrix", "body": "IF you haven't converted to oauth, I highly recommend doing so.", "created_utc": 1428534315, "gilded": 0, "name": "t1_cq5wrnt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/"}, {"author": "IAMA_YOU_AMA", "body": "I don't think it's an internal problem with praw. Reddit's servers have been a little less than stellar lately and just simple browsing has returned me more 500 errors than usual. Also last week when reddit released \"the button\" for April Fools, I'm pretty sure that caused a lot more traffic than normal. This is purely anecdotal, though.", "created_utc": 1428526076, "gilded": 0, "name": "t1_cq5rlem", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/"}, {"author": "damontoo", "body": "I don't think the button has anything to do with it. The button servers are separate from the Reddit app/db servers and the press it generated wasn't unusually high. Speaking of press, why don't you go ahead and do that you dirty non-presser!", "created_utc": 1428567893, "gilded": 0, "name": "t1_cq6ck6f", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31wpmp/praw_suddenly_returning_endless_http_errors/"}, {"author": "GoldenSights", "body": "What you're looking for is `comment.delete()` Also, it's good to encase your While loop inside a try-except so if reddit times out your script won't crash. That would just be: import traceback try: while True: ... except: traceback.print_exc() print('Resuming in 10...') time.sleep(10) You can catch individual praw exceptions, but this simplified version will catch *any* error and just ignore it. You may also prefer to just build this loop into your regular bot script, maybe making it only run after 10 usual loops, or something. That way you don't have to have two separate bots with separate praw instances.", "created_utc": 1428450973, "gilded": 0, "name": "t1_cq4qfyu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31t6py/autodelete_downvoted_bots_comments/"}, {"author": "TomSparkLabs", "body": "Thanks! I figured it would be something as simple as that but, as I said, the PRAW documentation is confusing to me. Also, thanks for the try-except suggestion, will be adding that shortly!", "created_utc": 1428451232, "gilded": 0, "name": "t1_cq4qljc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31t6py/autodelete_downvoted_bots_comments/"}, {"author": "emilvikstrom", "body": "The try-except blockbshould be inside the loop if we really want it to continue after the sleep.", "created_utc": 1428468465, "gilded": 0, "name": "t1_cq50whb", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31t6py/autodelete_downvoted_bots_comments/"}, {"author": "TomSparkLabs", "body": "That was the first thing I noticed. Sneak peek at my code: # Removal loop while True: try: for comment in user.get_comments(limit=None): if comment.score", "created_utc": 1428483236, "gilded": 0, "name": "t1_cq55gio", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31t6py/autodelete_downvoted_bots_comments/"}, {"author": "GoldenSights", "body": "Oops, you're completely right. Make sure to tell OP as well, thank you.", "created_utc": 1428468535, "gilded": 0, "name": "t1_cq50xkm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31t6py/autodelete_downvoted_bots_comments/"}, {"author": "hansolo669", "body": "Make sure to take a good read through the [PRAW docs](http://praw.readthedocs.org/en/latest/), and also the [reddit API docs](https://github.com/reddit/reddit/wiki/API), specifically the [JSON section.](https://github.com/reddit/reddit/wiki/JSON) I assume you're doing something along the lines of: for comment in submission.comments: print comment When you need to do (something along the lines of): for comment in submission.comments: print comment.body When you get a submission PRAW returns an object containing all the comments, each of those comments itself is an object containing attributes that correspond 1 to 1 with what's outlined in the reddit JSON docs.", "created_utc": 1428394740, "gilded": 0, "name": "t1_cq3x7m4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31pxl5/praw_get_comments_without_character_limit/"}, {"author": "gswhoops22", "body": "Thanks a lot!!", "created_utc": 1428399540, "gilded": 0, "name": "t1_cq3y7gx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/31pxl5/praw_get_comments_without_character_limit/"}, {"author": "bsimpson", "body": "Are you going to run this on the site, or are you just building it for your own sake? Bots like this are really annoying.", "created_utc": 1428086857, "gilded": 0, "name": "t1_cq07qvm", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/310mji/suggestions_for_grammar_bot/"}, {"author": "spoiler-walterdies", "body": "Yes, I have a suggestion. I can see that you're running locally, so I'm going to assume that you're running this bot more than once in a day. In that case, I'd suggest you to make the bot not reply the same comment more than once.", "created_utc": 1428593064, "gilded": 0, "name": "t1_cq6lq0q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/310mji/suggestions_for_grammar_bot/"}, {"author": "shaggorama", "body": "Your UA string should always contain your username. This will make it at least a little unique and also make it easy for the admins to contact you if they need to. I've actually been contacted like this before because they wanted to let me know about a change they were making to the API that would affect my bot. I was very impressed with the courtesy. My UA strings usually look something like * \"/u/shaggorama prototyping some shitty idea\" * \"/all/comment scraper for /u/someshittybot by /u/shaggorama\" * \"/u/shaggorama has nothing better to do tonight than scrape reddit\" etc.", "created_utc": 1427770214, "gilded": 0, "name": "t1_cpw9xko", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/"}, {"author": "IAMA_YOU_AMA", "body": "cool, thanks for the tip.", "created_utc": 1427771339, "gilded": 0, "name": "t1_cpwal78", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/"}, {"author": "shaggorama", "body": "FYI: This tip actually comes from the [praw documentation](https://github.com/reddit/reddit/wiki/API). Their full user-agent advice: >Change your client's User-Agent string to something unique and descriptive, including the target platform, a unique application identifier, a version string, and your username as contact information, in the following format: >:: (by /u/) > >* Example: User-Agent: android:com.example.myredditapp:v1.2.3 (by /u/kemitche) >* Many default User-Agents (like \"Python/urllib\" or \"Java\") are drastically limited to encourage unique and descriptive user-agent strings. >* Including the version number and updating it as your build your application allows us to safely block old buggy/broken versions of your app. >* NEVER lie about your user-agent. This includes spoofing popular browsers and spoofing other bots. We will ban liars with extreme prejudice.", "created_utc": 1427773630, "gilded": 0, "name": "t1_cpwbs8a", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/"}, {"author": "GoldenSights", "body": "My primary Useragent contains the word \"bot\", but it's also about 200 characters. I have helped several people who couldn't run any scripts, and found that they had a ua like \"bot test\". I think it's just a reminder to be descriptive.", "created_utc": 1427768043, "gilded": 0, "name": "t1_cpw8nb1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/"}, {"author": "IAMA_YOU_AMA", "body": "oh ok. Yeah, my ua is also fairly non-descriptive.", "created_utc": 1427769586, "gilded": 0, "name": "t1_cpw9kp6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/"}, {"author": "bboe", "body": "I added the warning because there were a fair number of posts with people unable to login because of their user-agent that happened to contain the word bot. Since the warning's addition, I've only seen a few of these sort of questions :).", "created_utc": 1428121849, "gilded": 0, "name": "t1_cq0p4v5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30vp0e/warning_in_praw_about_using_the_keyword_bot/"}, {"author": "PixelOrange", "body": "Error handling is a weak point for me. wrapping the entire script in error handlers will not give you expected results though. Have the part you want go retry be it's own function. On error, recall that function.", "created_utc": 1427648392, "gilded": 0, "name": "t1_cpulit0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30oqky/what_is_the_best_way_to_handle_errors_and/"}, {"author": "howbigis1gb", "body": "Thanks - that might just work, only - I don't know if it will just go into a loop for me and I'm not sure how to handle that. Perhaps I should keep track of my failures and try again - only that's only feasible for posts and not for comments. I really do need to get comments in order.", "created_utc": 1427650102, "gilded": 0, "name": "t1_cpumdk4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30oqky/what_is_the_best_way_to_handle_errors_and/"}, {"author": "kemitche", "body": "So there's a ton of things you can do here. At a high level, what you actually want to do is have a database (sqlite is a good starting point), not a CSV, so you can lookup/track which comments you've already gotten the data from (including, perhaps, child comments that you know of and have downloaded). For the code you've got here, my primary suggestion is to be more targeted with your `except` clauses. As written, you're going to catch *everything*, even silly bugs you accidentally write as you update things, which is going to make debugging a pain. Put your try/excepts around the specific code you expect to fail sometimes, and catch only the exceptions you're expecting to happen. That gives you the best shot of improving your code so that you get useful data out.", "created_utc": 1427735752, "gilded": 0, "name": "t1_cpvowpo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30oqky/what_is_the_best_way_to_handle_errors_and/"}, {"author": "howbigis1gb", "body": "I was going to write into a DB, but I decided against it for 2 reasons 1) I wanted to make the dataset available and figured CSV is good. I think a JSON/XML might be better, but I might face other issues with importing. I guess I could get around that by writing to both a JSON and a CSV, and it would add only a trivial amount of time to my code. 2) It adds complexity, while I could easily import the CSV into my database instead As for the second part of your suggestion - to use more targeted except, I think it's a great suggestion. But building upon that, do you have a suggestion for how to handle and retry errors? I could catch them, but I don't know what to do if I do catch them (and I'm not sure if being unable to get a comment causes an error) On another note - I came across this link blog.thehumangeo.com/2014/09/23/supercharging-your-reddit-api-access/ And I'm thinking it might be helpful, but I'm not sure", "created_utc": 1427745661, "gilded": 0, "name": "t1_cpvv7d9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30oqky/what_is_the_best_way_to_handle_errors_and/"}, {"author": "kemitche", "body": "Just to clarify, I'm advocating the use of a DB or some other lookup table *just for while you're gathering the data* as a way to check what comments you have or have not already downloaded, so you can continue after a crash without refetching data you already have. That's potentially separate from a CSV file that you distribute later. > But building upon that, do you have a suggestion for how to handle and retry errors? Separate your code into sections based on types of errors that might happen. For example, fetching the data you might get network errors; writing the data to CSV or DB, you might get disk errors, or duplicate key errors, etc. Those scenarios need to be handled differently. 1. Fetch data 2. Record data 3. Store that you've recorded that data If your program crashes, fix the issue, read the last value written in step 3, and start over from step 1. Retrying: Attempt to fetch things X number of times, with [exponential backoff](http://en.wikipedia.org/wiki/Exponential_backoff) in between attempts. Handling errors: Depends on the type of error. For 500 errors from reddit, exponential backoff is all you need. 4xx errors mean you probably have a bug in your code, so you should crash and figure out what to do differently. What errors are you seeing/concerned about?", "created_utc": 1427751052, "gilded": 0, "name": "t1_cpvyjx6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30oqky/what_is_the_best_way_to_handle_errors_and/"}, {"author": "howbigis1gb", "body": "Exponential backoff is a great idea. Thanks for the suggestion. I'll have a look at it and come back if I have any issues. Thanks", "created_utc": 1427779766, "gilded": 0, "name": "t1_cpwee1h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/30oqky/what_is_the_best_way_to_handle_errors_and/"}, {"author": "exoendo", "body": ">I run multiple bots . >keeps throwing the HTTP 429 There is your problem. 429 means you are being rate limited by reddit, which means you are making too many requests at once. Typically you are only allowed 30 requests a minute. The reason you aren't having problems when you are running it locally is because you are using a different IP address from the one your server has. If this is only happening to this one specific bot, my guess is it probably makes far more requests than your other bots, or does things more frequently. OR your other bots are already at the tipping point for total requests being made and starting this bot up is putting you over the rate-limiting edge. Your solutions are, get additional hosting (or run it locally) or write your script in a way that either doens't go beast mode with the requests, or you can just wrap everything in a try...except and catch the 429 errors and just sleep for a few minutes before continuing on with your script. --- edit: one last thing, if your user agent actually has the word 'bot' in it, I have heard/seen that this could lead to all sorts of problems with running praw scripts. So try taking that word out of your user agent.", "created_utc": 1427242870, "gilded": 0, "name": "t1_cpppedw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "AnSq", "body": "[praw-multiprocess](https://praw.readthedocs.org/en/v2.1.20/pages/multiprocess.html) can handle proper rate limiting of multiple bots running at the same time.", "created_utc": 1427263766, "gilded": 0, "name": "t1_cppznbn", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "noobit", "body": "Interesting, thanks for the link. I had a question on this line, though - > All requests made through the same praw-multiprocess server will respect reddit\u2019s API rate-limiting rules When it says \"same\" praw-multiprocess server - is there some unique string or something I need to pass the handler to ensure it's using the same one as all my other scripts...? Or does using `MultiprocessHandler()` without arguments always generate the same one or something? --- (I only ask because right after that it has this example for how to connect to a \"specific\" praw-multiprocess server: > If instead, you wish to connect to a praw-multiprocess server running at address 10.0.0.1 port 65000 then you would create the PRAW instance via > `handler = MultiprocessHandler('10.0.0.1', 65000)` Which makes it sound like Reddit has more than one or like I can create my own, and I need to specify which one I'm connecting to or something)", "created_utc": 1431654107, "gilded": 0, "name": "t1_cr9k511", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "AnSq", "body": "`MultiprocessHandler()` with no arguments will connect to a praw-multiprocess server running on localhost port 10101, i.e., one running on the same computer as the script, and started with no arguments. If you want, you can start the multiprocess server on another port, like `praw-multiprocess --port 22233`, and then connect to it using `MultiprocessHandler(port=22233)`. If you have multiple computers on a LAN all running praw scripts (i.e., they're all accessing reddit from the same public IP), then you'll need to have a centralized praw-multiprocess server for the whole LAN. Start praw-multiprocess on one of the computers, and have all the others connect to it with `MultiprocessHandler(host='server_LAN_IP_here')`. For example, say you have a LAN of two computers with local addresses 192.168.1.2 and 192.168.1.3 named Two and Three respectively. On Two you might run `praw-multiprocess --port 23456`. On Three you would connect to it using `MultiprocessHandler('192.168.1.2', 23456)`. On Two you can connect using the same thing, or with `MultiprocessHandler(port=23456)`, since the multiprocess server is on the same computer. To actually make the connection, you need to pass the result of `MultiprocessHandler()` to `Reddit()`. A complete example might look like this: handler = praw.handlers.MultiprocessHandler('192.168.1.2', 23456) r = praw.Reddit(user_agnet, handler=handler) In practice I have a function in each of my scripts to pick the right handler and make the connection: def reddit_connect(useragent, multi=False): \"\"\"connect to reddit\"\"\" handler = None if multi: handler = praw.handlers.MultiprocessHandler() print \"Connecting to praw-multiprocess\" else: handler = praw.handlers.DefaultHandler() r = praw.Reddit(useragent, handler=handler) return r and then I have a little extra around where I call it to make it a simple command line option: multi = False if \"--multi\" in sys.argv or \"-m\" in sys.argv: multi = True r = reddit_connect(user_agent, multi) Reddit doesn't provide praw-multiprocess servers for you to use. You have to start your own. If you have any more questions, feel free to ask.", "created_utc": 1431672652, "gilded": 0, "name": "t1_cr9sbpo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "noobit", "body": "> `MultiprocessHandler()` with no arguments will connect to a praw-multiprocess server running on localhost port 10101 I see - so it's something I have to have running before invoking `MultiProcessHandler()` from any scripts. > Reddit doesn't provide praw-multiprocess servers for you to use. You have to start your own. Ohh - I was thinking 'server' meant it was already communicating with reddit or something, to get my PRAW usage or such. This makes a lot more sense. --- > If you have any more questions, feel free to ask. Thanks! I have a couple more general questions. 1. What's the best way to handle bots with different timings - say I want bot 1 to run once every 2 minutes, and bot 2 to run every 10 seconds. Is this something `praw-multiprocess` can understand, or do I do something else to set that up? 2. What do you use to run `praw-multiprocess` or your bots on startup? I looked into adding them to `.bashrc` (am on linux) but think that only runs when I log in.", "created_utc": 1431761053, "gilded": 0, "name": "t1_craun02", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "AnSq", "body": "> I see - so it's something I have to have running before invoking `MultiprocessHandler()` from any scripts. Yeah, basically. *** >What's the best way to handle bots with different timings - say I want bot 1 to run once every 2 minutes, and bot 2 to run every 10 seconds. Is this something praw-multiprocess can understand, or do I do something else to set that up? praw-multiprocess basically just handles rate limiting while proxying requests to reddit, so yes it can understand it, although it can't set it up. In that case I'd just have two different scripts that connect to reddit with `MultiprocessHandler()`, and then have infinite loops that sleep for 10 or 120 seconds after each iteration, where the main body of the loop is whatever you want the bot to do (or more likely, a function call to it). >What do you use to run praw-multiprocess or your bots on startup? I looked into adding them to .bashrc (am on linux) but think that only runs when I log in. [I have a script set to run through mate-session-properties (also on Linux) when I log in](https://gist.github.com/AnSq/41aab9531e98de3bc518) (The reddit stuff is at the bottom). My setup is pretty specific to me though. You could put something in .bashrc (or [.bash_profile](http://www.joshstaiger.org/archives/2005/07/bash_profile_vs.html) more likely), but that might lead to strange effects if you log in twice. If you want to literally start it on boot, before you even log in, there are also ways to do that, such as [rc.local](https://www.raspberrypi.org/documentation/linux/usage/rc-local.md) (it will run your programs as root though, so be careful).", "created_utc": 1431792920, "gilded": 0, "name": "t1_crb4348", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "noobit", "body": "> In that case I'd just have two different scripts that connect to reddit with MultiprocessHandler(), and then have infinite loops that sleep for 10 or 120 seconds after each iteration, where the main body of the loop is whatever you want the bot to do (or more likely, a function call to it). Got it. I figured this was the 'naive' approach, and wanted to know if there were any best-practice implementations (using multiprocess or otherwise) that better programmers than myself would use. (I guess I can import the values from some config script or such) > If you want to literally start it on boot, before you even log in, there are also ways to do that, such as rc.local (it will run your programs as root though, so be careful). Ah, yeah - running as root sounds like something to avoid. Thanks for the rc.local and .bash_profile tips though, I'll look into them - there's gotta be some straightforward \"on-boot\" task manager I'm missing, using those as search keywords is much more likely to find me the one that would be best suited to this or that people use for deployed stuff. Thanks again for your help!", "created_utc": 1431820805, "gilded": 0, "name": "t1_crbh81a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/305o0j/praw_getting_httperror_429_too_many_client/"}, {"author": "GoldenSights", "body": "If you only need to deflair the easily visible posts -- those found in the /new or /top queues, it's just a matter of using `subreddit.get_new(limit=None)` and [each variation of get_top](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.objects.Subreddit.get_top_from_all) to get the posts, and doing `submission.set_flair('', '')` to clear the text and css. If you want a deep cleanse, we'll have to do something different. I have a script that gets every post on the subreddit, which I can use to build a file of post IDs for you, and then you can use those IDs to get the Submissions and deflair them. Let me know which route you want to take, and if I can provide any snippets for you.", "created_utc": 1427002526, "gilded": 0, "name": "t1_cpmpt48", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "5loon", "body": "Thanks man! If you could provide the snippets that collect all of the post IDs that'd be amazing. Also, this isn't urgent so don't go out of your way.", "created_utc": 1427007217, "gilded": 0, "name": "t1_cpmr7sf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "GoldenSights", "body": "That would be [this one](https://github.com/voussoir/reddit/blob/master/Prawtimestamps/timesearch.py), but I haven't updated PRAW in so long I don't know if it works without modifying anything. If it doesn't work I'd be happy to run it for you if you tell me the subreddit name.", "created_utc": 1427007337, "gilded": 0, "name": "t1_cpmr8xf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "5loon", "body": "Considering I'll be running the script in the future, I'll try to get it down myself. I tried it on 2.7 and 3.3.3 but I'm still getting the error: File \"C:\\Users\\me\\Desktop\\timesearch.py\", line 84 print ('Too few results, increasing interval', end='') ^ SyntaxError: invalid syntax What version are you running?", "created_utc": 1427008782, "gilded": 0, "name": "t1_cpmrm19", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "GoldenSights", "body": "All of my scripts are done in 3.4. You could delete the `end=''` part, the printouts just won't look exactly as they are supposed to but it doesn't matter. If that's the only error you hit then don't worry about upgrading or anything.", "created_utc": 1427008985, "gilded": 0, "name": "t1_cpmrns0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "5loon", "body": "Now I'm getting Traceback (most recent call last): File \"C:\\Users\\me\\Desktop\\timesearch.py\", line 172, in main() File \"C:\\Users\\me\\Desktop\\timesearch.py\", line 144, in main sub = input() File \"\", line 1, in NameError: name 'subredditnamehere' is not defined I even updated to 3.4 and installed praw under it. Same error.", "created_utc": 1427011966, "gilded": 0, "name": "t1_cpmsbla", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "GoldenSights", "body": "I'm very confused, there's nothing in my program called \"subredditnamehere\". Have you made any modifications to it?", "created_utc": 1427012304, "gilded": 0, "name": "t1_cpmsdzd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "5loon", "body": "~~Where do I enter the subreddit name? I guess I was screwing that bit up.~~ EDIT: Got it, noticed it was input() and not raw_input(). Thanks for your help!", "created_utc": 1427012645, "gilded": 0, "name": "t1_cpmsghy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "GoldenSights", "body": "When you launch the script it will ask you to type in a name, then a time to start searching from, which you will leave blank. Of course, make sure it launches using the correct version of Python as well. When it's done, you'll have a file called \"subredditname.db\", which contains all of the posts. Then, you'll need to create another script that does the deflairing. I think this should work: import sqlite3 import praw sql = sqlite3.connect('subredditname.db') cur = sql.cursor() r=praw.Reddit('useragent stuff') r.login('username', 'password') cur.execute('SELECT * FROM posts WHERE flair_text IS NOT NULL OR flair_css_class IS NOT NULL') fetch = cur.fetchall() fetch = ['t3_' + i[1] for i in fetch] while len(fetch) > 0: submissions = r.get_info(thing_id=fetch[:100]) for submission in submissions: submission.set_flair('', '') fetch = fetch[100:] but I haven't actually tested that.", "created_utc": 1427013016, "gilded": 0, "name": "t1_cpmsj5j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "5loon", "body": "Thanks for the deflairing script! I got the original script working until it gives the error: nowstamp = datetime.datetime.now(datetime.timezone.utc).timestamp() AttributeError: 'module' object has no attribute 'timezone' Is this a 3.4 issue? The script creates the subreddit.db file but stops at the time and datetime error", "created_utc": 1427013622, "gilded": 0, "name": "t1_cpmsnfz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "GoldenSights", "body": "That error tells me that it's not running in 3.4. I would start the script from the commandline and specify which python.exe to use.", "created_utc": 1427013699, "gilded": 0, "name": "t1_cpmsnzi", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "5loon", "body": "Oh boy doing this at 2am isn't helping. I finally got it working. Thanks for all of your help!", "created_utc": 1427013936, "gilded": 0, "name": "t1_cpmspna", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "GoldenSights", "body": "I know how that feels! No problem at all.", "created_utc": 1427013979, "gilded": 0, "name": "t1_cpmspy3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zvl1x/how_could_i_go_about_removing_every_flair_on/"}, {"author": "kemitche", "body": "What you're seeing is the effect of our CDN caching this page for logged out users - you'll see the same data for 2-5 minutes when logged out. Logged in, say, on your browser, it moves faster, because we can't & don't cache things the same way.", "created_utc": 1426699696, "gilded": 0, "name": "t1_cpiyds0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "Isolder", "body": "Ah okay.", "created_utc": 1426706758, "gilded": 0, "name": "t1_cpj2vvr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "faitswulff", "body": "Huh, interesting, looks like if you refresh it manually in-browser, it updates consistently: https://www.reddit.com/r/all/comments.json What interval are you currently using?", "created_utc": 1426649969, "gilded": 0, "name": "t1_cpig0p3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "Isolder", "body": "I was just trying it in the browser at first and earlier it wasn't updating at all. I was waiting about 10 seconds in between attempts. I just tried again and now it's doing it just fine. Strange.", "created_utc": 1426652976, "gilded": 0, "name": "t1_cpihdvg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "Mobilpadde", "body": "I remember reading somewhere, that you'd have to be logged in, in order to get fresh updates on the comments.json page, otherwise, you'd have to wait for like thirty seconds or so. Though I can't seem to find the thread again. I know this isn't going to help you in anyway, just wanted to tell you, sorry!", "created_utc": 1426694807, "gilded": 0, "name": "t1_cpiv9dn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "Isolder", "body": "Helped a lot actually. Thanks.", "created_utc": 1426706775, "gilded": 0, "name": "t1_cpj2wa5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "Mobilpadde", "body": "Oh, well! Just happy to help! I hope you succeed in whatever you're trying to do.", "created_utc": 1426713737, "gilded": 0, "name": "t1_cpj7ecl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "wscottsanders", "body": "I believe there is a set time period for the site to refresh itself. I think it's about every 30 minutes but take that with a grain of salt as I can't seem to find where I read that.", "created_utc": 1426689157, "gilded": 0, "name": "t1_cpiryyw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2zf5b1/whats_the_proper_way_to_call_rallcommentsjson/"}, {"author": "dakta", "body": "For that function, it's a wrapper on `get_content()` so you can find the accepted args under that function: http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit.get_content", "created_utc": 1426488247, "gilded": 0, "name": "t1_cpgd2lv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2z6rzn/praw_where_do_we_find_the_list_of_kwargs_for_a/"}, {"author": "Eathed", "body": "It does not appear to be possible with praw. [The API does allow you to edit those settings](https://www.reddit.com/dev/api#POST_wiki_settings_{page}), but it appears that praw has not implemented it yet. I may be wrong so you are more than welcome to investigate further. [Here](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_wiki_page) is the code for getting a wiki page in praw. [Here](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.objects.WikiPage) is a description of the WikiPage object. It appears that you can only create a page/update a page's content.", "created_utc": 1426306403, "gilded": 0, "name": "t1_cpeekv7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yys8k/praw_making_a_bot_change_the_who_can_edit_this/"}, {"author": "kemitche", "body": "using virtualenv or something, it looks like? In that virtualenv, do you need to install/upgrade six.moves.urllib? Some googling for the error got me this: https://github.com/influxdb/influxdb-python/issues/6", "created_utc": 1426264627, "gilded": 0, "name": "t1_cpdsgfj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yx982/praw_not_working_anymore/"}, {"author": "ibbignerd", "body": "Sorry, I forgot to mention I'm running on pythonanywhere.com. I'm already on the most recent versions. Unless I'm missing something, most of that thread just says to upgrade.", "created_utc": 1426265274, "gilded": 0, "name": "t1_cpdsury", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yx982/praw_not_working_anymore/"}, {"author": "kemitche", "body": "Sure, but there's something wrong with your `six.moves` module. It's might be installed incorrectly. The most recent version of `six` is 1.9, by the way. I jumped on a pythonanywhere.com console and they're using 1.8. However, 1.8 appears to have six.moves.urllib.parse, so I'm not sure why you're install is failing to load. You may have to check with pythonanywhere.com support or w/e, or reset your instance. (I've only just heard of them, so I can't do much from here)", "created_utc": 1426265709, "gilded": 0, "name": "t1_cpdt4g5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yx982/praw_not_working_anymore/"}, {"author": "ibbignerd", "body": "I tried installing `six` locally using `pip install six --user --upgrade`, which installed 1.9, but it resulted in the same error. I'm trying to get in contact with pythonanywhere support to help resolve this.", "created_utc": 1426268167, "gilded": 0, "name": "t1_cpdupdn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yx982/praw_not_working_anymore/"}, {"author": "zathegfx", "body": "Ok - I am retarded. To find the flair of the link, just use: `submission.link_flair_css_class` Hope this helps anyone with the same issues Also, this helped me: http://stackoverflow.com/questions/12719542/decoding-json-from-reddit-api-in-python-using-praw", "created_utc": 1426265401, "gilded": 0, "name": "t1_cpdsxlt", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yx828/praw_find_the_current_flair_for_a_specific_link/"}, {"author": "kemitche", "body": "200 is the max you can retrieve in a single API request; 1000 is the maximum you can retrieve by paging through recent comments.", "created_utc": 1426179025, "gilded": 0, "name": "t1_cpcoqyz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yt6lb/can_praw_get_the_last_2000_comments_in_a_sub/"}, {"author": "NewArithmetic", "body": "So if I were to want more than 1000, would I have to go through every post in the past x days and get all the comments from those posts? (still adhering 1 request per 2 seconds of course). The reason I ask is b/c I'm trying to build something similar to a word cloud and I think 1000 comments may be a bit small of a sample size.", "created_utc": 1426179507, "gilded": 0, "name": "t1_cpcp1zx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yt6lb/can_praw_get_the_last_2000_comments_in_a_sub/"}, {"author": "kemitche", "body": "If the subreddit has enough activity, you're better off watching for new comments until you've collected enough. If it's a slow subreddit though, yes, you'd need to trawl through various posts and slurp up as many comments as you can.", "created_utc": 1426180311, "gilded": 0, "name": "t1_cpcpjyb", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yt6lb/can_praw_get_the_last_2000_comments_in_a_sub/"}, {"author": "GoldenSights", "body": "The problem you're running into is different than what you think it is. It's not happening because you've gotten too many submissions, it's happening because one of the submissions has a large number of comments. I don't know what post it is because you're using `get_top` which is ambiguous and will behave differently based on whatever mode you used last. I always prefer to be explicit and use `get_top_from_all` or one of [the others](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.objects.Multireddit.get_top_from_all), so it will always act the same. *Anyway*, the MoreComments object represents the button at the bottom of the page that says \"Load x more comments\". Those objects don't have a body, obviously, and that's why `writer.writerow` is crashing when it gets there. Option 1: before doing `comments = praw.helpers....` , do `submission.replace_more_comments(limit=None)` so praw will automatically replace each one of those with the proper comments. This is astronomically slow on large threads so you might want to use a real limit if it's taking too long Option 2: Put an if statement before the writer that checks `if not isinstance(comment, praw.objects.MoreComments):` so the MoreComments just get ignored. If you're only looking for high-scoring comments this could work just fine. Hope that wasn't too wordy.", "created_utc": 1426044822, "gilded": 0, "name": "t1_cpb2q9e", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "tooproudtopose", "body": "Total side note - any idea how to return the time a post was created? I've poked around a fair amount but can't get anything to work (I keep getting the an attribute error ('list' has no object '____') - I've tried created and created_utc (as well as the obvious time, date, period, that I could think of). I should probably add that I've been trying to put it as a third option under writerow: writer.writerow((comment.body, comment.author, comment.created))", "created_utc": 1426411832, "gilded": 0, "name": "t1_cpfggwx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "GoldenSights", "body": "created and created_utc are definitely what you're looking for. They are in the form of unix timestamps, which you can convert to strings with the datetime module, if that's what you're looking for. What does that line produce? I don't think it should give you any errors.", "created_utc": 1426412262, "gilded": 0, "name": "t1_cpfgjth", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "tooproudtopose", "body": "I get AttributeError: 'list' object has no attribute 'created_utc'... I can't figure out why it's not working!", "created_utc": 1426414686, "gilded": 0, "name": "t1_cpfgzt7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "GoldenSights", "body": "Hmm, that gives me the impression that you typo'd and put something like `comments.created_utc` instead of `comment.created_utc`, since `comments` is a list and that's what the error is complaining about.", "created_utc": 1426415043, "gilded": 0, "name": "t1_cpfh25t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "tooproudtopose", "body": "Ahhh yes that was it! Haha boy do I feel silly. Thank you so much!", "created_utc": 1426415888, "gilded": 0, "name": "t1_cpfh7mw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "tooproudtopose", "body": "Thank you so much! This is exactly what I needed. Especially for the part about all comments versus only high scoring comments, as I'll eventually need both! As a note to anyone that might need this, you also need to put `f.close()`at the end (outside the loop). EDIT: This actually creates an assertion error. This problem is fixed in the changelog here: https://github.com/praw-dev/praw/blob/master/CHANGES.rst", "created_utc": 1426057978, "gilded": 0, "name": "t1_cpb7mu4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "kemitche", "body": "> you also need to put f.close() at the end In modern versions of python, you often want to use the `with` statement to automatically close files: with open(somefile, \"w\") as f: f.write(\"hello\") ... Upon exiting the `with` block, the file gets automatically closed. Further reading: http://preshing.com/20110920/the-python-with-statement-by-example/", "created_utc": 1426264855, "gilded": 0, "name": "t1_cpdslhy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "tooproudtopose", "body": "Oh! I thought I had that - for some reason when I didn't include f.close() at the end it resulted in an error. Side note - any idea how to return the time that a post was created? I've looked around and I can't find anything.", "created_utc": 1426410982, "gilded": 0, "name": "t1_cpfgb4p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "kemitche", "body": "Look for the `created_utc` field, it's a unix timestamp of when the item was created.", "created_utc": 1426443012, "gilded": 0, "name": "t1_cpfr6zw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yn0dd/comment_scraper_stops_working_once_the_limiting/"}, {"author": "kemitche", "body": "If using OAuth, over SSL. If not using OAuth, I'm not 100% sure, but it appears to be sent in plaintext.", "created_utc": 1425938777, "gilded": 0, "name": "t1_cp9l9d8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yhhm3/prawpythonare_requests_encrypted/"}, {"author": "Pathogen-David", "body": "It looks like PRAW is still using HTTP for most API endpoints still, unfortunately: https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L253 You could probably modify [line 197](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L197) to use https instead and it'd work just fine, but I've not tested it myself.", "created_utc": 1425939603, "gilded": 0, "name": "t1_cp9lr84", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yhhm3/prawpythonare_requests_encrypted/"}, {"author": "largenocream", "body": "Here's a monkey patch I use for forcing HTTPS with PRAW: https://gist.github.com/JordanMilne/04c161a5b66a087619ed . I haven't tested it very much, so you might want to!", "created_utc": 1425950689, "gilded": 0, "name": "t1_cp9s9h5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yhhm3/prawpythonare_requests_encrypted/"}, {"author": "kemitche", "body": "You don't need a refresh token! Just repeat the username/password process request.", "created_utc": 1425873425, "gilded": 0, "name": "t1_cp8t8so", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yekdx/how_do_i_get_an_oauth2_refresh_token_for_a_python/"}, {"author": "JBHUTT09", "body": "So I should just run get_new_token() when my token expires?", "created_utc": 1425873546, "gilded": 0, "name": "t1_cp8tan2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yekdx/how_do_i_get_an_oauth2_refresh_token_for_a_python/"}, {"author": "kemitche", "body": "Yup!", "created_utc": 1425875482, "gilded": 0, "name": "t1_cp8u4aa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yekdx/how_do_i_get_an_oauth2_refresh_token_for_a_python/"}, {"author": "JBHUTT09", "body": "Thanks. I have 2 more questions: 1. This is what I have for getting a token: def get_new_token( self ): current_time = time( ) client_auth = requests.auth.HTTPBasicAuth( self.client_settings[ 'client_id' ], self.client_settings[ 'client_secret' ] ) headers = { 'user-agent': self.client_settings[ 'user_agent' ] } post_data = { \"grant_type\": \"password\", \"username\": self.client_settings[ \"username\" ], \"password\": self.client_settings[ \"password\" ] } response = requests.post( \"https://www.reddit.com/api/v1/access_token\", auth = client_auth, data = post_data, headers = headers ) token_data = response.json( ) self.token_expiration = current_time + token_data[ 'expires_in' ] self.reddit.set_access_credentials( token_data[ 'scope' ], token_data[ 'access_token' ] ) I'm now getting this error from the set_access_credentials() call: >praw.errors.OAuthAppRequired: The OAuth app config parameters client_id, client_secret and redirect_url must be specified to use this function. I gather this means that I need to include those three parameters, but I'm not quite sure where to squeeze them in. My knee-jerk reaction would be to add them as parameters to the set_access_credentials() call, but, based on the [documentation](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.AuthenticatedReddit.set_access_credentials), it doesn't look like they are parameters. 2. What actions does the token need to be valid for? Should I check the token validity before every action I take with praw, like looking at new posts, or just things that only a user can do, like sending PMs or removing posts? Also, can I request a new token before my current one expires? I only ask because I'd like to throw in a several second grace period so that I can be sure that a nearly expired token doesn't expire while I'm doing something. Thanks for all your help so far.", "created_utc": 1425906748, "gilded": 0, "name": "t1_cp92l22", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yekdx/how_do_i_get_an_oauth2_refresh_token_for_a_python/"}, {"author": "kemitche", "body": "1 Ah. Didn't catch that you were using `praw` at first. You need to make a call to `set_oauth_app_info`: http://praw.readthedocs.org/en/latest/pages/oauth.html Last I checked, `praw` didn't handle the username/password flow for scripts, so you may have some slight oddities to handle when the initial token expires depending on how `praw` tries to renew it. 2 By default, the script tokens have access to all scopes; i.e., any documented reddit API is accessible. No need to check validity before requests, just use the token. I actually recommend writing your code to let the tokens expire, because it forces you to write-in handling of the 401 case (or deal with however `praw` ends up handling it), which is an important part of a well-written OAuth app - though admittedly less important for script-apps.", "created_utc": 1425919819, "gilded": 0, "name": "t1_cp998gu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2yekdx/how_do_i_get_an_oauth2_refresh_token_for_a_python/"}, {"author": "shaggorama", "body": "I'm going to hazard a guess that you have multiple versions of python running simultaneously. I'm guessing you had CPython installed already and then separately installed anaconda (or some other distribution that comes with spyder). When you ran `pip install praw` to install praw, the module got downloaded to the CPYthon installation instead of the anaconda installation. Spyder is connected to the anaconda distribution so doesn't see pacakges attached to the other installation. Does that sound like it might fit your situation? If so: poke around the files in your anaconda folder and look for the version of pip in there. Call pip using the absolute path to this version of pip and it'll install praw into your anaconda installation. Also, it's worth noting that this is really a *python* issue and not really a *reddit/praw* issue. In the future, you should post this sort of question to /r/learnpython.", "created_utc": 1425762660, "gilded": 0, "name": "t1_cp7jwxy", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "iforgotmylegs", "body": "Thanks, this worked perfectly. I know like you said it is a general python issue but just to wrap a bow around this issue once and for all for anyone who might find this thread later, could you answer one more thing for me? Is Anaconda3 like another, pre-configured version of Python? Like if I delete the Python34 folder from my C drive would Spyder still run scripts because it's accessing everything from C:\\Users\\myname\\Anaconda3?", "created_utc": 1425764321, "gilded": 0, "name": "t1_cp7kp9y", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "shaggorama", "body": "Yes, it would work fine. Anaconda and your CPython installations are operating off different binaries (exe files). For the record, there's probably a better way to uninstall your first python installation than just deleting the folder. If you're using windows, check in with \"add/remove programs\" in the control panel. The reason pip got confused was because the location of your first python is on your system's path variable (which you could edit manually). There's also probably some registry stuff to clean up too (not as easy or safe to do manually). If you have the option to do a formal uninstall, you should take it.", "created_utc": 1425764720, "gilded": 0, "name": "t1_cp7kw1j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "iforgotmylegs", "body": "Ah yes thank you this is exactly my situation. I thought maybe it was a praw issue because I didn't seem to have any trouble with other imports.", "created_utc": 1425762781, "gilded": 0, "name": "t1_cp7jz27", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "shaggorama", "body": "Glad I could help. To mitigate future confusion, I'd recommend removing the other python installation and/or getting really explicit about your environments by using virtualenv/conda.", "created_utc": 1425764168, "gilded": 0, "name": "t1_cp7kmqk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "kemitche", "body": ">>> import praw >>> print praw.__file__ /some/path/to/praw", "created_utc": 1425755400, "gilded": 0, "name": "t1_cp7gcoo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "iforgotmylegs", "body": "The path returned is C:\\Python34\\lib\\site_packages\\praw\\__init__.py In the PYTHONPATH manager is Spyder, I add the folder C:\\Python34\\lib\\site-packages\\praw then restart, but it still doesn't work.", "created_utc": 1425756679, "gilded": 0, "name": "t1_cp7gzpc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y9cs3/where_is_the_praw_module_stored/"}, {"author": "GoldenSights", "body": "Okay, this is possible and I've just done it, but you'll have to edit your PRAW installation because PRAW does not yet have anything for the /edited page. 1. Go to C:/python34/scripts/site-packages/praw 2. open objects.py in a text editor 3. Near lines 1318, you should find yourself inside the Subreddit object, looking at `get_top = _get_sorter('top')` 4. Beneath that, add a line: `get_edited = _get_sorter('about/edited')` Now, in your script's code, you can do this: subreddit = r.get_subreddit('redditdev') stream = praw.helpers._stream_generator(subreddit.get_edited, r) for item in stream: #Do your stuff Of course, using 'redditdev' gives you a 403 because it only works on subreddits you moderate. Edit: Wait, this will give you submissions and comments. The API has a parameter to return comments only, but if you just want to be simple I would do a check for t1_ or t3_ in the objects' fullname when in the stream.", "created_utc": 1425598362, "gilded": 1, "name": "t1_cp5p4vm", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y2rvj/praw_comment_stream_search_subredditaboutedited/"}, {"author": "lizardsrock4", "body": "Thank you so much! Enjoy the gold", "created_utc": 1425599437, "gilded": 0, "name": "t1_cp5pqhh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y2rvj/praw_comment_stream_search_subredditaboutedited/"}, {"author": "GoldenSights", "body": "Hey, thanks! I had to try a few ideas before I found the one that worked, but I think those instructions are all that's necessary. Let me know if you hit any errors.", "created_utc": 1425599606, "gilded": 0, "name": "t1_cp5ptw9", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y2rvj/praw_comment_stream_search_subredditaboutedited/"}, {"author": "joe-murray", "body": "What does this attribute represent? I know the voting stats have been modified fairly recently, but I'm not sure if \"likes\" is one of those variables.", "created_utc": 1425590177, "gilded": 0, "name": "t1_cp5k9da", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "Mekhami", "body": "likes would be True if the content is upvoted, False if downvoted, None if neutral. This is for the 'Voteable' duck type.", "created_utc": 1425591101, "gilded": 0, "name": "t1_cp5kuf8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "joe-murray", "body": "If it's a bool as you described, that sounds like a variable that only exists within a user-authenticated scope. Are you using the perspective of an authenticated user?", "created_utc": 1425592835, "gilded": 0, "name": "t1_cp5lxew", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "Mekhami", "body": "definitely.", "created_utc": 1425592968, "gilded": 0, "name": "t1_cp5m0cg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "joe-murray", "body": "Try this: from pprint import pprint pprint(vars(comment_object_here)) You might have seen this before -- it's in the PRAW docs -- but it will show you the variables available for the comment object. If \"likes\" isn't in there, then it's just not there. Reasons for this could include, among other things, 1. Your authentication scheme isn't working properly (you should be able to easily test this with a couple commands or print statements though, so this isn't likely to be the source of the error) 2. It was removed, for whatever reason (possibly related to the change in the voting system)", "created_utc": 1425593467, "gilded": 0, "name": "t1_cp5mbi0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "Mekhami", "body": "Authentication is definitely working. All sorts of other auth-scope features are working as intended. It's not just comment objects, but all voteable objects, and the PRAW docs don't make any mention of the likes attribute. There's this github issue which says that it existed at one point: https://github.com/praw-dev/praw/issues/208 but looking through the source code itself, I don't see anything that would point to a likes attribute.", "created_utc": 1425593642, "gilded": 0, "name": "t1_cp5mff8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "joe-murray", "body": "Oh wait, you are just looking for it in the docs? That's a completely different story -- the PRAW docs don't have everything in them. But the \"likes\" variable is shown in part of the PRAW docs (specifically, [where I code that code snippet from](https://praw.readthedocs.org/en/v2.1.20/pages/writing_a_bot.html)), so it definitely exists, or at least used to. In order to verify that is still exists, just do the test I mentioned or do something like this: print comment_object.likes Edit: it might actually be this: print comment_object['likes']", "created_utc": 1425593807, "gilded": 0, "name": "t1_cp5misd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "Mekhami", "body": "looks like it's there. Thanks for your help. Strangely undocumented.", "created_utc": 1425594687, "gilded": 0, "name": "t1_cp5n1iv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "joe-murray", "body": "Yea, I've had issues with this in the past -- PRAW doesn't record every little detail in the docs unfortunately, so you just have to kind of hack it. Just use the vars() function as an easy way to see the full depth of what attributes exist, and if you don't know what they do, try printing the variable itself to see what it looks like.", "created_utc": 1425598211, "gilded": 0, "name": "t1_cp5p1tp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2y26ty/praw_python_what_happened_to_the_likes_attribute/"}, {"author": "RuleIV", "body": "I tried using `print(vars(user))` for a shadow banned user and got this exception `requests.exceptions.HTTPError: 404 Client Error: Not Found` So a dirty way of finding if a user is deleted or shadow banned would be: shadow_banned = False try: user = r.get_redditor(user_name) except: shadow_banned = True Any thoughts?", "created_utc": 1425445901, "gilded": 0, "name": "t1_cp3sgm0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "Walter_Bishop_PhD", "body": "A page 404'ing is, afaik, the only way none-admins have of seeing if a user is shadowbanned", "created_utc": 1425469507, "gilded": 0, "name": "t1_cp3yysl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "RuleIV", "body": "Thanks. It's a shame that if Reddit hangs at that moment it will mark it as shadowbanned/deleted. Maybe I'll put in something to loop try three times to be sure.", "created_utc": 1425476163, "gilded": 0, "name": "t1_cp40usp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "bboe", "body": "A hang should result in something other than a 404.", "created_utc": 1425537667, "gilded": 0, "name": "t1_cp4yd45", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "RuleIV", "body": "Do you know how I can tell the difference? I don't have any experience differentiating except types.", "created_utc": 1425539999, "gilded": 0, "name": "t1_cp4z3eu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "zer0t3ch", "body": "I feel like this could only be useful for malicious activities.....", "created_utc": 1425465665, "gilded": 0, "name": "t1_cp3y6bm", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "RuleIV", "body": "In what possible way could it be malicious? I'm a moderator and I'm planning on running this once a week so we can easily see who the problem users are, and discuss potential moderator actions. The bot gives us all users with ten or more removals for the month, and sends each of those users a message warning them about their behavior.", "created_utc": 1425476076, "gilded": 0, "name": "t1_cp40tm9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "zer0t3ch", "body": "Ah ok. It seemed to me like something somebody running a bunch of malicious bots would want. IE, create 100 users, spam or upvote or whatever, then replace them when they get shadowbanned. Wasn't trying to accuse you or anything, just didn't consider alternate uses.", "created_utc": 1425476790, "gilded": 0, "name": "t1_cp41348", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xvhp4/praw_how_to_tell_if_a_user_shadow_banned_or_is_a/"}, {"author": "hiles", "body": "When you get the length you're advancing to the end of the generator. You should turn it into a list before working with it.", "created_utc": 1425163132, "gilded": 0, "name": "t1_cp0aggv", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xhrkc/iterating_through_unread_messages_isnt_working_as/"}, {"author": "PixelDJ", "body": "Okay. I didn't realize that's how generators worked. I should do some more research. I'm still learning. Thanks for the help!", "created_utc": 1425168884, "gilded": 0, "name": "t1_cp0d5s7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xhrkc/iterating_through_unread_messages_isnt_working_as/"}, {"author": "radd_it", "body": "I'm not familiar with PRAW-- but is message actually in messages? Or is it in ``messages.data.children.data`` or somesuch?", "created_utc": 1425157129, "gilded": 0, "name": "t1_cp07l2d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xhrkc/iterating_through_unread_messages_isnt_working_as/"}, {"author": "PixelDJ", "body": "Using PRAW, the messages should be stored in the \"messages\" variable as a generator, as far as I understand. If I run: messages = r.get_unread(limit = None) print messages I get the following: I also tried using messages = r.get_unread(limit = None) print list(messages) and get the following: [, ] Another version: print list(messages)[0] prints the message body: > From: PixelDJ > > Subject: Another message! > > > Hey, I'm sending another message for... So I know the message is there somewhere.", "created_utc": 1425157602, "gilded": 0, "name": "t1_cp07t9h", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2xhrkc/iterating_through_unread_messages_isnt_working_as/"}, {"author": "kannibalox", "body": "As it is, you aren't sending any form data with your POST requests. Try the [data argument](http://docs.python-requests.org/en/latest/user/quickstart/#more-complicated-post-requests).", "created_utc": 1424663332, "gilded": 0, "name": "t1_cou44pf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wtg1n/api_how_do_you_pass_variables_with_an_api_post/"}, {"author": "PixelOrange", "body": "I knew I wasn't passing anything. I couldn't figure out how. Your link appears to show me how. I'll try it tomorrow and see. Thank you!", "created_utc": 1424663766, "gilded": 0, "name": "t1_cou4e18", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wtg1n/api_how_do_you_pass_variables_with_an_api_post/"}, {"author": "GoldenSights", "body": "I haven't tested this, but I think that's only relevant if you're also sorting by /top. Then you can choose top by day, month, alltime, etc. It's not a time range like choosing specific dates. comments = user.get_comments(limit=100, sort='top', time='day') [Notice that it says](https://praw.readthedocs.org/en/v2.1.20/pages/code_overview.html#praw.objects.Redditor.get_comments) \"time period to return submissions **if applicable**\". And it gives you the different strings you can choose from. The parameter wouldn't work on subreddits because subreddits don't allow sorted comment listings like user pages do.", "created_utc": 1424565326, "gilded": 0, "name": "t1_cosysdu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wpkzd/how_does_time_work_for_get_comments_and_can_it_be/"}, {"author": "iforgotmylegs", "body": "I see... That is rather frustrating. Is there a way to check the time on a comment object manually? I was really hoping that I would be able to only check posts from a certain timeframe but I don't see any time property for a Comment object anywhere.", "created_utc": 1424566555, "gilded": 0, "name": "t1_cosze22", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wpkzd/how_does_time_work_for_get_comments_and_can_it_be/"}, {"author": "GoldenSights", "body": "Yeah, `comment.created_utc` is its unix timestamp in the UTC timezone.", "created_utc": 1424566602, "gilded": 0, "name": "t1_coszevu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wpkzd/how_does_time_work_for_get_comments_and_can_it_be/"}, {"author": "iforgotmylegs", "body": "Oh, awesome, thanks. Not that I'm trying to get you to make my bot for me but you seem to be very familiar with this package... In your mind would grabbing all the comments from a subreddit indiscriminately and then checking its timestamp against a pre-specified timeframe be the only feasible way of returning only comments from that timeframe? It seems like a very bruteforce method but if it is the only way I guess I will find a way to make it work.", "created_utc": 1424566860, "gilded": 0, "name": "t1_coszj94", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wpkzd/how_does_time_work_for_get_comments_and_can_it_be/"}, {"author": "GoldenSights", "body": "Hmm, I think you are. With submissions you can actually fetch a time range, but not comments. I think you'll have to get all 1k and drop whatever doesn't fit your range. That's another thing you might not know -- you will only be able to get the 1,000 most recent comments on the subreddit. If that's not enough... you're out of luck. Unless you want to grab submissions and then grab the comments on each, which is an astronomically slow process.", "created_utc": 1424567062, "gilded": 0, "name": "t1_coszmnu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wpkzd/how_does_time_work_for_get_comments_and_can_it_be/"}, {"author": "iforgotmylegs", "body": "You're onto me... But seriously thanks for your insight. I think the 1000 most recent submissions would be suitable for my needs.", "created_utc": 1424567761, "gilded": 0, "name": "t1_coszyfe", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wpkzd/how_does_time_work_for_get_comments_and_can_it_be/"}, {"author": "GoldenSights", "body": "Unless there's a particular reason for using Py2.7, I would highly recommend choosing Python 3.4 instead. It even comes with pip pre-packaged. You need to type `pip install praw` into a commandline while in whatever directory you installed pip to. My Pip is in C:/python34/scripts, so I do cd C:/python34/scripts pip install praw When praw receives an update, you can do `pip install praw --upgrade` to download it.", "created_utc": 1424549205, "gilded": 0, "name": "t1_cosqsha", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "Jinbuhuan", "body": "I like my Py with Ice Cream, a programmer's dream! (I'll show myself out.)", "created_utc": 1424575969, "gilded": 0, "name": "t1_cot3vhk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "iforgotmylegs", "body": "Thanks. I have Python 2.7 and 3.4 installed, just in different folder (Python27 and Python34). I was originally using Python 2.7 due to compatibility issues with another module. How can you switch between them? I thought you could just modfy if the path in Control Panel > System and Security > System > Advanced System Settings > Advanced > Change the last part of path to ...;C:\\Python34 instead of ...;C:\\Python 27 but when I load up IDLE, it still says Python 2.7, even when I open IDLE that is contained in the Python34 folder. EDIT: Nevermind, I got it! Changing the path did work, but the weird thing is, I can only get IDLE to open in 3.4 when I open up idle.bat in the Python34\\Lib\\idlelib folder. In fact, there doesn't actually seem to be an IDLE.py in there, just multiple shortcuts to it.", "created_utc": 1424549592, "gilded": 0, "name": "t1_cosqzim", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "GoldenSights", "body": "I don't have 2.7 so I can't tell you first-hand, but yeah I'd expect it's a PATH issue. You could also try starting IDLE from python34/lib/idlelib/idle.bat, which should choose the correct version.", "created_utc": 1424550015, "gilded": 0, "name": "t1_cosr6vu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "iforgotmylegs", "body": "Haha I think I stumbled upon the solution as you were typing. Thank you for your help, though.", "created_utc": 1424550280, "gilded": 0, "name": "t1_cosrbno", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "iforgotmylegs_bot", "body": "hey look its working", "created_utc": 1424554378, "gilded": 0, "name": "t1_costdps", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "iforgotmylegs", "body": "qwertyuiop", "created_utc": 1424554501, "gilded": 0, "name": "t1_costfzg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "iforgotmylegs_bot", "body": "asdfghjkl", "created_utc": 1424555082, "gilded": 0, "name": "t1_costqcu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "Jinbuhuan", "body": "Praw? You add two letters, as it is spelled Prawns, and you cook or bar-b-que them and eat them! Hyuk hyuk!", "created_utc": 1424575841, "gilded": 0, "name": "t1_cot3tac", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2woo12/i_dont_understand_how_you_are_supposed_to_install/"}, {"author": "mgrieger", "body": "Looks pretty good to me. Just two things: 1) I'm not sure if this is still the case, but I think you may be able to just compare comment.author with the username string instead of calling get_redditor(). 2) If you want this to run all the time instead of just once, place the bit of code where you check for new messages in an infinite loop with a sleep function call at the end.", "created_utc": 1424511398, "gilded": 0, "name": "t1_cosdjn9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "RudyH246", "body": "Thanks! I definitely want this to run at all times. How's this? import time import praw r = praw.Reddit(user_agent='RudyH246 big example user agent thingy') r.login('user', 'pass') while True: print 'loop' for comment in r.get_unread(): if comment.author() == r.get_redditor('username'): comment.reply('Reply.') comment.mark_as_read() time.sleep(60)", "created_utc": 1424531757, "gilded": 0, "name": "t1_cosi57c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "mgrieger", "body": "Looks good! I'm on my phone so the indentation is messed up, but if everything is indented correctly I think it should work.", "created_utc": 1424538419, "gilded": 0, "name": "t1_cosl6pd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "RudyH246", "body": "Turned out to have a small syntactical error, but now I've got it fully functioning. Thanks for the help! Python is pretty amazingly simple.", "created_utc": 1424539240, "gilded": 0, "name": "t1_coslm0s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "mgrieger", "body": "Cool! Yeah, python is pretty fun to use. PRAW is a great API wrapper.", "created_utc": 1424540325, "gilded": 0, "name": "t1_cosm6bs", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "RudyH246", "body": "Hm, I got home after work today and found this in my terminal window: Traceback (most recent call last): File \"stupid.py\", line 9, in for comment in r.get_unread(): File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 504, in get_conten t page_data = self.request_json(url, params=params) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 163, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 557, in request_js on retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw\\__init__.py\", line 399, in _request _raise_response_exceptions(response) File \"C:\\Python27\\lib\\site-packages\\praw\\internal.py\", line 178, in _raise_res ponse_exceptions response.raise_for_status() File \"C:\\Python27\\lib\\site-packages\\requests\\models.py\", line 831, in raise_fo r_status raise HTTPError(http_error_msg, response=self) requests.exceptions.HTTPError: 521 Server Error: Origin Down Looks like some kind of exception was thrown? After some short googling, it looks like I need to alter my code to look more like: import time import praw r = praw.Reddit(user_agent='RudyH246 big example user agent thingy') r.login('user', 'pass') while True: try: for comment in r.get_unread(): if comment.author == r.get_redditor('username'): comment.reply('Reply.') comment.mark_as_read() time.sleep(60) except: I'm not entirely sure what to put inside the except: bit. If I get an exception, I guess I just want to start the script over again after sleeping for maybe 5 minutes? Not entirely sure how I'd do that. I guess I'd just need to put \"time.sleep(300)\" inside the \"except:\" bit, and nothing else? Or would I need like, a goto line to make the script go back to the original \"try\" bit?", "created_utc": 1424573703, "gilded": 0, "name": "t1_cot2sx0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "mgrieger", "body": "Just putting the sleep statement would be fine. Once that sleep statement finishes, the program will reach the end of the current iteration of the loop and continue with a new loop.", "created_utc": 1424576728, "gilded": 0, "name": "t1_cot47z1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "RudyH246", "body": "Ahh, that makes sense. Awesome, thanks!", "created_utc": 1424577031, "gilded": 0, "name": "t1_cot4cw2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2wmvah/praw_a_simple_autoresponder_based_on_user_name/"}, {"author": "bboe", "body": "> 1) Not sure if it is possible to access random users' vote history even w/ OAuth You can only access this data (up to 1000 submission votes and 1000 comment votes) for users who explicitly give you permission. It won't work generally as you hope.", "created_utc": 1423498423, "gilded": 0, "name": "t1_cog1st2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2vaun5/wanted_to_run_an_idea_by_you_guys_im_new_to/"}, {"author": "Seventytvvo", "body": "Well, that completely shuts down this idea, I think...", "created_utc": 1423498912, "gilded": 0, "name": "t1_cog224f", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2vaun5/wanted_to_run_an_idea_by_you_guys_im_new_to/"}, {"author": "go1dfish", "body": "Yeah vote data is off by default, what you may be able to do is find people who hang out and contribute to threads together and that might have at least some overlap with what you're actually trying to find. Like how many times do person A and B both reply to the same comment? How many times do they both participate in the same thread at different levels?", "created_utc": 1423508392, "gilded": 0, "name": "t1_cog7hzk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2vaun5/wanted_to_run_an_idea_by_you_guys_im_new_to/"}, {"author": "Seventytvvo", "body": "Or, how often does UserA respond to UserB? And at what time after the submission does this occur? Or if you suspect UserA of fraud, look at the first 5 users to post in UserA's submitted threads to see if there's a pattern. yeah... there's definitely other options, but it's a lot harder to draw conclusions from these methods... I'm sure reddit has bots that are miles ahead of this in terms of vote fraud detection anyway... I was just so excited to be a detective!! I even got a couple good eye-rolls from my gf last night as I explained my idea.", "created_utc": 1423509334, "gilded": 0, "name": "t1_cog82fq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2vaun5/wanted_to_run_an_idea_by_you_guys_im_new_to/"}, {"author": "Falmarri", "body": "use sudo", "created_utc": 1423439337, "gilded": 0, "name": "t1_coffsac", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v8dic/unable_to_update_praw/"}, {"author": "Fireislander", "body": "Wow. Feeling a little dumb now. Thank you", "created_utc": 1423440094, "gilded": 0, "name": "t1_cofg6ou", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v8dic/unable_to_update_praw/"}, {"author": "howbigis1gb", "body": "Can you explain a little about what exactly it does? I'm looking at it, and I'm not exactly sure.", "created_utc": 1423378998, "gilded": 0, "name": "t1_coet3bd", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/"}, {"author": "chrisarr", "body": "Ha apologies if it is a bit hard to wrangle the meaning of at the moment. u/letgoandflow pretty well nailed it though. At the moment, it is more a collection of tools than a coherent power tool. This being my first web programming experience, I wanted to take some time learning each of the toolkits and libraries I felt the final version would need. I think the real value is in understanding how these kinds of tools can work together (better than this first iteration does)!", "created_utc": 1423418895, "gilded": 0, "name": "t1_cof4mbq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/"}, {"author": "letgoandflow", "body": "Not OP, but appears to do a couple things: * It filters the AMA [reddit nice maker](http://rnm.gauzza.com)-style so you only see questions that were answered. * It has some additional data in the sidebar about the AMA. \"Things mentioned\" seems to be a list of popular terms in the article. \"Timeline of comments from post submission\" shows the number of comments that were added to the thread over time (guessing this is by anyone and not just the OP). \"Most frequent commenters\" shows a bar graph of comments by user. Seems a little pointless, would probably be better just to show total comments left by the OP.", "created_utc": 1423407190, "gilded": 0, "name": "t1_coezexj", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/"}, {"author": "letgoandflow", "body": "Cool project, I think you have a start to something. Few things: 1. A major functionality of the app seems to be that it filters the AMA to only the questions that were answered. There are other apps that already do this (see reddit nice maker), so some redditors might overlook that feature. 2. Make it responsive when you get a chance. 3. See what you can do with the data in the sidebar of each AMA. I'd replace the \"top commenters\" bar graph and replace it with a total count of comments posted by the OP. Think about going deeper with the comments data. How many comments did the OP leave on the top-level, 2nd level, 3rd level and so on. Is the timeline graph just for the OP or for all commenters? Might be cool to see a graph for just the OP. How about a list of questions that were upvoted by the OP? Not sure if that's possible but would be cool. Don't have any specific ideas, but is there features you could add if you assumed that the OP was aware of your app and wanted to use it to analyze their AMA afterwards? Like my previous point of showing upvoted questions, if that isn't possible maybe the OP could be instructed to save comments or take some other action to feed them into a \"favorite questions\" widget in your app. How about a list of links mentioned by the OP in their post and comments? Alright I think I've done enough damage :) I guess my overall point is to think about the OP more than redditors. Redditors already have reddit and the reddit AMA app to read AMAs. OPs might be looking for more features and advanced data which you can provide with your app.", "created_utc": 1423408209, "gilded": 0, "name": "t1_coezra6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/"}, {"author": "chrisarr", "body": "This is great. Thanks for pointing out reddit nice maker, which I was unaware of! I've definitely been trying to reconcile where the focus lies between OP/author and redditor/commenter. At the moment, there is no ability to author or contribute (post) from within the app, but obviously that could be possible. The idea of looking at the AMA post hoc is sort of central to what the value is currently, not only because the lack of authoring tools, but I think there is a need for this kind of tool. AMA happens so quickly over a few hours or so, and then more AMAs overtake the previous days \u2013 many people don't have access to that info, and going back to find it or to read the summary of each discussion seems like a really valuable source of knowledge. I like all of your ideas about focusing more on the OP's activities (links, upvotes, comments-at-hierarchy, etc). That kind of focus really reinforces the author-oriented nature of AMA. Really valuable feedback! Thanks so much.", "created_utc": 1423425046, "gilded": 0, "name": "t1_cof7wyu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/"}, {"author": "letgoandflow", "body": "You can definitely make your tool more useful for both redditors and OPs. Take a hard look at reddit nice maker and see where you can do things better and add value. I'd also consider non-redditors, as they might have a desire to read AMAs in a simpler format. Looking at things from the OP's perspective is important because they have the ability to share your app with large audiences. If your app presents their AMA better than reddit and other tools, then you are onto something. In regards to posting an AMA from within the app, I've actually created an app that focuses on this functionality, see [easyama.co](http://easyama.co). It's built on flask as well.", "created_utc": 1423426378, "gilded": 0, "name": "t1_cof8nbo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v5s0c/building_a_tool_for_reddit_ama_take_a_peek/"}, {"author": "ThrustVectoring", "body": "Make a python script that posts in a subreddit, then set up a cron job that runs that script once a week.", "created_utc": 1423362679, "gilded": 0, "name": "t1_coemkgf", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v5ho4/help_making_a_bot_that_posts_once_a_week/"}, {"author": "Midgetapple5", "body": "thank you so much, I'll try this", "created_utc": 1423366088, "gilded": 0, "name": "t1_coeo36f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v5ho4/help_making_a_bot_that_posts_once_a_week/"}, {"author": "GoldenSights", "body": "Depending on what you need to post, Automoderator might work since it has [configurable schedules](http://www.reddit.com/r/AutoModerator/comments/1z7rlu/now_available_for_testing_wikiconfigurable/). I've never used them, but I'm sure a few searches on the subreddit will get you started.", "created_utc": 1423361569, "gilded": 0, "name": "t1_coem226", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2v5ho4/help_making_a_bot_that_posts_once_a_week/"}, {"author": "kemitche", "body": "PRAW is definitely the way you want to start! reddit's API has its fair share of oddities; PRAW wraps those up fairly nicely (and also keeps you from having to learn a bunch of HTTP semantics on top of python). > the top 10 hottest reddits for a subreddit The top 10 posts? Yes, that's possible. Item 6: https://praw.readthedocs.org/en/v2.1.20/", "created_utc": 1423239001, "gilded": 0, "name": "t1_cod5eav", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2uzqy7/first_timer_here_looking_for_guidance/"}, {"author": "Marorin", "body": "Thing is my professor wants a presentation on the API and it's use, so I was thinking that. He didn't want something that was 10 lines though so I'm trying to think of creative things to do with PRAW.", "created_utc": 1423240472, "gilded": 0, "name": "t1_cod69fg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2uzqy7/first_timer_here_looking_for_guidance/"}, {"author": "kemitche", "body": "Gotcha. Well I'm happy to answer any questions I can about reddit's API to help you along!", "created_utc": 1423240980, "gilded": 0, "name": "t1_cod6kgd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2uzqy7/first_timer_here_looking_for_guidance/"}, {"author": "Marorin", "body": "Let's see... how feasible would it be to take those top 10 topics in a certain subreddit and add a comment through the person's own credentials?", "created_utc": 1423241233, "gilded": 0, "name": "t1_cod6pya", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2uzqy7/first_timer_here_looking_for_guidance/"}, {"author": "kemitche", "body": "Certainly possible. Automatic replies like that can be frowned upon if the subreddit mods / community don't approve though.", "created_utc": 1423243550, "gilded": 0, "name": "t1_cod83wa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2uzqy7/first_timer_here_looking_for_guidance/"}, {"author": "GoldenSights", "body": "There is a reddit Gold feature where you can do /r/all-pics-funny to remove subreddits, but this obviously isn't ideal for a bot. The only thing that I can think of off the top of my head is to get posts from /r/all and then ignore them if they're from a certain subreddit by checking the `object.subreddit.display_name`. This often means a lot of wasted effort, but I don't remember any alternatives.", "created_utc": 1422404544, "gilded": 0, "name": "t1_co2ykmx", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2twjw9/excluding_specific_subs_from_bot_replies_praw/"}, {"author": "Kim___Jong___Un", "body": "Could you ELI5 what you mean by checking the `object.subreddit.display_name`? My bot is currently getting posts from /r/all, if that's the wasted effort you were referring to. I do want it to pull from almost all - just not from one or two of them. Thanks for your help!", "created_utc": 1422406017, "gilded": 0, "name": "t1_co2zfb9", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2twjw9/excluding_specific_subs_from_bot_replies_praw/"}, {"author": "GoldenSights", "body": "I think you do need to pull from /r/all, which means any posts from your blacklisted subreddits are going to be downloaded and immediately ignored, which is a bummer but I can't remember if there are any other methods. What I mean is this: BLACKLISTED = [\"pics\", \"funny\"] ... subreddit = r.get_subreddit('all') new = subreddit.get_new(limit=100) for submission in new: if all(submission.subreddit.display_name.lower() != blacklisted.lower() for blacklisted in BLACKLISTED): # Do things ... So the if-statement will fail if the post comes from a blacklisted subreddit. The way I wrote it means it's all case-insensitive.", "created_utc": 1422406346, "gilded": 0, "name": "t1_co2zm8u", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2twjw9/excluding_specific_subs_from_bot_replies_praw/"}, {"author": "Kim___Jong___Un", "body": "Excellent; thanks loads for your help.", "created_utc": 1422412557, "gilded": 0, "name": "t1_co334jt", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2twjw9/excluding_specific_subs_from_bot_replies_praw/"}, {"author": "bboe", "body": "PRAW is not thread safe. Remove your threading code and you'll be good to go.", "created_utc": 1422341496, "gilded": 0, "name": "t1_co25uio", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tnplt/comment_stream_error/"}, {"author": "SurviAvi", "body": "The problem is that it used to work.", "created_utc": 1422357825, "gilded": 0, "name": "t1_co29p6y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tnplt/comment_stream_error/"}, {"author": "bboe", "body": "Then you got lucky.", "created_utc": 1422373522, "gilded": 0, "name": "t1_co2fbwo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tnplt/comment_stream_error/"}, {"author": "bboe", "body": "Looks like a bug in the newest release. Can you pinpoint one or two submissions that reliably reproduce the issue?", "created_utc": 1422286256, "gilded": 0, "name": "t1_co1b1f1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "KnightOfDark", "body": "[This thread](http://www.reddit.com/r/Republican/comments/2tglp8/drug_testing_for_welfare_recipients_wisconsin/) and [this thread](http://www.reddit.com/r/socialism/comments/2tku4b/we_would_evict_queen_from_buckingham_palace_and/) consistently give the first error: assert len(self.children) == 1 and self.id == self.children[0] I've yet to run into the second error tonight, but I'll let you know if I do.", "created_utc": 1422310188, "gilded": 0, "name": "t1_co1poxk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "bboe", "body": "Thanks for reporting. Both the errors should be resolved. I won't be making a release just yet but you can jump onto the master branch via: pip install -U https://github.com/praw-dev/praw/archive/master.zip", "created_utc": 1422342389, "gilded": 0, "name": "t1_co2653y", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "advocat3", "body": "Hi really don't mean to hijack or anything but I found this thread via my google search of: \"assert len(self.children) == 1 and self.id == self.children[0]\". I'm working on my first bot and trying to parse the comments on [this thread](http://www.reddit.com/r/TopGear/comments/2yq388/prime_minister_of_the_united_kingdom_hails_jeremy/) I'm super super new to PRAW but loving it so far. Do I have the same issue as OP? Here's a [pastebin](http://pastebin.com/jLNjrd9m).", "created_utc": 1426150975, "gilded": 0, "name": "t1_cpcdv49", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "bboe", "body": "yes", "created_utc": 1426173964, "gilded": 0, "name": "t1_cpclnga", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "advocat3", "body": "I just installed the framework last night. Is the update you mentioned earlier still in progress? Not sure how to proceed but appreciate the quick reply.", "created_utc": 1426189098, "gilded": 0, "name": "t1_cpcv8k5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "i_practice_santeria", "body": "Yes, v2.1.20 is still the latest production release. /u/bboe is a hotfix release coming soon?", "created_utc": 1426539604, "gilded": 0, "name": "t1_cpgyjh2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "KnightOfDark", "body": "That's awesome, thank you!", "created_utc": 1422387635, "gilded": 0, "name": "t1_co2nxh4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/"}, {"author": "GoldenSights", "body": "I'm not actually on the computer at the moment, but the first thing that comes to mind is: Is that account still subscribed to the defaults? I know that some scripting doesnt work until your account has taken some \"subscription action\" in browser (either subscribing or unsubscribing from a single subeddit).", "created_utc": 1422030931, "gilded": 0, "name": "t1_cnyerie", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tevir/problems_subscribing_to_a_list_of_subreddits_from/"}, {"author": "gavin19", "body": "Do you not get any errors in the console/terminal when you run it? I use a very similar script that works fine import praw r = praw.Reddit('some useragent') r.login('user', 'password') with open('subs.txt', 'r') as e: subs = e.read().split('\\n') for sub in subs: r.get_subreddit(sub).subscribe()", "created_utc": 1422029638, "gilded": 0, "name": "t1_cnydz1k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tevir/problems_subscribing_to_a_list_of_subreddits_from/"}, {"author": "kemitche", "body": "Minor suggestion: This isn't the cause of your problem, but since you're writing python, I'd like to point out that your use of `i` is unnecessary (and considered \"unpythonic\"). You can do the following instead: for subreddit_name in liste: subreddit = r.get_subreddit(subreddit_name) ... You can even combine things: with open(\"subreddits.txt\") as sr_file: for line in file: subreddit_name = line.strip() subreddit = r.get_subreddit(subreddit_name) Your code looks fine; though you might have issues if you ever unsubscribe from a subreddit! I'd suggest making a publicly accessible multireddit, and having your bot view that instead.", "created_utc": 1422038793, "gilded": 0, "name": "t1_cnyjqj9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2tevir/problems_subscribing_to_a_list_of_subreddits_from/"}, {"author": "kemitche", "body": "This appears to be my fault! To be more compliant with the OAuth2 spec, we started returning scope strings as space-separated. Sending comma-separated values is still supported, but space separated is \"preferred\".", "created_utc": 1421952167, "gilded": 0, "name": "t1_cnxfbt5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "letgoandflow", "body": "Thanks so much! How can I incorporate your change? Do I have to wait until the next version of praw is released?", "created_utc": 1421957735, "gilded": 0, "name": "t1_cnxitax", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "kemitche", "body": "Uh, I'm not sure. /u/bboe, can you fill this person in on how PRAW releases work?", "created_utc": 1421962377, "gilded": 0, "name": "t1_cnxlrir", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "bboe", "body": "I'm preparing a release now.", "created_utc": 1421989810, "gilded": 0, "name": "t1_cny1392", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "bboe", "body": "I just released 2.1.20 which fixes the issue.", "created_utc": 1421990647, "gilded": 0, "name": "t1_cny1ggr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "letgoandflow", "body": "You're awesome, thanks.", "created_utc": 1421991057, "gilded": 0, "name": "t1_cny1mpm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "kemitche", "body": "/u/bboe merged [the fix I wrote](https://github.com/praw-dev/praw/pull/361) - sorry for the trouble!", "created_utc": 1421953295, "gilded": 0, "name": "t1_cnxg11e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2taq2w/praw_not_able_to_authorize_a_user_with_multiple/"}, {"author": "nath_schwarz", "body": "To indent your log please put four spaces in front of it, like so: Traceback (most recent call last): File \"redacted\", line 5, in r.login('redacted', 'redacted') File \"/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/praw/init.py\", line 1263, in login self.requestjson(self.config['login'], [...] Or upload it to a nopaste site like http://nopastie.info/ or http://pastebin.org/. For the error: Do you have https on those account activated?", "created_utc": 1421898527, "gilded": 0, "name": "t1_cnwuru8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "themetallurgist", "body": "Ok, sorry, I fixed it. I have tried it both ways with it enabled and disabled. Unless there is any more to do other than go to the account security settings and put in the password.", "created_utc": 1421903916, "gilded": 0, "name": "t1_cnwxgvs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "nath_schwarz", "body": "Mhm, in that case I'd suggest asking for help in the [official repo](https://github.com/praw-dev/praw) via an issue. Don't forget to mention that you tried it with and without https.", "created_utc": 1421905197, "gilded": 0, "name": "t1_cnwy159", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "themetallurgist", "body": "I did and got a response within a few minutes. The fix was to remove the word 'bot' from the user_agent string", "created_utc": 1422333008, "gilded": 0, "name": "t1_co2296d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "bboe", "body": "Must have missed my reply here from 4 days ago :-/.", "created_utc": 1422335716, "gilded": 0, "name": "t1_co23iy0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "themetallurgist", "body": "No problem. I actually mis interpreted what you said. So i still had the word 'bot' in the text along with my username.", "created_utc": 1422380000, "gilded": 0, "name": "t1_co2j4jt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "bboe", "body": "Try replacing the word `Bot` from your user agent with your username.", "created_utc": 1421905203, "gilded": 0, "name": "t1_cnwy18n", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2t8t9v/cant_login_with_praw/"}, {"author": "slyf", "body": "The first argument should likely be flair='Community' instead of 'flair=Community'?", "created_utc": 1421714664, "gilded": 0, "name": "t1_cnuex6c", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sz98i/praw_search_not_returning_posts_expected/"}, {"author": "zzpza", "body": "I believe the code is correct there. The \"flair=\" is part of the search, as there is no option to specify it as part of the call, so it is passed as part of the query (or in this case, the whole query). >search(query, subreddit=None, sort=None, syntax=None, period=None, *args, **kwargs) >Return a generator for submissions that match the search query. >Parameters: >query \u2013 The query string to search for. If query is a URL only submissions which link to that URL will be returned. >subreddit \u2013 Limit search results to the subreddit if provided. >sort \u2013 The sort order of the results. >syntax \u2013 The syntax of the search query. >period \u2013 The time period of the results. >The additional parameters are passed directly into get_content(). Note: the url and param parameters cannot be altered. >See http://www.reddit.com/help/search for more information on how to build a search query.", "created_utc": 1421742744, "gilded": 0, "name": "t1_cnurs81", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sz98i/praw_search_not_returning_posts_expected/"}, {"author": "slyf", "body": "Then I believe your query should be 'flair:community'", "created_utc": 1421766871, "gilded": 0, "name": "t1_cnuye99", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sz98i/praw_search_not_returning_posts_expected/"}, {"author": "zzpza", "body": "Bingo! Thank you so much! :D", "created_utc": 1421768110, "gilded": 0, "name": "t1_cnuz06w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sz98i/praw_search_not_returning_posts_expected/"}, {"author": "nemec", "body": "1. OAuth support is meant to allow users to authenticate with reddit through an application and allow that application to perform API calls *on their behalf*. This is slightly different from normal logins, because some other user could log into *your* PRAW instance without having to provide you with their password. 2. Now that we've established that OAuth is meant to let others log in to your PRAW application (which is typically hosted on a server somewhere), I can also assume that you want multiple users to be able to use PRAW commands *at the same time*. So, for instance, two users getting the contents of their inbox simultaneously. 3. Now, the warning is simply a reminder that a single instance of PRAW (e.g. `my_praw_instance = praw.Reddit(...)`) can only manage authentication for a single user. If you want multiple users, *each one* must have their own PRAW instance. In environments like a web server, requests may share the exact same set of global variables. So if you store a PRAW instance somewhere that every user request can access, some users will be performing queries on someone else's logged-in PRAW instance. Example: import praw r = praw.Reddit(\"...\") r.set_oauth_app_info(...) def perform_request(**access_information): # Method is called when a user connects to the web page # Pretend the rest of authorization with user was done before this point r.set_access_credentials(**access_information) # Now authenticated as user print(r.get_me()) I'm not really familiar with the PRAW OAuth flow, but imagine you had the above code and two users visited your site at the exact same time. The `perform_request` method would be called twice and set the `r` object's credentials to each user's account. Imagine if user 1 authenticated, then user 2 authenticated *before* user 1 could call `r.get_me()`. Now both user 1 and user 2 are calling that method using user 2's credentials which is very bad. However, it sounds to me like you're not yet at the stage where you're interested in running PRAW on a server for multiple other users to access, so feel free to keep on using the `r` variable as you go through learning to use PRAW.", "created_utc": 1421700599, "gilded": 0, "name": "t1_cnu6czd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2syxjd/not_sure_what_to_do_about_warning_in_the_praw/"}, {"author": "letgoandflow", "body": "Thanks for the info. Sorry I sounded like such a n00b, but I'm already serving an app that uses PRAW and OAuth. I do have some code that is similar to what you posted. I'm wondering what the alternative would be so that you don't run into the issue you outlined. Taking your code, would switching it to something like this work? import praw def perform_request(**access_information): # Method is called when a user connects to the web page r = praw.Reddit(\"...\") r.set_oauth_app_info(...) # Pretend the rest of authorization with user was done before this point r.set_access_credentials(**access_information) # Now authenticated as user print(r.get_me())", "created_utc": 1421707394, "gilded": 0, "name": "t1_cnuantd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2syxjd/not_sure_what_to_do_about_warning_in_the_praw/"}, {"author": "nemec", "body": "Ah, okay. Yes, your alternative looks correct. Basically, just make sure that a single PRAW instance isn't accessible to more than one user.", "created_utc": 1421708654, "gilded": 0, "name": "t1_cnubfip", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2syxjd/not_sure_what_to_do_about_warning_in_the_praw/"}, {"author": "letgoandflow", "body": "Ok, thanks.", "created_utc": 1421722791, "gilded": 0, "name": "t1_cnujkts", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2syxjd/not_sure_what_to_do_about_warning_in_the_praw/"}, {"author": "nath_schwarz", "body": "Check if the website has an api. That would be the easiest way. If not you'll need to pull the website and parse the information, how that works depends if you use python2 or python3.", "created_utc": 1421581050, "gilded": 0, "name": "t1_cnsqha7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2st4us/trying_to_pull_data_from_a_website/"}, {"author": "TBNL", "body": "You might look into the pypi packages requests (loading any web resource) and beautifulsoup (parsing HTML). Should allow you to set up a poc pretty fast.", "created_utc": 1421612039, "gilded": 0, "name": "t1_cnt24c6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2st4us/trying_to_pull_data_from_a_website/"}, {"author": "GoldenSights", "body": "You dont \"bypass\" the captcha. When your account has sufficient link karma, it will stop prompting you. This means you can head to /r/freekarma or make a submission somewhere. I think you only need like 10-15 to ease up on the limits.", "created_utc": 1421361491, "gilded": 0, "name": "t1_cnq9lm1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "wscottsanders", "body": "Got it. Thanks.", "created_utc": 1421361531, "gilded": 0, "name": "t1_cnq9mhe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "wscottsanders", "body": "BTW Just curious...What are they trying to explain in the praw documentation? Thanks for the help.", "created_utc": 1421361669, "gilded": 0, "name": "t1_cnq9pkt", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "GoldenSights", "body": "Well I haven't had to deal with captchas since I started PRAW so I don't remember how it all works. I remember that when I was testing things from the commandline, I would receive a CaptchaError, and it would print the url to a captcha. I'd paste that into Chrome, and then type the letters back into my CMD and the post would go through. If you were using PRAW to build a gui program, you could use the provided captcha iden to get an image from http://www.reddit.com/captcha/EBG2KudNny2pB6o5x2VTOIeyjT9n92SH.png and the user could solve it within your application. For the record, the solution to that image is different on my browser than what you put on the post, so maybe they expire or reset after a single use, I don't know. tl;dr There is a way to do captchas with PRAW but I don't know how. I'm guessing your hard-coded solution doesn't work because they only want each captcha to work once.", "created_utc": 1421366557, "gilded": 0, "name": "t1_cnqcjdq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "bboe", "body": "As /u/GoldenSights has suggested, by default PRAW will prompt and hang waiting for a captcha response to be typed into the shell where the PRAW program is running. By adding `raise_captcha_exception=True` a PRAW user can catch and subsequently write their own logic for fetching and obtaining the captcha response bypassing PRAW's built-in handler.", "created_utc": 1421426574, "gilded": 0, "name": "t1_cnr0gwc", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2skb9u/bypassing_captcha_to_post_updates_to_a_reddit/"}, {"author": "GoldenSights", "body": "Yep, formatting is just like usual. The only thing to keep in mind is that newlines, like hitting Enter, are represented by `\\n` and you need two of them to produce a linebreak. SETRESPONSE1 = \"[redditdev](http://reddit.com/r/redditdev)\\n\\n[learnpython](http://reddit.com/r/learnpython)\" equals [redditdev](http://reddit.com/r/redditdev) [learnpython](http://reddit.com/r/learnpython) _____ You can also use Python's multi-line string formatting, still using two linebreaks for every reddit one. SETRESPONSE1 = \"\"\"[redditdev](http://reddit.com/r/redditdev) [learnpython](http://reddit.com/r/learnpython)\"\"\"", "created_utc": 1421292096, "gilded": 0, "name": "t1_cnpf8ml", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sh6sp/how_can_i_use_reddit_formatting_with_a_bot_using/"}, {"author": "Brandon23z", "body": "Okay, the \\n thing worked. I never used it in C++, lol I usually manually endl at the end of each cout, but this is not for the console, it is actually sending the comment with that format, you know? Anyways it worked. Now how does a bot read formatting? For example, does it read **word** as \"word\" or \"asterisk asterisk word asterisk asterisk\"? Can it pick up hyperlinks or just the words on top of them?", "created_utc": 1421293034, "gilded": 0, "name": "t1_cnpfqpr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sh6sp/how_can_i_use_reddit_formatting_with_a_bot_using/"}, {"author": "GoldenSights", "body": "It will receive the comment exactly as it was typed, with no sense of formatting. To see for yourself, just use PRAW from the commandline, retrieve a comment, and `print(comment.body)` Also, http://www.reddit.com/api/info.json?id=t1_cnpfqpr", "created_utc": 1421293161, "gilded": 0, "name": "t1_cnpft6i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sh6sp/how_can_i_use_reddit_formatting_with_a_bot_using/"}, {"author": "Brandon23z", "body": "I don't know what that link means. I want to it and didn't understand it. I saw t1 stuff so I assume it has something to do with a reddit page but I have no idea. So it doesn't pick up formatting marks? It just picks up the plain text? **bold** Will it pick up \"bold\" or \"star star bold star star\"?", "created_utc": 1421296279, "gilded": 0, "name": "t1_cnphdgs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sh6sp/how_can_i_use_reddit_formatting_with_a_bot_using/"}, {"author": "GoldenSights", "body": "Like I said, you can print a comment body to see it for yourself. It will print **bold** exactly as the user typed it. That link contains all the information about [your comment](http://www.reddit.com/r/redditdev/comments/2sh6sp/how_can_i_use_reddit_formatting_with_a_bot_using/cnpfqpr). You can see the \"body \"attribute, \"body\": \"... Anyways it worked.\\n\\nNow how does a bot read formatting? For example, does it read **word** as \\\"word\\\" or \\\"asterisk asterisk word asterisk asterisk\\\"? ...\"", "created_utc": 1421298017, "gilded": 0, "name": "t1_cnpi6ma", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sh6sp/how_can_i_use_reddit_formatting_with_a_bot_using/"}, {"author": "V2Blast", "body": "Shows a thumbnail for me (now, at least).", "created_utc": 1421470770, "gilded": 0, "name": "t1_cnroiu3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2sdb6w/why_do_some_thumbnails_return_default/"}, {"author": "GoldenSights", "body": "Some libraries, like sqlite3 (and apparently requests) rely on an underlying system written in C which needs to be installed separately. On the raspberrypi this is done with 'sudo apt-get install'. Cant speak for ga but this may give you some ideas. Simply copying files probably wont work.", "created_utc": 1421101078, "gilded": 0, "name": "t1_cnmwtou", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2s7mcc/using_praw_with_google_app_engine/"}, {"author": "hassanchug", "body": "I see, thank you. I have a Raspberry Pi laying around that I haven't done much with, so I might consider just doing away with the App Engine altogether and running straight from the Pi, because after quite a bit of googling it doesn't seem like there's any way of getting GAE and PRAW to work together easily. Ah well.", "created_utc": 1421101404, "gilded": 0, "name": "t1_cnmx0n2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2s7mcc/using_praw_with_google_app_engine/"}, {"author": "GoldenSights", "body": "The Pi was frustrating to set up, especially installing Py3.4 with sqlite, but now that I've got it working it's been very nice. It's not a very fast device, though, so if your bot is supposed to be scanning /r/all you probably wouldn't keep up.", "created_utc": 1421109613, "gilded": 0, "name": "t1_cnn1iu5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2s7mcc/using_praw_with_google_app_engine/"}, {"author": "hassanchug", "body": "I will have to keep that in mind if I ever make a more serious bot (this one is just-for-fun and will only be enabled on a couple of my own subreddits.) It won't be scanning /r/all but it will be polling its own inbox every few seconds and replying to comments where it is mentioned ([this recent change](http://www.reddit.com/r/redditdev/comments/2rnzkd/attn_bot_maintainers_username_mentions_for/) makes things a lot easier). It should hopefully be fast enough for that, but I'll see how things go.", "created_utc": 1421110777, "gilded": 0, "name": "t1_cnn25i2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2s7mcc/using_praw_with_google_app_engine/"}, {"author": "damontoo", "body": "For whatever reason PRAW uses some less popular dependencies that require compiling, so it will never work on GAE. I gave up trying and just wrote a simple module to replace it that basically just handles before/after, setting UA string, parsing the JSON results and rate limiting. My use didn't require OAuth support or anything so it was pretty easy.", "created_utc": 1421181671, "gilded": 0, "name": "t1_cnnx8sn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2s7mcc/using_praw_with_google_app_engine/"}, {"author": "kemitche", "body": "Hello there! I'll try and help out as best as I can. First, looking at your code, you seem to be mixing OAuth access and cookie based access. Mixing those two authentication mechanisms won't work. I recommend dropping the login() and getModhash() functions, and using an OAuth \"script\" client: https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example Also or alternatively, I recommend looking up and using PRAW (Python Reddit API Wrapper), a 3rd party SDK for accessing reddit's API from python scripts. I'm also going to try and answer your specific questions, but bear the above in mind. > Why isn't https://ssl.reddit.com/api/v1/access_token in any documentation? It's documented under https://github.com/reddit/reddit/wiki/OAuth2. Apologies, reddit's documentation is split and incomplete. > Why the hell am I getting so many 429 errors? You need to set your custom `User-Agent` on ALL requests you make to the reddit servers (based on your code, you're only doing it in the `authorize()` function). > I also sometimes get 401, 403, and 404 errors with login() and getModhash() as well. 401: Most likely means your access token is expired (attempting to access oauth.reddit.com without an access token or with an expired token) 403: You don't have permission to access the resource 404: The URL you're attempting to access doesn't exist. > Even after login has returned a 200, getModhash() will still give me this: {\"json\": {\"errors\": [[\"USER_REQUIRED\", \"please sign in to do that\", null]]}} The way you're using the python `requests` lib is resulting in the result of your login calls to not be saved. You need to use a [session](http://docs.python-requests.org/en/latest/user/advanced/#session-objects) if you want to use cookie authentication with `requests` (but again, I recommend using OAuth - in which case, you need to be sending an `Authorization` header with an access token). > Am I making this much more complicated than it needs to be? A little bit, but mostly you're stumbling across a combination of (from what I can see) being new to HTTP, being new to APIs, being new to reddit's API, and reddit's API documentation being less than ideal. So don't feel bad about it! A lot of people run into similar problems.", "created_utc": 1420569016, "gilded": 0, "name": "t1_cngdoly", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "jakethespectre", "body": "That was very helpful! I didn't realize there were two types of authentication. And thanks for that second page of documentation. However, I now have a few more questions: **1.** To set the User-Agent on all requests, would I just add it to the end of listthing to everything I send over? **2.** How do I know which functions work with oauth and which functions work for cookie based? **3.** What's the difference between the two ways? **4.** How do I know which addresses should have ssl, oauth, or www in front of them? **5.** What is the redirect uri, why do I need it, and how do I make one that's useful? **6.** How was I able to get a token at all? I think I did *everything* wrong, but it still worked sometimes. EDIT: It still isn't working... [This](http://pastebin.com/y4qjX6j6) is my new code. I get the token fine, but authorize() always gives me a 403 and sendSS() always gives a 429.", "created_utc": 1420576224, "gilded": 0, "name": "t1_cngi53a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "kemitche", "body": "> 1. To set the User-Agent on all requests, would I just add it to the end of listthing to everything I send over? You'd need to pass in a `headers` dict with every request (PRAW, if you use that, manages this for you) > 2. How do I know which functions work with oauth and which functions work for cookie based? [The API list](/dev/api) indicates which API endpoints are OAuth accessible. Note: There are separate \"/api/me\" and \"/api/v1/me\" endpoints for cookies and OAuth respectively. > 3. What's the difference between the two ways? Cookie authentication is the \"old\" way. It's non-ideal because it encourages 3rd party developers to directly ask for a reddit user's password to access their account. OAuth 2 authentication allows a redditor to temporarily or permanently grant a 3rd party app or website access to their account (and to limit what actions the 3rd party can do with their credentials). > 4. How do I know which addresses should have ssl, oauth, or www in front of them? ssl is no longer needed (www can and SHOULD be accessed over https for all API access). Use `www.reddit.com` for cookie-authenticated requests. Use `www.reddit.com` when RETRIEVING an access token for use in OAuth. Use `oauth.reddit.com` when USING the access token to make API requests. > 5. What is the redirect uri, why do I need it, and how do I make one that's useful? The redirect URI is used in some OAuth 2 flows to return the user back to your website or app. For example, say I made \"supercool.com\" and I want to let redditors use my website to do something supercool with their reddit accounts. The redditor would do the following: 1. Go to supercool.com, and click a button that says \"Do the supercool thing\". 2. The button sends me to reddit.com, and the redditor sees a page that says \"Hey, supercool.com wants to use your reddit account to do X Y and Z. Is that ok?\" 3. If the user clicks the \"Allow\" button on that page, they are *redirected* back to supercool.com - how and where exactly? That's what the redirect URI is for - you register with reddit to tell us where on your site to send users after they click allow. The redirection URL will include information that allows supercool.com to create an access token, and that access token will let supercool.com's back-end web servers use the reddit API as if they were \"logged in\" as the redditor. > 6. How was I able to get a token at all? I think I did everything wrong, but it still worked sometimes. You used the `password` grant_type perfectly - that's how! (See, you didn't do everything wrong!) All you needed to do from there to get your script working (and really, I'm sorry, I should have *started* with this) is to SAVE the access token somewhere, and SEND it with each of your requests; e.g.: def get_my_info(access_token, user_agent): headers = {\"User-Agent\": user_agent, \"Authorization\": \"bearer \" + access_token} r = requests.get(\"https://oauth.reddit.com/api/v1/me\", headers=headers) print r.json()", "created_utc": 1420658087, "gilded": 0, "name": "t1_cnhiz0d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "jakethespectre", "body": "What if I don't need to redirect to anything? All I want is for my script to upload the stylesheet and get out. Can I just ignore the redirect uri completely?", "created_utc": 1420676583, "gilded": 0, "name": "t1_cnhugf2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "kemitche", "body": "Yes, you can ignore / use a dummy URL for redirect_uri when using the password grant_type.", "created_utc": 1420739954, "gilded": 0, "name": "t1_cnik97u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "jakethespectre", "body": "~~I'm constantly getting a 403 error for my redirect_uri when using /v1/authorize. I have it copied correctly, believe me, I've checked many times. I also read [this](https://www.reddit.com/r/redditdev/comments/197x36/using_oauth_to_send_valid_requests/) and it seems like he does /v1/authorize first, and then gets the token from /v1/access_token even though I thought I needed to do it the other way around. (get the token and use it to authorize)~~ When trying to post the stylesheet I get 429 errors still. Can I ignore the modhash since I'm using oauth? ~~EDIT: I did a thing! I added an 's' to http, and it stopped erroring. If I put the /v1/authorize code into my browser it shows me [this](http://imgur.com/RJRT5tt)! But this isn't really what I want. Shouldn't it already have my permission since it's a script and i'm the developer?~~ EDIT 2: I decided to go with a permanent token (it seemed complicated so earlier I didn't want to try), and now i'm getting an 'unsupported_grant_type' error. Currently I have 'grant_type': 'authorization_code' written. Wat do?", "created_utc": 1420789181, "gilded": 0, "name": "t1_cnjaij4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "kemitche", "body": "> Can I ignore the modhash since I'm using oauth? Yes, there's no modhash on OAuth. > Wat do? Let's start over - what are you trying to do, and who is your audience? You've sort of mixed and matched a couple different methods so I want to try and get things fleshed out from scratch to try and clarify everything.", "created_utc": 1420845439, "gilded": 0, "name": "t1_cnjzg9k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "jakethespectre", "body": "I want to upload a stylesheet from my computer to reddit. Audience: me. Also, thanks for your help. You've been very patient with me.", "created_utc": 1420850090, "gilded": 0, "name": "t1_cnk1whe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "kemitche", "body": "Cool, so if it's just for you, we can take the \"password\" grant_type approach that you sort-of started with! Have you followed the \"script quick start example\" I posted earlier? https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example There are essentially 2 \"parts\" you need to worry about. Part 1: Authenticating, a.k.a. getting a token. That's the call to https://www.reddit.com/api/v1/access_token Part 2: Using the access token. That's the part where you take the token you got above, put it into an \"Authorization\" header, and make API requests (such as uploading a new stylesheet) So, at a high level, you might want a Python file structured sort of like this: # imports import requests import requests.auth USER_AGENT = \"Your custom user agent\" def get_access_token(username, your_password, client_id, client_secret): # make the appropriate call to /api/v1/access_token headers = {\"User-Agent\": USER_AGENT} client_auth = requests.auth.HTTPBasicAuth(client_id, client_secret) post_data = {\"grant_type\": \"password\", \"username\": username, \"password\": your_password} response = requests.post(\"https://www.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data, headers=headers) response.raise_for_status() # Will raise an exception if the above call failed; you may want different error handling token = response.json()[\"access_token\"] return token def update_stylesheet(new_stylesheet, token): headers = {\"User-Agent\": USER_AGENT, \"Authorization\": \"bearer \" + token} post_data = {} response = requests.post(...) response.raise_for_status() # Will raise an exception if the above call failed; you may want different error handling def do_everything(): token = get_access_token(...) new_stylesheet = update_stylesheet(new_stylesheet, token) if __name__ == '__main__': do_everything()", "created_utc": 1420852214, "gilded": 0, "name": "t1_cnk2zr0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "jakethespectre", "body": "Wow. I was definitely making this much more complicated than it needed to be, probably because of all the different types of access and ways to go about access that exist. Thank you so much. It works!", "created_utc": 1420854393, "gilded": 0, "name": "t1_cnk42xr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "kemitche", "body": "I'm glad I could get you through it, and wish you good luck in your reddit dev'ing!", "created_utc": 1421085569, "gilded": 0, "name": "t1_cnmngkr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ri0r7/a_lot_of_questions_about_the_reddit_api/"}, {"author": "yappingboy", "body": "I am getting the same thing.. I have yet to find a solution", "created_utc": 1420645345, "gilded": 0, "name": "t1_cnhb9m3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rdtf1/praw_in_qpython/"}, {"author": "sallurocks", "body": "Ah, I think I'll contact the qpython guys and see if they have a solution.... If you do find something do let me know", "created_utc": 1420646011, "gilded": 0, "name": "t1_cnhbmi4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rdtf1/praw_in_qpython/"}, {"author": "nath_schwarz", "body": ">File \"C:\\Users\\Steam\\Desktop\\sendStylesheet.py\", line 6, in r.login('jakethespectre','[redacted]'); ... Also take a look at [this](https://github.com/nathschwarz/rwikibot) - it lets you push and pull the stylesheet including wiki, sidebar, etc pp. The error itself looks like that either reddit sent back invalid data or your praw-package is outdated. Edit: No, its actually your first line. `r = praw.Reddit(user_agent = 'here goes what you want')`. `user_agent` is an optional argument, not positional. Edit 2: you also might want to take a password that is a bit stronger. Also, take a look at KeePass and/or lastpass.", "created_utc": 1420436575, "gilded": 0, "name": "t1_cnev5os", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "jakethespectre", "body": "Whoops! I thought I checked it! Also, I'm not sure what you're saying I should do? I replaced my r=praw.Reddit() line with what you have, and I changed the text too, but it still gives me the same error.", "created_utc": 1420437637, "gilded": 0, "name": "t1_cnevkyp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "nath_schwarz", "body": "Then your praw package is probably out of date and you have to update it.", "created_utc": 1420437983, "gilded": 0, "name": "t1_cnevpre", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "jakethespectre", "body": "I downloaded it yesterday.", "created_utc": 1420439180, "gilded": 0, "name": "t1_cnew5wu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "nath_schwarz", "body": "Mhm, I just tried - same outcome as yours, yesterday it worked fine. I guess reddit changed the API. So we have to wait for a praw update.", "created_utc": 1420439373, "gilded": 0, "name": "t1_cnew8g0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "jakethespectre", "body": "is the rwikibot broken as well, then?", "created_utc": 1420441190, "gilded": 0, "name": "t1_cnewug1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "nath_schwarz", "body": "Aye, as long as this isn't fixed either by reddit or by praw there is nothing that can be done - aside from changing the praw code yourself, but that causes more trouble and takes longer than waiting.", "created_utc": 1420441308, "gilded": 0, "name": "t1_cnewvtm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "nath_schwarz", "body": "https://github.com/praw-dev/praw/issues/358 The issue is enabled https, you have to [disable it](https://www.reddit.com/prefs/security/) to use username/password auth with your account.", "created_utc": 1420568114, "gilded": 0, "name": "t1_cngd4nc", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "why_the_love", "body": "Thank you. Saved me a lot of time.", "created_utc": 1422136995, "gilded": 0, "name": "t1_cnzoesj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2rd7n3/cant_login_with_praw/"}, {"author": "bboe", "body": "Use the submission stream as it orders them in the correct order. https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html?highlight=stream#praw.helpers.submission_stream", "created_utc": 1420317246, "gilded": 0, "name": "t1_cnde01q", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2r8974/pushing_submissions_to_twitter_but_not_correct/"}, {"author": "armandg", "body": "Great, thanks!", "created_utc": 1420319836, "gilded": 0, "name": "t1_cndfaiw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2r8974/pushing_submissions_to_twitter_but_not_correct/"}, {"author": "zooparoo_skidoo", "body": "This is an example comment, t1_cnbign0", "created_utc": 1420149630, "gilded": 0, "name": "t1_cnbign0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2r1g5a/proper_way_to_access_comment_replies_in_praw/"}, {"author": "zooparoo_skidoo", "body": "This is an example reply to a comment t1_cnbigut", "created_utc": 1420149642, "gilded": 0, "name": "t1_cnbigut", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2r1g5a/proper_way_to_access_comment_replies_in_praw/"}, {"author": "exoendo", "body": "moderation queue contains comments, but **unmoderated links** is just submissions. If you are pretty confident that 99%+ isn't spam, it's probably more efficient to just auto approve everything in your unmoderated links queue (or just camp in the new queue) import praw import time user_agent = ('Automoderator for GAPINGCUNT') r = praw.Reddit(user_agent=user_agent) r.login(username,password) subreddit = reddit.get_subreddit('TestSubreddit') while True: for link in subreddit.get_unmoderated(limit=None): link.approve() time.sleep(1800) # sleep 30 minutes", "created_utc": 1420010695, "gilded": 0, "name": "t1_cna5m5b", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qw7gu/check_if_item_in_modqueue_is_a_flagged_post_or_a/"}, {"author": "GAPINGCUNT", "body": "Thank you! This works great. I did not even consider just getting unmoderated links. Luckily my subreddit only gets a few posts every day, so the spam submissions are incredibly rare.", "created_utc": 1420035769, "gilded": 0, "name": "t1_cnabbjl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qw7gu/check_if_item_in_modqueue_is_a_flagged_post_or_a/"}, {"author": "Deimorz", "body": "Sounds like you already have a solution that works, but for the record, AutoModerator does this by skipping over anything that doesn't have the `banned_by` attribute set: https://github.com/Deimos/AutoModerator/blob/master/automoderator.py#L907-L909 If `banned_by` has a false/none value, that means the item isn't removed, so it must be reported.", "created_utc": 1420063997, "gilded": 0, "name": "t1_cnapa7u", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qw7gu/check_if_item_in_modqueue_is_a_flagged_post_or_a/"}, {"author": "GAPINGCUNT", "body": "Thanks! I actually thought long and hard about using automoderator as it could do everything I need and more, but since I'm in the process of learning python I wanted to challenge myself to make my own moderator.", "created_utc": 1420090300, "gilded": 0, "name": "t1_cnazqgj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qw7gu/check_if_item_in_modqueue_is_a_flagged_post_or_a/"}, {"author": null, "body": "Unless you're running multiple bots, praw automatically inserts enough sleep() time in between api requests to reddit to conform to the rules. Is this the only bot that's running?", "created_utc": 1419967283, "gilded": 0, "name": "t1_cn9k6wg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": "haiguise1", "body": "Yes, its the only bot running on that pc and account.", "created_utc": 1419974968, "gilded": 0, "name": "t1_cn9oovv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": null, "body": "Try killing the extra wait time and see if it affects how the program runs.", "created_utc": 1419979877, "gilded": 0, "name": "t1_cn9rhp0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": "haiguise1", "body": "I didn't add any extra wait time myself, to find the timing for the requests I had a loop start time and then in the loop I put a time.time() after each reddit request. They had an average of 2 seconds between requests.", "created_utc": 1419981302, "gilded": 0, "name": "t1_cn9s9h2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": null, "body": "What I mean to say is, praw does all timing automatically. No need to worry about it unless you're running multiple scripts.", "created_utc": 1419981880, "gilded": 0, "name": "t1_cn9skio", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": "haiguise1", "body": "I figured out the problem, HistoricalWhatIf had been set to private, and it was number 30 in my list of subreddits. Thanks for the help.", "created_utc": 1420037188, "gilded": 0, "name": "t1_cnabtis", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qu0tu/403_error_using_praw/"}, {"author": "GoldenSights", "body": "If you have HTTPS enabled on your account, try disabling that and see if it goes through. PRAW3 will have https support, and you can download the repo in development https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/ If https isn't the problem, I'm not sure.", "created_utc": 1419886425, "gilded": 0, "name": "t1_cn8mmbw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qqm9j/cant_log_in_with_praw_any_more/"}, {"author": "rhiever", "body": "https doesn't seem to be the issue. Darn.", "created_utc": 1419909383, "gilded": 0, "name": "t1_cn8yupu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qqm9j/cant_log_in_with_praw_any_more/"}, {"author": "bboe", "body": "Have you tried either PRAW3 or the latest master branch?", "created_utc": 1421515361, "gilded": 0, "name": "t1_cns13ia", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qqm9j/cant_log_in_with_praw_any_more/"}, {"author": "rhiever", "body": "I've updated to the latest version of PRAW with: >sudo pip install --upgrade praw Unless now I need to install praw3 or something instead?", "created_utc": 1421517808, "gilded": 0, "name": "t1_cns29vq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qqm9j/cant_log_in_with_praw_any_more/"}, {"author": "largenocream", "body": "On reddit there's a user preference to automatically hide submissions with a score below a certain amount, and it defaults to `-4`. Are you able to fetch the submissions if you blank out that field?", "created_utc": 1419774480, "gilded": 0, "name": "t1_cn7co5y", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qm6ml/praw_prevent_get_new_hiding_low_karma_submissions/"}, {"author": "RuleIV", "body": "Interesting. I was aware of this preference, but I never considered the the preferences on the website would effect the bot. Yes, now that I've changed that setting I'm getting the hidden submission. Thank you.", "created_utc": 1419774818, "gilded": 0, "name": "t1_cn7cqxs", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2qm6ml/praw_prevent_get_new_hiding_low_karma_submissions/"}, {"author": "ELFAHBEHT_SOOP", "body": "Okay, so I think you need to use the raw REST request in order to get the list of all the comments. If you request this page http://www.reddit.com/r/millionairemakers/comments/2q36z6/reddit_lets_make_a_millionaire/.json?sort=new It will give you a list of all of the IDs of every comment. Now, I'm not sure if that is all of the top level comments or all of the comments in general. However, once you get that list of IDs you can then figure out whose comment it is by using http://www.reddit.com/r/millionairemakers/comments/2q36z6/reddit_lets_make_a_millionaire/ [comment ID] For example if I take the top one in the list when I request it (the ID is cn3fmq6), it goes like: http://www.reddit.com/r/millionairemakers/comments/2q36z6/reddit_lets_make_a_millionaire/cn3fmq6 So I say you should just request that page and parse out the IDs and only use the IDs when finding out who wins. If the comment ends up being not a top level comment you could just get the next comment in the list if you don't want to check every comment to see if it is a top level comment or not. Edit: There are also comments above the huge list of IDs, make sure to get those too.", "created_utc": 1419361382, "gilded": 0, "name": "t1_cn3hlud", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "thegrassygnome", "body": "Would you still be able to detect duplicate entries with this method?", "created_utc": 1419367934, "gilded": 0, "name": "t1_cn3l5lx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "ELFAHBEHT_SOOP", "body": "That was one of my concerns as well. I think once the comment is chosen, they can go to the user's comment history and make sure that they only posted to the thread one time. Same with all of the other restrictions. Then if posting was against the restriction, they can just go to the next comment.", "created_utc": 1419368172, "gilded": 0, "name": "t1_cn3la6d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "Overclock", "body": "If the posting was against the rules they should do a re-draw with the next bitcoin number mined, going to the next comment means a comment would benefit by having a large number of fake comments made before it.", "created_utc": 1419370763, "gilded": 0, "name": "t1_cn3mm5l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "10nix", "body": "But having posts against the rules in the population at all skews the odds. If that is deemed to be negligible or acceptable the mods, then them's the rules, but they should be aware of it.", "created_utc": 1419374836, "gilded": 0, "name": "t1_cn3olsp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "Overclock", "body": "Having posts against the rules wont skew the odds in any ones favor, unless you use the \"next comment\" method, then the the ineligible posts give an advantage to those that made an eligible post right after the ineligible posts.", "created_utc": 1419375428, "gilded": 0, "name": "t1_cn3ow0q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "ELFAHBEHT_SOOP", "body": "Good point. That's a better idea.", "created_utc": 1419371891, "gilded": 0, "name": "t1_cn3n6mw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "Lentil-Soup", "body": "Once you have the data you need, it's pretty easy to find duplicates, isn't it?", "created_utc": 1419368891, "gilded": 0, "name": "t1_cn3lnoj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "ELFAHBEHT_SOOP", "body": "Not necessarily, because the winner will be chosen from a list of IDs. Those IDs are different for every single comment. Then we would have to request every single comment in order to make sure it's not a duplicate of another comment in the list. I suppose once you get every single comment and all the data associated with each comment the process will become much faster because you could technically load all of the comments into your RAM. So yes it's easy once we have all of the data, but getting the data is going to be quite the process.", "created_utc": 1419369487, "gilded": 0, "name": "t1_cn3lyot", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "GoldenSights", "body": "Like I said, getting the data should take a solid 40 minutes, but the process won't be difficult. The really long part is checking for account activity.", "created_utc": 1419370168, "gilded": 0, "name": "t1_cn3mb7a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "ELFAHBEHT_SOOP", "body": "Oh yeah, I just meant it was going to take a while. Like 40 minutes. Then another huge while on top of that.", "created_utc": 1419370341, "gilded": 0, "name": "t1_cn3mef1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "Lentil-Soup", "body": "Your thought process was the same as mine. Haha. Agreed - Getting the data is going to take some time. I figured that much when I saw it break 100k comments.", "created_utc": 1419369755, "gilded": 0, "name": "t1_cn3m3m1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "GoldenSights", "body": "Actually, now that we have the list of ID numbers, it will only take about 40 minutes of continuous ID requesting to get the actual comment data for every comment in the thread. Then, it should be trivial to remove non-roots and roots with duplicate authors. However, the Millionaire thread states that the account must be older than the subreddit with some level of activity. This means an unknown number of account queries as well. This millionaire probably won't be getting his money for a few days...", "created_utc": 1419368761, "gilded": 0, "name": "t1_cn3ll9v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "ELFAHBEHT_SOOP", "body": "Yeah, that's why I suggested just going down the list. Although having non-entries in with the entries wouldn't be fair because it could skew the results. Good lord, this is going to take forever. They are also going to have to define in exact terms what an acceptable level of activity is in order to make this fair.", "created_utc": 1419369116, "gilded": 0, "name": "t1_cn3lrs4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "GoldenSights", "body": "Woah, that's really cool. But why do they have this system in place when they normally restrict us to 100 things at a time? Is this on purpose? Obviously they aren't giving us the post data, but still. Anyway, I've distilled it down to all potential comment ID numbers [here](https://raw.githubusercontent.com/voussoir/else/master/Trash/MillionaireThread.txt), was too big to put on Pastebin. The post data says there are 120,588 comments, but my file has 121,322 lines, but this will definitely speed everything up.", "created_utc": 1419367811, "gilded": 0, "name": "t1_cn3l39c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "ELFAHBEHT_SOOP", "body": "Yeah, I have no idea. This is the first time I saw this in a post request. Maybe it's for contest threads? I don't know. Also, the last number in your list is the ID of the post. I don't think that needs to be in there.", "created_utc": 1419368282, "gilded": 0, "name": "t1_cn3lcaw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "GoldenSights", "body": "No, t3_ is submission, t1_ is comment http://www.reddit.com/r/millionairemakers/comments/2q36z6/reddit_lets_make_a_millionaire/cn3gtrt A removed comment, but a comment nonetheless. Also, that ID number is reapeated at the top of my list, too. OP has some cleanup to do anyway, I'm just making his job a little harder", "created_utc": 1419368468, "gilded": 0, "name": "t1_cn3lfsz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "ELFAHBEHT_SOOP", "body": "Oh yeah. Okay, that makes sense.", "created_utc": 1419368603, "gilded": 0, "name": "t1_cn3libz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "GoldenSights", "body": "Re-invent your system. When the thread is started, tell the bot to start monitoring /r/millionairemakers/comments, and if a comment is made in the correct thread save it to a pool that can be referenced later. No more hassle with threaded comments, and the pool grows in real time. Obviously this answer isn't very helpful for the situation at hand. I've personally never had this error before.", "created_utc": 1419333486, "gilded": 0, "name": "t1_cn36eco", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "minlite", "body": "Thanks. I should consider doing this next time. I didn't expect to get 110k comments. I tested the bot with our last drawing that had 8k comments and it worked perfectly..", "created_utc": 1419333795, "gilded": 0, "name": "t1_cn36ggb", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "GoldenSights", "body": "That really is a lot of comments :/ It's going to be a major problem. The threaded system is going to be prohibitively slow, I think. The only alternative that comes to mind is to grab *all* comments sitewide, ranging from before the thread (roughly [cn2deld](http://www.reddit.com/api/info.json?id=t1_cn2deld)) to something fresh (roughly [cn36h2f](http://www.reddit.com/api/info.json?id=t1_cn36h2f)) which spans 1.3 Million IDs and would take 7.5 hours to fetch at 100 items per request. Shit, trying to expand all the threads would probably take even longer.", "created_utc": 1419334413, "gilded": 0, "name": "t1_cn36kmb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "GoldenSights", "body": "Hey OP, where are you at now? Since we've got all the comment IDs, it should be simple to rig up a request to get all the actual post data. If you'd like a hand with that, I'm more than happy to help.", "created_utc": 1419370588, "gilded": 0, "name": "t1_cn3mixf", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "minlite", "body": "Hey thanks. I am testing a new system now. I ditched praw and used reddit API directly. If that didn't work out, I will really appreciate your help! Thanks again..", "created_utc": 1419372111, "gilded": 0, "name": "t1_cn3najh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "sk8218", "body": "Pick me!", "created_utc": 1419382304, "gilded": 0, "name": "t1_cn3s2f1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "angrypotato1", "body": "Can you try contacting the admins? We need this to work, badly.", "created_utc": 1419345422, "gilded": 0, "name": "t1_cn39hn9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2q5zly/error_413_too_big_with_praw/"}, {"author": "bboe", "body": "https://www.reddit.com/r/redditdev/comments/2pumjt/issues_with_praw/", "created_utc": 1419151422, "gilded": 0, "name": "t1_cn16jl5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pyi07/value_error_when_trying_to_login/"}, {"author": "picflute", "body": "UGHHHH COMMAND PROMPT WHY WONT YOU LET ME INSTLL THIS", "created_utc": 1419151989, "gilded": 0, "name": "t1_cn16nvj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pyi07/value_error_when_trying_to_login/"}, {"author": "GoldenSights", "body": "You want all of the post's comments sorted by score, regardless of their depth in any comment chain? How about flat_comments = praw.helpers.flatten_tree(submission.comments) flat_comments.sort(key=lambda comment: comment.score, reverse=True) reverse=True puts the highest rated at the front of the list. Also, change your useragent!! Include a username, give detail.", "created_utc": 1419101574, "gilded": 0, "name": "t1_cn0mxnu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "chaz6", "body": "Thanks for the suggestion, but this will only work for that comments that have been downloaded. I only want to download the top comments for the submission. With your suggestion I would need to download every comment which could take many api calls. Ideally I want the top 10 comments only. P.S. The user agent is just for the purpose of this post.", "created_utc": 1419104735, "gilded": 0, "name": "t1_cn0obs5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "GoldenSights", "body": "That's correct, but what you're asking for is not possible. Since high-ranked comments are generally borne from high-ranked comments, you could sort the thread by /top and get the top few plus their replies, and you'll *probably* hit the top 10, but there is only one way to guarantee it.", "created_utc": 1419105038, "gilded": 0, "name": "t1_cn0ogk4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "chaz6", "body": "Is it at least possible to get the highest rated comments at the top level? I'm not sure how to do this. Thanks!", "created_utc": 1419106547, "gilded": 0, "name": "t1_cn0p483", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "GoldenSights", "body": "You'll need to do some experimenting from the commandline to figure out exactly how this mechanic works. Here's what I'm seeing in the PRAW docs: get_submission(url=None, submission_id=None, comment_limit=0, comment_sort=None, params={}) Return a Submission object for the given url or submission_id. Parameters: comment_limit \u2013 The desired number of comments to fetch. If", "created_utc": 1419106920, "gilded": 0, "name": "t1_cn0pacu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "chaz6", "body": "Brilliant, thank you for your help! I tried reading the docs but it wasn't obvious to me.", "created_utc": 1419112318, "gilded": 0, "name": "t1_cn0rn9q", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2pwgc9/praw_get_comments_for_a_submission_ordered_by/"}, {"author": "RuleIV", "body": "Really late getting back to you, but I had the same problem and found no direct solution. What I did was look at the most submissions my sub got per day, pulled more than that, then removed any that were older than the age I wanted. I just wanted anything less than 24 hours old. This is a snippit of the code. Hope it helps. subreddit = r.get_subreddit(SUB_NAME) new_submissions = subreddit.get_new(limit = SCAN_LIMIT) current_time = int(time.time()) sub_ids_of_past_24_hours = [] authors_of_past_24_hours = [] for submission in new_submissions: sub_age = (current_time - submission.created_utc) / 60 / 60 / 24 if sub_age", "created_utc": 1420597155, "gilded": 0, "name": "t1_cnguew4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2p48qb/praw_getting_all_posts_to_a_subreddit_within_a/"}, {"author": "AnalyseReddit", "body": "I am trying to write a bot that will fetch comments and/or posts from a specific time frame that might be a long time ago. With the limit of the reddit API I can only get about ~1000 items when I ask for contents. I assume that reddit will give me the newest submissions. This would make it impossible for me to fetch posts that are really old because I would have to fetch all newer items first. Is that correct? If so, do you know a solution for that problem?", "created_utc": 1422373356, "gilded": 0, "name": "t1_co2f8n2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2p48qb/praw_getting_all_posts_to_a_subreddit_within_a/"}, {"author": "RuleIV", "body": "I have the exact same problem and no solution. I need to get the last 30 days worth of submissions for a repost bot I'm writing, but the API limits you to about 1000 sorted by new. The only \"solution\" I have so far is to get the 1000 newest submissions, then pull 150 daily until I've got a full 30 days worth. At that point I can just cull anything older than 30 days off, and add anything new on each time the bot runs. By the sounds of things this is not appropriate for what you want to do. If you find a solution please let me know. Oh and funny thing, I haven't checked this account is about a week and a half, and I just looked at it for the first time since then and your message was ten minutes old. Great timing.", "created_utc": 1422374466, "gilded": 0, "name": "t1_co2fub0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2p48qb/praw_getting_all_posts_to_a_subreddit_within_a/"}, {"author": "AnalyseReddit", "body": "I figured I just did not understand the API and made a mistake but it seems that this is a real issue. I think this is a big weakness of the API, because bots cannot easily accses old content for analysis. I wonder if there is anything we can do about it. Maybe if I have the time I will look at the praw-tools code, but thats probably not gonna happen. I hope that we will find a solution.", "created_utc": 1422432840, "gilded": 0, "name": "t1_co3bwg4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2p48qb/praw_getting_all_posts_to_a_subreddit_within_a/"}, {"author": "bboe", "body": "If you want praw to actually make requests turn the generator into a list: submissions = list(r.get_subreddit('videos').get_hot(limit=30)) This will result in some sort of exception if you don't have an Internet connection.", "created_utc": 1418138152, "gilded": 0, "name": "t1_cmprcl7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "coolstorym8", "body": "Sweet, that worked like a charm. So it just turns a generator object into a list of submission objects, sweet. They seem to function exactly the same if I did this for both... for link in submissions: print link.url I just want to know if assigning the list() to submissions will then have any effect on my existing code. Is there anything I need to keep in mind when using list(submissions) over submissions? It seems like a plug and play fix.", "created_utc": 1418139498, "gilded": 0, "name": "t1_cmprzh4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "bboe", "body": "This latter approach is actually preferred as you don't really need to convert the generator to a list. I'd just go with this if it's what you need. One thing to pay attention to is depending on how many items you are iterating through, the generator function makes multiple network calls. So you may have some submissions that are processed, and then a failed network call.", "created_utc": 1418218538, "gilded": 0, "name": "t1_cmqqqr9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "coolstorym8", "body": "In regards to the multiple network calls. Does that go against the 1 request every 2 seconds limit for reddit bots? Thats fine if it gets an error, If it comes up, it'll just wait for a connection and retry the whole thing again.", "created_utc": 1418270560, "gilded": 0, "name": "t1_cmri2w0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "bboe", "body": "> Does that go against the 1 request every 2 seconds limit for reddit bots? No.", "created_utc": 1418402361, "gilded": 0, "name": "t1_cmsx1uk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "Falmarri", "body": "https://wiki.python.org/moin/Generators", "created_utc": 1418159768, "gilded": 0, "name": "t1_cmq3ufx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "coolstorym8", "body": "Or what would a good approach be to covering when my internet goes down and it waits for it to come back on?", "created_utc": 1418137992, "gilded": 0, "name": "t1_cmpra16", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ordbo/how_do_i_force_an_exception_error_instead_of/"}, {"author": "bboe", "body": "PRAW doesn't currently support the \"continue this thread\" links: https://github.com/praw-dev/praw/issues/321", "created_utc": 1418138732, "gilded": 0, "name": "t1_cmprm8l", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "howbigis1gb", "body": "If I'm reading correctly - the issue seems to be fixed https://github.com/praw-dev/praw/commit/12623275d9061e754f74dd05276aa71c7025edfd", "created_utc": 1422569375, "gilded": 0, "name": "t1_co52f4y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "bboe", "body": "Yes, it _should_ have been. But there was a bug which requires an unreleased version. Information here: https://www.reddit.com/r/redditdev/comments/2tlzxl/praw_assertion_errors_on_replace_more_comments/", "created_utc": 1422584066, "gilded": 0, "name": "t1_co5ara5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "hooked_dev", "body": "Thanks. I guess I'll just have to pull those deeper links from the website and not the API.", "created_utc": 1418175128, "gilded": 0, "name": "t1_cmqcarh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "howbigis1gb", "body": "bboe seems to indicate that the issue has been fixed https://github.com/praw-dev/praw/commit/12623275d9061e754f74dd05276aa71c7025edfd", "created_utc": 1422569402, "gilded": 0, "name": "t1_co52fpu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "hooked_dev", "body": "Thanks for the headup! I wrote my own website scraper when I detected these special cases but it would be great to keep everything in praw.", "created_utc": 1422571859, "gilded": 0, "name": "t1_co53x2p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "bboe", "body": "Maybe you could try adding the support to PRAW since it's something you'd like to have in the library. (_wishful thinking_)", "created_utc": 1418222961, "gilded": 0, "name": "t1_cmqsfei", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "hooked_dev", "body": "I was thinking of doing it. The problem is the api doesn't seem to give the children links. I would have to either check every message 10 deep or pull HTML.", "created_utc": 1418342648, "gilded": 0, "name": "t1_cmsbkku", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2onkd8/praw_maximum_depth_of_subtrees/"}, {"author": "bboe", "body": "There is no need for `me.json` as the data it returns is the same as `/u/youraccount/about.json`. This data is automatically pulled into `r.user` once you've authenticated.", "created_utc": 1416982150, "gilded": 0, "name": "t1_cmdd8pt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nfe4j/voting_by_api_cant_find_examples_of_mejson_with/"}, {"author": "beardgoggles", "body": "I need the modhash in order to vote. (Not available from r.user).", "created_utc": 1416988206, "gilded": 0, "name": "t1_cmdeykr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nfe4j/voting_by_api_cant_find_examples_of_mejson_with/"}, {"author": "bboe", "body": "If you're using PRAW you don't need to deal with the modhash. It's all taken care of for you.", "created_utc": 1416988420, "gilded": 0, "name": "t1_cmdf0i7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nfe4j/voting_by_api_cant_find_examples_of_mejson_with/"}, {"author": "beardgoggles", "body": "OK, so that's what I'm asking for examples of. The Reddit API had an example of voting, and it says it requires a modhash. I can't find any examples of voting using Praw. Can you be more specific about how the syntax would change using Praw?", "created_utc": 1416991621, "gilded": 0, "name": "t1_cmdfqc7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nfe4j/voting_by_api_cant_find_examples_of_mejson_with/"}, {"author": "GoldenSights", "body": "One problem lies in `flat_comments`. Since you're only looking for top-levels, we only want the roots of the tree, and therefore have no interest in a flattened set. Try the following from the command line: comments = submission.comments for x in comments: print(x) and flat = praw.helpers.flatten_tree(submission.comments) for x in flat: print(x) In the first case, you'll see a single More Comments object at the bottom, and in the second you'll see it all over the place. Secondly, you may want to replace Mores with a custom function instead of the usual `replace_more_comments` which will replace as many as possible, which we don't want. You may want something along these lines comments = submission.comments while isinstance(comments[-1], praw.objects.MoreComments): comments = comments[:-1] + comments[-1].comments() As we saw earlier, the last item in the non-flat comment list will be a More object. This loop will take it out and replace it with the comment data. When finished, ~~everything in `comments` will be a root-level Comment~~ you should have every root-level comment, and you could run it through another filter to get rid of any sneaky Mores. Furthermore, 1. Testing `is_root` does not require additional calls because it is included when you retrieve the comment, so don't worry about using that. 2. You may also want to check for username duplicates before writing to the file so nobody has an unfair advantage. Also double-check that they are all roots just to be safe. 3. Reddit is not returning 200 at a time. In my cmd testing, it appears that my while loop will produce 20 items at a time, and some of those are the sneaky Mores. This *will* be slow. I would get generous with the print statements so you can see the progress and it doesn't feel stuck. edit: It appears that some Mores have snuck into the list after running the While loop, but they shouldn't cause problems because the loop will not expand them. Just make sure to do some type-checking before trying to print it to the file. Could also do something more sophisticated that checks what's in the More data before adding it to the `comments` list. ____ I'm not at home at the moment, so please excuse minor errors.", "created_utc": 1416951645, "gilded": 0, "name": "t1_cmczdw5", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nesds/make_praw_only_replace_top_level_comments_not_the/"}, {"author": "bboe", "body": "No need to write you own replacement, as you can use `replace_more_comments` to simply remove all `More` objects without a single extra request: submission.replace_more_comments(limit=0) http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.replace_more_comments", "created_utc": 1416980432, "gilded": 0, "name": "t1_cmdcnax", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nesds/make_praw_only_replace_top_level_comments_not_the/"}, {"author": "minlite", "body": "Right, If you look at my code I already use this function. The problem is that it descends to the children comments and replaces them. I don't want that. I want only the top level comments to be replaced.", "created_utc": 1416995396, "gilded": 0, "name": "t1_cmdgfyd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nesds/make_praw_only_replace_top_level_comments_not_the/"}, {"author": "bboe", "body": "It's effectively a costless operation (relative to the 2 second request delay) to replace the `More` instances throughout the tree with the `limit=0`. After doing so, then you can iterate through only the top level comments with `for comment in submission.comments()`, and ignoring the `replies` of each.", "created_utc": 1417018481, "gilded": 0, "name": "t1_cmdn79u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nesds/make_praw_only_replace_top_level_comments_not_the/"}, {"author": "minlite", "body": "Thank you so much. I guess that would work. And of course I will be adding them to an array and verify for duplicates, etc.", "created_utc": 1416960924, "gilded": 0, "name": "t1_cmd47oe", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nesds/make_praw_only_replace_top_level_comments_not_the/"}, {"author": "kemitche", "body": "You're developing an Android app, so I'm a little confused - how does PRAW come into play here? What is your back-end server for? If you're just \"forwarding\" requests to reddit via your server, you don't need to do that. You can use OAuth2 to make the requests directly from the Android device. If you do in fact need a back end, then yes, you do need some concept of user's authenticating *to your server*, separate from OAuth-ing to reddit. You can do that and still be effectively REST-ful; the \"statelessness\" means more that each individual request is \"independent\"\\*, not that you don't have cookies/sessions. \\* For example, a \"stateful\" request would be a form with 4 pages, where the server tracks which page your on and your existing requests in the session.", "created_utc": 1416951553, "gilded": 0, "name": "t1_cmczbsg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2nblsd/praw_oauth_and_replying_to_comments/"}, {"author": "bboe", "body": "The problem is that you have https only redditing enabled. That functionality is not compatible with PRAW2. I suppose I could be more explicit about that in PRAW. Your options: * Disable the HTTPS only requirement * Use the beta version of [PRAW3](https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/) (there are no known bugs)", "created_utc": 1416459650, "gilded": 0, "name": "t1_cm7pwph", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "boibtest", "body": "Thanks for the reply - I'll try PRAW3 - but... Where do I have https only enabled? Is that under the reddit account settings or somewhere in praw? I just looked under the reddit settings and I don't see anything about https there. Regardless, thanks for the info.", "created_utc": 1416525437, "gilded": 0, "name": "t1_cm8ffs1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "bboe", "body": "https://www.reddit.com/prefs/security/", "created_utc": 1416543359, "gilded": 0, "name": "t1_cm8oak4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "boibtest", "body": "Thanks.", "created_utc": 1416601700, "gilded": 0, "name": "t1_cm9946a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "186394", "body": "I hope you changed your password.", "created_utc": 1416421932, "gilded": 0, "name": "t1_cm766av", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "boibtest", "body": "Yeah, I did. \"Stupid is as stupid does\" :)", "created_utc": 1416427983, "gilded": 0, "name": "t1_cm79opo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "186394", "body": "Haha, good. For what it's worth, I also got that error when trying to log in as you from python but was able to log in just fine from the browser. No idea what the problem is.", "created_utc": 1416428102, "gilded": 0, "name": "t1_cm79r7m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2msi7y/praw_no_json_object_could_be_decoded_error_from/"}, {"author": "chrisarr", "body": "The r.get_submission you are calling is a type of .get_content call in PRAW. The docs for that are found on the [praw docs](https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html?highlight=get_submission#praw.__init__.BaseReddit.get_content) What I like to do though is to just grab the JSON off of reddit, by appending \".json\" to a submission URL in the web browser. You can copy and paste that into python scripts and pprint it to see all of the fields you have access to. import pprint data = [copy/pasted JSON from reddit] pprint(data) Anything you see listed in the JSON for the top level comment post can be referenced within your Submission objects. e.g.: import praw r = praw.Reddit(\"App name /u/MoarMag\") r.config.store_json_result = True thread = r.get_submission(\"http://www.reddit.com/r/redditdev/comments/2mrcb8/\") description = thread.selftext.encode('utf-8') print description Probably other ways to do that, but hands on is easy enough! Cheers.", "created_utc": 1416411606, "gilded": 0, "name": "t1_cm70fp3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "letgoandflow", "body": "I see the \"upvote_ratio\" attribute when I append .json to a submission, but I'm not getting that value when I pull the submission with PRAW.", "created_utc": 1416443768, "gilded": 0, "name": "t1_cm7idsw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "chrisarr", "body": "Works for me, running the above code, swapping \"selftext\" for \"upvote_ratio\". As this is a float number, you can drop the .encode() from the call (else it will error).", "created_utc": 1416450281, "gilded": 0, "name": "t1_cm7lmo0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "letgoandflow", "body": "> r.config.store_json_result = True Is this a critical component? I just assumed that PRAW would give you all of the attributes (or at least note that they aren't).", "created_utc": 1416453707, "gilded": 0, "name": "t1_cm7nav2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "chrisarr", "body": "> .store_json_result I think it is all a matter of how you plan to reference the data after you request it. See [this answer on stack overflow](http://stackoverflow.com/a/24749142/3399542) which helped me get my PRAW JSON working correctly.", "created_utc": 1416454536, "gilded": 0, "name": "t1_cm7np2i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "letgoandflow", "body": "Hmm, well I'm stumped. I'm pulling a post in and pprinting the data and it doesn't include \"upvote_ratio\". You can see in the PRAW docs and the upvote_ratio attribute isn't included there either - https://praw.readthedocs.org/en/v2.1.19/pages/writing_a_bot.html", "created_utc": 1416454853, "gilded": 0, "name": "t1_cm7nuh8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "chrisarr", "body": "Also, re: your post topics question in specific, someone with a more certainty of the values in the JSON would answer, but I have been using 'score' as an aggregate upvote ranking signal for thread. I have heard that, at least at a user account level, the upvotes/score can change at random, to restrict the API. I am sure you could 'ups' / 'downs' or whatever kind of calculation you would want, so long as the numbers are indeed accurate.", "created_utc": 1416411856, "gilded": 0, "name": "t1_cm70k1k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mrcb8/help_how_do_i_get_the_upvoted_value_of_a/"}, {"author": "zer0t3ch", "body": "If you PM me, I'll give you a hand later. I'm on my Ile now and I'll forget if you don't.", "created_utc": 1416369766, "gilded": 0, "name": "t1_cm6oevh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "letgoandflow", "body": "Alright, I'll skip the PM and just comment here so you can reply in public and others can learn as well. I'm really just wondering if there are any other guides or info out there about how to navigate PRAW in general.", "created_utc": 1416371404, "gilded": 0, "name": "t1_cm6p5ex", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "zer0t3ch", "body": "Ok, well I'm on computer now, so here goes: First of all, look at other people's code, it will help you. Now, onto this specific problem, here's a little code snippet that should help you figure out whatever your problem is. import praw # Import the praw library agent = \"testScript/0.1 by zer0t3ch\" # Set a string for our user-agent variable r = praw.Reddit(user_agent=agent) # Create a new instance of a reddit connection (called r) r-slash-funny = r.get_subreddit(\"funny\") # Create a new instance of the funny sub (called r-slash-funny) top-10-funny = r-slash-funny.get_top(limit=10) # Gets the current top 10 (in the week, I think) top-10-all-time = r-slash-funny.get_top_from_all(limit=10) # Gets the top 10 of ALL TIME Now remember, your extra parameters are passed to [get_content()](https://praw.readthedocs.org/en/v2.1.19/pages/code_overview.html#praw.__init__.BaseReddit.get_content), so you have to go see what can be passed to it. In my example I showed a couple that you might need. Did that answer your question/problem? If not, let me know, and try to go into more detail, and I'll lend a hand where I can.", "created_utc": 1416375817, "gilded": 0, "name": "t1_cm6qxiv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "letgoandflow", "body": "Thanks for responding. I guess it was just confusing that the \"parameters get passed to get_content()\". Why not just list those parameters out again so the user knows what they can pass to this function? Also, I'm really just looking for more general advice on how to use PRAW. I'm not saying you have to provide it, but I'm more looking for I want to do \"X\" with PRAW, where do I start in terms of finding the right functions that will accomplish \"X\"?", "created_utc": 1416407723, "gilded": 0, "name": "t1_cm6ypdc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "zer0t3ch", "body": "1. I agree that listing each one out would make things easier, but if he did that, it's just too much, and too repetitive. 2. Most of the functions and whatnot on the docs page are named nicely, so you can just ctrl-f most of the time. But, I suggest finding other people's code to look through.", "created_utc": 1416414095, "gilded": 0, "name": "t1_cm71psu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "letgoandflow", "body": "Alright, cool. Thanks for the help!", "created_utc": 1416414567, "gilded": 0, "name": "t1_cm71z28", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "zer0t3ch", "body": "No problem. If you ever want some to look at, I have a couple of my reddit projects on my GitHub. (same username) And feel free to PM me if you're ever having a specific problem that I might be able to help you with.", "created_utc": 1416430448, "gilded": 0, "name": "t1_cm7b4p9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mqf5z/having_trouble_figuring_out_how_to_use_praw/"}, {"author": "souldeux", "body": "When you get a big, big, big submission like that you have a lot of \"more comments\" objects to expand and flatten. Each one is an API call, and subject to rate limiting. Unfortunately I don't known a way around this - the limits are built in by design.", "created_utc": 1416362048, "gilded": 0, "name": "t1_cm6kks2", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mpeqt/praw_why_does_it_take_an_hour_to_download_a_large/"}, {"author": "chrisarr", "body": "Dealing with the JSON is much, much faster, if you are willing to write the code to decipher it. YMMV, but I elected to use the r.request_json to fetch the thread, and then utilized a few nested loops to return for me the top, first, and second level of responses for threads. You could obviously go as deep as you want.", "created_utc": 1416412775, "gilded": 0, "name": "t1_cm710ru", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mpeqt/praw_why_does_it_take_an_hour_to_download_a_large/"}, {"author": "hooked_dev", "body": "Thanks for the response. Is the rate limiting factor PRAW itself, or the download restrictions it places on requests (as governed by the Reddit API)? Wouldn't `r.request_json` still end up having to send just as many requests (sorry if this is an obvious question)?", "created_utc": 1416423055, "gilded": 0, "name": "t1_cm76u4x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mpeqt/praw_why_does_it_take_an_hour_to_download_a_large/"}, {"author": "chrisarr", "body": "Reddit API will throttle you around 60 requests a minute. PRAW keeps a cadence of 2 seconds per request, so you don't get throttled. r.request_json is itself 1 request. .replace_more_comments will make a series of additional requests each time it encounters a entity. For very large threads, it is these additional requests being made that slows the process. Being said, .request_json will encounter a limit of its own, around 1000 (or 1500 if you are a reddit gold member, and your app is accessing the API with your credentials). At the tail of the JSON string you will see a bunch of id's for additional comments. These are the comments that occurred past the limit your account encountered. If speed is what you truly need, explore implementing JSON. otherwise, you will have to make due with the API limiting your requests speeds.", "created_utc": 1416431152, "gilded": 0, "name": "t1_cm7bjeo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mpeqt/praw_why_does_it_take_an_hour_to_download_a_large/"}, {"author": "wscottsanders", "body": "Sorry to resurrect and old post but could you explain in more detail what you mean by building the URL? I'm trying to get this working and I'm a bit confused. Do you need to create a dictionary of potential captcha responses?", "created_utc": 1421358401, "gilded": 0, "name": "t1_cnq7n3y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mm9sd/praw_how_to_get_a_captcha_image_url_after_you/"}, {"author": "letgoandflow", "body": "See [this post in the praw docs](https://praw.readthedocs.org/en/v2.1.19/pages/faq.html#how-can-i-handle-captchas-myself) which will tell you how to get the captcha ID. Once you have that, you can build URL for the captcha image by following this format: http://www.reddit.com/captcha/insert_captcha_id_here.png Then you can use that URL in an tag to display the captcha image to the user.", "created_utc": 1421364887, "gilded": 0, "name": "t1_cnqblkc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mm9sd/praw_how_to_get_a_captcha_image_url_after_you/"}, {"author": "rhiever", "body": "sudo pip install praw If that doesn't work, paste the error message that comes from that.", "created_utc": 1416177316, "gilded": 0, "name": "t1_cm4feob", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mht2a/how_to_install_praw_in_anaconda_in_mac/"}, {"author": "igeorgetaylor", "body": "your anaconda don't PRAW unless you got buns hun", "created_utc": 1416180128, "gilded": 0, "name": "t1_cm4guwo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mht2a/how_to_install_praw_in_anaconda_in_mac/"}, {"author": "pwang99", "body": "Can you do \"which pip\" to make sure that it's the one in anaconda/bin/? Otherwise, what might be happening is that pip in your *system python's* bin/ directory is getting picked up, and that then installs into the wrong site-packages.", "created_utc": 1416195224, "gilded": 0, "name": "t1_cm4ogzf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mht2a/how_to_install_praw_in_anaconda_in_mac/"}, {"author": "cruyff8", "body": "Almost seems like you're not installing it using the right python binary. Make sure you have the right one.", "created_utc": 1416221997, "gilded": 0, "name": "t1_cm4w63y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mht2a/how_to_install_praw_in_anaconda_in_mac/"}, {"author": "186394", "body": "There's an api limit of 1000 items per listing. You can't get more.", "created_utc": 1416090857, "gilded": 0, "name": "t1_cm3k35a", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2merm5/praw_hitting_a_limit_with_get_commentslimit_none/"}, {"author": "PasDeDeux", "body": "What would you recommend for accessing >1000 comments? Will direct url requests not allow after=xxx&count=1000 (more than 1000)? (Aside from my submission list -> comment get workaround.) Edit: Sorry, now that I know what I'm looking for, I googled better search terms and found the answer to my question. No, even direct url requests don't allow >1000. Maybe I'll setup a comment stream and run it for a day or something.", "created_utc": 1416115015, "gilded": 0, "name": "t1_cm3u806", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2merm5/praw_hitting_a_limit_with_get_commentslimit_none/"}, {"author": "PasDeDeux", "body": "Reading through the source for praw's get_content() method, I wonder if this is my problem: objects_found = 0 params = params or {} fetch_all = fetch_once = False if limit is None: fetch_all = True params['limit'] = 1024 # Just use a big number elif limit > 0: params['limit'] = limit else: fetch_once = True However, when I use very large limits (like 10,000), I still hit the ~1000 wall. After my program terminates, I get: sys:1: ResourceWarning: unclosed C:\\Users\\####\\Anaconda3\\lib\\importlib\\_bootstrap.py:2150: ImportWarning: sys.meta_path is empty sys:1: ResourceWarning: unclosed IIRC this is what I usually get when a download terminates early.", "created_utc": 1416086196, "gilded": 0, "name": "t1_cm3i26m", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2merm5/praw_hitting_a_limit_with_get_commentslimit_none/"}, {"author": "spam_me_not", "body": "Did you ever find a workaround for this problem to allow developers to download *all* comments for a sub?", "created_utc": 1423508891, "gilded": 0, "name": "t1_cog7sub", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2merm5/praw_hitting_a_limit_with_get_commentslimit_none/"}, {"author": "PasDeDeux", "body": "I'm trying to remember. I think getting **all** of the comments is impossible. If there was a solution, it was by specifying date ranges (but I don't remember if you can do that) using the search function in the api. If you're looking for just comments like I was, I iterated through submissions (that way you get 1024 threads of comments) and used the expand more function.", "created_utc": 1423529317, "gilded": 0, "name": "t1_cogjt2e", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2merm5/praw_hitting_a_limit_with_get_commentslimit_none/"}, {"author": "GoldenSights", "body": "If you print comments to the screen, it's probably going to overfill the screen and you wont be able to read it all. That said, you can try: print(comment.body) except AttributeError: pass I catch AttributeError in case there are any MoreComments objects left in the mix, which do not have a body attribute. _____ Can I make some other critiques? You're fetching submissions, appending their IDs to a list, and then re-fetching them by their IDs. The submissions returned by the search already contain the entire post data, so this is a waste of calls. You could actually append `x` instead of `x.id` and your set will contain full posts. You made `postids` a set, added entries to it, and then converted it to a list. You could have started it out as a list and used `postids.append(x)`. If this is a Python 2.7 quirk then you may correct me here, because I'm a py3 guy. Finally, your while loop would be much friendlier as for submission in postids: submission.replace_more_comments(...) Since postids will contain the actual submission objects as mentioned above. Also your useragent says top500 but you're searching for 'new'. Is this temporary or a mistake?", "created_utc": 1416019890, "gilded": 0, "name": "t1_cm2xz1j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "archon_rising", "body": "Thank you for your ideas. ~~I made the change you suggested, and it prints some of the comments but not all. Some of the others still print as . Why is that?~~ Your critiques are most welcome :). I'll retry the IDs part. Thanks! The list approach didn't work too well for me, I'm not sure why. I could give that another look. The useragent is temporary. I set that originally, but I've been modifying my search term to improve the processing speed. Can you suggest a more thorough documentation than praw.readthedocs? I'm sort of struggling with the level of detail in it(not enough for me) EDIT: the first problem was because of the repr() method, which I removed.", "created_utc": 1416020699, "gilded": 0, "name": "t1_cm2ybh6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "GoldenSights", "body": "Hmm, I'm not sure why the list method isn't working for you. Try this, and let's ditch that `break` of yours. results = list(r.search('subreddit:gunners', sort='new', period='day', limit=5)) for submission in results: submission.replace_more_comments(...) Boom. Cuts out like 9 lines of code. The only more thorough documentation on PRAW would be [the source code](https://github.com/praw-dev/praw/tree/master/praw) itself.", "created_utc": 1416023587, "gilded": 0, "name": "t1_cm2zh5b", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "archon_rising", "body": "Wow. This is great. Thanks again. Is there some way to print the comments WITH the number of upvotes?", "created_utc": 1416023837, "gilded": 0, "name": "t1_cm2zkfv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "PasDeDeux", "body": "This is less elegant if all you want to do is print, but for scaling or saving the data, I think it's helpful: list = [] # Or dict = {} for comment in commentlist: commscore = comment.score comment = comment.body # These aren't necessary but when I add features later # I like to have variables instead of calling the comment item list.append([commscore, comment]) # Or dict[commid] = comment print(list) My previous solution was to get the comments from each submission, which yielded much more data.", "created_utc": 1416085403, "gilded": 0, "name": "t1_cm3hq2k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "GoldenSights", "body": "Print the `comment.score` attribute. How you want to format it is up to you. `print(comment.score, '\\n', comment.body, '\\n\\n')` for example. Whatever you want to do with it.", "created_utc": 1416023924, "gilded": 0, "name": "t1_cm2zllo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "archon_rising", "body": "Brilliant. Thank you!", "created_utc": 1416024714, "gilded": 0, "name": "t1_cm2zwb1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2mcgcm/praw_trying_to_use_praw_to_scrape_all_comments/"}, {"author": "6086555", "body": "An useful thing to do with praw is to use dir on any object. I had forgotten the answer to your question but: import praw r = praw.Reddit('test') r.get_subreddit('redditdev') sub = r.get_subreddit('redditdev') print dir(sub) In the results you can find 'get_top_from_week' so sub.get_top_from_week works. Pass it limit = None as argument if you want to have all results (up to 1000)", "created_utc": 1415828995, "gilded": 0, "name": "t1_cm0rnza", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2m3qxk/praw_how_to_pull_all_top_rated_posts_from_last/"}, {"author": "Deimorz", "body": "There's not really anything built-in, but you could do something like use the `body_html` attribute of a comment and run that through BeautifulSoup, looking for any `` tags.", "created_utc": 1415525480, "gilded": 0, "name": "t1_clxck06", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2lqtpq/praw_any_way_to_tell_if_a_comment_contains_a_link/"}, {"author": "186394", "body": "Assuming the text of the comment is in variable 'comment_text': import re link_check = r.compile(r'\\[.+\\]\\(.+\\)') if link_check.search(comment_text): #Stuff to do if link found.", "created_utc": 1415529386, "gilded": 0, "name": "t1_clxd7we", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2lqtpq/praw_any_way_to_tell_if_a_comment_contains_a_link/"}, {"author": "aksios", "body": "Because of line 7: link_id = r.get_info(thing_id=link_id) and line 51: parent_id = r.get_info(thing_id=parent_id) You're pinging reddit's API agin here, which means that praw officially waits 2 seconds, but is usually anywhere between 1-3 seconds and can even be more. So since you're doing it twice, it adds up to about 5 seconds.", "created_utc": 1415050114, "gilded": 0, "name": "t1_cls1nlq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "mjgcfb", "body": "Is there a work around to speed up the loop? I'm not sure how to get the link and parent attributes without using that get_info.", "created_utc": 1415050913, "gilded": 0, "name": "t1_cls23kb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "aksios", "body": "You could try caching your results. And I don't know what you're doing with your results but if you're saving it somewhere, you could see if you've already gotten the information you need in it and get it from there rather than from reddit again.", "created_utc": 1415053725, "gilded": 0, "name": "t1_cls3mqm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "mjgcfb", "body": "I also just realized comment.permalink is another lazy object. Removed that and it sped it up by a second or two.", "created_utc": 1415062148, "gilded": 0, "name": "t1_cls7rd5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "aksios", "body": "You can actually work out a permalink for the comment if you have the submission: http(s)://www.reddit.com/r//comments///", "created_utc": 1415094662, "gilded": 0, "name": "t1_clsj549", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "mjgcfb", "body": "That may work. Thanks for the help!", "created_utc": 1415054613, "gilded": 0, "name": "t1_cls43is", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "fragmede", "body": "Is the included code inside a larger for loop? Without looking at the rest of your code, I'd be that the slow piece is that first line. How long does the script take if you take out everything except the first 4 lines? comments = praw.helpers.comment_stream(r, subreddit, limit=500) for comment in comments: comment_id = comment.id", "created_utc": 1415047409, "gilded": 0, "name": "t1_cls053m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2l6e6b/why_is_my_for_loop_take_around_5_seconds_for_each/"}, {"author": "Days_End", "body": "Send a http head request (http://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol#Request_methods) to the address and read back the headers. Imgur sets the content-type header to \"Content-Type:image/gif\" when it is animated.", "created_utc": 1414636950, "gilded": 0, "name": "t1_clnu9ss", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kqai0/checking_if_jpg_imgur_url_is_animated_gif_file/"}, {"author": "GoldenSights", "body": "The imgur API page you linked says that images have a boolean `animated` value. However, you need to register your program with them in order to use the api (because it is 100% separate from reddit's). This is *probably* the best way to go if you're the only one who is going to use this program. I've never done it before, but it probably isn't terribly complex. [See here](http://api.imgur.com/)", "created_utc": 1414626713, "gilded": 0, "name": "t1_clnozih", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kqai0/checking_if_jpg_imgur_url_is_animated_gif_file/"}, {"author": "phire", "body": "Yes, this would be the best method. Allows you to tell the difference between animated and non-animated gifs, rather than just assuming all gifs are animated.", "created_utc": 1414654652, "gilded": 0, "name": "t1_clo0d9p", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kqai0/checking_if_jpg_imgur_url_is_animated_gif_file/"}, {"author": null, "body": "[deleted]", "created_utc": 1414632190, "gilded": 0, "name": "t1_clnrtf8", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kqai0/checking_if_jpg_imgur_url_is_animated_gif_file/"}, {"author": "powderblock", "body": "This didn't end up working because I am not storing a local copy of the image. (Scanning hundreds of images an hour, so it just isn't logical to write a local copy when I can just do it in and out of memory.) This *did* however set me on the right path! Thanks! Here's what I ended up doing. url = \"http://i.imgur.com/9sYwSwz.jpg\" response = urllib2.urlopen(url) data = cStringIO.StringIO(response.read()) print(imghdr.what(data)) Basically it just asks for data after reading the response from urllib. Thanks again!", "created_utc": 1414697659, "gilded": 0, "name": "t1_clogqcg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kqai0/checking_if_jpg_imgur_url_is_animated_gif_file/"}, {"author": "gavin19", "body": "I believe it's due to the bot account being brand new so it gets hit with a captcha. https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html?highlight=send_message#praw.__init__.PrivateMessagesMixin.send_message Rather than try to figure out how to handle captchas and respond to each one you should just use the bot account to build up some link karma so you bypass them. I can't recall exactly how much you need to accumulate and if it's also tied to the account age. If you log into the bot using your own account (to temporarily test), then it should work fine.", "created_utc": 1414440741, "gilded": 0, "name": "t1_cllfxj7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kho9h/praw_error_when_sending_pm_valueerror_dictionary/"}, {"author": "bboe", "body": "Your send message syntax is incorrect for the user-specified function call. You either want to do: r.get_user('icedvariables').send_message('Pug Posts\", 'pugPosts) or r.send_message('icedvariables', 'Pug Posts\", 'pugPosts) Messages are always sent from the logged in account. The way you are calling it, PRAW thinks `pugPosts` is a captcha which it is not.", "created_utc": 1414469020, "gilded": 0, "name": "t1_cllv2zk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kho9h/praw_error_when_sending_pm_valueerror_dictionary/"}, {"author": "icedvariables", "body": "Thank you, it works correctly now.", "created_utc": 1414484048, "gilded": 0, "name": "t1_cllzady", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kho9h/praw_error_when_sending_pm_valueerror_dictionary/"}, {"author": "Ugleh", "body": "Once your bot gets black listed it stays that way until you message a moderator I believe. Did it just suddenly start and you can't fix it or is it still able to do stuff?", "created_utc": 1414410108, "gilded": 0, "name": "t1_cll1ezc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "teaearlgraycold", "body": "The bot *is* a moderator, though.", "created_utc": 1414410200, "gilded": 0, "name": "t1_cll1fr3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "Ugleh", "body": "Black listed from Reddit because it may have broke the API requests per minute. It black lists bots automatically that fail to follow it.", "created_utc": 1414410593, "gilded": 0, "name": "t1_cll1j2i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "teaearlgraycold", "body": "So I should have it PM a mod of ELI5?", "created_utc": 1414410702, "gilded": 0, "name": "t1_cll1k05", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "Ugleh", "body": "Its reddit-wide I believe, not ELI5. Does your bot still work if you disabled that?", "created_utc": 1414411351, "gilded": 0, "name": "t1_cll1pzg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "teaearlgraycold", "body": "Disable what?", "created_utc": 1414412684, "gilded": 0, "name": "t1_cll22k2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "Planecrazy1191", "body": "PRAW automatically limits your requests so that they fit within the API access rules. So you only need the sleep timers if you want your bot to periodically rest. As for the PMs, is it possible your bot is failing because of a captcha?", "created_utc": 1414455677, "gilded": 0, "name": "t1_cllnys9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "teaearlgraycold", "body": "Nah, it's got plenty of karma. I figured out that my sleep timers were causing the HTTP connection to time out :P All seems to be well now.", "created_utc": 1414455775, "gilded": 0, "name": "t1_cllo0qc", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "Planecrazy1191", "body": "Yeah, now that you mention it, looking at the code you posted you definitely don't want those sleep calls in the loops. The sleep(40) should still be fine though.", "created_utc": 1414456228, "gilded": 0, "name": "t1_cllo9n1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "teaearlgraycold", "body": "Nah, the sleep(40) was the problem. A 40 second delay without any contact will go over the Keep Alive for the connection.", "created_utc": 1414456315, "gilded": 0, "name": "t1_cllobd5", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "Planecrazy1191", "body": "Huh, I wouldn't have expected that. Looking at the PRAW documentation it seems you'd have to re-initialize the praw.reddit object after sleeping as best I can tell thats where the connection is opened.", "created_utc": 1414457449, "gilded": 0, "name": "t1_cllox8o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "teaearlgraycold", "body": "Well TBH I'm not sure which sleep timers were the problem as I removed all of them.", "created_utc": 1414457861, "gilded": 0, "name": "t1_cllp5ai", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "Planecrazy1191", "body": "I'm not sure anymore either. I've been trying to reproduce your problem but I'm able to sleep for at least 60 seconds without issue.", "created_utc": 1414459541, "gilded": 0, "name": "t1_cllq2r0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "teaearlgraycold", "body": "It wasn't a consistent problem. But it might depend on the version of Python/urllib/PRAW - the Keep Alive value isn't something *I* hard coded in. It's part of one of those modules. I'll let you know if the bot still doesn't work as it hasn't ran for as long in its current state as it did with the code I posted in this thread.", "created_utc": 1414459615, "gilded": 0, "name": "t1_cllq485", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kfiou/whats_wrong_with_my_bot/"}, {"author": "bboe", "body": "Yes, in the development version: https://github.com/praw-dev/praw/blob/master/CHANGES.rst#unreleased", "created_utc": 1414381986, "gilded": 0, "name": "t1_clkupbr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kex5m/does_praw_support_multiple_things_for_get_info/"}, {"author": "GoldenSights", "body": "Oh good! Does this mean I can simply copy-paste your [\\_\\_init__](https://github.com/praw-dev/praw/blob/88c037f70742d198530500bd60db265761eef7bb/praw/__init__.py#L777) file over my local one, or will this cause any problems?", "created_utc": 1414382230, "gilded": 0, "name": "t1_clkut58", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2kex5m/does_praw_support_multiple_things_for_get_info/"}, {"author": "GoldenSights", "body": "Try catching `praw.requests.exceptions.HTTPError` instead. I remember having a lot of trouble with errors until I used this. For testing, you could induce an error intentionally by fetching info about a private sub.", "created_utc": 1414127630, "gilded": 0, "name": "t1_clia2bj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2k6668/praw_504_httperror_exception_not_being_caught/"}, {"author": "bboe", "body": "> r.send_message('Zapurdead', \"[MOBILE-WIZARD] HTTPError\", msg) That function call may result in another HTTPError which you are not handling. That's probably what you're experiencing.", "created_utc": 1414129925, "gilded": 0, "name": "t1_cliarcy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2k6668/praw_504_httperror_exception_not_being_caught/"}, {"author": "bboe", "body": "> The problem is when I go to PRAW (with the domain changed to the appropriate value), the generator get_subreddit returns 'dicts' instead of praw.objects.Submission. Your object to `thing prefix` mapping is probably wrong: > xxx_kind: A string that maps the type returned by json results to a local object. xxx is one of: comment, message, more, redditor, submission, subreddit, userlist. This variable is needed as the object-to-kind mapping is created dynamically on site creation and thus isn\u2019t consistent across sites. You need to figure out the prefix for each object in your install and update you configuration so that the mapping matches.", "created_utc": 1414129768, "gilded": 0, "name": "t1_cliapqh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2k5ui0/figuring_out_the_api_on_a_reddit_clone/"}, {"author": "redditdev1", "body": "That was exactly it. Thank you very much.", "created_utc": 1414147052, "gilded": 0, "name": "t1_clie651", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2k5ui0/figuring_out_the_api_on_a_reddit_clone/"}, {"author": null, "body": "you need site-packages for pip install to work", "created_utc": 1413858283, "gilded": 0, "name": "t1_clf7rhl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "doob10163", "body": "> site-packages So how do I do this?", "created_utc": 1413858603, "gilded": 0, "name": "t1_clf7x4m", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": null, "body": "What OS are you using? edit: probably easier to download the files and run setup.py", "created_utc": 1413859297, "gilded": 0, "name": "t1_clf89gk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "doob10163", "body": "Windows 7. How do I do this?", "created_utc": 1413859832, "gilded": 0, "name": "t1_clf8iux", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": null, "body": "It looks like you need to add python to your path variables. After that pip should work fine. http://stackoverflow.com/questions/5599872/python-windows-importerror-no-module-named-site", "created_utc": 1413860487, "gilded": 0, "name": "t1_clf8u7r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "doob10163", "body": "I've set both my PATHS using set PYTHONHOME=c:\\Python27 set PYTHONPATH=c:\\Python27\\Lib set PATH=%PYTHONHOME%;%PATH% and have also tried unzipping to C:\\Python27\\lib then running python setup.py (I actually have to go into another directory to access setup.py within C:\\Python27\\lib) I have no idea what in the world is going on because in powershell I am still getting No module named site. Also tried this: Hey, I've been looking into this problem for myself for almost a day and finally had a breakthrough. Try this: >Setting the PYTHONPATH / PYTHONHOME variables >Right click the Computer icon in the start menu, go to properties. On the left tab, go to Advanced system settings. In the window that comes up, go to the Advanced tab, then at the bottom click Environment Variables. Click in the list of user variables and start typing Python, and repeat for System variables, just to make certain that you don't have mis-set variables for PYTHONPATH or PYTHONHOME. Next, add new variables (I did in System rather than User, although it may work for User too): PYTHONPATH, set to C:\\Python27\\Lib. PYTHONHOME, set to C:\\Python27. But python doesn't show up in either the system or uservariables within Environment Variables.", "created_utc": 1413860948, "gilded": 0, "name": "t1_clf91yu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": null, "body": "when you open powershell and you just type \"python\" and press enter, what happens?", "created_utc": 1413861210, "gilded": 0, "name": "t1_clf96gx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "doob10163", "body": "No module named site. WHAT THE HELL? This is the machine I did LPTHW on", "created_utc": 1413861378, "gilded": 0, "name": "t1_clf99cy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": null, "body": "that's okay. that just means you don't have python in your path. follow the steps in the long comment you made and you have to add one variable named \"PYTHONPATH\" with a value \"C:\\Python27\\\" and you should already have a variable name \"PATH\", edit the very end of it so you add \"; C:\\Python27\\\" Save the changes (I think you just have to click ok) then make sure powershell is closed and open it up and try again. edit: I'm heading to bed for the night. PM me if this doesn't solve your problem and i will make a better walkthrough when I get to a windows machine at work tomorrow.", "created_utc": 1413861697, "gilded": 0, "name": "t1_clf9epi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "doob10163", "body": "Well, i added PYTHONPATH with the value of C:\\Python27\\ to User variables and do I have to change the PATH variable in both User AND System Variables?", "created_utc": 1413863945, "gilded": 0, "name": "t1_clfaeb9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": null, "body": "changing it in system should be sufficient. Did you get it to work?", "created_utc": 1413903867, "gilded": 0, "name": "t1_clflfdk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "doob10163", "body": "No. I'm on my other machine right now and won't get back to that one until tonight which is like 10 hrs from now +. Thanks a lot for your help though. I don't know what else to try. I tried reinstalling Python27 and deleting the C:\\Python27 and everything.", "created_utc": 1413903962, "gilded": 0, "name": "t1_clflh5r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": null, "body": "it sounded like it was just a PATH issue. once you fixed that you probably just had to log off and log back on and it would work.", "created_utc": 1413904049, "gilded": 0, "name": "t1_clflisf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "doob10163", "body": "It didn't work. :/ I think i have a pretty serious problem on my hands if i can't run the python command in powershell", "created_utc": 1413946182, "gilded": 0, "name": "t1_clg802t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "doob10163", "body": "So you mean reset my computer, essentially? I'll try it again when I get home. I was frustrated last night and did not try that. Also, praw is working [nicely](http://www.reddit.com/r/CompetitiveHS/comments/2jw3oc/anyone_have_a_good_database_for_card_images/) on this machine", "created_utc": 1413904209, "gilded": 0, "name": "t1_clfllnh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": null, "body": "download it from [here](https://pypi.python.org/pypi/praw) unzip it to C:\\Python27\\lib open command prompt. cd C:\\Python27\\lib python setup.py", "created_utc": 1413860070, "gilded": 0, "name": "t1_clf8myh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2juell/cant_install_praw_on_this_other_machine/"}, {"author": "xcombelle", "body": "you can use ssl_domain in one of your config file like explained https://praw.readthedocs.org/en/latest/pages/configuration_files.html", "created_utc": 1413663101, "gilded": 0, "name": "t1_cld4v88", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "linuxydave", "body": "Oh man, thanks for pointing this out. I was struggling to understand why I couldn't log in with my account but my bots could log in with theirs.", "created_utc": 1413711907, "gilded": 0, "name": "t1_cldlivm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "jaacoppi", "body": "Looks like you're not providing your login details. Try r.login(yourloginname,yourloginpassword).", "created_utc": 1413630496, "gilded": 0, "name": "t1_clctqy8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "catzhoek", "body": "That's not the case, user and pswd are entered via terminal if left empty, but i tried it in all variations. That's why i mentioned that it reacts with proper exceptions if i provide incorrect login credentials. (praw.errors.InvalidUserPass)", "created_utc": 1413631411, "gilded": 0, "name": "t1_clctw2r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "jaacoppi", "body": "Yeah, sorry I missed that. I ran your code a few times and couldn't reproduce your traceback. it simply logged in as it should.", "created_utc": 1413634577, "gilded": 0, "name": "t1_clcug82", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "catzhoek", "body": "Thanks man, this kinda flushed my cloaked up brain. I require https on my account, which causes this behavior.", "created_utc": 1413638775, "gilded": 0, "name": "t1_clcvdei", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jl8ks/cant_get_praw_2118_to_login/"}, {"author": "kemitche", "body": "/u/cahaseler is correct - we categorize based on flair (categories are comprised of one or more flair CSS classes)", "created_utc": 1413489542, "gilded": 0, "name": "t1_clbdvms", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jg19x/how_is_the_categorization_on_the_official_ama_app/"}, {"author": "chrisarr", "body": "Awesome, thank you guys!", "created_utc": 1413491024, "gilded": 0, "name": "t1_clbeq2z", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jg19x/how_is_the_categorization_on_the_official_ama_app/"}, {"author": "cahaseler", "body": "Isn't it based on flair? Flair is manually assigned by the IAMA mod team.", "created_utc": 1413488759, "gilded": 0, "name": "t1_clbdf6t", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2jg19x/how_is_the_categorization_on_the_official_ama_app/"}, {"author": "bboe", "body": "Initialize praw via: r = praw.Reddit(user_agent='...', disable_update_check=True)", "created_utc": 1413341737, "gilded": 0, "name": "t1_cl9sznm", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j8dsv/using_praw_with_the_google_app_engine_no_such/"}, {"author": "HamsterBoo", "body": "Actually just went through the same problem. Have you noticed the Google App Engine giving RateLimitExceeded errors? All I'm trying to do is log in and I keep getting it, yet I can log in and post if I do it from a local script.", "created_utc": 1414081404, "gilded": 0, "name": "t1_clhmx30", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j8dsv/using_praw_with_the_google_app_engine_no_such/"}, {"author": "bboe", "body": "It's probably due to GAE proxy servers. I don't know if reddit can distinguish between GAE instances. That would be a good question for the admins.", "created_utc": 1414128683, "gilded": 0, "name": "t1_cliae9x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j8dsv/using_praw_with_the_google_app_engine_no_such/"}, {"author": "damontoo", "body": "I had this exact problem so I wrote my own pure python module that has no dependencies. It's not as fancy. Just fetches some arbitrary URL as json, handles before/after, and limits request frequency. Less than 100 lines.", "created_utc": 1413326563, "gilded": 0, "name": "t1_cl9ll9w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j8dsv/using_praw_with_the_google_app_engine_no_such/"}, {"author": "GoldenSights", "body": "http://www.reddit.com/r/announcements/comments/28hjga/reddit_changes_individual_updown_vote_counts_no/ There shouldn't be any way to do this.", "created_utc": 1413173701, "gilded": 0, "name": "t1_cl7yzyd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j364y/praw_getting_downvotes_for_comments/"}, {"author": "alexleavitt", "body": "Yes, and if you read my question and the post I link to, I acknowledge the change to the ratio. This link does not answer the question of if you can still get any vote count or vote ratio for comments.", "created_utc": 1413173877, "gilded": 0, "name": "t1_cl7z2bn", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j364y/praw_getting_downvotes_for_comments/"}, {"author": "GoldenSights", "body": "Comments only have controversiality measurements, not upvote ratios In fact, I can't even do `upvote_ratio` on submissions with praw, and I dont see it on the [code overview](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html). Has the name of it changed?", "created_utc": 1413174450, "gilded": 0, "name": "t1_cl7za5s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j364y/praw_getting_downvotes_for_comments/"}, {"author": "alexleavitt", "body": "It seems like you're correct, given this post: http://www.reddit.com/r/redditdev/comments/29i58s/reddit_change_api_availability_controversiality/ (I assume they would have mentioned it if it were possible...) As far as upvote_ratio, yes, it still works, like this: post = r.get_submission(submission_id=BLAH) ratio = post.upvote_ratio", "created_utc": 1413175333, "gilded": 0, "name": "t1_cl7zlhs", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j364y/praw_getting_downvotes_for_comments/"}, {"author": "GoldenSights", "body": "Interesting, because I was trying with `r.get_info(thing_id='t3_2j364y')` and that object didn't have an upvote_ratio attribute. Apparently get_submission gets more information, which seems like a bug to me", "created_utc": 1413175451, "gilded": 0, "name": "t1_cl7zmzm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2j364y/praw_getting_downvotes_for_comments/"}, {"author": "swissmcnoodle", "body": "for submission in r.get_submission(url='http://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/'): print(submission.title) I tried this code but I get an object is not iterable error, i understand why I get that error but I dont know how to get around it", "created_utc": 1413005803, "gilded": 0, "name": "t1_cl6cwg3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ix3aj/how_do_i_read_the_fields_of_praw_submission/"}, {"author": "aksios", "body": "PRAW doesn't use dicts, it uses objects, so instead of accessing fields like `submission['field']` you have to do `submission.field`. It's the same for comments and redditors: >>> comment = r.get_info(thing_id='t1_c9ehbkl') >>> comment.body \"Thank you! I'll have to play around with praw and see what it can do, but it does sound very fascinating!\" >>> s = comment.submission >>> s.score 11 >>> op = comment.submission.author >>> op.link_karma 4309 Now, the reason why your code above doesn't work is because `r.get_submission()` returns a submission object, which is not iterable. An iterable is something that you can go through one by one, like a list or a generator. Going through a submission one by one doesn't make sense, which is why python raises this error. Really, you can just do this: submission = r.get_submission(url='http://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/') print(submission.title) Here's something neat that's helpful for debugging; you can use `var()` to get a list of the fields of an object: >>> vars(submission) {'media': None, 'created': 1365765708.0, 'downs': 0, 'author': Redditor(user_name='aphexcoil'), 'likes': None, 'ups': 12, 'saved': False, '_underscore_names': None, 'url': 'http://www.reddit.com/r/redditdev/comments/1c75dh/lesson_1_how_to_get_submission_data_from_reddits/', 'author_flair_text': None, 'subreddit': Subreddit(display_name='redditdev'), 'reddit_session': , '_comments_by_id': {'t1_c9ehbkl': , 't1_c9dvbqu': , ... And another tip: when you want to display nice-looking code on reddit, indent all your lines by 4 spaces, or surround the text in backticks. (Look at the source of this comment to see examples of their usage).", "created_utc": 1413036268, "gilded": 0, "name": "t1_cl6ilak", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ix3aj/how_do_i_read_the_fields_of_praw_submission/"}, {"author": "swissmcnoodle", "body": "Ah that makes sense, I don't know what I was thinking. I've been iterating through so many mysqli variables the last few days its fried my brain. Thanks for the help and tips. I can finally finish my bot :)", "created_utc": 1413039003, "gilded": 0, "name": "t1_cl6jeia", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ix3aj/how_do_i_read_the_fields_of_praw_submission/"}, {"author": "bboe", "body": "> Is this the best way to use PRAW to exceed the non-oauth rate limit? The only way? For the time being, if you ensure you _only_ make OAuth requests then it's probably safe to change the request delay. You should be aware that some actions in PRAW when using OAuth are sent unauthenticated which may put you over the rate limit (I'm not sure) so try enabling request logging (http://praw.readthedocs.org/en/latest/pages/configuration_files.html [see log requests]) to ensure all the requests are going to the 'oauth' domain. You'll have to modify PRAW to access the response headers: https://github.com/praw-dev/praw/blob/5e8abf3fd7f08b5f27031b419defc44d43169048/praw/__init__.py#L382", "created_utc": 1413051335, "gilded": 0, "name": "t1_cl6oahl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2isomp/praw_using_oauth_with_reddit_api_ratelimits/"}, {"author": "NosajReddit", "body": "Forgot to reply to this, but thank you!", "created_utc": 1414691267, "gilded": 0, "name": "t1_clod5y9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2isomp/praw_using_oauth_with_reddit_api_ratelimits/"}, {"author": "sneaky_dragon", "body": "I figured it out. I thought it might have had to do with \"always use HTTPS\" setting on my account, and once I disabled it I was able to run my script like usual. The redirect exception isn't very helpful though...", "created_utc": 1412920473, "gilded": 0, "name": "t1_cl5gyhb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iqen5/keep_getting_redirectexception_using_set_flair/"}, {"author": "bboe", "body": "If you want to use HTTPS, give PRAW3 a try: https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/", "created_utc": 1413050866, "gilded": 0, "name": "t1_cl6o3fc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iqen5/keep_getting_redirectexception_using_set_flair/"}, {"author": "GoldenSights", "body": "- My comment rate is severely limited on this bot You should be seeing an error along the lines of \"Please wait 9 more minutes\" or something, right? The only real way to avoid these is to gain karma, either at /r/FreeKarma or by some other means. Otherwise, you could store comments that you know you want to reply to, and then keep work through them over time. PRAW will automatically limit your requests to comply with reddit rules. This is 1 call per 2 seconds. If you're seeing rates any slower than that, there are other restrictions on your account because of karma. ____ - How does the bot know how many comments to pull Well, you've done it in a bit of a round-about way, it looks like you were testing the bot on a specific submission just to see how it works. Normally you'd use `subreddit.get_comments(limit=100)` which will get the 100 newest comments to the subreddit. Instead, what you've done is grab a certain post and you're only looking at its comments using the flattened tree. This is comparable to simply opening your browser to the thread and reading comments as you scroll down the page. That means that the comments you're reading are sorted by score and do not necessarily represent the order they were posted. Furthermore, any time you see the \"Load more comments\" button in your browser, PRAW will automatically put a [MoreComments](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.MoreComments) object. If you pass over this object with `if '[[' in comment.body`, your bot will crash because those items don't have bodies. ____ - Ensuring recent posts Well as I said, you're not fetching recent posts right now, you're only fetching 2ioe5e. When you start using `subreddit.get_comments(limit=100)`, then you'll need to start keeping track of ID numbers (with `comment.id`) that you have already scanned and replied to. I use SQL databases to store these numbers. You could also use .txt files but I think that's clunky. [Here is an example of a bot that uses SQL](https://github.com/voussoir/reddit/blob/master/ReplyBot/replybot.py). In line 40-45 I open the file and make sure the table is in place. Line 62 checks for membership, 64 adds new members, and 72 saves the file, all very easy after some Googling ____ You've got quite a lot of questions going on at once, and your code is incomplete. If you want to get specific feel free to ask away.", "created_utc": 1412807597, "gilded": 0, "name": "t1_cl47ed8", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ipcik/completely_new_to_reddit_api_and_praw_with_python/"}, {"author": "bboe", "body": "> Normally you'd use `subreddit.get_comments(limit=100)` which will get the 100 newest comments to the subreddit. `get_comments`does not get the 100 newest comments. It depends on the underlying sort order, but in either case, it returns a tree which may include both new and old submissions.", "created_utc": 1413050802, "gilded": 0, "name": "t1_cl6o2g7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ipcik/completely_new_to_reddit_api_and_praw_with_python/"}, {"author": "PhilABustArr", "body": "Very helpful reply (I am not OP)", "created_utc": 1412839120, "gilded": 0, "name": "t1_cl4jyv9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ipcik/completely_new_to_reddit_api_and_praw_with_python/"}, {"author": "GoldenSights", "body": "Thanks! It can be a lot to take in at once, I still learn new things about the API from time to time.", "created_utc": 1412839176, "gilded": 0, "name": "t1_cl4jzap", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ipcik/completely_new_to_reddit_api_and_praw_with_python/"}, {"author": "GoldenSights", "body": "`comment.reply(r)` What are the contents of `r`? It's telling you that you're submitting a blank comment.", "created_utc": 1412739906, "gilded": 0, "name": "t1_cl3hd36", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2immwi/praw_apiexception_no_text/"}, {"author": "cylindrical418", "body": "Let me check that. **Edit**: It's true. Something happened on my comment builder class. Thanks for the help. Where do I get a list of all `praw.APIException` and details about them?", "created_utc": 1412740003, "gilded": 0, "name": "t1_cl3hel1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2immwi/praw_apiexception_no_text/"}, {"author": "GoldenSights", "body": "https://praw.readthedocs.org/en/v2.1.16/pages/exceptions.html That list isn't *super* detailed. You can read the source code for errors at `C:\\python34\\lib\\site-packages\\praw\\errors.py`, accommodating for your file path Most of the time you'll get HTTPErrors instead of APIErrors, and you can google those to see what they mean.", "created_utc": 1412740598, "gilded": 0, "name": "t1_cl3hng4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2immwi/praw_apiexception_no_text/"}, {"author": "cylindrical418", "body": "OK, thanks. This is a lot of help!", "created_utc": 1412741146, "gilded": 0, "name": "t1_cl3hvoa", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2immwi/praw_apiexception_no_text/"}, {"author": null, "body": "I don't think PRAW supports giving gold. if you have creddits, you should be able to gild a comment or submission by creating a POST request to [this](https://www.reddit.com/dev/api#POST_api_v1_gold_gild_{fullname}) enpoint. https://www.reddit.com/api/v1/gold/gild/[fullname] where [fullname] is of the form t1\\_[6 numbers/leters] for comments and t3\\_[6 numbers/letters] for posts. sending a POST request to [https://www.reddit.com/api/v1/gold/gild/t3_2ig2ci] would give gold to your submission.", "created_utc": 1412608620, "gilded": 0, "name": "t1_cl1wwa6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": "F3AR3DLEGEND", "body": "Can you give me a basic tutorial on how to do this? Here is what I've tried so far: import praw import requests reddit = praw.Reddit(\"/u/f3ar3dlegend test\") reddit.login(\"f3ar3dlegend\", \"MYPASSWORD\") client = requests.session() client.headers = {\"User-Agent\": \"/u/f3ar3dlegend test\"} resp = client.post(\"https://www.reddit.com/api/v1/gold/give\", data = {\"uh\": reddit.modhash, \"id\": \"t3_2ig2ci\"}) print resp.text # this seems to be the Reddit front page print resp.ok # False I'm not sure what \"uh\" is supposed to represent. I modified the [dictionary supplied here](http://stackoverflow.com/questions/12936274/reddit-api-and-voting-not-accepting-modhash-cookie-error-user-required).", "created_utc": 1412610649, "gilded": 0, "name": "t1_cl1xv0n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": null, "body": "you have /gold/give, but you specify a fullname. /gold/give takes a username and /gold/gild takes a fullname. I don't have access to the compiler on this machine so it will be a very basic python-esque tutorial import requests import json s = requests.session() s.headers={'user-agent:'user agent name as string'} messageData = {'user':USERNAME,'passwd':PASSWORD} login = s.post('https://www.reddit.com/r/login',params=messageData) loginData = json.loads(login.content) #This should give you a dictionary that has a key called 'uh' s.headers['uh']=loginData['uh'] s.post('https://www.reddit.com/api/v1/gold/gild/t3_cl1wwa6') #this would give gold to my first comment s.post('https://www.reddit.com/api/v1/gold/give/-Richard-',params = {'months':1}) #this would give gold to me as a user for 1 month", "created_utc": 1412613897, "gilded": 0, "name": "t1_cl1zhlg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": "F3AR3DLEGEND", "body": "I had to modify the login string to \"https://ssl.reddit.com/api/login\". Regardless, both POST calls of the gilding give me 403 errors :/.", "created_utc": 1412622510, "gilded": 0, "name": "t1_cl23xzm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": null, "body": "do you have creddits on your account? edit: tried it once I got home. I am getting the same errors. strange.", "created_utc": 1412622781, "gilded": 0, "name": "t1_cl2430y", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": "Deimorz", "body": "The `/give/` and `/gild/` endpoints are only available if you're using oauth for authentication.", "created_utc": 1412643001, "gilded": 0, "name": "t1_cl2ebqg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": null, "body": "pardon my ignorance, but does that mean someone would have to register their 'app' in order to use the API to give gold? edit: never mind, I got it. Thanks for helping us with this", "created_utc": 1412644649, "gilded": 0, "name": "t1_cl2f521", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": "Deimorz", "body": "More or less, yes. There's some explanation of how to get set up with OAuth on the wiki here: https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example", "created_utc": 1412647506, "gilded": 0, "name": "t1_cl2glob", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": null, "body": "yep, that's what I used. Thanks again. I posted some updated code for OP.", "created_utc": 1412647665, "gilded": 0, "name": "t1_cl2goka", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": null, "body": "okay. I got it. First you have to register an app under 'preferences' You will get a clientID (listed under the app's name) and a client secret (listed under \"secret'). Then the following code will work. import sys reload(sys) sys.setdefaultencoding('utf8') import requests import requests.auth def main(): clientSecret = [assigned by Reddit] clientName = [assigned by Reddit] client_auth = requests.auth.HTTPBasicAuth(clientName, clientSecret) post_data = {\"grant_type\": \"password\", \"username\": [account username], \"password\": [account password]} response = R.post(\"https://ssl.reddit.com/api/v1/access_token\", auth=client_auth, data=post_data) dictionary = response.json() headers = {\"Authorization\": 'bearer '+ dictionary['access_token'], 'User-Agent':'-Richard- Bot V2.0'} response = requests.post(\"https://oauth.reddit.com/api/v1/gold/gild/t1_ckxyvlw\",headers=headers) return 0 if __name__==\"__main__\": main()", "created_utc": 1412647425, "gilded": 0, "name": "t1_cl2gk6w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ig2ci/gild_commentsusers/"}, {"author": "GoldenSights", "body": "Getting mentions is working for me just fine Here are the steps I took: Logged into r as GoldenSights made [this comment](http://www.reddit.com/r/GoldTesting/comments/2gj3mu/politician_info_testing/cl0eql0) as 75000 mentions = list(r.get_mentions()) print(mentions[0].author) 75000 Notably, mentions does not keep track of read/unread, so fetching mentions multiple times will retreive ones you've already done. Can you do some tests from the commandline? Instead of jumping into a script, I like to test out features by hand so I only need to log in once.", "created_utc": 1412452154, "gilded": 0, "name": "t1_cl0evx3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "phil_s_stein", "body": "I can run tests from the command line, but still get the same result. `r.get_mentions()` returns an empty list even though I get an orangered about the mention. Any thoughts on how to debug this? Do you mind mentioning me in this thread to test?", "created_utc": 1412452697, "gilded": 0, "name": "t1_cl0f460", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "phil_s_stein", "body": "So when I mention in /r/GoldTesting it worked. It does not work when I mention in /r/phil_s_stein though. Odd. [These mentions worked](http://www.reddit.com/r/GoldTesting/comments/2gj3mu/politician_info_testing/cl0f88s) and [this one](http://www.reddit.com/r/phil_s_stein/comments/2ia95t/mention_test/cl0ezuf) did not.", "created_utc": 1412453152, "gilded": 0, "name": "t1_cl0fb4q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "GoldenSights", "body": "In [this post](http://www.reddit.com/r/phil_s_stein/comments/2ia95t/mention_test/cl0ezuf), you are replying to yourself (OP) and making a mention at the same time. I'm willing to bet they're canceling out.", "created_utc": 1412461290, "gilded": 0, "name": "t1_cl0ioi0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "phil_s_stein", "body": "This is a good bet. When posting in other subs, everything seems fine. It'd be nice to have this behavior documented, but eh?, what can you do?", "created_utc": 1412463858, "gilded": 0, "name": "t1_cl0jr5p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "GoldenSights", "body": "Yes, I wrote the comment just minutes after trying it. Is \"Test mention agent\" the actual useragent you're submitting or is that a placeholder text to make this post? That useragent is very nondescript and I'd change it to `\"/u/phil_s_stein commandline praw testing: fetching username mentions with praw\"`, that's generally how mine look. Your problem is probably something very simple, because there's really [no tricks](http://i.imgur.com/Ii27fLW.png) with this. [Useragents are known to cause trouble](http://www.reddit.com/r/redditdev/comments/2i6gcs/praw_httperror_when_trying_to_login/)", "created_utc": 1412453062, "gilded": 0, "name": "t1_cl0f9ry", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "phil_s_stein", "body": "Yeah, it is something simple which is why it's driving me a little crazy. Now I get mentions correctly from /r/GoldTesting but not other subs.", "created_utc": 1412453715, "gilded": 0, "name": "t1_cl0fjnx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2iaspn/how_to_use_praw_and_get_mentions/"}, {"author": "GoldenSights", "body": "This is just a shot in the dark, but what is your Useragent? Also, if you try doing praw stuff from the command line, does it behave differently?", "created_utc": 1412361215, "gilded": 0, "name": "t1_ckzkriw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2i6gcs/praw_httperror_when_trying_to_login/"}, {"author": "stats94", "body": "Turns out that it was to do with the useragent, a bit of tinkering around with it got it to work, d'oh! Thanks!", "created_utc": 1412418691, "gilded": 0, "name": "t1_cl046gq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2i6gcs/praw_httperror_when_trying_to_login/"}, {"author": null, "body": "[deleted]", "created_utc": 1412659792, "gilded": 0, "name": "t1_cl2lyjo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2i6gcs/praw_httperror_when_trying_to_login/"}, {"author": "stats94", "body": "To be honest, I'm not really sure - but just put it into a similar style to that of the bot for /r/Cricket which is: useragent = '/r/cricket sidebar updating and match thread creating bot by /u/rreyv. This does match updates every minute. Version 2.0' and after that it seemed to work fine!", "created_utc": 1412678002, "gilded": 0, "name": "t1_cl2pnhr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2i6gcs/praw_httperror_when_trying_to_login/"}, {"author": "GoldenSights", "body": "You're trying to remove every single post on the subreddit? PRAW is probably your best bet, since there is no reddit function to do this.", "created_utc": 1412009986, "gilded": 0, "name": "t1_ckvomrd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hsvhj/easiest_way_to_delete_all_posts_for_a_particular/"}, {"author": "nj47", "body": "NOTE: Everything I said below is still applicable, but there is a more pressing issue. The gold api requires you to be using oAuth - that is why the code you posted is not working - you need to authenticate with oauth to use that endpoint I _think_ (I'm not too familiar with praw, but looking through it's documentation and the reddit api docs, this is what seems most likely to me) Are you wanting to gild a post/comment? You want to use /v1/gold/gild, not the api endpoint you posted. The fullname parameter is a reference to the id of post/comment in question (poorly named imo, fullname could be confused with far to many other things). Read more here: https://www.reddit.com/dev/api#fullnames Also, here is a link to the actual source in question: https://github.com/reddit/reddit/blob/master/r2/r2/controllers/apiv1/gold.py#L101 If you are wanting to give gold as a gift directly to a user and not to a specific post, I apologize as what I posted is irrelevant to you.", "created_utc": 1411842620, "gilded": 0, "name": "t1_cku39fo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "F3AR3DLEGEND", "body": "I am looking to give gold directly to a user :P. How can I use oAuth for authentication?", "created_utc": 1411843180, "gilded": 0, "name": "t1_cku3ibs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "nj47", "body": "After digging a little more, I'm not 100% sure I'm correct. I would wait for someone else to reply and see what they have to say. Do other functions marked as oauth on this page (https://www.reddit.com/dev/api) work with praw when you login as opposed to using oauth? As for actually using oauth: https://praw.readthedocs.org/en/v2.1.16/pages/oauth.html https://github.com/reddit/reddit/wiki/OAuth2-Quick-Start-Example", "created_utc": 1411843580, "gilded": 0, "name": "t1_cku3omw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "F3AR3DLEGEND", "body": "Upvoting via PRAW works by just logging with PRAW (voting requires oAuth according to the docs).", "created_utc": 1411843958, "gilded": 0, "name": "t1_cku3uhc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "Ugleh", "body": "Within the scope of the (public) reddit API it is not possible to gild people gold as it is a security concern. The API feature is there for reddit clones. That's not to say it can't be done with a bit of post data. You send a GET request to this URL. Using the python requests library: https://ssl.reddit.com/gold?goldtype=gift&months=1&recipient=USER&signed=yes&giftmessage=MESSAGE_HERE This URL comes from, https://ssl.reddit.com/gold?goldtype=gift you can play around with different options etc.. (Don't worry, pressing Give just takes you to a confirmation screen). You then extract the required parameters from the \"sendcreddits\" form in the HTML of the page, and build up a POST request and send it; again you could use the python Requests library.", "created_utc": 1411895486, "gilded": 0, "name": "t1_ckul4m3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2hmh42/how_to_give_gold_with_the_api/"}, {"author": "GoldenSights", "body": "On a line beneath everything, you can step through the dictionary and print it's values: for key in karma_by_subreddit: print(key, karma_by_subreddit[key]) /r/learnpython might be a better place for future questions like this", "created_utc": 1411362874, "gilded": 0, "name": "t1_ckp391g", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2h3n2u/how_would_i_make_this_karma_breakdown_program/"}, {"author": "88rarely", "body": "That works thank you! But why is there parentheses wrapped around it when it prints with a u' in front of everything?", "created_utc": 1411363324, "gilded": 0, "name": "t1_ckp3e49", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2h3n2u/how_would_i_make_this_karma_breakdown_program/"}, {"author": "GoldenSights", "body": "http://stackoverflow.com/questions/2464959/whats-the-u-prefix-in-a-python-string I guess you're using Python 2? You can wrap it in str() like that one comment says if it really bothers you. Won't hurt anything though.", "created_utc": 1411363470, "gilded": 0, "name": "t1_ckp3fpc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2h3n2u/how_would_i_make_this_karma_breakdown_program/"}, {"author": "88rarely", "body": "Yep fixed. Now it's working 100% thank you.", "created_utc": 1411363644, "gilded": 0, "name": "t1_ckp3hkh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2h3n2u/how_would_i_make_this_karma_breakdown_program/"}, {"author": "bboe", "body": "> I could parse the URL and pull the thing id's out, and if there is a second thing id I would know for sure if the URL pointed to a comment. No there is not currently something in the library that does that because it hasn't been needed for anything.", "created_utc": 1411360825, "gilded": 0, "name": "t1_ckp2knc", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2h35s9/praw_is_there_a_way_to_tell_whether_a_url_links/"}, {"author": "ReBurnInator", "body": "Thanks for the response. I'll whip something up to parse the URL and make the decision. Thanks again!", "created_utc": 1411380948, "gilded": 0, "name": "t1_ckp70s7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2h35s9/praw_is_there_a_way_to_tell_whether_a_url_links/"}, {"author": "Mustermind", "body": "Most PRAW methods have a `limit` keyword. If you set run the method with `limit = None`, PRAW will keep going until it reaches the end of a listing. I made a bot that did that; my attempt looked something like this: user = r.get_user(\"b0wmz\") new = user.get_new(limit = None) posts_in_sub = (link for link in new if link.subreddit == \"redditdev\")", "created_utc": 1411145496, "gilded": 0, "name": "t1_ckmvp2f", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "b0wmz", "body": "Wouldn't that only retrieve the last 100 submissions of a user and then check if these last submissions are in the specified subreddit?", "created_utc": 1411148851, "gilded": 0, "name": "t1_ckmxf1u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "Mustermind", "body": "Nope, PRAW would keep going and making multiple requests to get all the content.", "created_utc": 1411149402, "gilded": 0, "name": "t1_ckmxpa3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "b0wmz", "body": "Alright I'll give that a shot, thanks so much :)", "created_utc": 1411149554, "gilded": 0, "name": "t1_ckmxs4x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "GoldenSights", "body": "You can get 100 items *per call*. The cache caps at [1,000](http://www.reddit.com/r/redditdev/comments/2ffide/listing_old_comments/ck8qlme) (I don't know if that's true for search results, though). /u/Mustermind's example is only pseudocode, here's a functional version: user = r.get_redditor('GoldenSights') sub_items = [] submitted = user.get_submitted(limit=None) for item in submitted: if item.subreddit.display_name.lower() == 'redditdev': sub_items.append(item) And here's how to do your search query: username = \"GoldenSights\" subreddit = \"RedditDev\" search = r.search('author:\"' + username + '\"', subreddit=subreddit, sort='new', limit=None) I have a bot almost exactly like this. I encourage you to solve the problem on your own, but you can look for pointers if you get stuck https://github.com/voussoir/reddit/tree/master/MoreFrom", "created_utc": 1411149994, "gilded": 0, "name": "t1_ckmy0dw", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "b0wmz", "body": "I was unable to find the `limit=None` parameter in the [documentation](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.search), that did the trick. Thanks a lot", "created_utc": 1411150943, "gilded": 0, "name": "t1_ckmyi5t", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "Mustermind", "body": "Nope, it's there in [`get_content`](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.__init__.BaseReddit.get_content), an internal method.", "created_utc": 1411152616, "gilded": 0, "name": "t1_ckmze6d", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "b0wmz", "body": "Ah damn, must've missed the part about additional parameters getting passed to `get_content`.", "created_utc": 1411152741, "gilded": 0, "name": "t1_ckmzgld", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "Mustermind", "body": "That's much better than my one-liner. Using reddit's search didn't occur to me.", "created_utc": 1411150470, "gilded": 0, "name": "t1_ckmy9gs", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gv6zy/praw_retrieve_more_than_25_search_results/"}, {"author": "paneer_burrito", "body": "Quick question: I remember reading about an issue where the proxy settings for https requests were not getting picked up from the environment variables. Does this branch have that fix? Thanks", "created_utc": 1411005112, "gilded": 0, "name": "t1_cklfmye", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/"}, {"author": "bboe", "body": "Yes it does. That fix is also in the master branch, we just haven't made a release for it (and we probably won't until 3.0).", "created_utc": 1411010034, "gilded": 0, "name": "t1_cklhv0f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/"}, {"author": "GoldenSights", "body": "I had this error after enabling HTTPS on my account. Same deal?", "created_utc": 1410831395, "gilded": 0, "name": "t1_ckjgt5q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "DAMN_it_Gary", "body": "yeah, https also. think it might be related to that then.", "created_utc": 1410831448, "gilded": 0, "name": "t1_ckjgu2u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "GoldenSights", "body": "I couldn't figure out how to fix it, so I just disabled https again. I don't know where the real solution is, but this could be a temp fix until you find one.", "created_utc": 1410831572, "gilded": 0, "name": "t1_ckjgway", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "DAMN_it_Gary", "body": "this fixed it. should try to contact praw so they can fix this.", "created_utc": 1410831737, "gilded": 0, "name": "t1_ckjgzbn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "bboe", "body": "Got it. I'll fix it tomorrow and just HTTPS everything.", "created_utc": 1410845117, "gilded": 0, "name": "t1_ckjmxx3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "DAMN_it_Gary", "body": "Great!", "created_utc": 1410864406, "gilded": 0, "name": "t1_ckjqycy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "bboe", "body": "See this post for a current solution: https://www.reddit.com/r/redditdev/comments/2gmzqe/praw_https_enabled_praw_testing_needed/ I appreciate any feedback you have.", "created_utc": 1410959310, "gilded": 0, "name": "t1_ckksavn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "DAMN_it_Gary", "body": "Ok, thanks", "created_utc": 1410959542, "gilded": 0, "name": "t1_ckksdwr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "bboe", "body": "Unfortunately I might not get to it today. Follow this bug for status updates: https://github.com/praw-dev/praw/issues/318", "created_utc": 1410890371, "gilded": 0, "name": "t1_ckk1ssx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gio0c/praw_cant_send_messages_pm/"}, {"author": "bboe", "body": "Not really. You can write your own function that alternates requests between the submission and comment listings. However, I personally think it would be simpler and cleaner to have two separate programs, one for comments, one for submissions, and use those in combination with [praw-multiprocess](http://praw.readthedocs.org/en/latest/pages/multiprocess.html) and some method of communicating between the two processes if necessary, e.g., a database.", "created_utc": 1410828627, "gilded": 0, "name": "t1_ckjfey1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gihdt/is_it_possible_to_run_a_submission_and_comment/"}, {"author": "Morgneer", "body": "Alright, thanks for the help", "created_utc": 1410829266, "gilded": 0, "name": "t1_ckjfqhr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gihdt/is_it_possible_to_run_a_submission_and_comment/"}, {"author": "aksios", "body": "For initial posting, 15000 *is* the character limit. The 45000 character limit is for subsequent edits.", "created_utc": 1410763249, "gilded": 0, "name": "t1_ckipfq3", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gfnzr/15000_character_limit_on_self_posts_in_praw/"}, {"author": "spinnelein", "body": "I tried creating a self post and editing it in praw, same error message submission.edit(text=posttext) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 285, in edit response = self.reddit_session.request_json(url, data=data) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 177, in wrapped raise error_list[0] praw.errors.APIException: (TOO_LONG) `this is too long (max: 15000.0)` on field `text`", "created_utc": 1410844785, "gilded": 0, "name": "t1_ckjmu5s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gfnzr/15000_character_limit_on_self_posts_in_praw/"}, {"author": "bboe", "body": "You can ignore the ResourceWarnings. They only appears because PRAW inadvertently enables the output of warnings, they otherwise occur but are simply ignored. Aside from the presence of warnings (out of our control) there is no difference in behavior with PRAW on python2 or python3 and if happens to be we'll fix it. I don't really understand your first question. If your program loops forever then you need someway to manage its process. If it only runs for a finite amount of time on each run, then you need some way to periodically execute the program.", "created_utc": 1410749218, "gilded": 0, "name": "t1_ckikffe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gezwe/two_simple_questions_from_a_praw_noob/"}, {"author": "TheLazarbeam", "body": "Okay thanks, is there any way to hide them? just a little annoying when I'm testing scripts and they pop up every time. as for my first question, I figured bots like AutoModerator or LinkFixerBot ran independently forever, not in someone's terminal. How can I make a program that will run automatically forever even when I turn my PC off?", "created_utc": 1410752259, "gilded": 0, "name": "t1_ckilt6y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gezwe/two_simple_questions_from_a_praw_noob/"}, {"author": "pointychimp", "body": "> How can I make a program that will run automatically forever even when I turn my PC off? Run the program on a computer that *is* on all the time. A cheap VPS (virtual private server), for example. If you have a second, low-power computer at home, it might be easier and cheaper to run it on that.", "created_utc": 1410756243, "gilded": 0, "name": "t1_ckinepp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2gezwe/two_simple_questions_from_a_praw_noob/"}, {"author": "Mustermind", "body": "/api/info doesn't support that. I remember having this problem a while back to store users in a web app and just ended up using the user's name instead. User (or even subreddit) fullnames rarely serve any purpose for API clients because a username or a subreddit URL can't be changed anyway, so you might as well use those instead.", "created_utc": 1410632294, "gilded": 0, "name": "t1_ckhbdg3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2g9rig/why_cant_i_get_info_with_an_account_fullname/"}, {"author": "GoldenSights", "body": "That's kinda strange. Sometimes it's nice to see the json information layed out like that, instead of having to launch Python+Praw and do vars(). Unless it's particularly taxing for their servers, I don't really get why this wouldn't be supported. Anyway, thanks", "created_utc": 1410645031, "gilded": 0, "name": "t1_ckhgu4d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2g9rig/why_cant_i_get_info_with_an_account_fullname/"}, {"author": "Mustermind", "body": "What do you mean? /about.json accomplishes the same task for users and subreddits: http://www.reddit.com/user/Mustermind/about.json http://www.reddit.com/r/redditdev/about.json", "created_utc": 1410645264, "gilded": 0, "name": "t1_ckhgxjt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2g9rig/why_cant_i_get_info_with_an_account_fullname/"}, {"author": "GoldenSights", "body": "Oh, brilliant, thank you!", "created_utc": 1410647635, "gilded": 0, "name": "t1_ckhhwna", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2g9rig/why_cant_i_get_info_with_an_account_fullname/"}, {"author": "GoldenSights", "body": "The first 1,000 are coming from the cache. Anything after that, I think, should be new information since you've started streaming. It sounds to me like you've caught up with the rate of comment creation in the subreddit, no? If people on the sub are making 5-10 comments per second, that's what you're going to get out of a stream. Or maybe I'm missing a glaring detail.", "created_utc": 1410259878, "gilded": 0, "name": "t1_ckdbh6x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2fw9fz/praw_slowdown_in_helpersget_comment_stream/"}, {"author": "ovooDE", "body": "mh, i understood that get_comment_stream gives an iterator that iterates over all comments from the past. So, can you only get the last 1000 comments per subreddit? Is there any way to go back further?", "created_utc": 1410260251, "gilded": 0, "name": "t1_ckdbjmx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2fw9fz/praw_slowdown_in_helpersget_comment_stream/"}, {"author": "GoldenSights", "body": "[That's unfortunately correct](https://www.reddit.com/r/redditdev/comments/2ffide/listing_old_comments/ck8qlme) There are some hypothetical ways around this, but nothing within the realm of possibility.", "created_utc": 1410260637, "gilded": 0, "name": "t1_ckdbmb8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2fw9fz/praw_slowdown_in_helpersget_comment_stream/"}, {"author": "bboe", "body": "It wasn't supported. I just added support. https://github.com/praw-dev/praw/commit/0fabfa930fc0a8d9ba1dcaf16794dbbac1f06d79", "created_utc": 1409984998, "gilded": 0, "name": "t1_ckalink", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2fle0e/praw_userget_liked_only_returns_50_items_does_not/"}, {"author": "GoldenSights", "body": "Oh wow, thank you! I'm very glad you saw this post.", "created_utc": 1409985802, "gilded": 0, "name": "t1_ckalpak", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2fle0e/praw_userget_liked_only_returns_50_items_does_not/"}, {"author": "Magzter", "body": "/u/largenocream assisted me in IRC. For future searchers, this is resolved by adding the following call: $client->setCurlOption(CURLOPT_USERAGENT,'ChangeMe/yourUsername 0.1'); Before you make your $response = $client->fetch(\"https://oauth.reddit.com/api/v1/me.json\"); call. Apparently the API is stricter about User Agents than it used to be and the tutorial just needs to be updated. ________________________________________________ Largenocream has actually updated the tutorial, what a beauty.", "created_utc": 1409827977, "gilded": 0, "name": "t1_ck8u1sz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ffxm9/user_agent_required_phpoauth/"}, {"author": "gavin19", "body": "If you don't need to worry about it checking old posts if the script stops (for whatever reason), you can just store the id of the comment that the bot has replied to in a list, add the id to the list, then check against that before making further replies replied_to = [] // After replying replied_to.append(comment.id) // Check before replying to another comment if comment.id not in replied_to: stuff", "created_utc": 1409674735, "gilded": 0, "name": "t1_ck765ws", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2f9lyn/praw_how_can_i_see_if_my_bot_already_replied_to_a/"}, {"author": "Leonlg", "body": "That worked. Thank you!", "created_utc": 1409678016, "gilded": 0, "name": "t1_ck77r1i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2f9lyn/praw_how_can_i_see_if_my_bot_already_replied_to_a/"}, {"author": null, "body": "I keep a database of that stuff. Everytime my bot has completed a task the postid of the post gets logged and will ignore the post if the bot has already been there", "created_utc": 1409677795, "gilded": 0, "name": "t1_ck77n7y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2f9lyn/praw_how_can_i_see_if_my_bot_already_replied_to_a/"}, {"author": "Mustermind", "body": "There are many ways to do that. 1. **A `set`**: As /u/gavin19 mentioned, you can just add to the set and check if the comment id is present in the set before doing the action. However, this does mean that if your bot crashes, your saved ids go with it and it may start commenting on the same comment again. One way to avoid it would be simply to escape common errors with `try/except`. 2. **`redis` or `memcached`**: These are separate databases that run alongside your bot and track the added ids. They store data on memory and on-disk, so if the bot crashes, you can just restart it and it will continue as normal. This may be overkill if you are new to programming or just want to make a simple bot, but it's worked quite well for me. 3. **A `commented.txt` file**: If a dedicated database is too overkill for you, just store the comments in a newline-separated file and use that like a list by doing something like `commented = open(\"commented.txt\").read().splitlines()` 4. **Checking `comment.replies`**: If you already replied to a comment, you can just check to see if your bot's comment is one of the replies. Although this is simple, this is a **terrible idea**, since reddit doesn't always return *all* the replies to a comment by default. Nevertheless, if you are using the set thing, I recommend also using this method with it as a sort of failsafe.", "created_utc": 1409678427, "gilded": 0, "name": "t1_ck77yhm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2f9lyn/praw_how_can_i_see_if_my_bot_already_replied_to_a/"}, {"author": "GoldenSights", "body": "This sounds like it might get spammy... u = r.get_redditor('tryme1029') submissions = u.get_submissions(limit=None) comments = u.get_comments(limit=None) for submission in submissions: submission.add_comment('The comment') for comment in comments: comment.reply('The comment') I'm not going to try running this, but I think it will work.", "created_utc": 1409518323, "gilded": 0, "name": "t1_ck5p1zk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2f3yz8/how_would_i_scan_for_all_contents_by_a_particular/"}, {"author": "Mustermind", "body": "AFAIK, there's no explicit `rank` attribute on a submission, but a listing of a subreddit returns submissions in order, so you can infer the submission's rank from there. import praw r = praw.Reddit(user_agent = \"Rank Finder\") submission = r.get_info(thing_id = \"t3_2f265d\") listing = r.get_subreddit(\"redditdev\").get_hot() # The best thing to do is to iterate through the generator # and return the index of the submission, but I'm lazy, so # here's a shortcut. rank = list(listing).index(submission) + 1 print(rank)", "created_utc": 1409499136, "gilded": 0, "name": "t1_ck5gmhl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2f265d/any_way_to_get_the_rank_of_a_submission_in_a/"}, {"author": "Lost_it", "body": "thank you!", "created_utc": 1409534675, "gilded": 0, "name": "t1_ck5w08a", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2f265d/any_way_to_get_the_rank_of_a_submission_in_a/"}, {"author": "_Daimon_", "body": "And if you *do* use PRAW, please say so in your post. Sometimes it can be hard to tell the difference.", "created_utc": 1409082488, "gilded": 0, "name": "t1_ck16gj8", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2enl27/help_us_help_you_include_important_debugging_info/"}, {"author": "aakilfernandes", "body": "Not sure about praw, but it looks like you can hit http://www.reddit.com/r/AskReddit/about/traffic/.json", "created_utc": 1409087786, "gilded": 0, "name": "t1_ck19e69", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2enkr2/prawapi_is_there_a_way_to_pull_traffic_stats_from/"}, {"author": "ELFAHBEHT_SOOP", "body": "Also, the layout for the JSON on that page is an array of arrays for days. The arrays in the \"days\" array is laid out like: [epoch time, uniques, pageviews, subscriptions] Then there are arrays of arrays for the hours and months. The arrays in both the \"hours\" and \"months\" arrays are laid out like: [epoch time, uniques, pageviews]", "created_utc": 1409426603, "gilded": 0, "name": "t1_ck4tzfl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2enkr2/prawapi_is_there_a_way_to_pull_traffic_stats_from/"}, {"author": null, "body": "As mentioned, reddit not longer publicly shows the number of downvotes, but it does show the ratio of upvotes to downvotes, and you can use that to get an estimate of the number of upvotes & downvotes: import praw r = praw.Reddit('downs test') subreddit = 'programming' for submission in r.get_subreddit(subreddit).get_hot(): ratio = r.get_submission(submission.permalink).upvote_ratio ups = round((ratio*submission.score)/(2*ratio - 1)) if ratio != 0.5 else round(submission.score/2) downs = ups - submission.score The only problem is that this is quite slow, since it needs to make a separate API call for each submission. Still, it's the best you can do for now.", "created_utc": 1408539436, "gilded": 0, "name": "t1_cjvhifc", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": "Dobias", "body": "Is there also a chance to calculate the downvotes for a comment? Exception: '' has no attribute 'upvote_ratio'", "created_utc": 1408567376, "gilded": 0, "name": "t1_cjvvq9o", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": null, "body": "Doesn't look like it.", "created_utc": 1408569926, "gilded": 0, "name": "t1_cjvx32w", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": "Dobias", "body": "Awesome, thank you very much. The calculated count seems to fit quite OK to the normal one. import praw r = praw.Reddit('downs test') subreddit = 'programming' for submission in r.get_subreddit(subreddit).get_hot(): ratio = r.get_submission(submission.permalink).upvote_ratio ups = int(round((ratio*submission.score)/(2*ratio - 1)) if ratio != 0.5 else round(submission.score/2)) downs = ups - submission.score print submission.ups, ups, downs Output: 203 220 17 1278 1480 202 23 31 8 17 23 6 80 93 13 31 43 12 24 33 9 55 65 10 5 5 0 160 201 41", "created_utc": 1408539952, "gilded": 0, "name": "t1_cjvhotl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": null, "body": "Nice. Just note the ninja edit I did for the case that the ratio is 0.5 And a quick note, your `int` call for calculating `downs` is redundant as both `ups` and `submission.score` are `int`s already. Unless I've got something wrong?", "created_utc": 1408540266, "gilded": 0, "name": "t1_cjvhsxs", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": "Dobias", "body": "Yes, you are right of course. Thanks.", "created_utc": 1408540547, "gilded": 0, "name": "t1_cjvhwkx", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": "gavin19", "body": "This is just what reddit returns since the up/down voting ratio was changed. It's the same no matter how you fetch the post info.", "created_utc": 1408538310, "gilded": 0, "name": "t1_cjvh54q", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2q2l/praw_downvote_count_always_zero/"}, {"author": "bboe", "body": "If you can follow the comment tree IDs deterministically via the json responses then it's a PRAW bug. It was at one point an issue on reddit's end, but that may be resolved.", "created_utc": 1408514636, "gilded": 0, "name": "t1_cjvbxca", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2e2191/praw_how_to_retrieve_replies_to_a_comment_past_10/"}, {"author": "_Daimon_", "body": "If you did not update PRAW, then PRAW has not changed in the past day. If it's stopped working then it's due to something else, such as a change to your code or Reddits. What excatly is happening, are you getting an exception or something like this? Do you have a code piece that replicates the issue?", "created_utc": 1408123145, "gilded": 0, "name": "t1_cjr5hnj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "snaysler", "body": "What's happening is that all of my bots have a connection timeout exception thrown every time they try to edit the sidebar. And now they are throwing random connection errors left and right for trying to edit or load several random wiki pages. It's like my subreddit is slowly taking less and less requests from PRAW. Everything was working perfectly yesterday with no problems, and nothing has changed since then...", "created_utc": 1408123480, "gilded": 0, "name": "t1_cjr5ohn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "_Daimon_", "body": "Sounds like you have a network connection issue, if the http requests keep timing out. It's either at your end or at Reddits end, where it could happen if the server is under particularly high load.", "created_utc": 1408123913, "gilded": 0, "name": "t1_cjr5x5w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "snaysler", "body": "I'm certain it isn't at my end. But is Reddit actually just this weak that it can't take an HTTP request every 10 minutes without breaking for a day? What on earth can I do? I have a lot riding on getting this to work properly...", "created_utc": 1408124046, "gilded": 0, "name": "t1_cjr5zu3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": null, "body": "[deleted]", "created_utc": 1408124421, "gilded": 0, "name": "t1_cjr67fr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "snaysler", "body": "My employer purchased a remote server to run all the bots. That is out of my hands. They guarantee constant up-time/reliability. Also, the server is running the bots using cronjobs, which is on an interval of once every few minutes. If a bot waits too long to connect, another iteration of the bot launches during that time, then perhaps another, etc. That would be bad.", "created_utc": 1408124689, "gilded": 0, "name": "t1_cjr6czi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": null, "body": "[deleted]", "created_utc": 1408124917, "gilded": 0, "name": "t1_cjr6hry", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "snaysler", "body": "How can a python program check to see if another iteration of itself is already running on my server? Is that really possible? How do I check for packet loss/what is that? I'm a programmer mostly, and am no expert in the technical side of the web.", "created_utc": 1408125126, "gilded": 0, "name": "t1_cjr6maw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": null, "body": "[deleted]", "created_utc": 1408125309, "gilded": 0, "name": "t1_cjr6q9q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "tst__", "body": "If you run a lot of bots from one server I would definitely check out [PRAW's multiprocess architecture](https://praw.readthedocs.org/en/v2.1.16/pages/multiprocess.html). Otherwise good debugging should help you isolate the problem. Ideas top of the head: * Check the bot from your connection instead of the server * Use links (or something equivalent) to browse reddit from your server * Try out different accounts / bots", "created_utc": 1408125230, "gilded": 0, "name": "t1_cjr6oj9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "snaysler", "body": "I've already looked into everything in your bullet points unfortunately :/ I'd never heard of the multiprocess architecture thought, thanks I'll check it out.", "created_utc": 1408125393, "gilded": 0, "name": "t1_cjr6s1l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "Deimorz", "body": "Are your bots using standard login, or oauth?", "created_utc": 1408130616, "gilded": 0, "name": "t1_cjr9r9o", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "snaysler", "body": "I think it is standard, but I'm not sure. Here's what it looks like: r = praw.Reddit('/r/ludobots TreeBot UVM') r.login(\"mcLudobot\", \"mypassword\")", "created_utc": 1408132122, "gilded": 0, "name": "t1_cjrakxa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dndq3/serious_issue_with_praw_in_my_sub_urgent/"}, {"author": "GoldenSights", "body": "[PRAW Code Overview](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html) You're looking for `comment.score`", "created_utc": 1407971941, "gilded": 0, "name": "t1_cjpj2hr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "irrestistable", "body": "I couldn't see comment.score in the Code Overwiew?", "created_utc": 1407973653, "gilded": 0, "name": "t1_cjpjvc9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "GoldenSights", "body": "Well, no, but if you do a ctrl+f for \"score\" one of the results you'll find is tied to [praw.Objects.Comment](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.Comment). The code overview is layed out by objects, and each object has its own attributes and functions.", "created_utc": 1407974960, "gilded": 0, "name": "t1_cjpkh1x", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "irrestistable", "body": "> get_hot(*args, **kwargs) > Return a get_content generator for some RedditContentObject type. > > The additional parameters are passed directly into get_content(). Note: the url parameter cannot be altered. > > May use the read oauth scope to seecontent only visible to the authenticated user. When it returns the get_content generator, can I turn it into a list like they do here? https://wiki.python.org/moin/Generators", "created_utc": 1407976543, "gilded": 0, "name": "t1_cjpl6wd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "GoldenSights", "body": "It seems to me that you can use `list()` to get a list of the generator, yeah sub = r.get_subreddit('redditdev') new = sub.get_new(limit=100) l = list(new) and now `l` contains all the items you get from /new", "created_utc": 1407977051, "gilded": 0, "name": "t1_cjplf6a", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "irrestistable", "body": "That's exactly what I was thinking! Thanks", "created_utc": 1407977922, "gilded": 0, "name": "t1_cjplt6m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "_Daimon_", "body": "You should use python introspection to find attributes and methods of the objects. See [this tutorial](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) in the PRAW documentation. For your specific question, both Comment and Submission objects have the 'score' attribute which is the score of the comment/submission.", "created_utc": 1407972022, "gilded": 0, "name": "t1_cjpj3wf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dhhrk/praw_command_list/"}, {"author": "Deimorz", "body": "The opt-out doesn't apply to \"new\" in /r/all, so you don't need to do anything special.", "created_utc": 1407873792, "gilded": 0, "name": "t1_cjodafp", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dd6n8/get_all_new_submissions_including_subreddits_who/"}, {"author": "echocage", "body": "Ahhh thank you very much, I hadn't realised that.", "created_utc": 1407874036, "gilded": 0, "name": "t1_cjodfdy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2dd6n8/get_all_new_submissions_including_subreddits_who/"}, {"author": "Mustermind", "body": "Me and /u/SavinaRoja actually compiled a file of subreddits where bots are not allowed or only partly allowed at /r/Bottiquette/wiki/robots_txt_json. I know that your idea is different and is spread out over various subreddits; but many bots look for submissions or comments in /r/all and querying each subreddit's robots_txt is very expensive (in terms of time) for the bot, so gathering all the rules first and then checking it within the logic of the program is much simpler. I've also never seen PRAW implement non-API features like that. I'm not much of a python person, but it might be possible to build a plugin for PRAW that does that.", "created_utc": 1407757366, "gilded": 0, "name": "t1_cjn009t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "deletedLink", "body": "Well, that's interesting. Yeah, it could be burdensome to go to each subreddit if a bot were just trolling /r/all. Sounds like a bot is needed to grab all of the robots.txt files from each wiki and put them in one -- in that case. The ultimate solution. Any idea of how many bots are using your system?", "created_utc": 1407762632, "gilded": 0, "name": "t1_cjn1f0s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "Mustermind", "body": "A bot that compiles every robots.txt and puts it into a file is not only very difficult to make (there is no definitive list of subreddits) but is the same thing as the /r/Bottiquette one. There is a bot running that adds subreddits to the list when messaged ([the format](https://www.reddit.com/r/Bottiquette/wiki/automatic_listings)). I don't think reddit provides traffic sites for wiki pages, so I don't know how many bots are using the file. I'm still getting messages about the file so I presume quite a few bots are already using it.", "created_utc": 1407763377, "gilded": 0, "name": "t1_cjn1nxb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "Ugleh", "body": "Wouldn't disabling the User-agent: * also disable every user, not just bots? Bots can make a custom User-agent, but as a developer, I always set my boot user-agent to my current version of firefox. If a unique name is needed that's when id give it a custom one.", "created_utc": 1407765299, "gilded": 0, "name": "t1_cjn2d78", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "deletedLink", "body": "The only user-agents that would be looking at the file are robots that participated.", "created_utc": 1407775851, "gilded": 0, "name": "t1_cjn7cy8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "Mustermind", "body": "In OP's analogy, he compared it to robots.txt, so I'm assuming the rules only apply to bots. Also, I might be misunderstanding you, but setting a bot's user-agent to a browser user-agent is against reddit's API rules.", "created_utc": 1407771783, "gilded": 0, "name": "t1_cjn5a9b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "kemitche", "body": "> I always set my boot user-agent to my current version of firefox I'm going to reiterate /u/Tjstretchalot's statement and say: Do not do that. It's bad form & against our API rules, and could cause us to assume your bots are nefarious.", "created_utc": 1407780840, "gilded": 0, "name": "t1_cjna1lq", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "Ugleh", "body": "I don't do this for Reddit. I make beats for other stuff as well, I should have noted.", "created_utc": 1407794560, "gilded": 0, "name": "t1_cjnhj9t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "Tjstretchalot", "body": "Do not do that. That is very bad form; please see: https://github.com/reddit/reddit/wiki/API Specifically >**NEVER lie about your user-agent.** This includes spoofing popular browsers and spoofing other bots. We will ban liars with extreme prejudice.", "created_utc": 1407771834, "gilded": 0, "name": "t1_cjn5b5l", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d7yqc/subreddit_robots_permission_file_analogous_to/"}, {"author": "Mustermind", "body": "In my experience building the picturegame bot, [get_new()](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_new) reliably returns results from newest to oldest every time. By the way, to avoid hitting submission you've already seen, have you considered using the slightly obscure but nonetheless cool [`praw.helpers.submission_stream`](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.helpers.submission_stream)? That way, you can avoid working with databases and keep the code simple. But keep in mind: you'll need to escape the bot from common errors ([see lines 547-574 to see how I did it](https://github.com/RequestABot/picturegamebot/blob/master/picturegamebot/bot.py#L457)) or you'll lose your users.", "created_utc": 1407726163, "gilded": 0, "name": "t1_cjmrfh3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "bsturtle", "body": "Thank you for the response. I had not come across submission_stream. I will look into it. I will probably still need the small dB to keep track of who has posted in the last X days to compare to the newer posts I'm looking at. edit: I did see that before. I did not want to use it because it starts from oldest and goes to new. And that is what made me wonder how information is returned with get_new() since it was not explicity stated in the docs. Though in retrospect from other reply's it would have been safe to assume it when newest to oldest. Thanks again though!", "created_utc": 1407727058, "gilded": 0, "name": "t1_cjmrtct", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "_Daimon_", "body": "[get_new](http://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_new) returns the [/new](http://www.reddit.com/r/redditdev/new/) listing for the subreddit. Just as on the webend it is returned new to old. The order is fully dependable. The time of creation is available as a unix timestamp in the created_utc attribute for the Submission object. Do not use created, as this timestamp is based on the server which is completely unreliable.", "created_utc": 1407726375, "gilded": 0, "name": "t1_cjmriuq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "bsturtle", "body": "Thanks for your response! I definitely intend to use the UTC timestamp to avoid issues with local time zones when checking how old the posts are.", "created_utc": 1407727141, "gilded": 0, "name": "t1_cjmruoo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "GoldenSights", "body": "This wouldn't be too hard to test yourself from the commandline. >>> import datetime >>> subreddit = r.get_subreddit('redditdev') >>> new = subreddit.get_new(limit=25) >>> for item in new: ... print(datetime.datetime.strftime(datetime.datetime.utcfromtimestamp(item.created_utc), \"%B %d %H:%M\") August 11 02:00", "created_utc": 1407726082, "gilded": 0, "name": "t1_cjmre6s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "bsturtle", "body": "Thanks for your response! I wish I had thought of that.", "created_utc": 1407727249, "gilded": 0, "name": "t1_cjmrwcp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2d72ou/praw_how_are_submissions_retrieved_from_a/"}, {"author": "bboe", "body": "The data obtained when fetching a message through `get_info` does not return any children objects. It looks like a message endpoint now exists which does, however, though PRAW does not currently have support. http://www.reddit.com/message/messages/ID.json", "created_utc": 1407399558, "gilded": 0, "name": "t1_cjjccko", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cv1ua/how_do_i_get_this_praw_object_to_populate/"}, {"author": "brucemo", "body": "Thank you. Am I stuck then, or is the JSON url you provided a means by which I should proceed?", "created_utc": 1407401117, "gilded": 0, "name": "t1_cjjco8d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cv1ua/how_do_i_get_this_praw_object_to_populate/"}, {"author": "RemindMeBotWrangler", "body": "Are you looking at your PMs before your bot? If you go to http://www.reddit.com/message/inbox/ before get_unread() does, although the messages say unread and are red, they actually are not. This is currently what I use. for comment in reddit.get_unread(unset_has_mail=True, update_user=True): redditPM = Search(comment) if \"RemindMe!\" in comment.body and str(type(comment)) == \"\": redditPM.run(privateMessage=True) comment.mark_as_read() time.sleep(30) I have to avoid going to /u/RemindMeBot's inbox or else messages `sent just now` are marked as read and it misses it.", "created_utc": 1407192349, "gilded": 0, "name": "t1_cjgzkqe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2clwr7/praw_get_unread_issue_please_help/"}, {"author": "mcLudobot", "body": "Thanks for the reply, but I actually never log in to mcLudobot except for this one time right now. I actually changed the program to call get_content(\"http://www.reddit.com/message/messages\"), which allows me to see the full message history, and which shows messages that mcLudobot has responded to, thereby addressing all those mcLudobot has not.", "created_utc": 1407263419, "gilded": 0, "name": "t1_cjhr364", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2clwr7/praw_get_unread_issue_please_help/"}, {"author": "aakilfernandes", "body": "Thats how most bots do it. You can set up a webhook using Reddit Alerts (full disclosure - its my company)", "created_utc": 1407091229, "gilded": 0, "name": "t1_cjfuu2h", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ciblc/praw_inefficiency_or_does_this_exist/"}, {"author": "Falmarri", "body": "If you have reddit gold, you can see when people mention your name in the style of /u/tingmakpuk .", "created_utc": 1407088823, "gilded": 0, "name": "t1_cjftrwg", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ciblc/praw_inefficiency_or_does_this_exist/"}, {"author": "Phteven_j", "body": "replies = comment.replies for reply in replies: if reply.author.name == target_username: return True Unless you want to include replies to sub-comments, in which case you will need to recurse like you said.", "created_utc": 1407038348, "gilded": 0, "name": "t1_cjffare", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ch33g/trying_to_use_praw_to_see_if_a_user_has_already/"}, {"author": "mathleet", "body": "This is what I originally wrote, but it would error out when the comment parse would encounter a MoreComments object. What I wrote above is an attempt(?) to make it load more comments then re-parse the submission. I added a pastebin of some test code in the OP to help illustrate what's happening.", "created_utc": 1407038461, "gilded": 0, "name": "t1_cjffcbk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ch33g/trying_to_use_praw_to_see_if_a_user_has_already/"}, {"author": "Phteven_j", "body": "so before you call it, do submission.replace_more_comments() and wrap it in a try-catch for unusually long trees.", "created_utc": 1407038557, "gilded": 0, "name": "t1_cjffdn4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2ch33g/trying_to_use_praw_to_see_if_a_user_has_already/"}, {"author": "Mustermind", "body": "AFAIK, this \"threat\" appears when an unnatural amount of links to possibly malicious sites or an overwhelming number of links to one site are detected, indicating that the website might be hacked to boost Google's rankings to the linked website. It's an illegal SEO \"trick\", thus the name [Black Hat](https://en.wikipedia.org/wiki/Black_hat_hacker#Black_hat). **It's probably a false positive.** It probably came up because the user submitted a lot from one domain (maybe i.imgur.com or self.something), so AVG is showing it. You should scrub through the result yourself to double-check that either you don't have a virus or that your ISP or VPN isn't adding stuff to your JSON.", "created_utc": 1406977060, "gilded": 0, "name": "t1_cjestn5", "num_comments": null, "score": 9, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "bobobo1618", "body": "OP, you should then replace AVG with something that isn't shit. It was a rather long time ago but [this](http://arstechnica.com/information-technology/2008/11/avg-incorrectly-flags-user32-dll-in-windows-xp-sp2sp3/) took away any doubt I had that AVG was the new Norton for me. I'd personally recommend [Bitdefender](http://www.bitdefender.com.au/solutions/free.html) but you can look [here](http://chart.av-comparatives.org/chart1.php) for an idea of effectiveness of various alternatives..", "created_utc": 1406979088, "gilded": 0, "name": "t1_cjet5ws", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "Theemuts", "body": "I'm actually considering dropping antivirus software completely. I found [this](http://www.syscan360.org/slides/2014_EN_BreakingAVSoftware_JoxeanKoret.pdf) on reddit a few days ago, and it isn't pretty.", "created_utc": 1406981722, "gilded": 0, "name": "t1_cjetns3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "Scientologist2a", "body": "well, the original report was on \"Nov 11 2008, 8:37pm EDT\" so they have had plenty of time to get their shit together. Not that anyone else is much better.", "created_utc": 1407005276, "gilded": 0, "name": "t1_cjf1wna", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "bobobo1618", "body": "However beautiful that may be, I don't think giving up on AV software completely is the way to go. If an exploit is actually released, it'll be picked up and detected pretty damn quick, just like any other malware. If you're running Windows, you really need AV. There are too many vulnerabilities floating around an average Windows PC.", "created_utc": 1406990638, "gilded": 0, "name": "t1_cjew0en", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": null, "body": "[deleted]", "created_utc": 1407025677, "gilded": 0, "name": "t1_cjfacu2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "bobobo1618", "body": "If you look at that graph I posted somewhere above, there's a line that shows everything caught by MSE. While your other points are valid, MSE isn't very effective compared to the other alternatives.", "created_utc": 1407027764, "gilded": 0, "name": "t1_cjfb73y", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": null, "body": "[deleted]", "created_utc": 1407030448, "gilded": 0, "name": "t1_cjfc959", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "bobobo1618", "body": "[Uhh](http://www.microsoft.com/security/pc-security/windows8.aspx) > In Windows 8, Windows Defender replaces Microsoft Security Essentials. [http://chart.av-comparatives.org/chart1.php](http://chart.av-comparatives.org/chart1.php) > Out-of-box protection (blocked) by Microsoft Windows 7 & 8 As for your \"statistically it was virtually no different than most of the other options\" \"month to month\" bullshit: - [March](http://i.imgur.com/eIuHSAp) - [April](http://i.imgur.com/Es8qph3) - [May](http://i.imgur.com/bweZqHa) - [June](http://i.imgur.com/bY7r7s0) Note how in _every_ test in the last six months, Microsoft has ranked below 90% while Kaspersky has consistently been above 99%. And Bitdefender, [which has a free edition, which I linked above](http://www.bitdefender.com.au/solutions/free.html), doesn't rank less than 99.5% a single time in the last six months. Maybe you're looking at a different graph to me but a 10% _minimum_ difference is definitely a 'practical difference in the effectiveness' to me. If your grandma exposes herself to 100 pieces of malware a month, she'll get 10 false negatives with MSE where she'd get 1 with Kaspersky or a 50/50 chance of getting one with Bitdefender. Sure MSE is easy to use but the difference between it and the better competitors is __definitely__ a practical one.", "created_utc": 1407037975, "gilded": 0, "name": "t1_cjff5ia", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "Theemuts", "body": "Thanks, since everything seems to be fine and there's no weird data associated with this user in my database, I assume it's a false positive.", "created_utc": 1406981783, "gilded": 0, "name": "t1_cjeto74", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2cex9y/virus_detected_in_data_returned_by_reddit/"}, {"author": "bboe", "body": "Having reddit gold on your account will maximize the number of comments you can fetch along with a submission. Otherwise, no, this is simply not possible on reddit (not a PRAW limitiation, nor API limitation).", "created_utc": 1406820341, "gilded": 0, "name": "t1_cjd1wha", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2c8lem/is_there_a_way_to_get_all_comments_in_a_thread/"}, {"author": "haiguise1", "body": "I see, thanks.", "created_utc": 1406825748, "gilded": 0, "name": "t1_cjd4w90", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2c8lem/is_there_a_way_to_get_all_comments_in_a_thread/"}, {"author": "tst__", "body": "If you want multiline variables with configparser you have to indent all the lines but the first. That means that in your example this should work: [misc] message=Hello world. *I have multiple lines* Notice that the \"empty\" line also has to start with two spaces. Here's the output from the REPL: >>> config.get(\"misc\", \"message\") 'Hello world.\\n*I have multiple lines*'", "created_utc": 1406814392, "gilded": 0, "name": "t1_cjcz2ka", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2c8k7m/using_an_external_cfg_file_for_multiline/"}, {"author": "FirestarterMethod", "body": "Thank you so much.", "created_utc": 1406814597, "gilded": 0, "name": "t1_cjcz5i9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2c8k7m/using_an_external_cfg_file_for_multiline/"}, {"author": "ytwang", "body": "You can't. Lists only have up to 1000 items due to the way reddit caches. You can get a few more by choosing different sorts (which are different cached lists), but you are unlikely to be able to get them all.", "created_utc": 1406642376, "gilded": 0, "name": "t1_cjazaf0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2c1iv5/how_to_get_all_comments_ever_written_in_a/"}, {"author": "Dobias", "body": "Too bad. But thank you for the clarification.", "created_utc": 1406644220, "gilded": 0, "name": "t1_cjb03mq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2c1iv5/how_to_get_all_comments_ever_written_in_a/"}, {"author": "_Daimon_", "body": "> so I'm just wondering if I need to process the body of the comment as a string and pattern match until I get any links Yes > or if there's a method that already exists for that? Nope.", "created_utc": 1406371210, "gilded": 0, "name": "t1_cj8981l", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2brrf9/praw_how_do_i_extract_links_from_a_comment/"}, {"author": "sworeiwouldntjoin", "body": "Hey, thanks for the quick response! Do you think it would be of benefit to the community if I were to (try to) add such a set of methods?", "created_utc": 1406371471, "gilded": 0, "name": "t1_cj899jr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2brrf9/praw_how_do_i_extract_links_from_a_comment/"}, {"author": "badpokerface12", "body": "If you start a github project I would be willing to help with a method(s) that will parse a comment for links", "created_utc": 1406398777, "gilded": 0, "name": "t1_cj8hnbx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2brrf9/praw_how_do_i_extract_links_from_a_comment/"}, {"author": "_Daimon_", "body": "I've seen others with the same question, so having access to such methods would be useful. You could create a github project, similar to replybot, that would help others with the same usecase. We won't accept a PR to add methods to parse content for a specific purpose inside the core PRAW, as that lies beyond the scope of a wrapper. However I think it would be fine to include it in the [helpers](https://github.com/praw-dev/praw/blob/master/praw/helpers.py) file which provide a few commonly needed methods that lies beyond PRAWs normal scope.", "created_utc": 1406373251, "gilded": 0, "name": "t1_cj89jyp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2brrf9/praw_how_do_i_extract_links_from_a_comment/"}, {"author": null, "body": "[deleted]", "created_utc": 1406279871, "gilded": 0, "name": "t1_cj7b4ms", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/"}, {"author": "naiyt", "body": "Good thoughts, thanks! It should perform fine under load as it's light and just sits on to of PRAW. I've set it watching /r/all without a problem, but I haven't done extensive testing for more popular keywords. And I've mainly used it in debug mod.e which just prints the reply rather than making it. Should be fine in a thread, although if you're running bots in the other threads you'll need to read the PRAW recommendations for they so you don't hit API limits. That's a good idea about expanding it to support other types of bots (moderation bots, bots that can PM, etc.).", "created_utc": 1406301421, "gilded": 0, "name": "t1_cj7hlys", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/"}, {"author": "nemec", "body": "Thanks for writing this. I think it would be worth expanding with different bot options built in (or at least mixins), to keep users from writing (potentially buggy) bots that reply to anything. Among other things: * Make it summonable, so `parse` is only called if the text also contains the summon (username, arbitrary text, url) * Detect when the bot is parsing a reply to one of its own comments, preventing the \"bot infinite loop\" where two (or more) bots correct each other in perpetuity. * Reply to only posts, only comments, or both. AutoWikiBot would probably only want comments, while TweetPoster replies only to posts. Not aware of any bots that reply to both.", "created_utc": 1406299664, "gilded": 0, "name": "t1_cj7gru6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/"}, {"author": "naiyt", "body": "Thanks for the comments! On my phone right now, so I'll respond to your comments fully a bit later.", "created_utc": 1406301494, "gilded": 0, "name": "t1_cj7hnbd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/"}, {"author": "naiyt", "body": "> Make it summonable, so parse is only called if the text also contains the summon (username, arbitrary text, url) It is summonable, but I leave it up to your Parser to determine whether or not to reply. It'll pipe all posts through your parser; if you don't want redditreplier to reply to a specific post, you return `False, None` rather than `True, message_to_reply_with`. That means your parser class can check all of the relevant things to see if it wants to reply. > Detect when the bot is parsing a reply to one of its own comments, preventing the \"bot infinite loop\" where two (or more) bots correct each other in perpetuity. Yep, it does this in the `_should_reply` method automatically! It should never reply to one of its own comments or to a comment that it has already replied to. (If you had to restart the bot for example, it may read the same posts on restart. It checks to make sure it hasn't already replied.) You can also define a BLACKLIST.txt with users that you don't want it to ever reply to. (To help prevent bot loops or whatever.) > Reply to only posts, only comments, or both. AutoWikiBot would probably only want comments, while TweetPoster replies only to posts. Not aware of any bots that reply to both. Good idea! Currently it only replies to comments. It would be good to add support for posts.", "created_utc": 1406306461, "gilded": 0, "name": "t1_cj7k7r4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bo4xn/i_wrote_a_python_module_to_make_writing_reddit/"}, {"author": "BS_in_BS", "body": "It's soft capping. Basically the score is adjusted to prevent the default subs from outranking smaller one when sorting see also: http://www.reddit.com/r/TheoryOfReddit/comments/29j5uh/reddit_still_artificially_introduces_downvotes_on/ciltyd7", "created_utc": 1406248447, "gilded": 0, "name": "t1_cj702p7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bmq3s/very_bizarre_submission_score_behavior/"}, {"author": "ThreeCorners", "body": "Thank you, I didn't know about this feature! So I would guess then if I remove all of the artificial drops in the score, I'd calculate the correct final score (which would be significantly higher)?", "created_utc": 1406266329, "gilded": 0, "name": "t1_cj77nmy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bmq3s/very_bizarre_submission_score_behavior/"}, {"author": "BS_in_BS", "body": "Basically, though I have no idea how much is actually subtracted from the votes.", "created_utc": 1406301689, "gilded": 0, "name": "t1_cj7hqtw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bmq3s/very_bizarre_submission_score_behavior/"}, {"author": "bboe", "body": "While reasonable, this is not a use-case I have previously encountered. If you look at the source for [Comment.reply](https://github.com/praw-dev/praw/blob/master/praw/objects.py#L341) you will see: response = self.reddit_session._add_comment(self.fullname, text) Thus, while I haven't tested it, you should be able to do: r_ss._add_comment(comment.fullname, \"COMMENT MESSAGE\") Note that `_add_comment` is currently a non-public method so expect that it may not exist, or behave the same in a future version of PRAW. For example, if this works for you, I just may decide to remove the `_` prefixing the function name, document the function, and provide this example in the documentation.", "created_utc": 1406232907, "gilded": 0, "name": "t1_cj6sfbz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bl8ew/praw_loading_with_one_instance_replying_with/"}, {"author": "DarkMio", "body": "This is a flawless solution. Since I am pretty verbose on my bot-actions, I will catch, if this function ever gets officially documented. Any idea where I should drop some example-code?", "created_utc": 1406311281, "gilded": 0, "name": "t1_cj7mt0x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bl8ew/praw_loading_with_one_instance_replying_with/"}, {"author": "bboe", "body": "Great! Glad it worked. > Any idea where I should drop some example-code? If you mean in the PRAW docs, I would suggest under the [few short examples](https://praw.readthedocs.org/en/v2.1.16/index.html#a-few-short-examples) section. Thanks!", "created_utc": 1406352196, "gilded": 0, "name": "t1_cj854lc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bl8ew/praw_loading_with_one_instance_replying_with/"}, {"author": "TrackReddit", "body": "/u/Spladug said OAuth will be up in [the near future](http://www.reddit.com/r/live/comments/2bj05b/requesting_access_to_reddit_live_api/cj5v5q9)", "created_utc": 1406157154, "gilded": 0, "name": "t1_cj5y653", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bj2h3/praw_and_creating_live_threads/"}, {"author": "spladug", "body": "Yup, I'll be rolling it out next week: https://github.com/reddit/reddit-plugin-liveupdate/pull/74", "created_utc": 1406331830, "gilded": 0, "name": "t1_cj7x605", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2bj2h3/praw_and_creating_live_threads/"}, {"author": "SummersRain", "body": "You run it from the windows command line in the folder that contains the pip program.", "created_utc": 1405738025, "gilded": 0, "name": "t1_cj1hn37", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "WilliamNyeTho", "body": "Thank you! This worked!", "created_utc": 1405738699, "gilded": 0, "name": "t1_cj1hwd1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "SummersRain", "body": "I made the same mistake. I got really frustrated trying to run it as a python script.", "created_utc": 1405779119, "gilded": 0, "name": "t1_cj1s0et", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "BurnBabyBurn71", "body": "I know you've already got it installed but here is a link to a step-by-step simple tutorial that I wrote that someone might find useful. http://www.reddit.com/r/burnbabyburn71/comments/2b4lst/tutorial_how_to_install_python_33_with_praw/ I could post the tutorial on this sub too if anyone wants me to.", "created_utc": 1405773795, "gilded": 0, "name": "t1_cj1qnqp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "captainmeta4", "body": "Unprivate this please", "created_utc": 1410395812, "gilded": 0, "name": "t1_ckewikd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "BurnBabyBurn71", "body": "Done, sorry about that, didn't know people wanted to use it.", "created_utc": 1410401438, "gilded": 0, "name": "t1_ckez4es", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "captainmeta4", "body": "Thank you very much! I just stumbled on here a few days ago via google.", "created_utc": 1410807469, "gilded": 0, "name": "t1_ckj4agf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "WilliamNyeTho", "body": "This looks good. The only thing I would add is the hint /u/SummersRain informed me about. (Remembering to have c:/python27/scripts or something similar as the location in the command line)", "created_utc": 1405782625, "gilded": 0, "name": "t1_cj1t4h7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "BurnBabyBurn71", "body": "Thanks for the suggestion :) Python 3.3 has the option set the Python path for you in the installer so setting the location manually shouldn't be necessary. I did add the error you would get if the path is not set in the 'Possible Errors' section with instructions on how to set the path of Python in Powershell.", "created_utc": 1405793382, "gilded": 0, "name": "t1_cj1xceh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b3nua/how_do_i_install_praw/"}, {"author": "bboe", "body": "Confirmed as a python3 bug: https://github.com/praw-dev/praw/issues/311", "created_utc": 1405658661, "gilded": 0, "name": "t1_cj0nnyp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b0f51/praw_is_get_mod_mail_broken/"}, {"author": "brucemo", "body": "Yay, thank you. Is there a workaround? My app dumps my mod mail to a searchable text file and I find that I cannot live without it.", "created_utc": 1405659112, "gilded": 0, "name": "t1_cj0ntwp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b0f51/praw_is_get_mod_mail_broken/"}, {"author": "bboe", "body": "I just fixed it, but won't make an immediate release. Here's the fix: https://github.com/praw-dev/praw/commit/e8b1dd848fffa86f28feba159fe3ef63babe89b3", "created_utc": 1405659241, "gilded": 0, "name": "t1_cj0nvkp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2b0f51/praw_is_get_mod_mail_broken/"}, {"author": "largenocream", "body": "What OS are you on? It's possible, but not likely that your CA cert bundle doesn't have the certs necessary to log in through `https://ssl.reddit.com`. What domain is your initial login request going to? If it's going to `https://www.reddit.com`, that doesn't have a valid SSL cert. You can check it by including some debug code for requests before you attempt the login: https://stackoverflow.com/questions/10588644/how-can-i-see-the-entire-request-thats-being-sent-to-paypal-in-my-python-applic", "created_utc": 1405538364, "gilded": 0, "name": "t1_ciz9j31", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2atnid/how_to_install_reddit_ssl_cert/"}, {"author": "elihusmails", "body": "I am on Mac 10.7.5. Ultimately I would like to transition this to a BeagleBone, which is where I originally found the problem. I'm using PRAW, so I don't specify a domain. I just have the code: r = praw.Reddit(user_agent=username) r.config._ssl_url = None r.login(username, password) I will check in to the PRAW lib, maybe there's a way to specify the URL I log in to.", "created_utc": 1405593649, "gilded": 0, "name": "t1_cizur0j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2atnid/how_to_install_reddit_ssl_cert/"}, {"author": "DangKilla", "body": "Reddit got its certificate through Gandi. This isn't a Certificate Authority (CA) I've ever heard of. Your Beaglebone's \"CA bundle\" probably doesn't have this CA cert. Install it and you should be fine. http://wiki.gandi.net/en/ssl/intermediate", "created_utc": 1406962045, "gilded": 0, "name": "t1_cjepxcf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2atnid/how_to_install_reddit_ssl_cert/"}, {"author": "kemitche", "body": "Please be aware that by making the change suggested in that StackOverflow post, you are sending your password in the clear, for any other computer to read and steal. It'd be better if you can get a proper set of SSL certificates installed.", "created_utc": 1405439431, "gilded": 0, "name": "t1_ciy35pi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2anne2/praw_2117_certificate_verify_failed/"}, {"author": "elihusmails", "body": "Agree. Are there steps for how to do that?", "created_utc": 1405454435, "gilded": 0, "name": "t1_ciybmug", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2anne2/praw_2117_certificate_verify_failed/"}, {"author": "kemitche", "body": "It really depends on the OS, how python was installed, etc. Unfortunately, I'm not well-versed in this area.", "created_utc": 1405465769, "gilded": 0, "name": "t1_ciyhoxs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2anne2/praw_2117_certificate_verify_failed/"}, {"author": "6086555", "body": "flat_comments[0] is a comment object : it contains plenty of thing. What's confusing you is that the print method will display its body. But flat_comments[0].id gives you the id", "created_utc": 1405336954, "gilded": 0, "name": "t1_ciwygcf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2anhlc/praw_retrieve_comment_id_from_body_then_reply_to/"}, {"author": "honeyplease", "body": "I see, indeed. This is helpful thanks.", "created_utc": 1405358632, "gilded": 0, "name": "t1_cix769r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2anhlc/praw_retrieve_comment_id_from_body_then_reply_to/"}, {"author": "bboe", "body": "You can also just call `reply` on the comment itself without needing to fetch the id. flat_comments[0].reply('msg')", "created_utc": 1405525367, "gilded": 0, "name": "t1_ciz2e9b", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2anhlc/praw_retrieve_comment_id_from_body_then_reply_to/"}, {"author": "honeyplease", "body": "Yes thanks, this is the last step in my code.", "created_utc": 1405539893, "gilded": 0, "name": "t1_cizadkv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2anhlc/praw_retrieve_comment_id_from_body_then_reply_to/"}, {"author": "IAmAnAnonymousCoward", "body": "Yeah, [same problem](http://www.reddit.com/r/redditdev/comments/28qbgd/praw_impossible_to_set_link_flair_unless/). I think the API doesn't allow it for some reason.", "created_utc": 1404420143, "gilded": 0, "name": "t1_cinuvrt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29isgn/using_praw_to_set_submission_flair_without/"}, {"author": "djimbob", "body": "In [1]: import praw In [2]: r = praw.Reddit('Testing v1.0 for /u/Cuisinier_Microsoft') In [3]: sub = r.get_submission(submission_id='29g2sd') In [4]: sub.created Out[4]: 1404129928.0 In [5]: sub.created_utc Out[5]: 1404101128.0 Note they are [unix timestamps](http://en.wikipedia.org/wiki/Unix_time) (basically # of seconds since Jan 1 1970), but its trivial to convert them to other formats. E.g., In[6]: import datetime In [7]: datetime.datetime.utcfromtimestamp(sub.created_utc) Out[7]: datetime.datetime(2014, 6, 30, 4, 5, 28) which matches the `Mon Jun 30 04:05:28 2014 UTC` displayed if I hover over the in \"submitted by Cusisinier_Microsoft\"", "created_utc": 1404106247, "gilded": 0, "name": "t1_cikml31", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "Cuisinier_Microsoft", "body": "This is exactly what I was looking for thank you.", "created_utc": 1404179483, "gilded": 0, "name": "t1_cild6a5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "okmkz", "body": "Not familiar with praw, but in the json data for a link, there is a created_utc value.", "created_utc": 1404101504, "gilded": 0, "name": "t1_cikkzjn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "Cuisinier_Microsoft", "body": "I see, if I can't find anything with praw I'll check it out. Thank you !", "created_utc": 1404101797, "gilded": 0, "name": "t1_cikl3ha", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "ThreeCorners", "body": "Actually, the data returned by PRAW is the JSON data, which I think is why the PRAW documentation there is a little light. So for example, if p is a comment object, just do print vars(p) to look at all of the data associated with that comment that you can access.", "created_utc": 1404233404, "gilded": 0, "name": "t1_ciltq1u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/29g2sd/getting_the_date_of_a_submission/"}, {"author": "gavin19", "body": "Use `get_comments` instead of `get_hot` and use `body` instead of `selftext`. Also, this msg = '[PRAW related thread](%s)' % submission.short_link won't work because comment objects don't have that attribute. You could use `link_title` instead of `short_link` so that you can at least identify the source thread.", "created_utc": 1403888512, "gilded": 0, "name": "t1_ciim9sc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2991wo/praw_writing_a_questiondiscover_program_for/"}, {"author": "_Daimon_", "body": "You want [update_settings](http://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.__init__.ModConfigMixin.update_settings). subreddit = r.get_subreddit(YOU_SUBREDDIT) subreddit.update_settings()", "created_utc": 1403735095, "gilded": 0, "name": "t1_cih2uhb", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/290jf7/praw_equivalent_of_clicking_the_save_button_on/"}, {"author": "BadgerBalls", "body": "Sadly, that's not doing it. The image is updated if I click on it in the list under the sub's CSS, but posts that use the image aren't don't reflect the changes even after calling update_settings(). Is there a specific flag I have to send update_settings()? I'm also not seeing any references to the sub images at http://www.reddit.com/r/MYSUB/about/edit/.json. Argh! =P", "created_utc": 1403739029, "gilded": 0, "name": "t1_cih4lsv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/290jf7/praw_equivalent_of_clicking_the_save_button_on/"}, {"author": "tst__", "body": "[Non-obvious behavior](https://praw.readthedocs.org/en/latest/pages/faq.html#non-obvious-behavior-and-other-need-to-know) > We can at most get 1000 results from every listing, this is an upstream limitation by reddit. There is nothing we can do to go past this limit. But we may be able to get the results we want with the search() method instead.", "created_utc": 1403591966, "gilded": 0, "name": "t1_cifk6h8", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "ThreeCorners", "body": "I was concerned that could be it. For my own user account, however, I only have 18 submissions and 143 comments, and yet it is still quite different: Reddit gives: 185 link karma, 332 comment karma PRAW gives: 276 link karma, 455 comment karma", "created_utc": 1403592405, "gilded": 0, "name": "t1_cifka9j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "6086555", "body": "This is normal, if you look at your own history the sum of your comments karma will be less than your comment karma, same for your link karma", "created_utc": 1403602771, "gilded": 0, "name": "t1_cifm8hy", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "tst__", "body": "Hm, did you look at individual contributions? I could imagine that older contributions maybe return other values via the API than via Reddit \u2013 but I could be wrong.", "created_utc": 1403592737, "gilded": 0, "name": "t1_cifkd1q", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "ThreeCorners", "body": "Oh geez, very good point. I seem to recall reading something about Reddit API changes not actually being reflected in older entries. I'll take a look, but this might take awhile. Thanks!", "created_utc": 1403592933, "gilded": 0, "name": "t1_cifkeoi", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "_Daimon_", "body": "If you just want the combined karma just use the attributes in the Redditor object. redditor = r.get_redditor('unidan') print redditor.link_karrma # I got 158820 print redditor.comment_karma # 2236503 As close as fuzzing will allow :)", "created_utc": 1403605759, "gilded": 0, "name": "t1_cifmqqk", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "ThreeCorners", "body": "Hi /u/_Daimon_, thanks for the reply! Amazingly, I didn't know about those attributes, that makes life a lot easier. Mind if I take this opportunity to throw some quick questions at you? * So my problem is probably just due to link fuzzing? * Was /u/tst__'s suspicion correct, if I look back far enough will I find reddit posts with different attributes? * An unrelated question: I needed an efficient way to load many submissions by ID that were otherwise unrelated. My solution was to append all of the IDs to the URL http://www.reddit.com/by_id/ and then call get_content(). Is there a better method? I think my method breaks if the URL is too long... Thank you!", "created_utc": 1403608532, "gilded": 0, "name": "t1_cifn8wx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "noobit", "body": "> Amazingly, I didn't know about those attributes Not all that amazing when you consider how useless the docs are with listing attributes", "created_utc": 1403888900, "gilded": 0, "name": "t1_ciimgth", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "_Daimon_", "body": "The user listings will return content that is visible to the viewer. So you won't see submissions/comments made to private subreddit or stuff that's been removed. This will obviously skrew the combined results. Additionally you can only go 1000 elements back, so for prolific users not everything will be returned. I'd also like to point out that PRAW doesn't manipulate the results from reddit in any way. What you see is what reddit gave PRAW. If you go to your own [submitted](http://www.reddit.com/user/ThreeCorners/submitted/) page and manually combined the score, then you will arrive roughly at what PRAW gave you. The differences are caused by reddits anti-spam closed-source algorithms. Not that I'm aware of no. Sounds like you want [get_submissions](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_submissions) which fit's your usecase pretty well.", "created_utc": 1403739803, "gilded": 0, "name": "t1_cih4xof", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "ThreeCorners", "body": "Thanks!", "created_utc": 1403750884, "gilded": 0, "name": "t1_cih9ml9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "kemitche", "body": "http://www.reddit.com/r/help/wiki/faq#wiki_my_karma_is_not_going_up_.2F_my_link_karma_is_stuck", "created_utc": 1403727959, "gilded": 0, "name": "t1_cigzb7b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28y0c8/very_inaccurate_karma_via_praw/"}, {"author": "TerraMaris", "body": "Have you tried \"sudo pip install praw\"?", "created_utc": 1403477566, "gilded": 0, "name": "t1_ciedklv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": null, "body": "[deleted]", "created_utc": 1403492904, "gilded": 0, "name": "t1_ciejuep", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": null, "body": "[deleted]", "created_utc": 1403493016, "gilded": 0, "name": "t1_ciejw26", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": "6086555", "body": "I had the same problem, this is a problem of path. To solve it, I launched that instruction in the good folder ( which is something like ) Python/Scripts I think. There's probably a cleaner way to do it by editing the path but I don't know how to do it", "created_utc": 1403481278, "gilded": 0, "name": "t1_cief3lp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": null, "body": "[deleted]", "created_utc": 1403492936, "gilded": 0, "name": "t1_ciejuwd", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": "6086555", "body": "The path is where your command line is going to look for instructions (like pip). If you edit it, it should work. I don't know how to do it though", "created_utc": 1403493157, "gilded": 0, "name": "t1_ciejy4s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": "nandhp", "body": "You're typing \"pip install praw\" into Python. You need to type it into your shell instead. Ordinarily, the shell would be the thing you run Python from, but on Windows you're probably just clicking an icon and going directly to a Python interpreter, rather than using the Command Prompt (the command-line shell for Windows). A command prompt window will look like this: Microsoft Windows [Version 6.3] C:\\WINDOWS> Try opening Command Prompt and you should see a prompt like the one above (although you it may start in a folder (directory) other than C:\\WINDOWS). Then try `pip install praw` in there. If `pip` doesn't work, try `python`, which should give you a Python interpreter like you had before. If `python` works, but not `pip` there may be a problem with the way `pip` is installed, but try using the `cd` commands below first. If you get an error like `not recognized as an internal or external command, operable program, or batch file`, then you need to tell the Command Prompt where your Python is installed. You can try changing the current directory with the `cd` command: `cd C:\\Python33` or maybe `cd C:\\Python33\\scripts` and then try the `pip` command again. However, it's recommended that Python be in your system's PATH environment variable (so that that it always knows which directory to look in); I believe the Python installer offers to do this for you, you can also add it manually. There's a bit more information about all this here: https://docs.python.org/3/faq/windows.html#how-do-i-run-a-python-program-under-windows", "created_utc": 1403633541, "gilded": 0, "name": "t1_cify2v3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": "wet_bias", "body": "fyi, when you get an error message with a caret in it, the position of that caret is very important. Pasting the message here doesn't help us figure out your problem.", "created_utc": 1403889162, "gilded": 0, "name": "t1_ciimll1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28tqod/having_problems_with_installing_praw_using_pip/"}, {"author": "reostra", "body": "See: http://www.reddit.com/r/redditdev/comments/28houf/attributeerror_cant_set_attribute/ (It doesn't initially look related, but Deimorz's fix there did in fact fix this problem when I had it)", "created_utc": 1403127825, "gilded": 0, "name": "t1_cib335o", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28hwaz/error_trying_to_get_comments_from_a_submission_in/"}, {"author": "baloneyeagle123", "body": "Thanks, I'll give it a try.", "created_utc": 1403128166, "gilded": 0, "name": "t1_cib393b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28hwaz/error_trying_to_get_comments_from_a_submission_in/"}, {"author": "tst__", "body": "See this thread: http://www.reddit.com/r/redditdev/comments/28houf/attributeerror_cant_set_attribute/", "created_utc": 1403125353, "gilded": 0, "name": "t1_cib1ue6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28hrir/praw_comment_errors/"}, {"author": "stickytruth", "body": "D'oh, thanks.", "created_utc": 1403125692, "gilded": 0, "name": "t1_cib20px", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28hrir/praw_comment_errors/"}, {"author": "reostra", "body": "Can confirm; even though from the tracebacks it doesn't appear related, the fix that Deimorz posted works on this issue too.", "created_utc": 1403125668, "gilded": 0, "name": "t1_cib208t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28hrir/praw_comment_errors/"}, {"author": "gavin19", "body": "https://praw.readthedocs.org/en/latest/pages/code_overview.html?highlight=submit#praw.__init__.SubmitMixin.submit", "created_utc": 1403046098, "gilded": 0, "name": "t1_cia8izw", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28etuk/submit_a_post_with_praw/"}, {"author": "laptopdude90", "body": "That's embarrassing. Thanks!", "created_utc": 1403046168, "gilded": 0, "name": "t1_cia8k4f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28etuk/submit_a_post_with_praw/"}, {"author": "kemitche", "body": "That data hasn't yet been added to the API, sorry.", "created_utc": 1402942600, "gilded": 0, "name": "t1_ci92rs9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28aga6/is_there_any_way_to_get_a_list_of_subreddits_that/"}, {"author": "djimbob", "body": "Doesn't seem to be a way in the API. But you can do something easy like simply request `http://www.reddit.com/user/AnSq` and get the list of subreddits with a little bit of JQuery like: $('#side-mod-list li a').map(function() {return $(this).text()}).toArray() or do the equivalent in your favorite language/library (e.g., in python with pyquery & requests - both available through PIP): >>> import requests >>> import pyquery >>> r=requests.get('http://www.reddit.com/user/ManWithoutModem', headers={'User-Agent':'test for AnSq'}) >>> pq = pyquery.PyQuery(r.content) >>> [x.text for x in pq('#side-mod-list li a')] ['/r/EarthPorn', '/r/television', '/r/atheism', '/r/space', '/r/Showerthoughts', '/r/cringepics', ... Obviously less elegant than through the api, but gets the job done if necessary.", "created_utc": 1402947616, "gilded": 0, "name": "t1_ci95de0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28aga6/is_there_any_way_to_get_a_list_of_subreddits_that/"}, {"author": "AnSq", "body": "I was afraid of that. Although I hadn't heard of pyquery before, so thanks for that.", "created_utc": 1402978437, "gilded": 0, "name": "t1_ci9jstf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28aga6/is_there_any_way_to_get_a_list_of_subreddits_that/"}, {"author": "bboe", "body": "As far as I know, reddit's API does not provide an endpoint that supports multiple replies, so the answer is no: http://www.reddit.com/dev/api", "created_utc": 1402928587, "gilded": 0, "name": "t1_ci8w4qt", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/288anc/reply_to_multiple_messages_in_one_request/"}, {"author": "SavingPrivateMe", "body": "First and foremost, I haven't tried running this code. But I think that you're better off looking at the ReplyBot and working with the code from there. All you're doing is looking for a specific comment from a subreddit. This is a much more simplified approach. subreddit = r.get_subreddit(\"nerdcubed\") while True: comments = subreddit.get_comments() for comment in comments: if comment.id not in already_tried: if \"he is banned from germany\" in comment.body.lower(): comment.reply(\"He's banned from many places\") already_tried.append(comment.id) time.sleep(5) I think the main issue that you're having is that no one is actually commenting what you're searching for. This will search all NEW comments, not ones that have been posted before your bot's creation. Hope I could help somewhat.", "created_utc": 1402881844, "gilded": 0, "name": "t1_ci8j0fv", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2880cz/comment_bot_not_commenting_what_have_i_done_wrong/"}, {"author": "bboe", "body": "Just FYI this only looks at top-level comments. If you want lower level comments, you may want to look at the `flatten_tree` helper function as covered here: http://praw.readthedocs.org/en/latest/pages/comment_parsing.html", "created_utc": 1402928384, "gilded": 0, "name": "t1_ci8w1yw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2880cz/comment_bot_not_commenting_what_have_i_done_wrong/"}, {"author": "Alexratman", "body": "Brilliant, thank you for your help. I'll look at replybot and see if it's suitable", "created_utc": 1402900991, "gilded": 0, "name": "t1_ci8q6eg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2880cz/comment_bot_not_commenting_what_have_i_done_wrong/"}, {"author": "markerz", "body": "Wouldn't that just be a list of posts before a certain timestamp? The search feature supports that I believe. Specifically, it would be posts closest to, but before a certain timestamp.", "created_utc": 1402706238, "gilded": 0, "name": "t1_ci71aqo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/283bqr/praw_how_to_get_newest_posts_of_last_week_from_a/"}, {"author": "BananaPotion", "body": "Thank you", "created_utc": 1402746967, "gilded": 0, "name": "t1_ci7bin3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/283bqr/praw_how_to_get_newest_posts_of_last_week_from_a/"}, {"author": "agentlame", "body": "I could probably have the mod button show up on the flair page like it does on the mod and contributors page. Is you save the sub to your mod button list, it should just be a matter of changing the action to approved and checking off the sub in question. Still manual, but a bit easier.", "created_utc": 1402669395, "gilded": 0, "name": "t1_ci6k6sg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/281u72/syncing_a_flaired_user_list_to_approved_submitter/"}, {"author": "nallen", "body": "It would be a ton easier than what I do now, but I'm not sure if it solves the bigger problem: identification of newly flaired users that aren't on the approved submitter list. Right now I generate a list of flaired users and compare it to the approved submitter list by hand, it's a huge pain. I seem to recall that there is a programmer way to subtract a list from another and make a list of the differences. That alone would solve my problem since it would only be a handful of new users, compared to reading through a list of hundreds or more than 1000", "created_utc": 1402669900, "gilded": 0, "name": "t1_ci6keek", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/281u72/syncing_a_flaired_user_list_to_approved_submitter/"}, {"author": "gavin19", "body": "Something like import praw def main(): approved = [] flaired = [] target_sub = 'some_sub' # migrating to this sub current_sub = 'another_sub' r = praw.Reddit(user_agent=\"/u/someone for /r/somesub\") r.login('user', 'password') target = r.get_subreddit(target_sub) current = r.get_subreddit(current_sub) target_approved = target.get_contributors(limit=None) current_flaired = current.get_flair_list(limit=None) for i in target_approved: approved.append(i.name) for i in current_flaired: flaired.append(i['user']) to_approve = [i for i in flaired if i not in approved] print(len(to_approve), \"to approve\") for i in to_approve: try: target.add_contributor(i) print(\"Added\", i) except praw.errors.InvalidUser: pass if __name__ == '__main__': main() It could do with a review (or two), but it worked fine in a couple of subs I tested. I noticed an issue with users who had flair at one time, but since deleted their account. The name of the user is still returned when fetching the flair, so trying to add them as an approved submitter raises `praw.errors.InvalidUser`, hence the try/except.", "created_utc": 1402676096, "gilded": 0, "name": "t1_ci6n9fm", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/281u72/syncing_a_flaired_user_list_to_approved_submitter/"}, {"author": "nallen", "body": "I just ran it and it works lovely for small samples, like in test subreddits, but when I applied it to the real deal, which has a very long list, I hit the subreddit rate limit. How many actions does the script perform when checking a user that was already added? I get the sense that it's checking the first 100, and then hitting the limit, which might be a problem. If I run the script again in 2 hours will it check those same 100 and hit the rate limit? or is it only the adding of the submitters that counts? Update: 2 hours later, I ran it again, and it picks up where it left off, so there is no problem as I can see.", "created_utc": 1402705202, "gilded": 0, "name": "t1_ci70xgq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/281u72/syncing_a_flaired_user_list_to_approved_submitter/"}, {"author": "gavin19", "body": "I used something very similar on a sub with ~4000 flairs and I remember that I had to run it about 6 times or so before it got through everyone. There is a way to fetch the flairs in x amount of pages, then if it errors on flair 2500, you only need to go back to the previous page and start over, instead of back to 0. Never could figure that bit out.", "created_utc": 1402741429, "gilded": 0, "name": "t1_ci7anle", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/281u72/syncing_a_flaired_user_list_to_approved_submitter/"}, {"author": "nallen", "body": "I'll give this a try when I get home. It's pretty easy to look over the flair list and remove deleted accounts, so I'm not worried about that. Thanks gavin19!", "created_utc": 1402679863, "gilded": 0, "name": "t1_ci6p5o2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/281u72/syncing_a_flaired_user_list_to_approved_submitter/"}, {"author": "6086555", "body": "[This](http://www.reddit.com/r/redditdev/comments/191bn8/does_praw_support_the_before_field_for_getting/c8k4cxd) should help you", "created_utc": 1402613932, "gilded": 0, "name": "t1_ci63fr3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28037u/praw_cant_get_prawobjectsubredditget_new_to_honor/"}, {"author": "polhemic", "body": "It does just that. Thanks.", "created_utc": 1402614499, "gilded": 0, "name": "t1_ci63on7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/28037u/praw_cant_get_prawobjectsubredditget_new_to_honor/"}, {"author": "bboe", "body": "Unfortunately, without a single reproducible request there isn't much that we (PRAW devs) can do. From what you've written it seems like the response from reddit is being truncated due to some sort of network issue. If that is the case, raising an exception is the appropriate action to take so you can handle it as you've done. Restarting the stream is not that inelegant / inefficient.", "created_utc": 1402498237, "gilded": 0, "name": "t1_ci4t83f", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/27vebn/praw_valueerror_in_prawhelperssubmission_stream/"}, {"author": "VoterBot", "body": "Thanks for the response. Just knowing that it's not crashing because of my clumsiness is good. PRAW is solid as a rock on my local machine and it's only started playing up on Heroku in the last few days.", "created_utc": 1402535305, "gilded": 0, "name": "t1_ci5asya", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/27vebn/praw_valueerror_in_prawhelperssubmission_stream/"}, {"author": "bboe", "body": "https://github.com/kennethreitz/requests/issues/1882", "created_utc": 1402326749, "gilded": 0, "name": "t1_ci307dg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/27os73/recent_socket_timeouts_under_heroku_and_praw/"}, {"author": "bboe", "body": "It sounds like you are running into a caching issue where the second execution of a similar command within a single execution doesn't receive the updated parameters. As per reddit's API guidelines that specify not to make the same request within 30 seconds, PRAW maintains a cache of the same request for 30 seconds. You can either wait 30 seconds, or you can add an arbitrary query parameter to a GET request to bypass the caching functionality.", "created_utc": 1402066004, "gilded": 0, "name": "t1_ci0pg4v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/27ey83/im_designing_online_class_with_reddit_praw_has/"}, {"author": "snaysler", "body": "A I see, thank you for telling me. How do I \"add an arbitrary query parameter to a GET request to bypass the caching functionality\"? That would be fantastic", "created_utc": 1402080094, "gilded": 0, "name": "t1_ci0w3zy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/27ey83/im_designing_online_class_with_reddit_praw_has/"}, {"author": "bboe", "body": "It depends on the function you're using but you can try something similar to: r.get_submission(..., params={'count': 1}) Then adjust the count parameter for subsequent requests.", "created_utc": 1402085370, "gilded": 0, "name": "t1_ci0ynsy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/27ey83/im_designing_online_class_with_reddit_praw_has/"}, {"author": "_lowell", "body": "I found [this](https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html#praw.objects.Subreddit.add_contributor). Sounds like what you're looking for. Maybe try it, if you haven't already?", "created_utc": 1401972755, "gilded": 0, "name": "t1_chzrg0t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/279qlu/adding_a_user_as_an_approved_submitter_with_praw/"}, {"author": "quadnix", "body": "works perfectly, thanks", "created_utc": 1401981089, "gilded": 0, "name": "t1_chzuqkb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/279qlu/adding_a_user_as_an_approved_submitter_with_praw/"}, {"author": "GoldenSights", "body": "I can't test this extensively right now, but I think it wouldn't be too hard to get the current Unix time, compare it to the comments' posted times, and break the loop once you find one that's older than 24 hours. I'm not sure how that *time* argument is supposed to work, so this is kind of my hack around that. import datetime def getTime(bool): timeNow = datetime.datetime.now(datetime.timezone.utc) timeUnix = timeNow.timestamp() if bool == False: return timeNow else: return timeUnix So when you start the loop, you'd do `currenttime = getTime(True)`, and there would be some sort of if currenttime - comment.created_utc >= 86400: break to break out of the loop. That's my idea at the moment, but there's probably some more efficient way.", "created_utc": 1401381476, "gilded": 0, "name": "t1_chu39i9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26rr50/praw_get_redditors_comments_for_today_only/"}, {"author": "halaal_sandwich", "body": "This bot would have a heart attack in the average /r/AskReddit thread :) Does it stop checking after a certain level?", "created_utc": 1401207831, "gilded": 0, "name": "t1_chsb7mp", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "5225225", "body": "or in /r/counting.", "created_utc": 1401573143, "gilded": 0, "name": "t1_chvxrvq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "halaal_sandwich", "body": "Out of all the weird/bizarre subreddits I've seen that is the most bizarre. Simply because it's so... normal.", "created_utc": 1401573309, "gilded": 0, "name": "t1_chvxu46", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "5225225", "body": "Yeah, all they're doing is counting... To ~~infinity~~ until they break reddit. Which apparently has happened before.", "created_utc": 1401573420, "gilded": 0, "name": "t1_chvxvqe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DrJosh", "body": "For small threads I use submission = r.get_submission(submission_id='26lrs8') submission.replace_more_comments(limit=None,threshold=0); for large ones, just submission = r.get_submission(submission_id='26lrs8')", "created_utc": 1401208096, "gilded": 0, "name": "t1_chsbc0s", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "halaal_sandwich", "body": "Ah ok, so it's about 200 for the large ones.", "created_utc": 1401208186, "gilded": 0, "name": "t1_chsbdi6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DrJosh", "body": "Yes.", "created_utc": 1401208682, "gilded": 0, "name": "t1_chsblqp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "halaal_sandwich", "body": "If you want to process massive threads you could probably store the comments into something like an amqp server so you can process them piece-meal :)", "created_utc": 1401209075, "gilded": 0, "name": "t1_chsbs7w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DEADB33F", "body": "If you wanted to process massive threads effectively you'd probably want to do that but retrieve the comments by polling /r/all/comments (or the comments listing of a multireddit of the subreddits the bot operates in) then reconstruct the comment hierarchy relationships yourself. It could then dynamically adjust the comment map as new comments are added (would need to host its own images to do that though).", "created_utc": 1401214730, "gilded": 0, "name": "t1_chseg6n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "halaal_sandwich", "body": "If you want a specific thread's comments then logically it would be better to simply trawl that thread's comment tree.", "created_utc": 1401214884, "gilded": 0, "name": "t1_chseiv8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DEADB33F", "body": "On very large threads that means trawling an awful lot of 'more comments' links. It'll also have to trawl through them all each time you want to update your map and add newly added replies. If the bot is generating maps for all submissions on a subreddit (or selection of subreddits) then it's going to be far better to capture comments as they're generated and update the tree maps dynamically as new replies are added. --- If you only want to generate the maps for popular threads then you can store comment relationships and only generate the image once the thread has X replies.", "created_utc": 1401306203, "gilded": 0, "name": "t1_chtddyu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DrJosh", "body": "Thanks for the tip.", "created_utc": 1401209693, "gilded": 0, "name": "t1_chsc2gb", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "pgl", "body": "Interesting. And very cool!", "created_utc": 1401277768, "gilded": 0, "name": "t1_cht131y", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DrJosh", "body": "Thanks pgl.", "created_utc": 1401281793, "gilded": 0, "name": "t1_cht26es", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "bob000000005555", "body": "Could you associate usernames with major nodes?", "created_utc": 1401282804, "gilded": 0, "name": "t1_cht2i7s", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DrJosh", "body": "That's a great idea. I'm going to post the full code here, so people can add their own improvements.", "created_utc": 1401284037, "gilded": 0, "name": "t1_cht2y1l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "rhiever", "body": "Neat stuff Josh. Are you analyzing these trees at all? For example, to look at how nested conversations become on reddit? It could be interesting to size the nodes in the visualization by their degree, and perhaps color them by their # of upvotes (red = many upvotes, blue = no upvotes).", "created_utc": 1401243091, "gilded": 0, "name": "t1_chsrh39", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DrJosh", "body": "Thanks Randy. We're actually going to use this method for some other reddit-based projects soon. I'd like to color the nodes by # of votes, but I've read that the reddit API obfuscates the popularity of a posting. --Josh", "created_utc": 1401273525, "gilded": 0, "name": "t1_cht07fy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "rhiever", "body": "Reddit obfuscates the score, but only very slightly. The relative score of all comments remains more-or-less the same. So I'd say it would be reliable.", "created_utc": 1401281915, "gilded": 0, "name": "t1_cht27tq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DrJosh", "body": "Ah! Good to know; thanks.", "created_utc": 1401282388, "gilded": 0, "name": "t1_cht2d6v", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "DrJosh", "body": "As a lot of people have proposed improvements, please find the full code for CommentTreeBot below. Feel free to improve it and launch your own CommentTreeBots: import matplotlib.pyplot as plt import networkx as nx import random import praw G = nx.Graph(); def Parse_Forest(replies,tabAmount,parentID): if ( replies == [] ): return; for comment in replies: isComment = type(comment) is praw.objects.Comment if ( isComment ): myID = comment.id; G.add_node(myID); G.add_edge(parentID,myID); Parse_Forest(comment.replies,tabAmount+' ',myID); r = praw.Reddit('testing') submission = r.get_submission(submission_id='26lrs8') submission.replace_more_comments(limit=None,threshold=0); forest_comments = submission.comments G.add_node(0); runningID = 0; Parse_Forest(forest_comments,'',0); pos=nx.graphviz_layout(G,prog='twopi',args='') nx.draw(G,pos,node_size=20,alpha=0.5,node_color=\"blue\", with_labels=False) plt.show();", "created_utc": 1401284229, "gilded": 0, "name": "t1_cht30lu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "Antrikshy", "body": "You should put this on GitHub and also post to /r/botwatch.", "created_utc": 1401832425, "gilded": 0, "name": "t1_chyeo2d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26m45h/i_created_commenttreebot/"}, {"author": "kemitche", "body": "Couple questions for you: 1. How many requests per hour, combined, are you sending? 1. Have you set an appropriate User Agent? Is it the same for each? 1. Is there any chance this server is on a shared IP with other users? 1. What are the [ratelimit headers](https://github.com/reddit/reddit/wiki/API#rules) telling you?", "created_utc": 1400982958, "gilded": 0, "name": "t1_chqeu50", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26ex2a/rate_limit_exceeded_errors_with_praw/"}, {"author": "gavin19", "body": "Sorry for the delay. GMT. Nodded off. The four scripts are a [flair bot](https://dl.dropboxusercontent.com/u/2552046/_scripts/py/reddit/t.py) that checks for new PMs and updates the flair based on that info. Two others are like [this](https://dl.dropboxusercontent.com/u/2552046/_scripts/py/reddit/top3.py) and request the top 50 posts from new and update the sidebar contents based on that. The last is very similar except it only updates the sidebar. No other requests are made to reddit. Then there are the two AutoMod instances. Adding the second instance is where it seems the problems began. The UA isn't unique for those run via the same account, like /u/RFootballBot. They're all just '/u/someone for /r/somesub'. DO faq states that the IP is unique per server (droplet). I'm unsure how to fetch the header info. All I can relay at the minute is that PRAW tells me praw.errors.RateLimitExceeded: `you are doing that too much. try again in X hours.` on field `vdelay`", "created_utc": 1401019068, "gilded": 0, "name": "t1_chqn7wa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26ex2a/rate_limit_exceeded_errors_with_praw/"}, {"author": "bboe", "body": "If you're running separate instances of PRAW behind the same IP address you should use the multiprocess proxy server to manage the rate limiting: http://praw.readthedocs.org/en/latest/pages/multiprocess.html", "created_utc": 1401040711, "gilded": 1, "name": "t1_chqtu2x", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26ex2a/rate_limit_exceeded_errors_with_praw/"}, {"author": "gavin19", "body": "Unsurprisingly, it worked. Thanks! Both instances logged in and are running concurrently without issue. I wasn't 100% on the addr/port combination so I just went with 0.0.0.0 and port 80.", "created_utc": 1401042710, "gilded": 0, "name": "t1_chqulpr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26ex2a/rate_limit_exceeded_errors_with_praw/"}, {"author": "kemitche", "body": "~~It doesn't look like PRAW exposes the \"subreddit_type\" JSON field directly, however, you can access it if needed like so:~~ /u/bboe has the proper solution below. sr = r.get_subreddit(\"redditdev\") sr._get_json_dict()['subreddit_type']", "created_utc": 1400866976, "gilded": 0, "name": "t1_chpeawj", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "bboe", "body": "PRAW exposes the parameters on demand so the following will work just fine: r.get_subreddit('redditdev').subreddit_type Running the following does not actually make any request as fetching the subreddit about page is not often required. r.get_subreddit('redditdev')", "created_utc": 1400877224, "gilded": 0, "name": "t1_chpiyl3", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "tehusername", "body": "Hey! Sorry for reviving this old thread but I think I ran into a problem. I tried this: print r.get_subreddit('askreddit').subreddit_type and it prints *public* as expected. But when I try this one: print r.get_subreddit('gaming').subreddit_type I get a *403 Client Error: Forbidden* which doesn't make sense since r/gaming is a public subreddit. I tried print r.get_subreddit('pyongyang').subreddit_type and I got a *restricted* response. Any idea what's happening here? Edit: I forgot to add, I also tried logging in but to no avail.", "created_utc": 1401069161, "gilded": 0, "name": "t1_chr4n0u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "bboe", "body": "I cannot reproduce your error: import praw r = praw.Reddit('bboe test') print(r.get_subreddit('askreddit').subreddit_type) print(r.get_subreddit('gaming').subreddit_type) print(r.get_subreddit('pyongyang').subreddit_type) Its output is: public public restricted", "created_utc": 1401089236, "gilded": 0, "name": "t1_chravm7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "tehusername", "body": "Oh wait, I think it's the internet filter here. Thanks for the prompt reply!", "created_utc": 1401093799, "gilded": 0, "name": "t1_chrbpw9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "LinkFixerBotSnr", "body": "/r/gaming ***** [^report ^a ^**problem**](http://reddit.com/r/LinkFixerBotSnr) ^| [^delete ^comment](http://www.reddit.com/message/compose?to=LinkFixerBotSnr&subject=Comment%20Deletion%20%28Parent%20Commenter%20Only%29&message=%2Bdelete+chr4n5b) ^| [^source ^code](http://github.com/WinneonSword/LFB) ^| [^contact ^developer](http://reddit.com/user/WinneonSword)", "created_utc": 1401069170, "gilded": 0, "name": "t1_chr4n5b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": "tehusername", "body": "Perfect! Thanks!", "created_utc": 1400987397, "gilded": 0, "name": "t1_chqgcq4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26aqel/praw_how_to_check_if_a_subreddit_is/"}, {"author": null, "body": "I'll have a look in a bit for you after lunch. I'm sure it's there or can be coded", "created_utc": 1400767892, "gilded": 0, "name": "t1_chodvnj", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2678bh/re_praw_is_there_a_function_for_getting_unread/"}, {"author": null, "body": "Still messing about in praw. Ill see if I can get a decent function for you.", "created_utc": 1400785188, "gilded": 0, "name": "t1_chom5b6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2678bh/re_praw_is_there_a_function_for_getting_unread/"}, {"author": "jsmcgd", "body": "Ah, I believe this is what I'm looking for: https://praw.readthedocs.org/en/v2.1.16/pages/code_overview.html", "created_utc": 1400755360, "gilded": 0, "name": "t1_choadxl", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/26762p/is_there_a_documented_list_of_praws_api_functions/"}, {"author": "spladug", "body": "Please do not do this. Sending unsolicited (by the recipients) PMs is considered spam and will get your bot banned.", "created_utc": 1400742617, "gilded": 0, "name": "t1_cho87hr", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "Base10Propaganda", "body": "OP specifically says that the user requests it, so it's not unsolicited.", "created_utc": 1400743470, "gilded": 0, "name": "t1_cho8e06", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "spladug", "body": "> The bot will send a pm to each user in a thread if the op requests it That's not the same as the individual recipients requesting it.", "created_utc": 1400773942, "gilded": 0, "name": "t1_choglt6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "SyedAJafri", "body": "Sorry I guess a better way would be for the users to \"suscribe\" then the suscribed users will be pmed.", "created_utc": 1400807168, "gilded": 0, "name": "t1_chow2or", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "Base10Propaganda", "body": "Ah sorry, I misunderstood.", "created_utc": 1400774498, "gilded": 0, "name": "t1_chogvao", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "notbusyatall", "body": "It shouldn't be an issue if the bot sends the message to each person who *comments* in the thread (that way it won't try to send them all at once), and it will be able to message new commenters. Try to make it so that people will know what they are getting into, probably with a requirement in the thread title, and the chances of the bot getting banned will decrease.", "created_utc": 1400779745, "gilded": 0, "name": "t1_chojf5g", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "SyedAJafri", "body": "Yeah but if I have 100 users to pm then I have to wait 200 seconds for one thread. Since I have to wait 2 seconds per request. I want a way to mass pm with just one request.", "created_utc": 1400807400, "gilded": 0, "name": "t1_chow63z", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "notbusyatall", "body": "200 seconds is the *total* time it will take to pm 100 people. You'll never have 100 users comment in the one thread in 200 seconds. If the bot responds in real time, you won't ever have to worry about a buildup of people to pm. I'm saying to predefine whether or not the bot should pm anybody in the thread, because you *can't* pm multiple people simultaneously.", "created_utc": 1400812845, "gilded": 0, "name": "t1_choye7e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2664jz/trying_to_pm_multiple_people_without_waiting_2/"}, {"author": "spike77wbs", "body": "So it looks to me looking at the documentation that PRAW doesn't address the resubmit option of the API, am I reading that correctly?", "created_utc": 1400470727, "gilded": 0, "name": "t1_chli8fy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "bboe", "body": "Support will be added in the next version of PRAW: https://github.com/praw-dev/praw/commit/24ac157d369fd9c54615e2f19a1f2f6b92f8f9b4", "created_utc": 1400631632, "gilded": 0, "name": "t1_chn3iev", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "spike77wbs", "body": "that worked perfectly, thanks", "created_utc": 1400655331, "gilded": 0, "name": "t1_chnc4at", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "bboe", "body": "You can manually add the parameter to the request. I'm not sure what the exact parameter is but it should be something like: r.submit('xyzsubreddit', title, url=url, params={'resubmit': True})", "created_utc": 1400553217, "gilded": 0, "name": "t1_chmb0gh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "spike77wbs", "body": "I tried the following: redd.submit('borntoday', title, url=url, params={'resubmit': True}) *TypeError: submit() got an unexpected keyword argument 'params'* redd.submit('borntoday', title, url=url, param={'resubmit': True}) *TypeError: submit() got an unexpected keyword argument 'param'* redd.submit('borntoday', title, url=url, parameter={'resubmit': True}) *TypeError: submit() got an unexpected keyword argument 'parameter'* redd.submit('borntoday', title, url=url, resubmit='True'}) *TypeError: submit() got an unexpected keyword argument 'resubmit'* I tried looking at decorators.py to see if I could figure it out, but I suspect I have not yet gotten to a level of understanding to be able to.", "created_utc": 1400563513, "gilded": 0, "name": "t1_chmf2im", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "bboe", "body": "Ah, looks like it doesn't support extra arguments. I suppose I'll fix that.", "created_utc": 1400606177, "gilded": 0, "name": "t1_chmrjec", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "spike77wbs", "body": "Here is the full error sequence: Version 2.1.11 of praw is outdated. Version 2.1.16 was released 7 days ago. Traceback (most recent call last): File \"C:/Documents and Settings/Family/Desktop/Python/reddpost3.py\", line 62, in redd.submit('borntoday', title, url=url, params={'resubmit': True}) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 320, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw\\decorators.py\", line 222, in wrapped return function(obj, *args, **kwargs) TypeError: submit() got an unexpected keyword argument 'params'", "created_utc": 1400564193, "gilded": 0, "name": "t1_chmf9ov", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25uii4/is_it_possible_to_use_praw_to_resubmit/"}, {"author": "SummersRain", "body": "Pip isn't a python script, it's a regular tools for windows. You don't run them from within python, you run them from the windows shell.", "created_utc": 1400674743, "gilded": 0, "name": "t1_chnfmxj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25u5gx/unable_to_install_praw_please_help/"}, {"author": "kemitche", "body": "There's two primary uses for the `state` variable. One is very important, as it has security implications for YOUR app. The other is just a tool that your app can make use of. ### As a CSRF token The normal OAuth flow is something like this, assuming you're running a web app: 1. User visits your website. 1. User chooses to perform some action that requires your website to have temporary access to their reddit account. 1. Your website redirects the user over to ssl.reddit.com/api/v1/authorize(...) 1. The user chooses to allow your website to have access to their reddit account. 1. reddit redirects the user to your REDIRECT_URI 1. Your webserver grabs the code out of the query params of the redirect URI, exchanges it for an access token. 1. Your webserver then performs some reddit API calls on that user's behalf However, on step 6 - there's actually nothing about the OAuth2 spec that guarantees that steps 1-3 actually happened. Consider this alternate scenario: 1. User visits shady-website.com 1. shady-website.com redirects the user to YOUR authorize URL on reddit.com (your client ID is public info, and your client secret isn't part of the authorize URL, so anyone can construct the reddit URL that your website directs people to!) 1. User (perhaps foolishly) agrees to allow your website access to their reddit account. 1. User is redirected to YOUR website's REDIRECT_URI 1. ??? Who knows? What does your webserver do? Maybe your app always just displays private messages, maybe it posts on a user's behalf, maybe something else. But it happens automatically. This is where the `state` variable comes in. It should be considered a [nonce](http://en.wikipedia.org/wiki/Cryptographic_nonce). When your server crafts the authorize URL in step 3 of the first flow, it includes a unique state value. When the user returns to your website on step 6, you validate if the state value given is one that your website created - if not, bail out! Someone could be trying something. Once a specific state value has been \"used\" on your end, you should discard its value or mark it as \"used\" in some fashion so it doesn't get re-used. Further reading: https://developers.google.com/accounts/docs/OAuth2Login#createxsrftoken ### As an indicator of what to do Another possible use for the `state` value (one that can be used in *addition to*, not instead of, as a CSRF token) is to indicate where on your website to send the user. For example, say you have 2 flows in your web app, mail checking and managing subreddit subscriptions. When the user gets redirected back to your website after authorizing your app, you might not know which of those 2 paths you started the user on. Here, you can use the state variable to keep track of that, in one of two ways: 1. When you generate and store the state value, also record where to send that user when they get back to your site 2. Append information to the end of the state variable that directly indicates what to do. For example, if you generated a state variable \"abc123\", you could send \"abc123:mail\" or \"abc123:subreddits\". Your website then would break apart the state variable and (after validating the CSRF portion), redirect the user to the \"mail\" or \"subreddits\" part of your website.", "created_utc": 1400260687, "gilded": 0, "name": "t1_chjnc9c", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25o8xq/praw_get_authorize_url_method_requires_a_unique/"}, {"author": "redditratings", "body": "Wow, lots of great info here. Very helpful. So I think I just need to create a random string of characters, pass it to get_authorize_url, and save it in a session variable. Then when the user comes back to the site, make sure that the state variable matches what is saved in the session. I'm sure I can figure that out, I'm just never sure on the details like how many characters long should it be and how exactly should it be generated.", "created_utc": 1400265718, "gilded": 0, "name": "t1_chjpr22", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25o8xq/praw_get_authorize_url_method_requires_a_unique/"}, {"author": "kemitche", "body": "Here's what [we use at reddit](https://github.com/reddit/reddit/blob/master/r2/r2/models/token.py#L37) to securely generate one time use tokens. Key takeaways: Use an operating-system level function to ask for some number of cryptographically random bytes, then encode the bytes as needed for transport. For size, I recommend around 30 characters.", "created_utc": 1400274467, "gilded": 0, "name": "t1_chjtrzt", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25o8xq/praw_get_authorize_url_method_requires_a_unique/"}, {"author": "redditratings", "body": "Awesome, thanks!", "created_utc": 1400349806, "gilded": 0, "name": "t1_chkfhiy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25o8xq/praw_get_authorize_url_method_requires_a_unique/"}, {"author": "shrayas", "body": "Hey, while this seems like a nice bot to have. There was another bot that existed that did the same thing. It was called /u/Reads_Small_Text_Bot but I believe he was banned. Read about it [here](http://www.reddit.com/r/OutOfTheLoop/comments/1ro1du/why_was_ureads_small_text_bot_banned). With that said, here is how I would do it: * Get the list of comments, I'm sure you have to use a `limit=None` parameter there to get all the comments * Iterate through the comments looking for a pattern that would be something like `^^^TEXT_HERE^^^` using regex. * When such a pattern is found, extract `TEXT_HERE` and reply to that particular comment with `**TEXT_HERE**`. This of course would be a very basic implementation. Much more can be done. But i'm just giving you a nudge to get you along on your way. In addition, /u/Reads_Small_Text_Bot's source is open. Check [it](https://github.com/tomwadley/reads-small-text-bot) out. It is a great practice to read other peoples code, teaches you a lot. Hope it helps. Cheers and may the force be with you.", "created_utc": 1399867893, "gilded": 0, "name": "t1_chfp9uw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25bvld/please_help_on_first_bot/"}, {"author": "Seeing_Eye_Bot", "body": "Thank you so much for your help", "created_utc": 1399868351, "gilded": 0, "name": "t1_chfpfr0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25bvld/please_help_on_first_bot/"}, {"author": "iateacrayon", "body": "Well the way the comment would show up when you use get_comment is like \\^this \\^text into \\*\\*bold\\*\\* So just check for the \\^ character and grab the 'word' that follows it, then add \\*\\* to both sides of the word. To look for stuff in comments, you'll have to split the comment into individual words. Then you'll look for the \\^ with an if statement like so: for comment in all_comments: #split word into a bunch of words words[0]='^this' if '^' in words: #replace ^ in word with '' word='**'+words+'**' print('I found'+ word) which would hopefully output \\*\\*this\\*\\* when it works. Start with single words and work your way up from there to make it easier. Here's some helpful info on how to [replace](http://www.tutorialspoint.com/python/string_replace.htm) and [split](http://www.tutorialspoint.com/python/string_split.htm) parts of a string.", "created_utc": 1399867070, "gilded": 0, "name": "t1_chfoz3l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25bvld/please_help_on_first_bot/"}, {"author": "Seeing_Eye_Bot", "body": "Thanks a bunch, I'll look into those links you posted", "created_utc": 1399868452, "gilded": 0, "name": "t1_chfph02", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/25bvld/please_help_on_first_bot/"}, {"author": null, "body": "All it's saying is, there's no point using the \"strict\" option any more, it has no effect. The thing it used to do is now done by default whether you want it or not.", "created_utc": 1399547244, "gilded": 0, "name": "t1_chcmdnm", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/250uqn/html_deprecationwarning_when_replying_to_a_comment/"}, {"author": "MattDrumz", "body": "I've been running a bot for quite awhile with this warning and I never noticed anything bad happen.", "created_utc": 1399532506, "gilded": 0, "name": "t1_chcjwx3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/250uqn/html_deprecationwarning_when_replying_to_a_comment/"}, {"author": "BrandieBrands", "body": "Alright, I guess I'll just ignore it then. Thanks for the reply!", "created_utc": 1399537456, "gilded": 0, "name": "t1_chcku4x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/250uqn/html_deprecationwarning_when_replying_to_a_comment/"}, {"author": "Falmarri", "body": "sudo", "created_utc": 1399440943, "gilded": 0, "name": "t1_chbmmkk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "ubccompscistudent", "body": "not sure what that means. I should have said I'm a noob with command line.", "created_utc": 1399483701, "gilded": 0, "name": "t1_chbz165", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "Falmarri", "body": "man sudo", "created_utc": 1399497198, "gilded": 0, "name": "t1_chc5xrl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "ubccompscistudent", "body": "Thanks, that did it", "created_utc": 1399503511, "gilded": 0, "name": "t1_chc8w2r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": null, "body": "[deleted]", "created_utc": 1399448318, "gilded": 0, "name": "t1_chbodk3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "ubccompscistudent", "body": "Sorry, but every time I searched for pip/praw, the threads were all in this subreddit, so I assumed it was the right one. My bad", "created_utc": 1399483660, "gilded": 0, "name": "t1_chbz0fo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "kemitche", "body": "Looks like that ez_setup.py script has a `--user` option. Use it.", "created_utc": 1399441810, "gilded": 0, "name": "t1_chbmv9z", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "ubccompscistudent", "body": "not sure what that means. I don't see --user anywhere. I should have mentioned I'm a noob with command line (I guess I've been sheltered by my IDEs)", "created_utc": 1399483757, "gilded": 0, "name": "t1_chbz266", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "kemitche", "body": "You'll probably want to get comfortable with the command line before going crazy with `pip`. Also, it's generally not a great idea to `curl` a script and then run it directly without at least looking it over.", "created_utc": 1399483942, "gilded": 0, "name": "t1_chbz5j0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "ubccompscistudent", "body": "These are all things I wish were written in a nice concise manual", "created_utc": 1399484517, "gilded": 0, "name": "t1_chbzft7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": "Falmarri", "body": "man man", "created_utc": 1399497247, "gilded": 0, "name": "t1_chc5ynt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24xdaj/trying_to_install_pip_but_getting_administrator/"}, {"author": null, "body": "[deleted]", "created_utc": 1399397185, "gilded": 0, "name": "t1_chb2sz4", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "JBHUTT09", "body": "Thank you so much. I wonder why I couldn't find created_utc anywhere in the documentation. I assume it's a property of a redditor object?", "created_utc": 1399397911, "gilded": 0, "name": "t1_chb35th", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "reseph", "body": "Documented here: https://github.com/reddit/reddit/wiki/JSON", "created_utc": 1399406219, "gilded": 0, "name": "t1_chb79w6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "_Daimon_", "body": "Attributes of objects are dynamically determined at runtime based on what is exposed by Reddits API. This is a really cool feature that allos us to instantly integrate new features of the API to all versions of PRAW. No upgrade necessary. But it does mean, that there is no way of knowing what attribute are available on what objects at the time of compiling the documentation. This is why [PRAW's documentation](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) details how to use standard python introspection tools to discover what attributes are available on an object.", "created_utc": 1399409429, "gilded": 0, "name": "t1_chb8wgg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "JBHUTT09", "body": "I don't understand. Redditor objects will always have the 'created_utc' attribute, will they not? Wouldn't it make sense to list all the attributes in the documentation? How much do the attributes actually change? I would assume that they remain pretty consistent or there would be no stable bots.", "created_utc": 1399421582, "gilded": 0, "name": "t1_chbelsn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "nemec", "body": "But then if Reddit adds another attribute (like when they introduced flair) the documentation will now be *inaccurate*, which could be considered worse than having none at all and requiring you to check yourself.", "created_utc": 1399434045, "gilded": 0, "name": "t1_chbk8b7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "JBHUTT09", "body": "I think that slightly inaccurate documentation (with a warning that there may be some slight inaccuracies) is still much more useful than none at all. And how long would the documentation really be inaccurate? I can't see it taking more than a day or two for someone to update it.", "created_utc": 1399460148, "gilded": 0, "name": "t1_chbq9rt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": null, "body": "[deleted]", "created_utc": 1399398133, "gilded": 0, "name": "t1_chb39nf", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "_Daimon_", "body": "PRAW is open source. You're welcome to contribute..", "created_utc": 1399409305, "gilded": 0, "name": "t1_chb8u7j", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": null, "body": "[deleted]", "created_utc": 1399412265, "gilded": 0, "name": "t1_chbaaqd", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "_Daimon_", "body": "Indeed. You can just take, take and take. Never giving back. You can even feel pleased with yourself about shittaking the work you're given for free.", "created_utc": 1399413545, "gilded": 0, "name": "t1_chbax3n", "num_comments": null, "score": 8, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "vicstudent", "body": "You can start by doing something like this and move forward. import praw import time r = praw.Reddit('Blah') u = r.get_redditor('vicstudent') timestamp = u.created_utc # Convert the timestamp date = time.ctime(timestamp) # date = 'Thu Jul 25 22:08:04 2013'", "created_utc": 1399397639, "gilded": 0, "name": "t1_chb3114", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "JBHUTT09", "body": "Thank you very much!", "created_utc": 1399397921, "gilded": 0, "name": "t1_chb35zk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24vkho/is_there_a_way_to_get_the_age_of_an_account/"}, {"author": "OnceAndForever", "body": "You are not actually logging in with a user account. `r.login()` takes the username and password of a reddit account as two parameters. Your error message says this: `praw.errors.NotLoggedIn: 'please login to do that' on field 'None'.` Keep in mind that you can only delete comments and submissions that your account has posted.", "created_utc": 1399353663, "gilded": 0, "name": "t1_chaqqs0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ua4d/cant_get_praw_to_delete_a_post/"}, {"author": "Phteven_j", "body": "No exception at all? Could be a problem with your hardware.", "created_utc": 1399273292, "gilded": 0, "name": "t1_ch9w60m", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24qx0w/is_it_normal_for_praw_bots_to_just_stop_for_some/"}, {"author": "shaggorama", "body": "try throwing a \"ctrl-c\" at the bot to jolt it and see what happens?", "created_utc": 1399295012, "gilded": 0, "name": "t1_cha08ht", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24qx0w/is_it_normal_for_praw_bots_to_just_stop_for_some/"}, {"author": "ScredditAttackBot", "body": "I'm new. It looks like you have praw installed. What makes you think you don't? Make sure you are running just python in the command prompt and not python3.", "created_utc": 1399179191, "gilded": 0, "name": "t1_ch91p6l", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "pietrod21", "body": "or in python 3 IDLE or in the command line by writing \"python\" (and the default it's python 3), it tells me: Traceback (most recent call last): File \"\", line 1, in import praw ImportError: No module named 'praw'", "created_utc": 1399203982, "gilded": 0, "name": "t1_ch96kdy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "_Daimon_", "body": "The output of your command says Requirement already satisfied (use --upgrade to upgrade): praw in c:\\users\\admin \\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages It means that pip thinks praw is installed and in the correct path. The likely cause of your problem is that the site-packages directory, where PRAW is located, isn't on your PYTHONPATH. Which is basically a list of directories where Python looks for libraries when you have an import statement in your code. So Python cannot find the module, even though it actually is on your system. I haven't used python in a windows environment in many years. So this is moving out of my area of expertise. You might be able to solve the problem by adding to your pythonpath as mentioned in this [stackoverflow answer](http://stackoverflow.com/a/4855685/1368070). You might also consider starting a new question on SO or /r/learnpython as this is a more general question, that has little to do with PRAW and a lot to do with running python on windows. The guyes at those two places will know much more about this subject, and will be better able to help you. Best of luck!", "created_utc": 1399242897, "gilded": 0, "name": "t1_ch9kunz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "pietrod21", "body": "Thanks a lot, it seems no more automatically add folder instruction in the envinroment variables instructions... I post the question [here](http://www.reddit.com/r/learnpython/comments/24snw2/prompt_doesnt_recognize_pip_install_command_after/) anyway, I really hope in some divine help!!!", "created_utc": 1399315846, "gilded": 0, "name": "t1_cha9f18", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": null, "body": "it's a **very** hacky workaround start your script with: import sys sys.path.append('c:\\users\\admin\\appdata\\local\\enthought\\canopy\\user\\lib\\site-packages') import praw for some reason PRAW was installed in a directory that wasn't in your path.", "created_utc": 1399254175, "gilded": 0, "name": "t1_ch9pits", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "pietrod21", "body": "thanks I very near solving but for error cancel also another setting and obviously Windows doesn't backup and so there isnt any option to repair **the most importan instruction on it**...so now probably i will destroy every windows i will find and on this pc i will use ubuntu", "created_utc": 1399310320, "gilded": 0, "name": "t1_cha6ogr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24o064/i_install_praw_via_pip_install_but_doesnt_work/"}, {"author": "_Daimon_", "body": "No problem! Glad we could help :)", "created_utc": 1399193018, "gilded": 0, "name": "t1_ch94yvw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24nm86/praw_checking_content_of_comment_parent/"}, {"author": "slyf", "body": "Update your requests to 2.3, googling it looks like this was a requests bug (and yours is 2.2.1, latest is 2.3)", "created_utc": 1399074058, "gilded": 0, "name": "t1_ch85bmf", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": "actual_factual_bear", "body": "That's odd, I already did an \"easy_install -U requests\" and it stayed at 2.2.1. Is requests 2.3 available for Python 2.6?", "created_utc": 1399077286, "gilded": 0, "name": "t1_ch86j0q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": "slyf", "body": "http://docs.python-requests.org/en/latest/ clearly says \"Release v2.3.0. (Installation)\"....I guess it is not out yet though. None of the releases have it. Anyway, pretty sure it is a requests bug.", "created_utc": 1399078232, "gilded": 0, "name": "t1_ch86vh4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": "actual_factual_bear", "body": "my machine rebooted the other day and i just got around to re-starting my PRAW client and found it had the same error I mentioned in this post two months ago. I have no idea how I originally fixed it, but this time I did an `easy_install -U requests` and it updated to 2.3 and the problem was fixed.", "created_utc": 1405713541, "gilded": 0, "name": "t1_cj172rx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": "_Daimon_", "body": "Yeah incrementing the version count was apparently a mistake on requests side. I filed a [issue](https://github.com/kennethreitz/requests/issues/2034) and they fixed it rapidly. 2.2.1 is the latest released version of requests.", "created_utc": 1399111510, "gilded": 0, "name": "t1_ch8fp0l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": "PasswordIsntHAMSTER", "body": "You should probably use Markdown's code feature for your error report.", "created_utc": 1399070045, "gilded": 0, "name": "t1_ch83rf8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": "actual_factual_bear", "body": "Thanks, fixed...", "created_utc": 1399070695, "gilded": 0, "name": "t1_ch84125", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ku36/is_praw_really_compatible_with_python_26/"}, {"author": null, "body": "it's technically the 'note' field", "created_utc": 1398968747, "gilded": 0, "name": "t1_ch71w7y", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24gpae/feature_request_add_reason_to_get_banned_in_praw/"}, {"author": "bboe", "body": "This isn't the place for feature requests. https://github.com/praw-dev/praw/issues", "created_utc": 1398980196, "gilded": 0, "name": "t1_ch77gaa", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24gpae/feature_request_add_reason_to_get_banned_in_praw/"}, {"author": "Phteven_j", "body": "Heroku is the preferred choice here for a single python app. If you want a simple web server, you can also get a DigitalOcean server for $5/mo with whatever Linux OS you want and full control. Heroku is like $30/mo for 2 or more apps.", "created_utc": 1398911372, "gilded": 0, "name": "t1_ch6jq2q", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "RUbernerd", "body": ">free Amazon EC2 Free Tier. It's about the only way to get free services without having to cozy up and get real warm with someone you may not like.", "created_utc": 1398915971, "gilded": 0, "name": "t1_ch6lrea", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "0x14", "body": "How easy is this to get set up?", "created_utc": 1398952694, "gilded": 0, "name": "t1_ch6uluk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "RemindMeBotWrangler", "body": "Based on your linux knowledge. Most work you'll have to do out of the box is yum install the programs you need.", "created_utc": 1398959256, "gilded": 0, "name": "t1_ch6xcj9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "0x14", "body": "Do Amazon expect you to hand over any payment details, personal information? How easy is registration? So they use a fedora/redhat/centos system (i had to look up yum, i use debian) Do you get a pretty basic user account in this linux environment (CLI and GUI?) and you can easily install with a package manager... sounds good can you install pip nice and easy with yum? How does it handle praw/reddit logins, i know pythonanywhere doesnt like praw going through SSL. Finally are redditbots pretty stable on it? Pythonanywhere will switch your bot off after a few days if its going over its CPU allowance, is EC2 more generous?", "created_utc": 1398962031, "gilded": 0, "name": "t1_ch6ynfk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "RemindMeBotWrangler", "body": ">Do Amazon expect you to hand over any payment details, personal information? How easy is registration? Credit card info >So they use a fedora/redhat/centos system (i had to look up yum, i use debian) https://aws.amazon.com/marketplace/b/2649367011?page=1&category=2649367011 for a list but free only get to pick like 8. There are debian systems you can use though I believe. >can you install pip nice and easy with yum? How does it handle praw/reddit logins, i know pythonanywhere doesnt like praw going through SSL. Same as apt-get really. sudo yum install pip. sudo pip install praw. It works fine on there for me. >Finally are redditbots pretty stable on it? Pythonanywhere will switch your bot off after a few days if its going over its CPU allowance, is EC2 more generous? I've only been using it for a few days but so far so good. http://aws.amazon.com/free/faqs/ There are the limitations you get for free. If you go over you pay for that month but I believe you still get the 12 months for free. But I don't think you'll go over if you are only searching and replying. I read that if you are always at 100% CPU they will throttle you but you won't notice as they just lower your speed, but you'll still be at 100%. Not sure if it really matters, I'll notice or if it's true.", "created_utc": 1398963481, "gilded": 0, "name": "t1_ch6zchi", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "0x14", "body": "> Credit card info Dealbreaker for me, but awesome reply, i will probably explain all that to someone at some point so thanks!", "created_utc": 1398969034, "gilded": 0, "name": "t1_ch7219w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "drkabob", "body": "If you can get it working on [PythonAnywhere](https://www.pythonanywhere.com/), its free. The servers are a bit slow, but you can't beat free.", "created_utc": 1398919475, "gilded": 0, "name": "t1_ch6n780", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "GHETTO_CHiLD", "body": "only issue is that you only get a limited number of runs a day and i also believe free cannot connect to certain db's like mongohq and they can only run on the hour so if you need every x mins then it kind of stinks.", "created_utc": 1398961170, "gilded": 0, "name": "t1_ch6y8nv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "damontoo", "body": "Google app engine also has a free tier but you can't use PRAW I don't think. I just wrote my own pure python API wrapper. It's not fancy but it works.", "created_utc": 1398950688, "gilded": 0, "name": "t1_ch6tx4p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "bboe", "body": "PRAW should work on GAE.", "created_utc": 1398980608, "gilded": 0, "name": "t1_ch77n8f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "damontoo", "body": "IIRC there was some dependency I couldn't get working because it had a compiled module.", "created_utc": 1398980868, "gilded": 0, "name": "t1_ch77rhn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "IAmAnAnonymousCoward", "body": "Ramnode is about $5 a year iirc. Search for coupons.", "created_utc": 1398953997, "gilded": 0, "name": "t1_ch6v3e0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "albireox", "body": "[123Systems](http://123systems.net/) is extremely cheap ($9/yr). If you want to go free though, Heroku is the best.", "created_utc": 1398914889, "gilded": 0, "name": "t1_ch6lamo", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24ezpi/where_to_host_a_simple_praw_bot_that_just_scans/"}, {"author": "judasblue", "body": "I don't know about the exception, but you can answer a lot of this pretty trivially by starting a throwaway sub, then banning your bot account from it and testing.", "created_utc": 1398884981, "gilded": 0, "name": "t1_ch67az9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24dzm3/praw_best_way_to_handle_being_banned_from_a/"}, {"author": "mgrieger", "body": "I'll go try that out soon. Just thought I would post this up just in case anybody else has the same question.", "created_utc": 1398885428, "gilded": 0, "name": "t1_ch67j1s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24dzm3/praw_best_way_to_handle_being_banned_from_a/"}, {"author": "RemindMeBotWrangler", "body": "The API exception you get is except (HTTPError, ConnectionError, Timeout, timeout), e: #PM instead if the banned from the subreddit if str(e) == \"403 Client Error: Forbidden\": for anyone wondering.", "created_utc": 1398959758, "gilded": 0, "name": "t1_ch6xl0l", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24dzm3/praw_best_way_to_handle_being_banned_from_a/"}, {"author": "mgrieger", "body": "Awesome, thanks!", "created_utc": 1398962557, "gilded": 0, "name": "t1_ch6ywe6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24dzm3/praw_best_way_to_handle_being_banned_from_a/"}, {"author": "Phteven_j", "body": "You will get a specific exception and I've seen it before. You'll have to find it out yourself if you don't want a catch all.", "created_utc": 1398940370, "gilded": 0, "name": "t1_ch6rkeu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24dzm3/praw_best_way_to_handle_being_banned_from_a/"}, {"author": "mgrieger", "body": "Cool, I was hoping there was a specific one. I'm going to mess around with it later today and I'll edit the OP if I find anything.", "created_utc": 1398957802, "gilded": 0, "name": "t1_ch6woop", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/24dzm3/praw_best_way_to_handle_being_banned_from_a/"}, {"author": "Phteven_j", "body": "What do you mean by this? Like copying a hyperlink that is in a comment body? Or the hyperlink to a comment? You get to a comment with Comment.permalink (if I recall) but getting a URL from a comment body is just a String operation. You'd do something like: text = comment.body.split() for word in text: if 'http://' in word: #process it however you want. Keep in mind you will have to handle [this kind](of link) using regex or partition(). To strip a [bracketed](reddit markup link) you just do something like: word = word.partition('](')[2] #returns right half of []() word = word.partition(')')[0] #returns left half of final ) So if you start with [text here](URL_HERE), you would end up with URL_HERE.", "created_utc": 1398742579, "gilded": 0, "name": "t1_ch4rhxl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248v0w/looking_for_a_way_to_grab_url_links_out_of/"}, {"author": "yaph", "body": "Since reddit comments are in markdown you can use the markdown package. You can see an example of how to extract all URLs from a comment in [this script's extract_urls](https://github.com/yaph/threaddit/blob/master/threaddit/bot.py) function.", "created_utc": 1398761692, "gilded": 0, "name": "t1_ch4ws19", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248v0w/looking_for_a_way_to_grab_url_links_out_of/"}, {"author": "elihusmails", "body": "Thank you for the response. I will try this.", "created_utc": 1398773002, "gilded": 0, "name": "t1_ch4yqot", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248v0w/looking_for_a_way_to_grab_url_links_out_of/"}, {"author": "picflute", "body": "Remember that you can only grab a certain amount of comments in a run through submissions. But /u/Phteven_j posted was right. you can change 'http' to anything you want. if you only want to catch imgur links you can if 'imgur.com' in word:", "created_utc": 1398793622, "gilded": 0, "name": "t1_ch57a7k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248v0w/looking_for_a_way_to_grab_url_links_out_of/"}, {"author": "vicstudent", "body": "The msg argument you are passing to is used for the 'subject' parameter. [Here's the docs](https://praw.readthedocs.org/en/latest/pages/code_overview.html?highlight=send_message#praw.__init__.PrivateMessagesMixin.send_message). Passing print(r.user.send_message('MyUserName', 'subject', msg)) should work.", "created_utc": 1398736813, "gilded": 0, "name": "t1_ch4onq7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "body": "Did you read the pastebin? Line 17, `msg = '[PRAW related thread](%s)' , submission.short_link` shouldn't this fill both?", "created_utc": 1398736923, "gilded": 0, "name": "t1_ch4opq3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "vicstudent", "body": "From what I understand, this won't work. However, I don't really understand what you're trying to get at with that variable. Is that an attempt at string formatting? Because that won't work with Py3.4. Otherwise, what you're doing is sending send_message a tuple, which won't work. This is the method according to the docs: send_message(recipient, subject, message, captcha=None). If you're trying to format the string just do: msg = '[PRAW related thread]({}).format(submission.short_link) #Then pass: print(r.user.send_message('MyUserName', 'Subject Title', msg)) Edit: The point is that you're sending the msg argument to the subject title parameter which does not have a default argument.", "created_utc": 1398737749, "gilded": 0, "name": "t1_ch4p4tl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "body": "Okay, I changed it to `print(r.user.send_message('HellFireKoder', 'From Your Reddit Bot', msg, None))` and now it says Traceback (most recent call last): File \"C:\\Users\\w_000\\Desktop\\questionsearch.py\", line 22, in print(r.user.send_message('HellFireKoder', 'From Your Reddit Bot', msg, None)) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 58, in wrapped return function(self.reddit_session, self, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 225, in wrapped return function(obj, *args, **kwargs) TypeError: send_message() takes from 4 to 5 positional arguments but 6 were given Any clues? I don't get it because I only count 4 arguments... Thank you for your help and patience so far.", "created_utc": 1398738381, "gilded": 0, "name": "t1_ch4pgio", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "vicstudent", "body": "What does your msg variable look like? Also, you don't need to add the None argument. That's what the default parameter for captcha is for.", "created_utc": 1398738466, "gilded": 0, "name": "t1_ch4pi2t", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "body": ">What does your msg variable look like? `msg = submission.short_link` >Also, you don't need to add the None argument Okay. Edit: now it asks for captcha every time, like this Captcha URL: http://www.reddit.com/captcha/randomstring.png Captcha:", "created_utc": 1398738850, "gilded": 0, "name": "t1_ch4pp07", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "vicstudent", "body": "Oh. Try: print(r.send_message('HellFireKoder', 'From Your Reddit Bot', msg, None))", "created_utc": 1398739461, "gilded": 0, "name": "t1_ch4q01u", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "body": "It still asks for Captcha, in the same way. Thank you very much for your help to this point, and any help you will provide soon (which I will thank you for again, when it happens). Edit: Maybe it matters that the user that I am logging in as is new to reddit (new user solely for my bots)?", "created_utc": 1398739712, "gilded": 0, "name": "t1_ch4q4gv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "vicstudent", "body": "It might because you're a relatively new user.", "created_utc": 1398739820, "gilded": 0, "name": "t1_ch4q6f6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "body": ">It might because you're a relatively new user. *(sigh)*... okay, I verified my email though, isn't that supposed to make it so that I don't need to do that?", "created_utc": 1398740403, "gilded": 0, "name": "t1_ch4qgvb", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "Phteven_j", "body": "No, you need at least 2 link karma.", "created_utc": 1398768953, "gilded": 0, "name": "t1_ch4xx17", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "body": "Oh, ***link*** karma! well, that would explain why comment thing didn't work... ^*sigh*", "created_utc": 1398797426, "gilded": 0, "name": "t1_ch597qt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "vicstudent", "body": "Okay that's fine. Just goto the url and fill in the captcha and tell me if it sends the message.", "created_utc": 1398739802, "gilded": 0, "name": "t1_ch4q63u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "body": "It returns Traceback: Traceback (most recent call last): File \"C:\\Users\\w_000\\Desktop\\questionsearch.py\", line 22, in print('HellFireKoder', r.send_message('HellFireKoder', 'From Your Reddit Bot', msg, None)) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python34\\lib\\site-packages\\praw\\decorators.py\", line 225, in wrapped return function(obj, *args, **kwargs) TypeError: send_message() got multiple values for argument 'captcha' Edit: I am going to try again with `None` in there... Edit 2: It worked without `None` in it, still asking for captcha though :(", "created_utc": 1398739935, "gilded": 0, "name": "t1_ch4q8hp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "vicstudent", "body": "I'm not sure about the captcha. I have to go to bed atm b/c I have an exam tomorrow morning. If you don't figure it out, I'll try to help out. I'm sure it's just because you need to gain more karma or something silly.", "created_utc": 1398742070, "gilded": 0, "name": "t1_ch4r9mf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "HellFireKoder", "body": "Okay, thank you :)", "created_utc": 1398742194, "gilded": 0, "name": "t1_ch4rbqv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/248ku8/usersend_message_returning_errors_and_not_sending/"}, {"author": "_Daimon_", "body": "You're seeing a SSL Certificate error while using pip. [This stackoverflow answer](http://stackoverflow.com/a/16370731/1368070) should help solve your problem. :)", "created_utc": 1398610605, "gilded": 0, "name": "t1_ch3b8n7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/243ueb/error_when_installing_praw/"}, {"author": "Sc1F1", "body": "Thank you! I don't like having to use easy-install, but as long as it works it's fine.", "created_utc": 1398611359, "gilded": 0, "name": "t1_ch3bj2h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/243ueb/error_when_installing_praw/"}, {"author": "Phteven_j", "body": "Increasing your karma in that sub helps, as does being an approved submitter. I just catch that exception and wait for the amount of time it says.", "created_utc": 1398578880, "gilded": 0, "name": "t1_ch34vpe", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2432ye/what_is_the_rate_limit_exceeded_message_and_how/"}, {"author": "xjcl", "body": "[Reddit has a rate limit in place to keep unreputable users and bots from spamming reddit.](http://www.reddit.com/r/help/comments/14erjc/psa_if_you_get_the_looks_like_youre_either_a/) Workarounds: * Use a try-except-block and keep the bot running until reddit accepts the response. * Verify the bot account's email address. * Get karma. There are global and a subreddit-specific rate limits. You can get around the global limit by getting your comment karma up to ~3 (e.g. in /r/FreeKarma). DON'T upvote your bot's posts manually, as this can get you banned.", "created_utc": 1398622305, "gilded": 0, "name": "t1_ch3fzoa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2432ye/what_is_the_rate_limit_exceeded_message_and_how/"}, {"author": "YouBetterGoFindIt", "body": "So I think I verified my bot's account email address but I still have to wait 9 minutes between each post...", "created_utc": 1398626565, "gilded": 0, "name": "t1_ch3humh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2432ye/what_is_the_rate_limit_exceeded_message_and_how/"}, {"author": "markerz", "body": "Did you log in and fail too many times? Reddit is cruel for that. Aside from that, reddit limits requests to 30 per minute.", "created_utc": 1398577146, "gilded": 0, "name": "t1_ch34f83", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2432ye/what_is_the_rate_limit_exceeded_message_and_how/"}, {"author": "gavin19", "body": "It's for x in not if x in", "created_utc": 1398539804, "gilded": 0, "name": "t1_ch2qkfk", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/241rkt/having_trouble_parsing_inbox_messages_python_27/"}, {"author": "5loon", "body": "And yet another reminder not to think up programs at 2 in the morning. Thanks!", "created_utc": 1398539953, "gilded": 0, "name": "t1_ch2qmjb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/241rkt/having_trouble_parsing_inbox_messages_python_27/"}, {"author": "bboe", "body": "First, there's no need for `subbed_reddits` as you already have a listing with your Subreddit objects. Second, you can omit the `pass` from your functions as it's only needed if you have a block that is to do nothing. Just like in Java, after a `return` no code executes within the same scope. Try: hot_posts = [next(x.get_hot(limit=1)) for x in subscribed_reddits] for post in hot_posts: print(post.title) # output vars(post) for a complete list of available attributes Edit: host_posts -> hot_posts", "created_utc": 1398628790, "gilded": 0, "name": "t1_ch3iued", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "CastleCorp", "body": "I have this: def getHotPosts(): hot_posts = [next(x.get_hot(limit=1)) for x in subscribed_reddits] for post in hot_posts: print(post.title) But it does not output anything. Thoughts?", "created_utc": 1398630909, "gilded": 0, "name": "t1_ch3js58", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "bboe", "body": "Assuming the second line above is just indented incorrectly, what's len(subscribed_reddits)?", "created_utc": 1398632976, "gilded": 0, "name": "t1_ch3kpfd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "CastleCorp", "body": "When I tried to print len(subscribed_reddits) it said \"Object of type generator has no attribute len()\"", "created_utc": 1398634142, "gilded": 0, "name": "t1_ch3l8a6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "bboe", "body": "Sorry it needs to be `len(list(subscribed_reddits))`.", "created_utc": 1398638324, "gilded": 0, "name": "t1_ch3n3zd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "CastleCorp", "body": "it returns 0", "created_utc": 1398639648, "gilded": 0, "name": "t1_ch3nof6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "bboe", "body": "Well there's your issue. Are you logged in? Does that user actually have subscribed subreddits? Also a possibility -- are you're using a function to alter the value of a variable at the global scope? If the you need to use the `global` keyword.", "created_utc": 1398641765, "gilded": 0, "name": "t1_ch3ol4g", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "CastleCorp", "body": "Yes, did login, at least I think I did it correctly. It is logging in under my account, so it does have subscribed subreddits. As for the second part, I am running on like 2 hours of sleep here, and I am not really processing what the heck you are talking about. I don't think I am but I could be very wrong. Here is what I have: user_agent = (\"PlaceHolder bot by /u/CastleCorp\" \"github link\") # Should this be open source? r = praw.Reddit(user_agent = user_agent) # Leave on this line. r.login(username,password) already_done = [] # Submissions already checked user = r.get_redditor(username) subscribed_reddits = r.get_my_subreddits(limit=500) # get all subscribed reddits, up to 500 subbed_reddits = [] # Array to hold all subscribed subreddits hot_posts= [] # Array to hold all the hot posts def getSubscribedReddits(): # store each subscribed reddit in array, print the array for subreddit in subscribed_reddits: subbed_reddits.append(subreddit) # add the subreddit's name to the array print subreddit.display_name print len(list(subscribed_reddits)) return subbed_reddits # return for function call def getHotPosts(): # Get the top \"Hot\" post from each of the user's subscribed reddits hot_posts = [next(x.get_hot(limit=1)) for x in subscribed_reddits] for post in hot_posts: print(post.title) Thank you very much for all your help, it is really awesome!", "created_utc": 1398644523, "gilded": 0, "name": "t1_ch3pqj0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "bboe", "body": "Where do you call `getHotPosts`? And it seems there is no reason for getSubscribedReddits at all as I prevously mentioned.", "created_utc": 1398654130, "gilded": 0, "name": "t1_ch3u0sf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "CastleCorp", "body": "I call both methods at the bottom of the code like this: print \"Subreddits you are subscribed to:\\n\" getSubscribedReddits() print \"\\nThe first hot post from each subreddit:\\n\" getHotPosts() And I understand that it might not be necessary to have the getSubscribedReddits I just kept it because I don't know why.", "created_utc": 1398707716, "gilded": 0, "name": "t1_ch4agis", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "CastleCorp", "body": "Sorry, but I'm not at my computer now. I will get back to you tomorrow when I can take a look.", "created_utc": 1398654486, "gilded": 0, "name": "t1_ch3u6m6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "CastleCorp", "body": "I will give that a try. Thank you. Will report back soon.", "created_utc": 1398629011, "gilded": 0, "name": "t1_ch3ixx2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2415uq/need_some_help_iterating_through_submissions_in/"}, {"author": "calebkeith", "body": "Can you post a fiddler request and the headers of the response?", "created_utc": 1398467685, "gilded": 0, "name": "t1_ch25ekm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23ziu3/api_returning_401_response_when_using_oauth2client/"}, {"author": "Ambit", "body": "Here's everything I could get. # First Call to https://ssl.reddit.com/api/v1/authorize ## Request GET https://ssl.reddit.com/api/v1/authorize?scope=identity%2Cprivatemessages%2Cread&redirect_uri=http%3A%2F%2F127.0.0.1%3A65010%2Freddit_authorize_callback&response_type=code&client_id=E2gCdoXth-x_LQ&access_type=offline&state=AJH60W8WO3WEQ6VDMD23VFTNSJ54GN2S&duration=permanent HTTP/1.1 Host: ssl.reddit.com User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:27.0) Gecko/20100101 Firefox/27.0.2 Waterfox/27.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate Cookie: Connection: keep-alive ## Response HTTP/1.1 200 OK Date: Fri, 25 Apr 2014 23:29:35 GMT Connection: close content-type: text/html; charset=UTF-8 cache-control: no-cache pragma: no-cache x-frame-options: SAMEORIGIN x-content-type-options: nosniff x-xss-protection: 1; mode=block Vary: accept-encoding Content-Length: 25263 Server: '; DROP TABLE servertypes; -- Body: # Second Call to https://ssl.reddit.com/api/v1/authorize ## Request POST https://ssl.reddit.com/api/v1/authorize HTTP/1.1 Host: ssl.reddit.com User-Agent: Mozilla/5.0 (Windows NT 6.3; Win64; x64; rv:27.0) Gecko/20100101 Firefox/27.0.2 Waterfox/27.0 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8 Accept-Language: en-US,en;q=0.5 Accept-Encoding: gzip, deflate Referer: https://ssl.reddit.com/api/v1/authorize?scope=identity%2Cprivatemessages%2Cread&redirect_uri=http%3A%2F%2F127.0.0.1%3A65010%2Freddit_authorize_callback&response_type=code&client_id=E2gCdoXth-x_LQ&access_type=offline&state=AJH60W8WO3WEQ6VDMD23VFTNSJ54GN2S&duration=permanent Cookie: Connection: keep-alive Content-Type: application/x-www-form-urlencoded Content-Length: 264 Body: client_id=E2gCdoXth-x_LQ&redirect_uri=http%3A%2F%2F127.0.0.1%3A65010%2Freddit_authorize_callback&scope=identity%2Cprivatemessages%2Cread&state=AJH60W8WO3WEQ6VDMD23VFTNSJ54GN2S&duration=permanent&uh=&authorize=Allow ## Response HTTP/1.1 302 Found Date: Fri, 25 Apr 2014 23:29:42 GMT Connection: close content-length: 0 content-type: text/html; charset=UTF-8 location: http://127.0.0.1:65010/reddit_authorize_callback?state=AJH60W8WO3WEQ6VDMD23VFTNSJ54GN2S&code= cache-control: no-cache pragma: no-cache x-frame-options: SAMEORIGIN x-content-type-options: nosniff x-xss-protection: 1; mode=block Server: '; DROP TABLE servertypes; -- # Call to https://ssl.reddit.com/api/v1/access_token This is what the debugger is telling me is sent. Fiddler wasn't capturing requests originating from the server. ## Request POST https://ssl.reddit.com/api/v1/access_token HTTP/1.1 content-type: application/x-www-form-urlencoded user-agent: /u/Ambit's Reddit To Glass application Body: 'code=&redirect_uri=http%3A%2F%2F127.0.0.1%3A65010%2Freddit_authorize_callback&client_id=E2gCdoXth-x_LQ&scope=identity%2Cprivatemessages%2Cread&client_secret=&grant_type=authorization_code' ## Response HTTP/1.1 401 Unauthorized Date: Fri, 25 Apr 2014 23:59:13 GMT Connection: close content-length: 14 content-type: application/json; charset=UTF-8 www-authenticate: Basic realm=\"reddit\" x-frame-options: SAMEORIGIN x-content-type-options: nosniff x-xss-protection: 1; mode=block Server: '; DROP TABLE servertypes; -- Body: {\"error\": 401}", "created_utc": 1398470852, "gilded": 0, "name": "t1_ch26nkc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23ziu3/api_returning_401_response_when_using_oauth2client/"}, {"author": "calebkeith", "body": "Are you using HTTP Basic authentication when making the access token request? It doesn't seem like you are.", "created_utc": 1398471744, "gilded": 0, "name": "t1_ch270ed", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23ziu3/api_returning_401_response_when_using_oauth2client/"}, {"author": "Ambit", "body": "The documentation does mention the Authorization header but it only says what to send in there after you have the access token. What needs to be present in the header when requesting the access token?", "created_utc": 1398472651, "gilded": 0, "name": "t1_ch27d87", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23ziu3/api_returning_401_response_when_using_oauth2client/"}, {"author": "calebkeith", "body": "You only need it during getting the access token. Authorization: Basic Base64(client_Id:client_secret) After you get the token, use the Auth header with the bearer token", "created_utc": 1398475514, "gilded": 0, "name": "t1_ch28i7y", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23ziu3/api_returning_401_response_when_using_oauth2client/"}, {"author": "Ambit", "body": "That did it! Thank you! :D", "created_utc": 1398477783, "gilded": 0, "name": "t1_ch29esp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23ziu3/api_returning_401_response_when_using_oauth2client/"}, {"author": "calebkeith", "body": "No problem, glad it worked \ud83c\udf7a", "created_utc": 1398496423, "gilded": 0, "name": "t1_ch2fiko", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23ziu3/api_returning_401_response_when_using_oauth2client/"}, {"author": "scandinavian-", "body": "[Have you tried Markdown?](https://pythonhosted.org/Markdown/) All markdown libraries translate to HTML, that is the entire point of markdown. Edit: If you need speed, try [misaka](http://misaka.61924.nl/). [And here's a list of different libraries.](http://www.w3.org/community/markdown/wiki/MarkdownImplementations)", "created_utc": 1398370944, "gilded": 0, "name": "t1_ch1452p", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23w0jq/is_there_a_library_to_translate_markdown_to_html/"}, {"author": "27oz_is_800ml", "body": "It's worth noting that [snudown](https://github.com/reddit/snudown) has some differences to markdown. PRAW has submission.selftext_html which should be easier to use.", "created_utc": 1398375979, "gilded": 0, "name": "t1_ch16opg", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23w0jq/is_there_a_library_to_translate_markdown_to_html/"}, {"author": "slyf", "body": "> snudown has some differences to markdown. Nah, markdown is just basically undefined", "created_utc": 1398388331, "gilded": 0, "name": "t1_ch1c886", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23w0jq/is_there_a_library_to_translate_markdown_to_html/"}, {"author": "110011001100", "body": "Thanks, selftext_html is what I needed!", "created_utc": 1398410316, "gilded": 0, "name": "t1_ch1kemy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23w0jq/is_there_a_library_to_translate_markdown_to_html/"}, {"author": "markerz", "body": "There should be a field returned named body_html from the API. Edit it's actually selftext_html https://github.com/reddit/reddit/wiki/JSON", "created_utc": 1398385437, "gilded": 0, "name": "t1_ch1axnw", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23w0jq/is_there_a_library_to_translate_markdown_to_html/"}, {"author": "_Daimon_", "body": "Yes. They are in the attributes `ups` for upvotes and `downs` for downvotes. You can use [standard python introspection](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) to see what attributes an object has, sometimes you might think the name is a bit strange. But it's always easy to find when you know what value the value the attribute you're looking for should be.", "created_utc": 1398323594, "gilded": 0, "name": "t1_ch0nn2s", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23u4eh/getting_the_number_of_upvotes_and_downvotes_of_a/"}, {"author": null, "body": "[removed]", "created_utc": 1398628556, "gilded": 0, "name": "t1_ch3iqgn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tpns/how_to_get_bot_out_of_moderationspam_queue/"}, {"author": "fedo_tip_bot", "body": "**192** link karma **12** comment karma subreddit link comment Eve 176 12 FreeKarma 16 0 Is there a page that shows rough thresholds required?", "created_utc": 1398641930, "gilded": 0, "name": "t1_ch3onmp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tpns/how_to_get_bot_out_of_moderationspam_queue/"}, {"author": "6086555", "body": "It looks like your bot has been shadowbanned", "created_utc": 1398373113, "gilded": 0, "name": "t1_ch15abu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tpns/how_to_get_bot_out_of_moderationspam_queue/"}, {"author": "fedo_tip_bot", "body": "But only for bot posts? I didn't know that was possible.", "created_utc": 1398378159, "gilded": 0, "name": "t1_ch17pmr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tpns/how_to_get_bot_out_of_moderationspam_queue/"}, {"author": "Doctor_McKay", "body": "I've never used praw, but try setting the limit to 100, as that's the most reddit will return anyway. Passing None might result in praw sending no limit parameter to reddit, making it default to 25.", "created_utc": 1398356891, "gilded": 0, "name": "t1_ch0ww7t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bboe", "body": "`limit=None` here passes in an appropriate no-restriction limit. The issue as I believe is there is a certain amount of error in relying on real-time listings. `comment_stream` does not keep a huge cache of seen comments, and stops yielding new comments (in that iteration) as soon as it finds one it's already seen: https://github.com/praw-dev/praw/blob/master/praw/helpers.py#L108 If you play around with the parameters in the _stream_generator, you may be able to make it more sound.", "created_utc": 1398357758, "gilded": 0, "name": "t1_ch0xbw7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "linuxydave", "body": "The API guidelines say that you shouldn't be making more than 30 requests per minute. If you make more you're liable to get limited. Now, this means that even if you made a request every 2 seconds you'll miss a lot of comments gives then size of Reddit. Consider pushing your requests through a praw multiprocess handler. That way you'll be able to see the actual requests being made which will help with debugging :)", "created_utc": 1398370927, "gilded": 0, "name": "t1_ch144r8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": null, "body": "[removed]", "created_utc": 1398629276, "gilded": 0, "name": "t1_ch3j211", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bboe", "body": "Each HTTP request only returns at most 100 comments this way. The server-side saves at most 1000 (per subreddit in a multireddit) comments for the listing, but reddit's API requires 10 requests to fetch them all.", "created_utc": 1398629671, "gilded": 0, "name": "t1_ch3j8c4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": null, "body": "[removed]", "created_utc": 1398637152, "gilded": 0, "name": "t1_ch3mlib", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bboe", "body": "Which docs are you referring to?", "created_utc": 1398638303, "gilded": 0, "name": "t1_ch3n3mz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": null, "body": "[removed]", "created_utc": 1398640962, "gilded": 0, "name": "t1_ch3o8je", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bboe", "body": "Gotcha. The PRAW docs do not really discuss the relationship between a function call and its underlying requests so I understand the confusion.", "created_utc": 1398642134, "gilded": 0, "name": "t1_ch3oqrz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bboe", "body": "On average there are fewer than 50 comments made to all of reddit each second. Thus missed comments is not really a problem.", "created_utc": 1398379545, "gilded": 0, "name": "t1_ch18c0g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "linuxydave", "body": "Do you have a source for that? It seems highly unlikely given the large amount of subreddits.", "created_utc": 1398413331, "gilded": 0, "name": "t1_ch1l02u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bboe", "body": "Run the script I wrote (mentioned [here](http://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/ch17xe8)) and see for yourself.", "created_utc": 1398453203, "gilded": 0, "name": "t1_ch1yuhl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "linuxydave", "body": "Cool, I'll have a look when I get the chance :)", "created_utc": 1398525754, "gilded": 0, "name": "t1_ch2l9ku", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "JackOfCandles", "body": "It seems believable to me given that a large majority of subreddits are not very active. Plus 1 second goes by pretty quick. If you think about it, there are 86400 seconds in a day. If there are an average of say, 30 comments per second, that's about 2.6 million comments per day.", "created_utc": 1398444034, "gilded": 0, "name": "t1_ch1u7gs", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "JackOfCandles", "body": "> The API guidelines say that you shouldn't be making more than 30 requests per minute. If you make more you're liable to get limited. But the praw documentation says that it handles all the reddit API rules internally. Should this still be an issue?", "created_utc": 1398374979, "gilded": 0, "name": "t1_ch16799", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "linuxydave", "body": "To be honest I've only used it with a multiprocess handler so I'm not entirely sure.", "created_utc": 1398413206, "gilded": 0, "name": "t1_ch1kz8v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "bboe", "body": "It's not an issue. Surprisingly across all of reddit, the average second does not contain more than 50 new comments (the level at which issues may arise with limits in the API). For the last minute the average is under 30 comments per second as computed by a little tool I wrote: https://gist.github.com/bboe/11271738", "created_utc": 1398378633, "gilded": 0, "name": "t1_ch17xe8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23tmto/how_many_comments_per_second_should_a_bot_be/"}, {"author": "test_huang", "body": "I have the same problem... You got any feedback?", "created_utc": 1413254858, "gilded": 0, "name": "t1_cl8trnv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23jii6/prawreddit_api_using_the_period_parameter_of_the/"}, {"author": "bobdammit", "body": "ahhh, I haven't tested this out, yet, but I bet I have to add the 'submit' scope to the get_authorize_url(). Let me know if I am wrong here. Edit: Yep, another case of RTFM. I was looking at the source code, instead of the praw docs. Example usage, just in case someone gets stuck in the future. scopes = ['identity', 'submit'] # and whatever else you want to add r.get_authorize_url('uniqueKey', scopes, True)", "created_utc": 1397940068, "gilded": 0, "name": "t1_cgwrvfy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23gl7i/submitting_comments_with_oauth2/"}, {"author": "_Daimon_", "body": "Yep, sounds about right. If you get an error about insufficient permissions while using OAuth, the required scope should be listed in the exception. For most functions, the Oauth scope required is stored in the docstring of each function. For instance [sticky](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.sticky) requires modposts scope to use via OAuth.", "created_utc": 1397941257, "gilded": 0, "name": "t1_cgwsdlm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23gl7i/submitting_comments_with_oauth2/"}, {"author": "Phteven_j", "body": "Get_flair(subreddit,username)", "created_utc": 1397885626, "gilded": 0, "name": "t1_cgwcryo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23es5v/getting_the_flair_of_a_comment/"}, {"author": "KnightHawk3", "body": "The problem is that i have to be a moderator, which I am not.", "created_utc": 1397885678, "gilded": 0, "name": "t1_cgwcsfy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23es5v/getting_the_flair_of_a_comment/"}, {"author": "_Daimon_", "body": "Yeah, Reddit recently changed the [permissions to use get_flair](http://www.reddit.com/r/redditdev/comments/1xreor/has_there_been_a_change_to_the_permissions/) to moderators only. There's no way via the API (which is what PRAW uses ) to directly retrieve a users flair on a subreddit. If you have a submisson/text, then you can still retrieve the authors flair. import praw r = praw.Reddit(UNQUE_AND_DESCRIPTIVE_USERAGENT) s = r.get_submission(\"http://www.reddit.com/r/DotA2/comments/23f929/live_discussion_sltv_star_series_season_9_day_3/\") print(s.author_flair_css_class)", "created_utc": 1397900129, "gilded": 0, "name": "t1_cgwfl7r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23es5v/getting_the_flair_of_a_comment/"}, {"author": "KnightHawk3", "body": "Thanks!", "created_utc": 1397901294, "gilded": 0, "name": "t1_cgwfqy1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23es5v/getting_the_flair_of_a_comment/"}, {"author": "Phteven_j", "body": "Is that right? Not sure then dude.", "created_utc": 1397886009, "gilded": 0, "name": "t1_cgwcvma", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23es5v/getting_the_flair_of_a_comment/"}, {"author": "KnightHawk3", "body": "The solution I have is to get the URL of the comment on the subreddit, then pull the JSON of that post, then get the flair from the json. Slow as shit but it works.", "created_utc": 1397889634, "gilded": 0, "name": "t1_cgwdqxc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/23es5v/getting_the_flair_of_a_comment/"}, {"author": "kortank", "body": "NOTE: I do not recommend you do this, reddit's admins frown upon this and will block IPs if it gets out of hand. You can append a randomly generated GET variable to the end of your requests to help with cache busting. (i.e. \"http://reddit.com/r/redditdev.json?\" + timestamp()) Though depending on the number of requests, this can start to be a hassle on reddit's servers. For example an app I am working on made several hundred requests per second with cache busting enabled and reddit emailed me and began blocking machines that were making such requests.", "created_utc": 1397673099, "gilded": 0, "name": "t1_cgu3wkj", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2370hr/comment_stream_is_there_anyway_to_bypass_reddits/"}, {"author": null, "body": "[deleted]", "created_utc": 1397673374, "gilded": 0, "name": "t1_cgu41rs", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2370hr/comment_stream_is_there_anyway_to_bypass_reddits/"}, {"author": "kortank", "body": "No problem! That rate should be fine as well.", "created_utc": 1397673446, "gilded": 0, "name": "t1_cgu433c", "num_comments": null, "score": -4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2370hr/comment_stream_is_there_anyway_to_bypass_reddits/"}, {"author": "alienth", "body": "Please do not use cache busting techniques like that. It tends to cause problems for us, and we may end up stripping the cache busting parameter if things get out of hand. If you pass a reddit_session cookie to us, both our CDN and our app will not send cached results. Please also be sure to review the [API usage rules](https://github.com/reddit/reddit/wiki/API).", "created_utc": 1397688211, "gilded": 0, "name": "t1_cgubnvn", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2370hr/comment_stream_is_there_anyway_to_bypass_reddits/"}, {"author": "testingpraw", "body": "Okay, so if I do something like: credentials = {'user': username, 'passwd': password, 'api_type': 'json',} headers = {'user-agent': '/u/testingpraw comment stream',} session_client = requests.session() r = session_client.post('http://www.reddit.com/api/login', data = credentials, headers=headers) the_json = json.loads(r.text) session_client.modhash = j['json']['data']['modhash'] ...grab the data with credentials session_client.get('http://reddit.com/r/subreddit/12345.json', headers=headers) Reddit will not send cached results?", "created_utc": 1397748619, "gilded": 0, "name": "t1_cguw5kr", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2370hr/comment_stream_is_there_anyway_to_bypass_reddits/"}, {"author": "alienth", "body": "If the login is successful and you get a cookie back, then the subsequent request (providing it contains the cookie) will get uncached results. Technically you could send a completely bogus reddit_session cookie in and the same thing would happen. However that's a bit of inside baseball and is subject to change, so I'd recommend avoiding it.", "created_utc": 1397758500, "gilded": 0, "name": "t1_cgv17t8", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2370hr/comment_stream_is_there_anyway_to_bypass_reddits/"}, {"author": "_Daimon_", "body": "Why don't you just use [OAtuh2](http://praw.readthedocs.org/en/latest/pages/oauth.html)? In this way users can give you permission to do some stuff on your behalf. It can be limited, so users know for certain that you won't say delete their old submissions or pm girls on /r/gonewild from their account. Plus the access can last forever, which is nice, but the user can also revoke your authorization at any point they want. Which is even nicer. Also. PRAW has excellent support of OAuth and nobody seems to be really using it, which kinda annoys me since we spent quite some time last year refactoring the library to allow OAuth to be easily used. So as a bonus you'll make me happier :)", "created_utc": 1397483623, "gilded": 0, "name": "t1_cgs2jiq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "testingpraw", "body": "Follow up question. I got everything working, however, is there some sort of boolean that I can check to see if the user has allowed the use of oauth? For logins, PRAW has is_logged_in, is there anything similar to that? I want users that aren't logged in to be able to read the feed, however, if users want to comment on a thread they must be logged in.", "created_utc": 1397502174, "gilded": 0, "name": "t1_cgsbbv9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "_Daimon_", "body": "Yep. It is called [is_oauth_session](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.AuthenticatedReddit.is_oauth_session). You can also use [get_me](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L1136) which returns the user currently authenticated via oauth login. It is a `LoggedInRedditor` object, so it works just like `r.user` which returns the user/passwd authenticated user. The methods that deal with uauthentication are located in [AuthenticatedReddit](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.AuthenticatedReddit). This object is a subset of the main `Reddit` object, mainly seperated out for clarity and maintainability.", "created_utc": 1397502743, "gilded": 0, "name": "t1_cgsbm79", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "testingpraw", "body": "Thanks!", "created_utc": 1397503110, "gilded": 0, "name": "t1_cgsbsuz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "testingpraw", "body": "I think this is exactly what I was looking for! Thanks! I'll read the docs, and reply again, if I have any questions.", "created_utc": 1397484177, "gilded": 0, "name": "t1_cgs2r29", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2303hw/grabbing_login_session_data_with_praw/"}, {"author": "OnceAndForever", "body": "Are you getting any exceptions when you try to run the script? I notice a couple things in your snippet that could cause problems. On line 5, you have written `submission_id = \"id\"`. Since `id` is in quotations, python interprets that as a string and does not actually use the value of the `id` variable on line 4. You are also missing a quotation mark after output.csv on line 14. I don't think that line 6 is necessary. You could include the comment limit as a parameter to the `get_submission` method on line 5, which would look like this: submission = r.get_submission(submission_id=id, comment_limit=None) Also, line 8 would throw an exception because there is no `subm` object in your given snippet. That line should instead read `submission.replace_more_comments()`. To flatten the comment tree, use `praw.helpers.flatten_tree()`. Line 9 would then look like this: comments = praw.helpers.flatten_tree(submission.comments) As for you for loop from lines 11 to 13, you do not need to use `enumerate` because you do not actually use the index (`idx`) for anything within the loop so there is no need to keep track of it. You could simply loop through them by using `for comment in comments:`. You have also already `replaced_more_comments()` on line 8, so there is no need to check the comment type against `praw.objects.MoreComments` on line 12. If you wish to grab a comments body, author, and author comment karma, you would just use the various attributes of the `comment` object. This section may look like this: for comment in comments: users.append([comment.author.name, comment.body, comment.author.comment_karma]) It may be easier to use python's `csv` module to write the csv file. It is part of the standard library and you could read more about it [here](https://docs.python.org/2/library/csv.html). You can use it select your delimiter and quotation handling among other things, but a simple example could look like this: with open(\"output.csv\", \"wb\") as output: writer = csv.writer(output) writer.writerows(users) Here is everything in it's entirety to make it easier to see: import praw import csv r = praw.Reddit(user_agent='RaffleCommentBot by /u/alexratman') r.login('RaffleCommentBot', '##Password##') id = \"22p30f\" submission = r.get_submission(submission_id=id, comment_limit=None) print \"submission get!\" submission.replace_more_comments() comments = praw.helpers.flatten_tree(submission.comments) users = [] for comment in comments: users.append([comment.author.name, comment.body, comment.author.comment_karma]) with open(\"output.csv\", \"wb\") as output: writer = csv.writer(output) writer.writerows(users) Keep in mind that praw sends another request to the API every time it accesses a users comment karma. There is a 2 second delay between requests, so this script could take a while to run depending on how many comments there are in a submission. You may also want to handle the formatting of `comments.body`s because new line characters could mess with the formatting of your csv file. You may also want to handle unicode characters that may appear in comments because they could throw `UnicodeEncodeError` exceptions when trying to write them to the csv. The csv module does not support unicode input, but there is some information on the csv documentation page for handling this. If it is too much of a problem, you could also install other csv writing/reading modules that are not part of the standard lib. Finally, if you are trying to select a random comment from a submission based on certain criteria for a raffle, you may find this tool helpful: http://redditraffle.com/ Hopefully this is clear. I had a slightly more detailed reply typed but I somehow lost it when I tried to comment the first time.", "created_utc": 1397203409, "gilded": 0, "name": "t1_cgpnxi8", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ql4t/is_there_a_way_to_pull_comments_and_additional/"}, {"author": "Alexratman", "body": "I wasn't getting any output from python until I added -v and then it was just showing me libraries loaded. However it was still not showing any output from the script itself. I tried adding import logging logging.basicConfig(filename='debug.log',level=logging.DEBUG) to the beginning of the script, however it didn't output to debug.log. I was probably doing something wrong with syntax. All of what you've written makes sense to me! Thank you so much for your help! I wasn't aware that the reddit API had a 2 second delay, so thanks for bringing that to my attention! I was originally creating this because http://redditraffle.com seems to be broken, giving no valid responses every time it is used. Again, Thank you for your help!", "created_utc": 1397213329, "gilded": 0, "name": "t1_cgppdwa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ql4t/is_there_a_way_to_pull_comments_and_additional/"}, {"author": "_Daimon_", "body": "A submission object has the `num_comments` attribute, which, you guessed it, is the number of comments that object has. As for your related question. Why don't you just test it out in /r/test :) That's usually the fastest way to figure out stuff like that.", "created_utc": 1397160965, "gilded": 0, "name": "t1_cgp8a3h", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22pvic/praw_ive_got_a_question_about_counting_comments/"}, {"author": "Carsonbizotica", "body": "I hate life... where was that little devil hiding?? I swear no such sorcery existed the other 500 times I scoured those docs! Also, I had no idea that subreddit exisited! Thanks much! **Edit for posterity**: `num_comments` eliminated the need for my related question. Part of my process was trying to ensure that my script wasn't counting deleted comments, with `num_comments` doesn't do by default. Although if anyone is wondering: for x in submission.comments: print x returns '[deleted]' for deleted posts.", "created_utc": 1397161369, "gilded": 0, "name": "t1_cgp8he0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22pvic/praw_ive_got_a_question_about_counting_comments/"}, {"author": "reseph", "body": "Your bot has literally no karma. http://www.reddit.com/r/help/wiki/faq#wiki_why_am_i_being_told_.22you.27re_doing_that_too_much....22_i.27ve_been_here_for_years.21 That timer can be up to at least 7 minutes.", "created_utc": 1397067817, "gilded": 0, "name": "t1_cgo93q5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22m9wv/praw_i_keep_getting_ratelimitexceeded_exception/"}, {"author": "d2lp_test2", "body": "okay. so you think the code looks fine?", "created_utc": 1397069440, "gilded": 0, "name": "t1_cgo9w72", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22m9wv/praw_i_keep_getting_ratelimitexceeded_exception/"}, {"author": "linuxydave", "body": "PRAW is ideal for this. It's a powerful library for interacting with the Reddit API and that's why it's so popular with bot creators (and probably why so many bots are written in Python, haha).", "created_utc": 1396884543, "gilded": 0, "name": "t1_cgmafrn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22fhy9/praw_or_just_do_direct_harvesting_for_collecting/"}, {"author": "zynix", "body": "Hmm, I guess I will need to pull praw into a ipython notebook and do some experimentation. As a sanity check, I didn't see anywhere where I could use my raw username/password credentials ( if my computer is compromised I'm screwed anyway, my reddit accounts being the least of my concern ). edit: Also through the oauth privileges request API, I saw that these endpoints `get_comments, get_new_by_date (and the other listing funcs), get_submission, get_subreddit, get_content, from_url` are available which is 50-60% of what I want, but does that include harvesting my (up|down)vote's? Additional: http://www.reddit.com/dev/api#GET_user_{username}_disliked This is the other endpoint I need but I don't see what oauth privilege I need to provide to get access to it.", "created_utc": 1396885526, "gilded": 0, "name": "t1_cgmaw74", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22fhy9/praw_or_just_do_direct_harvesting_for_collecting/"}, {"author": "linuxydave", "body": ">As a sanity check, I didn't see anywhere where I could use my raw username/password credentials Most of the stuff can be found in the [code overview](https://praw.readthedocs.org/en/latest/pages/code_overview.html) import praw r = praw.Reddit(user_agent='Bot UserAgent') r.login(username='', password='') Hope that helps :)", "created_utc": 1396885879, "gilded": 0, "name": "t1_cgmb298", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22fhy9/praw_or_just_do_direct_harvesting_for_collecting/"}, {"author": "zynix", "body": "Indeed it does!", "created_utc": 1396894247, "gilded": 0, "name": "t1_cgmf9f1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22fhy9/praw_or_just_do_direct_harvesting_for_collecting/"}, {"author": "_Daimon_", "body": "The method [get_unread](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.PrivateMessagesMixin.get_unread) returns a listing of unread messages. If it's empty, then there's no unread orangereds. You can also use the `has_mail` attribute of the authenticated user object `r.user`. But this is a bit uncertain, as those attributes are set on logging in and not updated whenever the user object is accessed.", "created_utc": 1396806057, "gilded": 0, "name": "t1_cgljukl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22crls/praw_check_for_new_mail/"}, {"author": "Codyd51", "body": "Great, thanks!", "created_utc": 1396806780, "gilded": 0, "name": "t1_cglk5m8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22crls/praw_check_for_new_mail/"}, {"author": "WinneonSword", "body": "Thid is great! Thank you!", "created_utc": 1396809462, "gilded": 0, "name": "t1_cglla6i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ccgn/wrote_a_small_gist_on_how_to_update_a_subreddits/"}, {"author": "shrayas", "body": "No problem :)", "created_utc": 1396843990, "gilded": 0, "name": "t1_cgm04xk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ccgn/wrote_a_small_gist_on_how_to_update_a_subreddits/"}, {"author": "boib", "body": "Is the GIST gone? I'd like to see it...", "created_utc": 1406138658, "gilded": 0, "name": "t1_cj5nw4k", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ccgn/wrote_a_small_gist_on_how_to_update_a_subreddits/"}, {"author": "shrayas", "body": "Hey, I changed my username so this link was dead. Sorry, fixed! :)", "created_utc": 1406172735, "gilded": 0, "name": "t1_cj65drk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ccgn/wrote_a_small_gist_on_how_to_update_a_subreddits/"}, {"author": "boib", "body": "Thanks. And thanks for providing the info.", "created_utc": 1406213491, "gilded": 0, "name": "t1_cj6heb1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ccgn/wrote_a_small_gist_on_how_to_update_a_subreddits/"}, {"author": "shrayas", "body": "No problem, anytime. Buzz me if you have any problems running it. Cheers!", "created_utc": 1406214293, "gilded": 0, "name": "t1_cj6ht6q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ccgn/wrote_a_small_gist_on_how_to_update_a_subreddits/"}, {"author": "boib", "body": "I'm working on the stylesheet page now. Do you know if I can get a list of the uploaded images?", "created_utc": 1406215406, "gilded": 0, "name": "t1_cj6ie6p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ccgn/wrote_a_small_gist_on_how_to_update_a_subreddits/"}, {"author": "Mustermind", "body": "This should work: import praw r = praw.Reddit(user_agent=\"parentfinder or something\") comment = r.get_info(thing_id=\"t1_cgmdnwg\") # or some other comment if not(comment.is_root): parent_author = r.get_info(thing_id=comment.parent_id).author print(str(parent_author))", "created_utc": 1396904434, "gilded": 0, "name": "t1_cgmkiey", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ap4k/author_of_parent_comment/"}, {"author": "205", "body": "Works great, thanks!", "created_utc": 1396908489, "gilded": 0, "name": "t1_cgmmgz5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ap4k/author_of_parent_comment/"}, {"author": "editer63", "body": "I had a similar question, and this answered it. Thanks!", "created_utc": 1397258625, "gilded": 0, "name": "t1_cgq72ai", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ap4k/author_of_parent_comment/"}, {"author": "205", "body": "No problem :)", "created_utc": 1397261573, "gilded": 0, "name": "t1_cgq86od", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/22ap4k/author_of_parent_comment/"}, {"author": null, "body": "[removed]", "created_utc": 1396582185, "gilded": 0, "name": "t1_cgjn7po", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/225rhj/how_can_i_get_historical_reddit_data/"}, {"author": "redditTopics", "body": "thanks, i'll do that!", "created_utc": 1396620150, "gilded": 0, "name": "t1_cgjwajd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/225rhj/how_can_i_get_historical_reddit_data/"}, {"author": "aquateen", "body": "I don't see a direct method, however I've never used the API. I think if you had the ID of a particular submission, you could sort a subreddit's submissions by creation date and work through them a 100 at a time. Use /r/all as the subreddit to get the entirety of reddit. In 2008 I scraped usernames from reddit off the wayback machine. I then went through each username's history to find all the posts they submitted. They also offered me a data dump rather than scraping their site, however that was a long time ago.", "created_utc": 1396601794, "gilded": 0, "name": "t1_cgjs8ac", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/225rhj/how_can_i_get_historical_reddit_data/"}, {"author": "redditTopics", "body": "thanks!", "created_utc": 1396620138, "gilded": 0, "name": "t1_cgjwac9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/225rhj/how_can_i_get_historical_reddit_data/"}, {"author": "_Daimon_", "body": "> I have written some code with Praw that uploads (overwrites) some already existing images in the stylesheet. This image is coded into the sidebar. The problem that I have is if I upload the the new image to the stylesheet, it won't be updated in the sidebar until I save the stylesheet. This is how reddit rolls and is the same no matter how you change an image. What's the problem with saving the stylesheet after updating the image?", "created_utc": 1396458026, "gilded": 0, "name": "t1_cgi9duk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/220tbz/update_image_in_sidebar/"}, {"author": "argutus1", "body": "Sorry for the wait, I had to get back from a trip. Here is the code and the resulting error: http://pastebin.com/LSBT6xNT", "created_utc": 1396580095, "gilded": 0, "name": "t1_cgjmdr2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/220tbz/update_image_in_sidebar/"}, {"author": "_Daimon_", "body": "Yes you should. Basically before \"get_new\" returned *either* the new listing or the reddit listing depending on your reddit preference, which caught quite a few people unaware. This wasn't a PRAW thing, it was how the Reddit API worked. If you wanted to be sure you actually got the new listing, you had to send a argument with the request. Which is what get_new_by_date did. Then Reddit decided it would be a good idea to split new and rising into separete listings (and it is a good idea). Which made get_new_by_date obsolete, because now get_new always returned the new listing. What specific problem are you encountering in your transition?", "created_utc": 1396457930, "gilded": 0, "name": "t1_cgi9c5m", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/220p0f/question_about_praws_get_new_by_date_vs_get_new/"}, {"author": "JBHUTT09", "body": "I just now solved it, but it turns out that it wasn't a problem with get_new like I thought it was. I compose a message in which I insert a post's short link, and that's where it was breaking. I assumed that it had to do with the switch to get_new not returning the same post objects as get_new_by_date, but the problem was actually in the string itself. I had a url that had '%' signs in it. I thought that I could use the escape character '\\\\' to ignore them, but apparently I had to use a '%%' to do that. If anything, this has taught me to test after every change, because a problem might not be caused by what you think it is. Thanks for the reply. I have a better understanding of how things work/evolved now.", "created_utc": 1396458845, "gilded": 0, "name": "t1_cgi9s94", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/220p0f/question_about_praws_get_new_by_date_vs_get_new/"}, {"author": "SOTB-human", "body": "I can't say for certain without seeing your code, but I think your issue might be that you're trying to reply to a comment that was fetched with an unauthenticated session. In PRAW, reddit \"things\" (like comments, submissions, etc.) are associated with the session in which you fetched them, which is not going to be the same as the `self.r` session attached to the bot objects. In order to handle the comment with a different session, you have to re-fetch it with `get_url` or whatever it is.", "created_utc": 1396051542, "gilded": 0, "name": "t1_cgekghl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/21mtuq/praw_multiple_bots_multiple_threads/"}, {"author": "vafrederico", "body": "Yeah, i discovered the link between session, but i thought it was possible to reassign a object to a reddit session, by doing like `comment.reddit_session` since it changes the session associated with, but it seems that it is not enough. EDIT: Also, how do I re-fetch a Comment object?", "created_utc": 1396051963, "gilded": 0, "name": "t1_cgekme9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/21mtuq/praw_multiple_bots_multiple_threads/"}, {"author": "tazzy531", "body": "This is the basic architecture that I used: https://github.com/tazzy531/reddit-screenshot-bot", "created_utc": 1396074423, "gilded": 0, "name": "t1_cges1hr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/21mtuq/praw_multiple_bots_multiple_threads/"}, {"author": "vafrederico", "body": "Thanks but not really what my problem is. Basically, you are only logging in one account with the parser, and using it to reply. What i want is to reply with different accounts, one for each bot. I've managed to do that now, i've just added above.", "created_utc": 1396108494, "gilded": 0, "name": "t1_cgeykqe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/21mtuq/praw_multiple_bots_multiple_threads/"}, {"author": "Phteven_j", "body": "Comments are pretty easy. If I remember, you just do: comments = subreddit.get_comments(limit=X) Check the code overview page on that site for a list of all functions you can use and their arguments.", "created_utc": 1395933683, "gilded": 0, "name": "t1_cgdamgl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/21i9iz/trying_to_write_something_simple_and_its_devolved/"}, {"author": "tazzy531", "body": "Take a look at my code: https://github.com/tazzy531/reddit-screenshot-bot I've basically done what you're looking for.", "created_utc": 1395934066, "gilded": 0, "name": "t1_cgdat3e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/21i9iz/trying_to_write_something_simple_and_its_devolved/"}, {"author": "_Daimon_", "body": "As of a recent change to Reddit yes you do. The recency of the is why this throws a 403 and not a ModeratorRequired exception.", "created_utc": 1395734528, "gilded": 0, "name": "t1_cgbbnqb", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/21ann0/praw_get_my_flair_in_a_particular_subreddit/"}, {"author": "glider_integral", "body": "too bad, I could use curl to obtain my flair but I think it doesn't worth the effort.", "created_utc": 1395762825, "gilded": 0, "name": "t1_cgbitk4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/21ann0/praw_get_my_flair_in_a_particular_subreddit/"}, {"author": "linuxydave", "body": ">I installed praw (i guess) What steps did you do? From the error message I take it you're running Mac OS?", "created_utc": 1395651355, "gilded": 0, "name": "t1_cgafyuw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/217ivx/im_a_noob_that_cant_seem_to_be_able_to_install/"}, {"author": "nissoPT", "body": "It's working now, i think i installed praw in the wrong folder.", "created_utc": 1395678959, "gilded": 0, "name": "t1_cganmn3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/217ivx/im_a_noob_that_cant_seem_to_be_able_to_install/"}, {"author": "linuxydave", "body": "Cool :)", "created_utc": 1395679181, "gilded": 0, "name": "t1_cganqk9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/217ivx/im_a_noob_that_cant_seem_to_be_able_to_install/"}, {"author": "nissoPT", "body": "thanks anyway :)", "created_utc": 1395679284, "gilded": 0, "name": "t1_cgansex", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/217ivx/im_a_noob_that_cant_seem_to_be_able_to_install/"}, {"author": "kemitche", "body": "Looks like you want to pass in \"cloudsearch\" as the syntax parameter.", "created_utc": 1395559884, "gilded": 0, "name": "t1_cg9mziu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": null, "body": "[deleted]", "created_utc": 1395589759, "gilded": 0, "name": "t1_cg9srnv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "kemitche", "body": "Hrm. PRAW docs aren't super clear, but if the arguments map the way I think they do, you need to send your query like so: query = scraper.search(query=\"(and text:'FOTD' %s\" % postperiod, subreddit='makeupaddiction',sort='new',syntax='cloudsearch') I *believe* the `period` parameter maps to the \"today/this week/this month/this year/all time\" drop down, not the semi-hidden timestamp stuff.", "created_utc": 1395590931, "gilded": 0, "name": "t1_cg9t7cv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "makeupdev", "body": "Thanks... this seems to return no submissions unfortunately, and only one if I pass in 'all time' or 'today' instead of the timestamp. I was basing the post period on [this](https://github.com/praw-dev/praw/issues/231) GitHub issue.", "created_utc": 1395591990, "gilded": 0, "name": "t1_cg9tlvq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "OnceAndForever", "body": "I don't think you need to bother with the `datetime` unless you need a specific time period. Otherwise sorting by new and setting your limit to 3000 will give you the latest posts that match your search terms. I got it to work using this code: import praw r = praw.Reddit('YOUR USER AGENT') subreddit = r.get_subreddit('MakeupAddiction') search = subreddit.search('FOTD', sort='new', limit=3000) for submission in search: print submission This returned 998 results, which is the same amount I got when using the [reddit search](http://www.reddit.com/r/MakeupAddiction/search?q=fotd&sort=new&restrict_sr=on). I don't believe reddit allows you look further back than 1000 posts, so if you needed to get 3000 entries you would have to run this search every few days and add the new entries to your list. Otherwise, you could try using the [after_field parameter](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit.get_content) and pass the thing_id of the oldest available entry to see if you can restart the search with the oldest entry as your starting point. Here is a small sample of my results: 3 :: [FOTD] I was feeling a little red today. First daytime outing without my... 47 :: I've been doing my makeup Korean style for a long time. I wanted to sho... 40 :: The lighting was too good to not take a bunch of pictures! [FOTD] 17 :: [FOTD] Haven't posted recently, and wanted to do something bright and f... 6 :: Tried something new, FOTD using MAC Dreaming Dahlia. CCW! 7 :: [FOTD] Standard Red Lip + Wings. CCW 18 :: Saturday FOTD. A little more dramatic eye than I'm used too! CCW 16 :: My FOTD, running around Manhattan. It finally feels like spring in NYC!... 320 :: FOTD: Using my Too Faced Natural Eye palette! I don't know what data you need for your project, but in the `for` loop you can type `print vars(submission)` to see the attributes available to you. I would set the limit on line 6 to a much smaller number when you check out the attributes though. There is also no need to do `time.sleep()` when using praw because praw handles that for you and already delays every request by 2 seconds. Hope this helps and leads you in the right direction.", "created_utc": 1395563277, "gilded": 0, "name": "t1_cg9njgj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "makeupdev", "body": "Thank you. I'm a little confused how to use the after_field parameter, maybe I'm not passing it correctly since I get the same problem: subreddit = scraper.get_subreddit('MakeupAddiction') query = subreddit.search('FOTD', sort='new', limit=998) for x in range(0,4): thing_id = self.do_query(query) if x != 3: query = subreddit.search('FOTD', sort='new', limit=15, after_field=thing_id) def do_query(self,query): thing_id = '' for post in query: thing_id = post.id # other filtering stuff return thing_id", "created_utc": 1395591684, "gilded": 0, "name": "t1_cg9thlj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "OnceAndForever", "body": "It looks like you're using the after_field parameter correctly, but I'm not positive if it works with search. I've just been playing around with this myself and I couldn't get more than 1000 posts with the search. Using the regular reddit search, RES would not let me go back further than 1000 posts. I don't know how quickly you need to finish this project, but you could also run this search every few days to build up a database. You could collect new FOTD posts as they appear. My other thought would be to simply store as many possible posts in /r/MakeupAddiction as you can, working your way backwards. Still, I'm not sure exactly how far you can go back, but let's say you get 10,000 posts. From there, you could search through those posts locally and then remove submissions that do not contain 'FOTD' in the title and narrow your 10,000 posts to hopefully around 3,000 that contain your search terms. Also, in the snippet you posted, what are you trying to do from lines 3-6? I get that you're trying to get the oldest thing_id, but I'm not sure why you're using a for loop and iterating 4 times? Perhaps it makes more sense within the context of the other code you have. Here is a gist of what I have been playing with so far to get my results: https://gist.github.com/anonymous/f3782e025af2f0d86494 I've been storing everything in an SQLite3 database instead of using a csv. SQLite3 can export as a csv though if you need the data in that format.", "created_utc": 1395606227, "gilded": 0, "name": "t1_cg9zjpb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "KnightsWhoSayNiBot", "body": "Pleas ignore this comment.It is a test of the initial programming of this bot. Please have an upvote in compensation", "created_utc": 1395543731, "gilded": 0, "name": "t1_cg9iaxf", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/213ht6/want_to_grab_last_3000_records/"}, {"author": "bboe", "body": "`send_message` exists in two contexts. Either the context of the user you want to send the message to: r.get_user('Harakou').send_message('subject', 'message') or a version that is directly called upon the `reddit` instance which you can specify the recipient: r.send_message('Harakou', 'subject', 'message')", "created_utc": 1395419895, "gilded": 0, "name": "t1_cg8d39u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20ykci/having_some_trouble_with_praw_send_message/"}, {"author": "Harakou", "body": "That clears things up a lot for me, thank you! Now I understand why it was behaving that way.", "created_utc": 1395420594, "gilded": 0, "name": "t1_cg8ded2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20ykci/having_some_trouble_with_praw_send_message/"}, {"author": null, "body": "[deleted]", "created_utc": 1395197501, "gilded": 0, "name": "t1_cg670oa", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20r8nb/any_praw_wizards_in_austin/"}, {"author": "editer63", "body": "Getting back to you: This helped. I got together with a Python whiz, and we used a good portion of the code in that script. Thanks again!", "created_utc": 1395526222, "gilded": 0, "name": "t1_cg9brbn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20r8nb/any_praw_wizards_in_austin/"}, {"author": "editer63", "body": "Thanks! I'll give it a look.", "created_utc": 1395200198, "gilded": 0, "name": "t1_cg686hi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20r8nb/any_praw_wizards_in_austin/"}, {"author": "andytuba", "body": "You should maybe xpost to /r/austin too.", "created_utc": 1395196995, "gilded": 0, "name": "t1_cg66sgp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20r8nb/any_praw_wizards_in_austin/"}, {"author": "editer63", "body": "Thanks, I hadn't thought of that.", "created_utc": 1395200241, "gilded": 0, "name": "t1_cg68750", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20r8nb/any_praw_wizards_in_austin/"}, {"author": "_Daimon_", "body": "That really depends on how RES stores and retrieves information. I don't know much about RES implementation or advanced functionality. I would ask or crosspost in /r/enhancement", "created_utc": 1394744655, "gilded": 0, "name": "t1_cg1wcx2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20bmac/coordinating_res_and_praw/"}, {"author": "nnnannn", "body": "Thanks for the advice. I did some poking around in /r/Enhancement and it seems like my second idea is bad because it could too easily fill up RES's JSON file, which apparently can [cause problems](http://fr.reddit.com/r/Enhancement/comments/1nnq17/i_have_a_warning_concerning_user_tags/). My first idea *should* be possible, but I've yet to discern how to access RESStorage outside of the RES command line.", "created_utc": 1394761580, "gilded": 0, "name": "t1_cg23yym", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20bmac/coordinating_res_and_praw/"}, {"author": "Plague_Bot", "body": "I haven't done any testing on this so I can't confirm, but it looks like RES [stores its data locally](http://redditenhancementsuite.com/wiki/index.php?title=Where_is_RES%27_data_stored%3F) at this address (Chrome/Windows 7): C:\\Users\\\\AppData\\Local\\Google\\Chrome\\User Data\\Default\\Local Storage\\chrome-extension_kbmfpngjjgdllneeigpgjifpgocmfgmb_0.localstorage Sneaking a peak at the data, it looks like it's stored in SQLite format. Presumably you could use python to modify this file, although again, I haven't tested this.", "created_utc": 1394845357, "gilded": 0, "name": "t1_cg2wmq7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20bmac/coordinating_res_and_praw/"}, {"author": "eugay", "body": "This emoticon requires a double backslash on reddit because of markdown. Could that be the cause?", "created_utc": 1394692458, "gilded": 0, "name": "t1_cg1fn69", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20aehd/trying_to_get_notified_when_a_certain_string_\u30c4_is/"}, {"author": "SOTB-human", "body": ">>> print u'\u00af\\_(\u30c4)_/\u00af' \u00af\\_(\u30c4)_/\u00af >>> print u'\u00af\\\\_(\u30c4)_/\u00af' \u00af\\_(\u30c4)_/\u00af >>> print u'\u00af\\\\\\_(\u30c4)_/\u00af' \u00af\\\\_(\u30c4)_/\u00af >>> print u'\u00af\\\\\\\\_(\u30c4)_/\u00af' \u00af\\\\_(\u30c4)_/\u00af It gets confusing because you're encoding markdown as a Python string, while markdown itself is an encoding of the text displayed on the screen. So, I think you actually need a triple or quadruple backslash in the Python code to represent a single backslash in the displayed comment.", "created_utc": 1394712206, "gilded": 0, "name": "t1_cg1imtc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20aehd/trying_to_get_notified_when_a_certain_string_\u30c4_is/"}, {"author": "Actualacc", "body": "I'm looking to find when people mess up the face and type \"\u00af\\_(\u30c4)_/\u00af\" or \"\u00af\\\\_(\u30c4)_/\u00af\". Should I use two and four?", "created_utc": 1394729860, "gilded": 0, "name": "t1_cg1oz3u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20aehd/trying_to_get_notified_when_a_certain_string_\u30c4_is/"}, {"author": "SOTB-human", "body": "I think so. I'm pretty sure the reason why people mess it up is because they don't escape the backslashes properly in markdown. \u00af\\_(\u30c4)_/\u00af \u00af\\\\_(\u30c4)_/\u00af \u00af\\\\\\_(\u30c4)_/\u00af [Source of this comment](http://i.imgur.com/vAofDzG.png)", "created_utc": 1394731255, "gilded": 0, "name": "t1_cg1pmv6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20aehd/trying_to_get_notified_when_a_certain_string_\u30c4_is/"}, {"author": "Dvorak_Simplified_Kb", "body": "Is your code which was supposed to be in the while loop not indented (so the loop is empty) or is that just a copy paste error?", "created_utc": 1394687880, "gilded": 0, "name": "t1_cg1edp9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20aehd/trying_to_get_notified_when_a_certain_string_\u30c4_is/"}, {"author": "Actualacc", "body": "Just a copy paste error, sorry.", "created_utc": 1394688235, "gilded": 0, "name": "t1_cg1ehlo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20aehd/trying_to_get_notified_when_a_certain_string_\u30c4_is/"}, {"author": "_Daimon_", "body": "Update your version of PRAW to 2.1.13 or later and it will work like the other listings like get_hot. pip install praw -U There was a [recent change to banlist](http://www.reddit.com/r/changelog/comments/1ydu2d/reddit_change_paginated_ban_lists/) that fundamentally changed how the data for banlists are returned, which means older versions of PRAW doesn't really understand these listings anymore. Shoutout to **Andre-d** for sending the [pull request](https://github.com/praw-dev/praw/pull/280) that updated PRAWs functionality to handle the new userlistings.", "created_utc": 1394744573, "gilded": 0, "name": "t1_cg1wbha", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/20a9hl/how_to_iterate_the_ban_list_with_praw/"}, {"author": "scandinavian-", "body": "Something like this I guess. from requests.exceptions import HTTPError ... while True: try: subreddit = r.get_subreddit('dota2') for submission in subreddit.get_new(limit=15): # make it lowercase for now #blah blah, more code here that wont be executed if there is an exception except HTTPError: print \"Timeout or whatever\" pass #The while loop will be continued except: print \"Other exceptions, will stop the script if we don't pass\"", "created_utc": 1394534831, "gilded": 0, "name": "t1_cfzqjr0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/204i97/learning_python_cant_deal_with_httperrors/"}, {"author": "DarkMio", "body": "It is easy like that? Nothing more, nothing less? Cool! I will try it out!", "created_utc": 1394536367, "gilded": 0, "name": "t1_cfzqsco", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/204i97/learning_python_cant_deal_with_httperrors/"}, {"author": "scandinavian-", "body": "Just be aware that it will not execute the rest of the for loop if there is an exception, so it will instantly jump to the while loop again. Therefore it will not sleep(120).", "created_utc": 1394536689, "gilded": 0, "name": "t1_cfzqu75", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/204i97/learning_python_cant_deal_with_httperrors/"}, {"author": "DarkMio", "body": "then I will just add this line: while True: try: #stuff except HTTPError: # stuff time.sleep(300) except: print \"Standby, unknown error.\" time.sleep(600)", "created_utc": 1394536943, "gilded": 0, "name": "t1_cfzqvp0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/204i97/learning_python_cant_deal_with_httperrors/"}, {"author": "Leverno", "body": "Do note that with this code you are catching the HTTPError exception and other exceptions, but you do not know which exceptions are actually raised. Depending on how verbose you want the exception handling to be, you can print just the string representation of an exception, or even the entire traceback. import traceback while True: try: #stuff except HTTPError as error: # stuff print \"HTTPError caught: {0}, HTTP Error Code: {1}\".format(str(error), error.code) traceb = str(traceback.format_exc()) print traceb time.sleep(300) except Exception as error: print \"Standby, caught error: {0}\".format(str(error)) traceb = str(traceback.format_exc()) print traceb time.sleep(600)", "created_utc": 1394554736, "gilded": 0, "name": "t1_cfzwhgh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/204i97/learning_python_cant_deal_with_httperrors/"}, {"author": "scandinavian-", "body": "Sure thing, just remember to pass when you want execution to continue after the try block.", "created_utc": 1394537263, "gilded": 0, "name": "t1_cfzqxmq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/204i97/learning_python_cant_deal_with_httperrors/"}, {"author": "DarkMio", "body": "Thanks for all the help! :)", "created_utc": 1394538182, "gilded": 0, "name": "t1_cfzr3ga", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/204i97/learning_python_cant_deal_with_httperrors/"}, {"author": "nemec", "body": "Maybe it got banned because it does nothing to contribute to any conversation?", "created_utc": 1394493717, "gilded": 0, "name": "t1_cfzdo4t", "num_comments": null, "score": 8, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2032ul/what_is_wrong_with_this_bot/"}, {"author": "bassguitarman", "body": "It hasn't worked once", "created_utc": 1394493822, "gilded": 0, "name": "t1_cfzdpw1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2032ul/what_is_wrong_with_this_bot/"}, {"author": "LungFungus", "body": "You are looping over comments of r/all, the only time your bot takes action is if one of those comments is from your friend. The problem is that you are not exhaustively looping over all comments made to r/all, so the likelihood of finding a comment from your friend is low. For a better way of searching comments for your friend, why not just periodically grab the comments he actually makes? Alternatively, maybe look at something like praw's comment stream helper.", "created_utc": 1394520516, "gilded": 0, "name": "t1_cfzo741", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2032ul/what_is_wrong_with_this_bot/"}, {"author": "bassguitarman", "body": "How would I do that?", "created_utc": 1394567396, "gilded": 0, "name": "t1_cg02fhk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2032ul/what_is_wrong_with_this_bot/"}, {"author": "LungFungus", "body": "It looks like you're already doing it somewhat with user.get_comments, I believe user.get_comments only returns comments for that user. I would do something like: import time import praw from random import randint r=praw.Reddit(\"Annoybot\") r.login(\"annoysterninator\",\"secret\") responses = [\"lick\", \"I found the vegan!\", \"DdddddddDrop the bass\", \"twerk eth twerk eth twerk\", \"what nice leg muscles you have\", \"The qua-a-adratic formula\", \"it is ethjck twerk music swag money\"] already_seen = set() while True: user = r.get_redditor(\"bassguitarman\") user_comments = user.get_comments(limit=25) # tweak this for comment in user_comments: if comment.id not in already_seen: already_seen.add (comment.id) reply = responses[randint(0, len(responses) - 1)] comment.reply(reply) time.sleep(60 * 60) # sleep for an hour. likely no need to constantly check I didn't test this, I essentially just tweaked your code. So if there was some other bug in there, it will still be there. If you get errors, you can post them. Food for though: if your bot crashes, and you restart it, you will very likely re-reply to some comment. I recommend storing the already_seen set to disk somehow, perhaps just a file with a comment id per line for something really simple.", "created_utc": 1395540147, "gilded": 0, "name": "t1_cg9gzmm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/2032ul/what_is_wrong_with_this_bot/"}, {"author": "gavin19", "body": "I can't see anything wrong with what you have, although reddit.login(username = username, password = password) should just be reddit.login(username, password) I think your issue is simply that [the comment](http://www.reddit.com/r/pokemon/comments/1zxpx2/art_porygon_decides_to_trace_back_its_roots/cfycwfg) wasn't one of the ones that was fetched. IIRC, get_comments by itself, when you ran it, would have only grabbed the first 100 from /r/pokemon/comments. Use a test subreddit and set up a few dummy posts/comments. That way, you don't have to fetch a bunch of comments you don't need and it makes it easier to test these kind of things.", "created_utc": 1394467675, "gilded": 0, "name": "t1_cfz15lf", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/201vo4/praw_having_trouble_with_comment_parsing/"}, {"author": "myessail", "body": "Yep, the comment wasn't one of the ones fetched. Everything else was working fine. Thank you!", "created_utc": 1394486882, "gilded": 0, "name": "t1_cfzafhx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/201vo4/praw_having_trouble_with_comment_parsing/"}, {"author": "TacticalNarwhal", "body": "I'm kindof new to Reddit but I usually just wait my hour so I can post more. It's just their way of preventing their servers from getting overloaded and it's also a bot prevention method. \"What to do?\": Wait it out.", "created_utc": 1394269827, "gilded": 0, "name": "t1_cfxemox", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zurpt/praw_facing_quota_error_while_submit_more_than/"}, {"author": "ravik78c", "body": "Actually I have my local instance of redit installed and I want to remove this check from it.", "created_utc": 1394305350, "gilded": 0, "name": "t1_cfxn3d9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zurpt/praw_facing_quota_error_while_submit_more_than/"}, {"author": "_Daimon_", "body": "PRAW doesn't inhibit the rate you can submit stuff, only reddit does that. There are some lower limit on how often you can submit stuff and this is based on your karma both total and on a subreddit level. So there's fewer restrictions on submissions in a subreddit where you've gained loads of karma, then in one where you've never posted before like r/test or other testing subreddits.", "created_utc": 1394217512, "gilded": 0, "name": "t1_cfwvuhn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "LinkFixerBotSnr", "body": "/r/test ***** ^This ^is ^an [^automated ^bot](http://github.com/WinneonSword/LFB)^. ^For ^reporting ^**problems**, ^contact ^/u/WinneonSword.", "created_utc": 1394217538, "gilded": 0, "name": "t1_cfwvux4", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "ravik78c", "body": "I have a local reddit instance installed on my local server. Is there a way to remove this check and submit as many links as I want?", "created_utc": 1394219787, "gilded": 0, "name": "t1_cfwwx8i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "_Daimon_", "body": "Yes. I don't have a local instance running on my machine, so I cannot test this myself. But looking at reddit's code I would say it looks like the ratelimiting happens [here](https://github.com/reddit/reddit/blob/master/r2/r2/controllers/api.py#L373) and changing this code should allow you to submit posts without ratelimits.", "created_utc": 1394221894, "gilded": 0, "name": "t1_cfwxxt4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "ravik78c", "body": "According to my information RATE_LIMIT exception is thrown by the server if the something has happened too frequently. My error which is being thrown is : praw.errors.APIException: (QUOTA_FILLED) `You've submitted too many links recently. Please try again in an hour.` on field `None` If it can be fixed by RATE_LIMIT then I can change the respective tag in the configuration file (development.ini) in my reddit source code. But I think the problem is somewhere else. Please correct me if I am wrong.", "created_utc": 1394226910, "gilded": 0, "name": "t1_cfx0b8u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "ravik78c", "body": "I also tried to submit more links with RATE_LIMIT=0 in the development.ini file in my reddit source code but It is throwing the same error mentioned above.", "created_utc": 1394235570, "gilded": 0, "name": "t1_cfx42rn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zrysj/praw_how_many_submissions_can_be_made_using_praw/"}, {"author": "chpwssn", "body": "I actually asked this 35 days ago. http://www.reddit.com/r/redditdev/comments/1wkn3s/praw_on_a_reddit_clone/ Edit: sorry for the lazy link I'm on mobile", "created_utc": 1394155555, "gilded": 0, "name": "t1_cfwc9hu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zroo7/praw_using_praw_for_local_reddit_instance/"}, {"author": "ravik78c", "body": "I am able to able to submit links now to my local instance. Thanks for the help but I know about the link you are referring to. 'READTHEDOCS' site is not clear as what to do exactly to do this. However, I can submit the links after logging into my local instance but after submitting the link it is giving this error - >>> r.submit('REDDIT_TEST1', 'REDDIT_API_TEST2', text=None, url='www.google.com', captcha=None, save=True, send_replies=None) Traceback (most recent call last): File \"\", line 1, in File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 225, in wrapped return function(obj, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 2029, in submit return self.get_submission(url) File \"/usr/local/lib/python2.7/dist-packages/praw/__init__.py\", line 891, in get_submission params=params) File \"/usr/local/lib/python2.7/dist-packages/praw/decorators.py\", line 323, in wrapped return function(cls, *args, **kwargs) File \"/usr/local/lib/python2.7/dist-packages/praw/objects.py\", line 851, in from_url submission.comments = c_info['data']['children'] AttributeError: 'dict' object has no attribute 'comments' Link is getting submitted though. But I suppose I can add only one link in one hour which is not good for my project as I need to add unlimited links to my local instance.", "created_utc": 1394156697, "gilded": 0, "name": "t1_cfwcrj8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zroo7/praw_using_praw_for_local_reddit_instance/"}, {"author": "_Daimon_", "body": "Hi, you're looking for the `over_18` boolean that both `Submission` and `Subreddit` objects have.", "created_utc": 1393962570, "gilded": 0, "name": "t1_cfubgh2", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "IAmAnAnonymousCoward", "body": "Oh I need to use `fetch=True` to see all the attributes. Not quite sure why it's necessary with `Subreddit` objects but not with `Submission` objects. Interestingly it's `over18` instead of `over_18` for `Subreddit` objects as well.", "created_utc": 1402756829, "gilded": 0, "name": "t1_ci7drct", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "_Daimon_", "body": "That's because of lazy objects, which PRAW utilizes to greatly optimize the run time of projects. See this article https://praw.readthedocs.org/en/v2.1.16/pages/lazy-loading.html in our documentation for more details.", "created_utc": 1402771404, "gilded": 0, "name": "t1_ci7iyz3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "IAmAnAnonymousCoward", "body": "> Hi, you're looking for the `over_18` boolean that both `Submission` and `Subreddit` objects have. Do `Subreddit` objects have that attribute? This is all I'm getting: {'_info_url': 'http://www.reddit.com/r/RealGirls/about/', '_listing_urls': ['http://www.reddit.com/r/RealGirls/new/.json', 'http://www.reddit.com/r/RealGirls/.json', 'http://www.reddit.com/r/RealGirls/top/.json', 'http://www.reddit.com/r/RealGirls/controversial/.json', 'http://www.reddit.com/r/RealGirls/rising/.json'], '_underscore_names': None, '_url': 'http://www.reddit.com/r/RealGirls/', 'display_name': 'RealGirls', 'has_fetched': False, 'json_dict': None, 'reddit_session': }", "created_utc": 1402754842, "gilded": 0, "name": "t1_ci7d7mv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "Abe_Linkin", "body": "Thank you!", "created_utc": 1393962651, "gilded": 0, "name": "t1_cfubhvw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "_Daimon_", "body": "No problem. Btw, check out the tutorial /u/gavin19 linked to. It shows how to introspect objects to find out stuff like this.", "created_utc": 1393963438, "gilded": 0, "name": "t1_cfubvit", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "gavin19", "body": "From the [tutorial](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html#finding-what-we-need). See the *over18* property.", "created_utc": 1393962606, "gilded": 0, "name": "t1_cfubh4d", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "procrastinatorDaz", "body": "fs", "created_utc": 1394103278, "gilded": 0, "name": "t1_cfvrdy8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "procrastinatorDaz", "body": "das", "created_utc": 1394103309, "gilded": 0, "name": "t1_cfvre47", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zk07r/praw_does_praw_allow_you_to_see_if_a_post_is/"}, {"author": "slyf", "body": "Their Kind is \"Listing\": More info at the top of http://www.reddit.com/dev/api", "created_utc": 1393960979, "gilded": 0, "name": "t1_cfuao2i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zhcrq/enumerating_the_ban_list/"}, {"author": "brucemo", "body": "Thank you for the answer. Subsequent calls to \"get_banned\", which include \"count\" or \"after\", return an API error that says that the keyword is unexpected. Am I supposed to call something other than \"get_banned\" the second time?", "created_utc": 1394396131, "gilded": 0, "name": "t1_cfyej84", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zhcrq/enumerating_the_ban_list/"}, {"author": "_Daimon_", "body": "Yes. comment.created_utc returns a utc timestamps of the creation of the comment. There's also an attribute called created, which is the timesstamp of creation *in the timezone of the server*. Which is very unreliable. Don't use this. Always use created_utc.", "created_utc": 1393881406, "gilded": 0, "name": "t1_cftigcz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zgt0d/basic_praw_question_getting_comment_posted_time/"}, {"author": "TheTretheway", "body": "Thanks!", "created_utc": 1393882407, "gilded": 0, "name": "t1_cftiym9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zgt0d/basic_praw_question_getting_comment_posted_time/"}, {"author": "saltyjohnson", "body": "For your reference, https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html Scroll down a few paragraphs and you'll see all the attributes and their values when you look at a specific submission in PRAW. ^(I may have posted this from the wrong account at first. My apologies.)", "created_utc": 1393984365, "gilded": 0, "name": "t1_cfulyk5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1zgt0d/basic_praw_question_getting_comment_posted_time/"}, {"author": "_Daimon_", "body": "What version of python are you using? EDIT: Also, what's your version of six? EDIT2: Problem found. PRAW now requires version 1.4 or later of the six module. To fix this problem run pip install six==1.4 or pip install six -U if you want the latest version. Which is better ff you don't have any need for a specific version of six, say you only have it installed due to PRAW. I'm running the test suite with six 1.4 installed now, if no problems occur I'll push another version out tonight. Thanks for bringing this to my attention.", "created_utc": 1393278239, "gilded": 0, "name": "t1_cfnptyn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ytpmx/latest_praw_not_working/"}, {"author": "unigee", "body": "I just upgraded six to the latest version and the problem is solved. Thank you", "created_utc": 1393279598, "gilded": 0, "name": "t1_cfnqiqa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ytpmx/latest_praw_not_working/"}, {"author": "unigee", "body": "My python version is 2.7.5 and my six version is 1.3.0", "created_utc": 1393279459, "gilded": 0, "name": "t1_cfnqgah", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ytpmx/latest_praw_not_working/"}, {"author": "_Daimon_", "body": "Yeah, the version of six is the problem. See my edit. I'm sorry you're experiencing this problem.", "created_utc": 1393279673, "gilded": 0, "name": "t1_cfnqk4p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ytpmx/latest_praw_not_working/"}, {"author": "gavin19", "body": "Rough guess. try: comment.reply(trigger_reply % (comment.author, parent_author)) except requests.exceptions.HTTPError: pass", "created_utc": 1393096305, "gilded": 0, "name": "t1_cflyx5e", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1yn3ae/praw_how_to_keep_bot_from_crashing_when_it_fails/"}, {"author": "chancrescolex", "body": "Thanks, that seems to work for now.", "created_utc": 1393098999, "gilded": 0, "name": "t1_cflzyy4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1yn3ae/praw_how_to_keep_bot_from_crashing_when_it_fails/"}, {"author": "RisingStar", "body": "I would also look at possibly saving the sub-reddit to a config file as a banned sub so that you don't waste effort trying to post to it.", "created_utc": 1393096592, "gilded": 0, "name": "t1_cflz17r", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1yn3ae/praw_how_to_keep_bot_from_crashing_when_it_fails/"}, {"author": "gavin19", "body": "That would be ideal - and make the error handling specific to the 403/Forbidden so that it's not just adding subs based on any old HTTPError.", "created_utc": 1393096895, "gilded": 0, "name": "t1_cflz5d5", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1yn3ae/praw_how_to_keep_bot_from_crashing_when_it_fails/"}, {"author": "IAmAnAnonymousCoward", "body": "Don't forget you get forbidden as well for subreddits that go private, more often than not it's only temporary.", "created_utc": 1393097661, "gilded": 0, "name": "t1_cflzg66", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1yn3ae/praw_how_to_keep_bot_from_crashing_when_it_fails/"}, {"author": "ExceedinglyEdible", "body": "The `for ... in ...` statement does not work this way. Here is how it is used: for item in iterable: do_something(item) Therefore, the `for .. in ...` statement loops over an iterable, assigning the item that it is currently iterating over to a readily available variable. The C-like language equivalent would look like this: for (i=0; i", "created_utc": 1393040057, "gilded": 0, "name": "t1_cflkgls", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ylb52/searching_comments_for_two_words_regardless_of/"}, {"author": "ExceedinglyEdible", "body": "By the way, your use of `or` seems appropriate. Watch for operator precedence though. In boolean logic, `and` is often compared to multiplication and `or` to addition, therefore use the same precedence as you would for those operations. This: if a in list or b in list and general_condition is True; ... is not the same as: if (a in list or b in list) and general_condition is True: ...", "created_utc": 1393040459, "gilded": 0, "name": "t1_cflklsf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ylb52/searching_comments_for_two_words_regardless_of/"}, {"author": "argutus1", "body": "I fixed it earlier, I just forgot to reupload the code, check it now. --- I have gotten the bot to start and finish, however I am having a problem with the bot: conf_count = str(comment.replies).lower().count(\"confirmed\") This line does not work, it returns zero even if it should be two or three. What am I doing wrong?", "created_utc": 1393040328, "gilded": 0, "name": "t1_cflkk69", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ylb52/searching_comments_for_two_words_regardless_of/"}, {"author": "ExceedinglyEdible", "body": "`Comment.replies` returns a list of comments. Transforming it to a string only returns the textual representation of that Python object. >>> str(comment.replies) '[, ]' Also, as a general recommendation, since determining if a string can be found inside another is probably only marginally more efficient than counting its occurrences altogether, you should not check for presence first; just count the occurrences from the start. If that number is 0, then you have no match. The only case that I can think of where \"string in text\" would be faster is if the string can be found near the beginning of the text. ---- Try this: conf_count = sum([reply.body.count(\"confirmed\") for reply in comment.replies]) In Python, this is called a [list comprehension](http://docs.python.org/2/tutorial/datastructures.html#list-comprehensions) (the syntax is `[ for thing in iterable]`). Basically, this line will build a list made of the counts of \"confirmed\" found in every comment body that is a reply to your initial comment.", "created_utc": 1393041190, "gilded": 0, "name": "t1_cflkuxa", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ylb52/searching_comments_for_two_words_regardless_of/"}, {"author": "argutus1", "body": "This worked, however it had one flaw. Some users replied \"confirmed\" to another comment that says \"confirmed\". Is there a way that I can have the bot check the full tree of replies in the chance this happens again? Example: [BOUGHT] ... [SOLD] ... | |(Reply to Post) |_Confirmed | |(Reply to Reply) |_Confirmed", "created_utc": 1393042112, "gilded": 0, "name": "t1_cfll6mv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ylb52/searching_comments_for_two_words_regardless_of/"}, {"author": "ExceedinglyEdible", "body": "Look up \"flattening\" a list, which basically extracts every leaf in a tree and lines them up into a single list.", "created_utc": 1393042506, "gilded": 0, "name": "t1_cfllbna", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ylb52/searching_comments_for_two_words_regardless_of/"}, {"author": "argutus1", "body": "Alright, thanks", "created_utc": 1393042982, "gilded": 0, "name": "t1_cfllhsj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ylb52/searching_comments_for_two_words_regardless_of/"}, {"author": "_Daimon_", "body": "Here is the [issue raised with urllib3](https://github.com/shazow/urllib3/issues/238) for your problem. PRAW uses requests which uses urllib3, so the warning will go away when you have a version of urllib3 with the merged fix. However, from what I can see it *should* be in the latest version of requests. Which it obviously isn't. The fix was included in version 1.71 of urllib3, which was created 5 months ago and the version of urllib included within requests is 1 month old, or more precisely [this commit](https://github.com/shazow/urllib3/commit/9346c5c8b97ccd06d4210401f3fdb9ed71ed1cd3). Looking at the [release history of requests](https://pypi.python.org/pypi/requests/2.2.1) there should have been several releases after the fix was included in urllib3. Either I misread something and your issue is related to something non fixed or there's something else I've misunderstood about the inclusion of urllib3 into requests and their release schedule. If you want more info, then you should raise an issue with urllib3/requests or send them a mail. Either way. This is a *warning*, not an *error* and it is harmless with python version below", "created_utc": 1392929135, "gilded": 0, "name": "t1_cfkeznc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "colorcodebot", "body": "I've detected a hexadecimal color code in your comment. Please allow me to provide visual representation. [#879208](http://color.re/879208.png) *** [^^Learn ^^more ^^about ^^me](http://color.re) ^^| ^^Don't ^^want ^^me ^^replying ^^on ^^your ^^comments ^^again? ^^Respond ^^to ^^this ^^comment ^^with: ^^'colorcodebot ^^leave ^^me ^^alone'", "created_utc": 1392929139, "gilded": 0, "name": "t1_cfkezpv", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "m1ss1ontomars2k4", "body": "Wat. That wasn't even his comment. lrn2markdown", "created_utc": 1392948422, "gilded": 0, "name": "t1_cfknzad", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "chancrescolex", "body": "It read the end of the link from stackoverflow - \"http://stackoverflow.com/questions/879173/how-to-ignore-deprecation-warnings-in-python/879208#879208\"", "created_utc": 1392948564, "gilded": 0, "name": "t1_cfko1ke", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "m1ss1ontomars2k4", "body": "Yes, but it's not in the actual text of the comment, since the text of the link is \"this stackoverflow\".", "created_utc": 1392948669, "gilded": 0, "name": "t1_cfko387", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "chancrescolex", "body": "Yeah, the bot's reading the comment source which it probably shouldn't be", "created_utc": 1392948912, "gilded": 0, "name": "t1_cfko6yf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "_Daimon_", "body": "colorcodebot don't be incompetent", "created_utc": 1392929252, "gilded": 0, "name": "t1_cfkf1wc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ygwc7/requests_deprecationwarning_when_trying_to_log_in/"}, {"author": "markerz", "body": "Interesting. With just the math, 4500 comments / (25 minutes * 30 requests / minute) makes for 6 comments per request. I don't use PRAW and I believe the it should already employ these optimizations but you can specify the limit query parameter which allows you to pull more comments per request. For example, http://reddit.com/?limit=100 gives you the first 100 links from the front page. Alternatively, I believe the max for comments is 500 and even higher if you have reddit gold. Edit: just did a quick test and got this message: Sorry, the maximum number of comments is 500. (However, if you subscribe to reddit gold, it goes up to 1500.) The more children api should support it all the same.", "created_utc": 1392753656, "gilded": 0, "name": "t1_cfik168", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y9lu7/praw_replace_more_comments_rate_limiting/"}, {"author": "bboe", "body": "If you spend some time with the \"load more comments\" links on the site you'll notice that for many of those links, the browser will only pull in 1 additional comment. The API suffers the same limitation. If you want a speed improvement, use a reasonable threshold for `replace_more_comments` so that it only makes requests that yield a reasonable number of comments. Edit: For posterity, when I wrote the more comments handling in PRAW I tried to group all the unfetched comment ids together to reduce the number of requests. However, reddit's API, does not work in that way, thus if you're trying to fetch a large comment tree it's going to take some time.", "created_utc": 1392761711, "gilded": 0, "name": "t1_cfioa13", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y9lu7/praw_replace_more_comments_rate_limiting/"}, {"author": "kemitche", "body": "Filtering of the fields returned (requesting specific fields) is not supported. For now, you have to read in the whole response, and pull out the fields you're interested in.", "created_utc": 1392751020, "gilded": 0, "name": "t1_cfiipct", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y9hzq/how_can_i_limit_the_information_returned_in_an/"}, {"author": "_Daimon_", "body": "You can't. It's not a feature in reddit so PRAW cannot implement it. See your own inbox http://www.reddit.com/message/inbox/ and notice there's no way to delete it in the webview. If this is the case then it would be extremely unlikely to be possible via the API which PRAW uses.", "created_utc": 1392736987, "gilded": 0, "name": "t1_cficacy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y8gt3/what_is_the_proper_way_to_delete_a_message/"}, {"author": "MetricPleaseBot", "body": "Wonderful. In that case is it possible to poll only unread messages with messages = list(r.get_inbox()) messlen = len(messages)", "created_utc": 1392737154, "gilded": 0, "name": "t1_cficclk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y8gt3/what_is_the_proper_way_to_delete_a_message/"}, {"author": "_Daimon_", "body": "[`get_unread`](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.PrivateMessagesMixin.get_unread) is like `get_inbox` except it only returns unread messages. You can use that instead to easily poll for unread messages :) On a sidenote. It would be great if you *could* download messages.", "created_utc": 1392738970, "gilded": 0, "name": "t1_cfid2s7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y8gt3/what_is_the_proper_way_to_delete_a_message/"}, {"author": "bboe", "body": "There's a bug in how reddit's json for a submission returns the equivalent of \"load more comments\" and \"continue this thread\" as they both appear as a \"More\" type in the json. Take a look here for instance: http://www.reddit.com/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/c5w6n63 Notice the \"continue this thread\" link which takes you to: http://www.reddit.com/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/c64b5qb First of all, even through the web interface, when you specify to limit the number of comments, you will be oblivious to the fact that the above comment exists (after loading more comments): http://www.reddit.com/r/redditdev/comments/yjk55/proposed_change_to_the_users_online_count_for_low/c5w6n63?limit=14 The primary issue, however, is that the \"continue this thread\" in the json view is not correct as it appears like: \"kind\":\"more\", \"data\":{ \"count\":0, \"parent_id\":\"t1_c64b5qb\", \"children\":[ ], \"name\":\"t1__\", \"id\":\"_\" } PRAW ignores such requests which means all \"continue this thread\" comments are not being included. If you find a nice way to handle that problem, please consider a PRAW pull-request. Regarding the sort order, you can specify the comment sort order in `get_submission`: https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.UnauthenticatedReddit.get_submission", "created_utc": 1392762983, "gilded": 0, "name": "t1_cfiox5t", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y7pq7/help_with_retrieving_all_comments_for_a_given/"}, {"author": "jonathan_morgan", "body": "I will look into it, see if I can fins something elegant. I have a collector doing a first collection on posts that have a comment count under 1500, since it looks like for posts with comment counts under your limit, there are no mores, so I have a little time to play with this while I wait for it to finish. For the post I was testing on, setting the comment_order to \"old\" and the comment_limit to 1500 in the get_submission pulled back 14020, but the time order is still a little wonky. But that is fine. I'm intrigued by the differences between the \"mores\". I'll let you know if I figure out anything in terms of dealing with it (I'll have to wade into some JSON). Thanks for the help. Glad to know it wasn't just me.", "created_utc": 1392778006, "gilded": 0, "name": "t1_cfiw29f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y7pq7/help_with_retrieving_all_comments_for_a_given/"}, {"author": "slyf", "body": "get_stylesheet() and set_stylesheet() on a subreddit object", "created_utc": 1392662260, "gilded": 0, "name": "t1_cfhm9db", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y5t6e/need_help_gettingupdating_css/"}, {"author": "timotab", "body": "I'm an idiot. I meant the sidebar, not the stylesheet", "created_utc": 1392677878, "gilded": 0, "name": "t1_cfhtnd9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y5t6e/need_help_gettingupdating_css/"}, {"author": "slyf", "body": "use .update_settings(description=new_sidebar_content)", "created_utc": 1392678065, "gilded": 0, "name": "t1_cfhtqli", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y5t6e/need_help_gettingupdating_css/"}, {"author": "timotab", "body": "Thanks. It looks like I can use the get_settings() method which will give me a dictionary, from which I can grab the current sidebar.", "created_utc": 1392678148, "gilded": 0, "name": "t1_cfhts00", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y5t6e/need_help_gettingupdating_css/"}, {"author": "timotab", "body": "Thanks! That should give me enough to go on. I'll figure this out :)", "created_utc": 1392662368, "gilded": 0, "name": "t1_cfhmb2f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y5t6e/need_help_gettingupdating_css/"}, {"author": "halaal_sandwich", "body": "Ah, this will come in handy", "created_utc": 1392664443, "gilded": 0, "name": "t1_cfhn7gx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y5o0i/antiabuse_functions_for_new_botmasters_using_praw/"}, {"author": "Wiki_Bot", "body": "Well, that might have been useful a few hours ago...", "created_utc": 1392685475, "gilded": 0, "name": "t1_cfhx0nh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y5o0i/antiabuse_functions_for_new_botmasters_using_praw/"}, {"author": "acini", "body": "Line 18 try if str(submission.id) not in already_done", "created_utc": 1392655598, "gilded": 0, "name": "t1_cfhjkws", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y4t0j/praw_spamming_issue/"}, {"author": "MrBonez", "body": "Nope, it still replies to the same post again. Thanks for trying.!", "created_utc": 1392715183, "gilded": 0, "name": "t1_cfi7ene", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y4t0j/praw_spamming_issue/"}, {"author": "_Daimon_", "body": "Generally you want to replace all MoreComment objects with the Comment objects they represent. It's detailed in [the Comment Parsing](http://praw.readthedocs.org/en/latest/pages/comment_parsing.html#the-number-of-comments) page in the documentation. If you just want to replace a single MoreComment object, then see the link kemitche provided,", "created_utc": 1392564212, "gilded": 0, "name": "t1_cfgpdxa", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/"}, {"author": "globalglasnost", "body": "thanks for your help! so MoreComments.comments(update=True) returns either type list or type NoneType, when it returns NoneType is that a deleted comment?", "created_utc": 1392571039, "gilded": 0, "name": "t1_cfgrq8j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/"}, {"author": "kemitche", "body": "http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.MoreComments", "created_utc": 1392529119, "gilded": 0, "name": "t1_cfgiaib", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/"}, {"author": "globalglasnost", "body": "thanks!", "created_utc": 1392571029, "gilded": 0, "name": "t1_cfgrq43", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1y0t1v/i_want_to_load_all_the_comments_from_a_specific/"}, {"author": "thunder_afternoon", "body": "Why don't you just [search](https://praw.readthedocs.org/en/latest/pages/code_overview.html?highlight=search#praw.objects.Subreddit.search) for the URL in question? You can use the URL query. For example: http://www.reddit.com/r/nfl/search?q=url%3Ahttp%3A%2F%2Fwww.nola.com%2Fsaints%2Findex.ssf%2F2014%2F02%2Fnew_orleans_saints_will_smithr.html&restrict_sr=on", "created_utc": 1392304212, "gilded": 0, "name": "t1_cfecdkd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xrkhi/get_a_submission_object_based_on_the_url_of_the/"}, {"author": "mattster42", "body": "Great! Thanks!", "created_utc": 1392438551, "gilded": 0, "name": "t1_cffr8l1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xrkhi/get_a_submission_object_based_on_the_url_of_the/"}, {"author": "kemitche", "body": "> I'm trying to write a function to register new Reddit accounts. Now why would you want to do that?", "created_utc": 1392093457, "gilded": 0, "name": "t1_cfcbhbh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xkp8n/praw_captcha_handling_problem/"}, {"author": "metalayer", "body": "I'm writing a reddit client. Not sure why you would assume bad intentions when registering new accounts is an API feature.", "created_utc": 1392119408, "gilded": 0, "name": "t1_cfchhym", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xkp8n/praw_captcha_handling_problem/"}, {"author": "OnceAndForever", "body": "I've been playing around with this a bit, and I think I got it working. If you look look at the permalink of an inbox message, it looks something like this: `http://www.reddit.com/message/messages/1jkdxs`. You can use praw's `get_content()` method to fetch this url. For example: import praw r = praw.Reddit(USER AGENT) r.login(USERNAME, PASSWORD) message_permalink = r.get_content(url='http://www.reddit.com/message/messages/1jkdxs') for message in message_permalink: message.reply('This is your reply to the message') In this example, `message_permalink` is a [get_content generator](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit.get_content), and message is a [praw.objects.Message object](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Message), which is based off of the [Inboxable object](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Inboxable). As for storing the message ids, use the `get_inbox()` ( or `get_unread()` depending on what you're doing) and then loop through all your messages. The `id` attribute is the one you're looking for. messages = r.get_inbox() for message in messages: print message.id print message.body >>> 1jkdxs >>> this is a test You can use the `id` to build the url you will use for `get_content()` because it will always be `'http://www.reddit.com/message/messages/' + id`. Hope this helps.", "created_utc": 1392026749, "gilded": 0, "name": "t1_cfblhs8", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xi8ol/praw_how_to_store_an_inbox_message_in_a_database/"}, {"author": "decho", "body": "Yes. subreddit = r.get_subreddit('all') comments = subreddit.get_comments(limit=5) for x in comments: print x.subreddit This will print a list where each comment is coming from. EDIT: Also try the following: Set the limit to 1 and then: from pprint import pprint for x in comments: pprint(vars(x)) and for x in comments: print dir(x)", "created_utc": 1391900131, "gilded": 0, "name": "t1_cfahlrb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xe0fw/praw_check_what_subreddit_a_comment_is_from/"}, {"author": "zd9", "body": "Okay, thanks! I was looking through the PRAW docs, and didn't see `.subreddit` as a property of `comment`", "created_utc": 1391900340, "gilded": 0, "name": "t1_cfaholo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xe0fw/praw_check_what_subreddit_a_comment_is_from/"}, {"author": "decho", "body": "Those aren't listed IIRC. You need to use dir and pprint. Check my edit. Very useful commands.", "created_utc": 1391900481, "gilded": 0, "name": "t1_cfahqib", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1xe0fw/praw_check_what_subreddit_a_comment_is_from/"}, {"author": "_Daimon_", "body": "It's a warning in a dependency of a dependency of PRAW that it's doing something that isn't supported anymore. What's your version of `requets`? Run this command in the terminal `pip freeze | grep requests`. You should try upgrading it `pip install requests -U` and see if that helps. That said, it looks like a non-serious warning so you probably won't encounter problems related to it.", "created_utc": 1391786414, "gilded": 0, "name": "t1_cf9euze", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9wzh/having_trouble_logging_in_with_praw/"}, {"author": "notverysmart_bot", "body": "It sais that requests is up to date, when I try to upgrade it. > That said, it looks like a non-serious warning so you probably won't encounter problems related to it. So you mean, I am actually logged in and this is no error message?", "created_utc": 1391786679, "gilded": 0, "name": "t1_cf9eyqi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9wzh/having_trouble_logging_in_with_praw/"}, {"author": "_Daimon_", "body": "Yeah, what you're getting is a `DepreciationWarning` not an Error. Basically the dependency does something that has given an argument that doesn't exist anymore. but it still works. There's a setting to disable/enable depreciation level both on a User and Module level. Are you using an old version of Python, IIRC from 2.7 and forward `DepreciationWarnings` are silenced by default.", "created_utc": 1391787532, "gilded": 0, "name": "t1_cf9fb44", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9wzh/having_trouble_logging_in_with_praw/"}, {"author": "notverysmart_bot", "body": "I am using Python 3.3, but apparently warinings are still there. How can I disable them?", "created_utc": 1391788527, "gilded": 0, "name": "t1_cf9fpua", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9wzh/having_trouble_logging_in_with_praw/"}, {"author": "_Daimon_", "body": "There is a pretty good [stack overflow](http://stackoverflow.com/a/879208/1368070) answer on this. Try it out and see if it helps.", "created_utc": 1391795723, "gilded": 0, "name": "t1_cf9iy09", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9wzh/having_trouble_logging_in_with_praw/"}, {"author": "notverysmart_bot", "body": "That worked. Thank you for your help! You're awesome!", "created_utc": 1391797030, "gilded": 0, "name": "t1_cf9jjpz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9wzh/having_trouble_logging_in_with_praw/"}, {"author": "Deimorz", "body": "> In the praw \"tutorial\" for writing a bot, they store a list of comments ids for each time an action is performed. What if the bot needs to be restarted? If it's not a quick script and a record of what it's already done is something that you need to store over a longer period, you should be persisting it across runs somehow by using something like a sqlite database (or just a text file, if your needs are basic). > What about checking the replies to a comment to see if the bot has already replied? Is there anything that can go wrong there? The biggest problem with this is that it will probably require far more requests (which slows your bot down a lot, since it can only make one request every 2 seconds). Most bots that are replying to things are using an \"out-of-context\" listing, like http://www.reddit.com/r/all/comments, or the /comments page for a particular multireddit/subreddit. You won't have any way to tell which comments you've replied to from there except by loading the comment thread for each comment, which will require an additional request each.", "created_utc": 1391808387, "gilded": 0, "name": "t1_cf9p1sj", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "thunder_afternoon", "body": "Check out the 'after' and 'before' fields in several queries. See listing description at the top: http://www.reddit.com/dev/api . You can then save only the most recent ID you have processed. You can also use a cloud_search syntax and absolute UTC times.", "created_utc": 1391790505, "gilded": 0, "name": "t1_cf9gk40", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "decho", "body": "First of all sorry if I'm wrong somewhere, I'm new to python, but I\\`ll give my best shot. I assume you can do that without the need for a list to store bot`s comments. Say you have already grabbed the comments for a particular thread and your bot detects that he needs to perform an action. Simple code as an example how to test it: submission = r.get_submission(submission_id='1wya5l') comments = submission.comments for x in comments: for z in x.replies: try: if z.author.name != 'yourbotname': z.reply('something') except AttributeError: pass Line by line explanation: \\#3 Here you start iterating through comments. \\#4 Another for loop to check every single comment's replies. Here (between lines 3 and 4) you should add the condition in which you want your bot to perform an action else it's going to reply on everything. \\#5 Try is needed because many of the comments won't have a reply and you script will throw an Attribute error. \\#6 Your bot checks if any of the replies for a given comment are his and if they aren't then he replies to the given comment. I hope this is enough to get you started. Hope I've been helpful. PS: Not completely sure if the command is add_reply or just reply, but I'm sure you can figure that out yourself.", "created_utc": 1391868563, "gilded": 0, "name": "t1_cfa60ov", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "Antrikshy", "body": "I stumbled upon this comment while writing my own bot. Won't line 6 cause the bot to keep replying to z every time it finds a comment that's not written by him?", "created_utc": 1397058323, "gilded": 0, "name": "t1_cgo4jig", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "decho", "body": "Yes, that is indeed what is going to happen, but I just used that as an example to point the guy in the right direction. Look what I also wrote, this is very important: > #4 Another for loop to check every single comment's replies. **Here (between lines 3 and 4) you should add the condition in which you want your bot to perform an action else it's going to reply on everything.** Let me know if you want to know anything else. By the way, there is also a little trick you can find useful. You can hide text within a comment, and that text will stay invisible to anyone but bots or if a person clicks source on your comment. You can use that to help your bot. Click source on this comment to see how it's done. ^^^^^^^^^^^^^^^^There_are_also_other_ways_to_achieve_this_as_well_._Check_any_of_autowikibot's_comment's_source.", "created_utc": 1397061786, "gilded": 0, "name": "t1_cgo65qw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "Mustermind", "body": "Hi! I just learnt python last weekend. The funny thing is, your code is pretty similar to mine :). Except x.replies will only respond with an empty list instead of an AttributeError, so there's no need for that try thing. This idea makes total sense, right? But it turns out large threads or comments not loaded from a submission (e.g. /r/all/comments) will not come with replies. So the bot kept replying to the same comments over and over again because x.replies gave me nothing, so it assumed it hadn't commented before :/", "created_utc": 1391877286, "gilded": 0, "name": "t1_cfa8mzc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "decho", "body": "Oh yeah, when you \"scan\" an exact thread it will not come with replies, unlike when you scan a subreddit if that's what you meant. But you can use the praw helpers. I just tested it and returned both comments and replies. submission = r.get_submission(submission_id='1wya5l') comments = submission.comments comments_and_replies = praw.helpers.flatten_tree(comments) for x in comments_and_replies: print x ## it should return everything in the thread. Try it out, it think this might be exactly what you need.", "created_utc": 1391878285, "gilded": 0, "name": "t1_cfa8zuo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x9kzl/is_there_more_than_one_way_to_make_sure_a_bot/"}, {"author": "_Daimon_", "body": "Do you know those problems where you struggle for a long time only to realise it's something trivial? This is one of those problems, [get_mod_queue](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModOnlyMixin.get_mod_queue) has a space in the name :P", "created_utc": 1391764606, "gilded": 0, "name": "t1_cf9a6cb", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x8q1b/am_i_not_using_get_modqueue_correctly/"}, {"author": "FramesPerSushi", "body": "Thanks for that. I was guessing get_modqueue() was from an outdated version. There are plenty of docs and old repos out there that feature that function.", "created_utc": 1391796978, "gilded": 0, "name": "t1_cf9jiuk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1x8q1b/am_i_not_using_get_modqueue_correctly/"}, {"author": "Plague_Bot", "body": "[set_flair(subreddit, item, flair_text='', flair_css_class='')](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModFlairMixin.set_flair) is what you want. To clear the previous one, just have a variable that tracks the last post that the flair was assigned to. When you update the new post, check that variable, and call `set_flair()` on the old post.", "created_utc": 1391395266, "gilded": 0, "name": "t1_cf5lkdc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "mattster42", "body": "Thank you! I'm assuming I can immediately set that flair when the submission is posted, but I'm having trouble referencing it. From what I understand, praw returns the submission object immediately with a message like: yet when I try to immediately implement: reddit.set_flair('subredditredacted', submission.id, flair_text=\"Flair text\", flair_css_class=\"test\") Traceback (most recent call last): File \"\", line 1, in NameError: name 'submission' is not defined So obviously my assumption that praw automatically converts my submission into a \"submission\" object is flawed. How can I call the latest submission to use in a function like set_flair?", "created_utc": 1391407772, "gilded": 0, "name": "t1_cf5qnmn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "Plague_Bot", "body": "First, what you're trying to pass in here is the submission id. I believe you need to pass the entire object (lose the .id part). Second, you probably have to set the object to a variable when Reddit returns it. Something like `submission = reddit.submit(params...)` As a side note, when you're setting the variable that tracks the old submission that I mentioned before, `submission = reddit.get_submission(submission_id = \"\")` might come in handy.", "created_utc": 1391409421, "gilded": 0, "name": "t1_cf5r62g", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "mattster42", "body": "There it is! Thank you so much!", "created_utc": 1391409849, "gilded": 0, "name": "t1_cf5ragq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "Plague_Bot", "body": "No problem, glad to help!", "created_utc": 1391410237, "gilded": 0, "name": "t1_cf5rear", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "mattster42", "body": "Also, only moderators can set link flair, but the bot is a moderator. EDIT: One more thing: I have my own instance of AutoModerator running, in case that would be better to do this.", "created_utc": 1391383840, "gilded": 0, "name": "t1_cf5ghvz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wug8a/set_link_flair_upon_submission/"}, {"author": "Plague_Bot", "body": "Can you show what code you were trying?", "created_utc": 1391248749, "gilded": 0, "name": "t1_cf4axj4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wptfz/what_is_the_correct_way_to_return_the_comment/"}, {"author": "newpong", "body": "I figured it out. I was trying to check the scores of comments returned from get_inbox() rather than get_comments, assuming messages and comments had the same structure. This more or less what I was trying: scores = [item.score for item in ua.get_inbox()] whereas I was looking for the result produced by this: messages = ua.get_inbox() scores = [ua.get_info(thing_id = m.fullname).score for m in messages] Not that either of these things are necessarily useful. It was just a learning exercise, and I was confused.", "created_utc": 1391251340, "gilded": 0, "name": "t1_cf4baay", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wptfz/what_is_the_correct_way_to_return_the_comment/"}, {"author": "_Daimon_", "body": "Use the `created_utc` timestamp to see when the submission was created. Then add it to a task list to check the submission 5 minutes after creation.", "created_utc": 1391168134, "gilded": 0, "name": "t1_cf3imuq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wmt4p/praw_is_it_possible_to_use_praw_to_check_on_posts/"}, {"author": "TeroTheTerror", "body": "Thanks man, exactly what I was looking for.", "created_utc": 1391199235, "gilded": 0, "name": "t1_cf3u4ra", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wmt4p/praw_is_it_possible_to_use_praw_to_check_on_posts/"}, {"author": "_Daimon_", "body": "Sure :) Look at the [configuration files](https://praw.readthedocs.org/en/latest/pages/configuration_files.html) page and the documentation for [BaseReddit](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.BaseReddit) for details on how to implement this.", "created_utc": 1391109549, "gilded": 0, "name": "t1_cf2wv73", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wkn3s/praw_on_a_reddit_clone/"}, {"author": "Plague_Bot", "body": "One thing I've found useful is Python's dir() function. If there's something you want to know the attributes to, like the body attribute for comments, just do something like this: print dir(comment) This will give you something like: ['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattr__', '__getattribute__', '__hash__', '__init__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_get_json_dict', '_info_url', '_populate', '_replies', '_submission', '_underscore_names', '_update_submission', 'approve', 'approved_by', 'author', 'author_flair_css_class', 'author_flair_text', 'banned_by', 'body', 'body_html', 'clear_vote', 'created', 'created_utc', 'delete', 'distinguish', 'distinguished', 'downs', 'downvote', 'edit', 'edited', 'from_api_response', 'fullname', 'gilded', 'has_fetched', 'id', 'ignore_reports', 'is_root', 'likes', 'link_author', 'link_id', 'link_title', 'mark_as_read', 'mark_as_unread', 'name', 'num_reports', 'parent_id', 'permalink', 'reddit_session', 'remove', 'replies', 'reply', 'report', 'saved', 'score', 'score_hidden', 'submission', 'subreddit', 'subreddit_id', 'undistinguish', 'unignore_reports', 'ups', 'upvote', 'vote'] It's still a pain, but at least they're mostly self explanatory.", "created_utc": 1390986074, "gilded": 0, "name": "t1_cf1oxzj", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "gyroda", "body": "This doesn't seem to be working for me, I'm likely doing something wrong here. Python 2.7.3 (default, Sep 26 2013, 21:37:06) [GCC 4.6.3] on linux2 Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import praw >>> print dir(comment) Traceback (most recent call last): File \"\", line 1, in NameError: name 'comment' is not defined >>> It also doesn't work if I try from praw import * Any idea what's not right?", "created_utc": 1391175310, "gilded": 0, "name": "t1_cf3k1oj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "Plague_Bot", "body": "The problem is you haven't actually defined `comment` yet. `comment` was just an example variable name. You want to do something like this: import praw r = praw.Reddit(user_agent='UNIQUE USER AGENT') comments = praw.helpers.comment_stream(r, 'all', limit=None, verbosity=0) for comment in comments: print dir(comment) break", "created_utc": 1391214188, "gilded": 0, "name": "t1_cf40xbt", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "gabrieldain", "body": "+1 I learnt praw by using `dir()` and `help()`. You can learn most simple libraries by following a pattern of `dir(library)`, `help(library.element)`, `dir(library.element)` and so on.", "created_utc": 1391005611, "gilded": 0, "name": "t1_cf1sis5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "IAMA_YOU_AMA", "body": "Thanks, this is very useful to know.", "created_utc": 1391003209, "gilded": 0, "name": "t1_cf1rudb", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1wfb55/praw_documentation/"}, {"author": "notwhereyouare", "body": "~~So, isn't this like the auto wiki bot that was just created a week or so ago? What's different?~~ Ok, I see the difference now", "created_utc": 1390753451, "gilded": 0, "name": "t1_cezayry", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w642h/about_wikipediacitationbot/"}, {"author": "bboe", "body": "In python 3.2+ unclosed file descriptors (those that are not explicitly closed) raise warnings. By default warnings are not logged anywhere, however, upon importing PRAW warnings begin being logged to the console. PRAW probably shouldn't interfere with the console logging settings, nevertheless, even without PRAW the program still has the ResourceWarning as the file descriptor requests uses is not explicitly closed. If I recall correctly, the error will go away if you do: resp = requests.get('http://example.com') resp.close() return 'idk'", "created_utc": 1390622719, "gilded": 0, "name": "t1_ceyb8ku", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "FramesPerSushi", "body": "Tried that, still getting the warning. Thanks though.", "created_utc": 1390632834, "gilded": 0, "name": "t1_ceye9d4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "_Daimon_", "body": "It's a python 3.3 warning for unclosed resouce. It got opened and never closed. At first glance id guess it may be a problem with PRAWs `requests` session that's never closed and I cannot see how to do that without magic. Anyway, this is not a critical problem indeed you are unlikely to see any bad effects of this. I have no idea why importing praw would cause this effect. We only do imports and and setting of global variables at startup, no establishing of session or any or any other resource.", "created_utc": 1390590555, "gilded": 0, "name": "t1_cexx9bg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "FramesPerSushi", "body": "Yeah praw must have messed something up recently (even if it was my fault), because I wasn't getting this error yesterday. For this reason I'm confident that this won't be a problem once I reboot my computer, but it would be nice to know how/what happened so I can prevent it in the future. Sure it may not me fatal to my program, but it hurts internally every time an error pops up.", "created_utc": 1390591440, "gilded": 0, "name": "t1_cexxpq2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "_Daimon_", "body": "Have you updated your version of PRAW between yesterday and today? AFAIK resource warning is not a warning that will consistently pop up, so it may simple be due to random chance. To be fair it's not an error, it's a warning. But yeah, that's something I'd like to get fixed as well if possible.", "created_utc": 1390601017, "gilded": 0, "name": "t1_cey2i2h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "rhiever", "body": "Is there a github issue filed for this?", "created_utc": 1390659716, "gilded": 0, "name": "t1_ceyiq6d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "bboe", "body": "https://github.com/kennethreitz/requests/issues/1882", "created_utc": 1390678309, "gilded": 0, "name": "t1_ceyozd9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "FramesPerSushi", "body": "Hi, reading through their responses I don't understand how get rid of these warnings. Could you explain?", "created_utc": 1391287809, "gilded": 0, "name": "t1_cf4lfx0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "bboe", "body": "For now, you can disable the reporting of warnings. PRAW _shouldn't_ effect that setting, but for some reason it currently does.", "created_utc": 1391312581, "gilded": 0, "name": "t1_cf4v161", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "FramesPerSushi", "body": "Is it possible to disable warnings for only that specific issue?", "created_utc": 1391312903, "gilded": 0, "name": "t1_cf4v5ec", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "_Daimon_", "body": "No. Neither on PRAW nor requests page. You're welcome to file one if you want to. I'm not sure where it should be filed, probably on the requests page with another issue on the PRAW page linking to the requests page.", "created_utc": 1390668721, "gilded": 0, "name": "t1_ceylc8l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "FramesPerSushi", "body": "No I haven't updated PRAW. I also rebooted my computer and am still getting the warning. This is strange, because the same code yesterday was working fine. I'm not sure what I've done in the meantime to invoke it.", "created_utc": 1390632950, "gilded": 0, "name": "t1_ceyeack", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1w1w1s/cant_use_requests_module_if_ive_imported_praw/"}, {"author": "Jhinra", "body": "The subreddit attribute should tell ya. The [JSON](https://github.com/reddit/reddit/wiki/JSON) page tells you all the attributes of a comment.", "created_utc": 1390365567, "gilded": 0, "name": "t1_cevpox6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vtpmp/get_which_subreddit_a_comment_is_in_praw/"}, {"author": "Link_Correction_Bot", "body": "Thank you very much.", "created_utc": 1390366560, "gilded": 0, "name": "t1_cevq3o3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vtpmp/get_which_subreddit_a_comment_is_in_praw/"}, {"author": "_Daimon_", "body": "A reference to the subreddit is stored in the `subreddit` attribute. If your comment object is called `comment` then you just do `comment.subreddit`.", "created_utc": 1390380730, "gilded": 0, "name": "t1_cevtxka", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vtpmp/get_which_subreddit_a_comment_is_in_praw/"}, {"author": "HAEC_EST_SPARTA", "body": "Thank you for the information; it was very helpful.", "created_utc": 1390398534, "gilded": 0, "name": "t1_cevwxnu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vtpmp/get_which_subreddit_a_comment_is_in_praw/"}, {"author": "_Daimon_", "body": "`comment` is not defined. The element in the loop is called `c` instead. Change line 51 to `c.reply(\"Looks like.... \")`", "created_utc": 1390335882, "gilded": 0, "name": "t1_cevbmk5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vs7v5/replying_to_a_comment_with_praw/"}, {"author": "Dwarflord", "body": "D'oh! thank you Daimon!", "created_utc": 1390336015, "gilded": 0, "name": "t1_cevboxv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vs7v5/replying_to_a_comment_with_praw/"}, {"author": "_Daimon_", "body": "post code.", "created_utc": 1390335056, "gilded": 0, "name": "t1_cevb7uf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vs7v5/replying_to_a_comment_with_praw/"}, {"author": "Dwarflord", "body": "Added code!", "created_utc": 1390335248, "gilded": 0, "name": "t1_cevbb77", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vs7v5/replying_to_a_comment_with_praw/"}, {"author": "thunder_afternoon", "body": "> Using get_spam(), is there a way to return comments only (instead of comments and posts)? Not sure if this is what you're after but posts start with t3_ and comments start with t1_. It should be fairly easy to distinguish what is a comment and what's a post. As for tracking already-replied comments, the safest method might be to keep/save a local list of comments replied and compare. I think removed comments' author shows up as \"[deleted]\" IIRC.", "created_utc": 1390321886, "gilded": 0, "name": "t1_cev4wwh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vrfu3/praw_how_can_i_tell_if_a_postcomment_has_been/"}, {"author": "Jyrroe", "body": "I tried out a few different things. I've been using comment.id, but I found out about comment.name, which does indeed start with t1_. Not sure if I can use that yet, but it's good to know. :P About the author, it still returns the original poster (I'm assuming that's because the bot is a moderator). And this script is running online and I'm fairly new to python, so I have no idea how I would read/write from a file or where I could store it for quick access. If there really isn't a way to check if a post has been removed, I guess I'll have to learn about python file IO. I wonder if I could check the comment's children for an existing comment from the bot? Seems inefficient...", "created_utc": 1390325363, "gilded": 0, "name": "t1_cev6hql", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vrfu3/praw_how_can_i_tell_if_a_postcomment_has_been/"}, {"author": "letgoandflow", "body": "A simple database would be ideal for this. Create a table that stores comment ids of removed comments and then have your bot skip over the removed comments. Bonus fun if you don't know how to use a database with python (it's not hard, look for sqlite tutorials).", "created_utc": 1390326110, "gilded": 0, "name": "t1_cev6uh7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vrfu3/praw_how_can_i_tell_if_a_postcomment_has_been/"}, {"author": "OnceAndForever", "body": "How often are you trying to fetch new comments? Most pages on reddit are cached for 30 seconds, so you won't get new data if you keep checking all that frequently. From the [API wiki](https://github.com/reddit/reddit/wiki/API#rules): >* Most pages are cached for 30 seconds, so you won't get fresh data if you request the same page that often. Don't hit the same page more than once per 30 seconds. When using PRAW, the cache would be the same. The other issue you may have is that the cache is longer for unauthenticated requests, so logging in may allow you to get fresh comments more frequently.", "created_utc": 1390297654, "gilded": 0, "name": "t1_ceuyqke", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vqywg/praw_comment_fetching_problem/"}, {"author": "HAEC_EST_SPARTA", "body": "Thanks for the information.", "created_utc": 1390336027, "gilded": 0, "name": "t1_cevbp6d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vqywg/praw_comment_fetching_problem/"}, {"author": "gabrieldain", "body": "Have you tried using comment_stream? reddit = praw.Reddit(\"app\") counter = 0 for post in praw.helpers.comment_stream(reddit, 'all'): counter += 1 if counter > 100: break else: [code] It reads a bit more hacky, but if it works...", "created_utc": 1390342337, "gilded": 0, "name": "t1_cevewor", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vqywg/praw_comment_fetching_problem/"}, {"author": "_Daimon_", "body": "Hey, you're probably looking for `get_unread` and [`set_flair`](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModFlairMixin.set_flair). import praw r = praw.Reddit(UNIQUE_AND_DESCRIPTIVE_USERAGENT_HERE) r.login() # As the moderator that will be receiving the mails sub = r.get_subreddit(YOUR_SUBREDDIT) for message in r.get_unread(): # Parse the message to get the username and css class sub.set_flair(USERNAME, flair_css_class=THE_CSS_CLASS)", "created_utc": 1390207095, "gilded": 0, "name": "t1_ceu347h", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "Decopolis", "body": "I'm very new as well. I'm not having this specific issue, but I don't understand where I can find more methods like this. For example, let's say I'm trying to return the title of new submissions tagged with certain keywords. Where can I find the methods to let me do this? I don't understand what the reddit api is telling me, and on the methods from readthedocs I don't understand what objects to use them on or what they return. Sorry if this sounds whiney, but I can't find where to start.", "created_utc": 1390254525, "gilded": 0, "name": "t1_ceuiiuy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "_Daimon_", "body": "Have you read the [writing a bot](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) guide? (After [getting started](http://praw.readthedocs.org/en/latest/pages/getting_started.html) obviously) It talks about standard python introspection and how to find the methods and attributes you need.", "created_utc": 1390299543, "gilded": 0, "name": "t1_ceuz179", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "5loon", "body": "Sorry, I'm still fairly new at this. I'm confused on those last two lines. How would I format the message to make it input the flair and css class? Also, is there a way to add the flair text to the message? Do I just add flair_text='' to the set_flair line? Thanks in advance, thanks for your time.", "created_utc": 1390208315, "gilded": 0, "name": "t1_ceu3d4y", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "_Daimon_", "body": "Each `Inboxable` object has the `body` attribute which is the main text part of the `Message` or `Comment`. It is a regular string. So if say, people are sending you messages with a body of \"username css_class\" then you can do username, css_class = message.split(\" \") Since it's a regular string you can do regular python string operations and parse it that way. If you need more details on this, then i recommend googling around. There are more, and more indepth, tutorials on python string parsing than what I can provide here. Messages cannot be flaired as far as I know.", "created_utc": 1390259849, "gilded": 0, "name": "t1_ceuky3m", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "5loon", "body": "Thanks!", "created_utc": 1390260066, "gilded": 0, "name": "t1_ceul1l6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vnq5y/im_new_to_using_praw_on_python_i_have_one_simple/"}, {"author": "_Daimon_", "body": "Looks like something has gone wrong on Reddit's end. Because accessing the 'alaska88' userpage via the webend results in a 404, but the [json version of the page works fine](http://www.reddit.com/user/Alaska88/about.json). This is why you don't get a 404 in the `get_redditor`line, because everything looks good to PRAW.", "created_utc": 1390206352, "gilded": 0, "name": "t1_ceu2yds", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vn25q/praw_how_to_do_a_tryexcept_for_a_lazy_object/"}, {"author": "collred", "body": "Yes, I believe something did go wrong with reddit. I also want to point out though that in my original code I was importing the urllib2 library when I should have been importing \"requests\" and using \"requests.excetpions.HTTPError\" to catch the error.", "created_utc": 1390766727, "gilded": 0, "name": "t1_cezgal0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vn25q/praw_how_to_do_a_tryexcept_for_a_lazy_object/"}, {"author": null, "body": "If you want to save data \u2026 use a database?", "created_utc": 1390109878, "gilded": 0, "name": "t1_cet7rse", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "Ugleh", "body": "I like your thinking. Its like a base but for data.", "created_utc": 1390111393, "gilded": 0, "name": "t1_cet897c", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "Ugleh", "body": "If you can't run a database maybe save the posts on reddit using the save feature?", "created_utc": 1390111451, "gilded": 0, "name": "t1_cet89th", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "voltagex", "body": "Why not PostgreSQL?", "created_utc": 1390134794, "gilded": 0, "name": "t1_cetcq6a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "Rosco_the_Dude", "body": "Actually, there isn't even a \"why not\" anything at this point! I'm open to anything that'll work well with my application. I actually started using shelve to save and load class attributes from a gdbm database, so I think for now I found something up my alley. In your opinion, why would you choose PostgreSQL over my set up or any other set up, if you would at all?", "created_utc": 1390154582, "gilded": 0, "name": "t1_ceti937", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "OnceAndForever", "body": "If you aren't familiar with databases, I would probably start off using SQLite. It would definitely meet your needs, and it is part of the python standard library, so there would be no issues using it. You can read the official documentation [here](http://docs.python.org/2/library/sqlite3.html). This would be the best option for storing information like submission_ids. Other than using a database, your other option would be to write the data to text files. This would work fine if you are just trying to store an instance of an object. You could dump the object with pickle or even json and then save that to a text file. Information that you need to retain long term like post_ids are better off stored in a database. Here is the python documentation for [file objects](http://docs.python.org/2/library/stdtypes.html#file-objects) If gdbm is working for you though, then keep with it! Also, this is more of a python specific question and not a question specific to the reddit API, so you may have a better response in /r/python or /r/learnprogramming. If you'd like to see how other people have handled this same problem in their bots, I could recommend a few open source bots that you could check out: * /u/VideoLinkBot -- [Source](https://github.com/dmarx/VideoLinkBot/) * /u/TweetPoster -- [Source](https://github.com/buttscicles/TweetPoster/) * /u/astro-bot -- [Source](https://github.com/RobSis/astrobot) Good luck!", "created_utc": 1390167805, "gilded": 0, "name": "t1_ceto0jf", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "Rosco_the_Dude", "body": "Thanks so much for the detailed response! I'll check these bots' source code. I'm getting pretty great results with shelve, but I want to learn as much as I can", "created_utc": 1390168818, "gilded": 0, "name": "t1_cetoh6p", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "OnceAndForever", "body": "No problem! I'm glad you got something working. If you'd like some more examples, I can link you to some more bots. The bots that I linked are well written which makes them easier to follow and understand.", "created_utc": 1390168963, "gilded": 0, "name": "t1_cetojiz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": "Rosco_the_Dude", "body": "Honestly, I want to read as much source code as I can. I'm just starting to branch out and find new libraries to use, and I've just started writing my own classes in Python. I feel like I've learned a lot in the past couple weeks just reading documentation and reading other people's code. I know I keep straying away from redditdev topics, but my current project in Python is primarily an application that works with Reddit objects. It doesn't really matter to me if these bots are written in another language, btw.", "created_utc": 1390170015, "gilded": 0, "name": "t1_cetp0la", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1vkqib/im_writing_my_first_reddit_bot_and_i_was/"}, {"author": null, "body": "[deleted]", "created_utc": 1390067243, "gilded": 0, "name": "t1_cess3sn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": "m1ss1ontomars2k4", "body": "Obviously not, because he's asking how to do it here...", "created_utc": 1390073351, "gilded": 0, "name": "t1_cesuehu", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": null, "body": "[deleted]", "created_utc": 1390051205, "gilded": 0, "name": "t1_cesnme4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": "stats94", "body": "Thanks for your help! I uninstalled and reinstalled python/pip and I got rid of that particular error message and it now instead says [this](http://pastebin.com/M0BgqHsi). I don't suppose you know anything about that? Again, thanks for the help!", "created_utc": 1390051494, "gilded": 0, "name": "t1_cesno30", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": null, "body": "[deleted]", "created_utc": 1390051932, "gilded": 0, "name": "t1_cesnqrk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": "stats94", "body": "Thank you so much, I thought I'd downloaded and installed everything in the right order. Uninstalled everything, tried it all again in that order and works like a charm. Thanks!", "created_utc": 1390067194, "gilded": 0, "name": "t1_cess36e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": "_Daimon_", "body": "Sounds like a problem with `pip` that has nothing to do with PRAW or the reddit API. I don't know what the problem is and cannot help you and this seem like the wrong place to ask your question. I'm sure you'll have more luck by asking this this question in r/python, /r/learnpython or on stackoverflow.", "created_utc": 1390048277, "gilded": 0, "name": "t1_cesn6ie", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": "stats94", "body": "Thanks!", "created_utc": 1390048800, "gilded": 0, "name": "t1_cesn96g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1viomp/pip_unable_to_download_praw/"}, {"author": "SeaCowVengeance", "body": "I don't know if there is a testing framework for PRAW, if there is, my life could've been much easier. I did manage to make a workaround. I recently developed a [bot](https://github.com/renfredxh/compilebot) and ran into a similar issue to yours. I mocked PRAW functionality by creating inner classes that are identical to the classes in PRAW that my bot needed to use for testing. Check out the [checkProcessUnread class](https://github.com/renfredxh/compilebot/blob/master/compilebot/tests/test_reply.py#L147) in the unit test module for examples of this. The first inner classes are meant to model PRAW classes and have some of the attributes and methods needed for them. In the `setUp` method that is run before each test, you can see `self.r = self.Reddit()` is called, which creates a new \"fake\" reddit object that was defined locally that can be used for testing without actually logging into reddit via the \"real\" Reddit object in the praw module. Similarly, in the actual tests when I need to pass a comment into a function I pass in a new `self.Comment` instead of the comment object from the praw module. This way I have complete control over what all of these objects do during the tests, and additionally none of these tests require the PRAW module to make any requests to reddit.com. Depending on how your bot is set up, it might be best to actually reassign those new \"fake\" classes to the corresponding class in the praw module. So if you make your own reddit Submission class you can say `praw.objects.Submission = self.Comment`. Hopefully this makes sense. Let me know if you have any other questions. **EDIT:** Just realized from the other comments there's actually a library called [mock](http://www.voidspace.org.uk/python/mock/) that pretty much does this for you. Wish I knew about it earlier.", "created_utc": 1389764345, "gilded": 0, "name": "t1_cepy4pc", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v8sbx/using_praw_with_a_unit_testing_framework_and_mocks/"}, {"author": "_Daimon_", "body": "Why not [mock](http://www.voidspace.org.uk/python/mock/)?", "created_utc": 1389775576, "gilded": 0, "name": "t1_ceq0y7h", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v8sbx/using_praw_with_a_unit_testing_framework_and_mocks/"}, {"author": "idProQuo", "body": "I'm going to try using mock, but I wanted to see if anyone had a solution tailored to this domain (reddit bots) and this framework (praw).", "created_utc": 1389780864, "gilded": 0, "name": "t1_ceq1s9v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v8sbx/using_praw_with_a_unit_testing_framework_and_mocks/"}, {"author": "LungFungus", "body": "There was a feature recently added to praw that allows praw objects to export themselves into JSON format. Later, you can reinstantiate the object using the json. This could be useful for testing if you want to have the same objects each time. (Sorry I just woke up)", "created_utc": 1389800323, "gilded": 0, "name": "t1_ceq69kg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v8sbx/using_praw_with_a_unit_testing_framework_and_mocks/"}, {"author": "_Daimon_", "body": "Try [standard introspection](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) on a `Comment` object.", "created_utc": 1389583851, "gilded": 0, "name": "t1_ceo6q0r", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "PointsOutTrains", "body": "Also, I didn't realize until now that you are THE praw guy. Subtle", "created_utc": 1389740353, "gilded": 0, "name": "t1_cepnpxz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "_Daimon_", "body": "I'm not. There's no \"THE\" PRAW guy. I'm just a maintainer who has written some of the documentation. /u/mellort is the original author and /u/bboe is the other maintainer. He has contributed more commits than me and was the architect of the version 2.0 redesign.", "created_utc": 1389745755, "gilded": 0, "name": "t1_cepq7ex", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "PointsOutTrains", "body": "Ahhh, okay.", "created_utc": 1389784267, "gilded": 0, "name": "t1_ceq28r7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "letgoandflow", "body": "Side question: why doesn't reddit include standard objects for each of the major object types in their api docs and wikis? I feel like this is the most basic thing I want to look at after I know what api method to use. Or is it better practice to check out the object yourself with whatever api/language you are using?", "created_utc": 1389590247, "gilded": 0, "name": "t1_ceo96f2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "_Daimon_", "body": "Do you mean descriptions of the attributes of the endpoints and what types they are? They have that in the wiki https://github.com/reddit/reddit/wiki/JSON", "created_utc": 1389606522, "gilded": 0, "name": "t1_ceod4kz", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "letgoandflow", "body": "o damn, there it is! Thanks. I'm still wondering why that isn't linked from the [API documentation](http://www.reddit.com/dev/api). Seems like a lot of API's don't make that info easy to find (twitter is the same way). But my knowledge is limited on this topic so maybe it's not really like that for most apis.", "created_utc": 1389626012, "gilded": 0, "name": "t1_ceogu1q", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "PointsOutTrains", "body": "Hey, thanks. This looks like what I need. I will get back to you (may be a few days) to update.", "created_utc": 1389584717, "gilded": 0, "name": "t1_ceo72cg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": "PointsOutTrains", "body": "I tried out what you said when messing around with Python this morning, and to get what I needed to work was getting the submission ID from the comments and using the introspection from there. I am elated right now, thank you for your help.", "created_utc": 1389613561, "gilded": 0, "name": "t1_ceoe3ta", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1v2ho0/is_it_possible_to_get_the_subreddit_of_a_comment/"}, {"author": null, "body": "you try RTFM? https://praw.readthedocs.org/en/latest/pages/getting_started.html >Next we can use the functions get_comments() and get_submitted() to get that redditor\u2019s comments and submissions. Both are a part of the superclass Thing as mentioned on the reddit API wiki page. Both functions can be called with the parameter limit, which limits how many things we receive. As a default, reddit returns 25 items. When the limit is set to None, PRAW will try to retrieve all the things.", "created_utc": 1389105154, "gilded": 0, "name": "t1_cejk1cy", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1umg87/how_to_get_all_posts_from_a_user/"}, {"author": "SwedishBoatlover", "body": "Sorry, no I didn't find the correct page. I'm completely new to python as well as PRAW (installed both yesterday), and the documentation is a complete djungle to me thus far. But thank you for helping despite my lack of RTFM *embarrased* A followup question: I'm having trouble finding how to format the time-argument. \"All\" is kind of self-explanatory, but where can I find other ways to format this argument? Can I for example choose to only retrieve submissions/comments posted between two dates? Or say the last six months? I'm used to documentation showing how to actually use the different arguments, so I'm assuming it's just that I can't find it (yes, I have searched for it both on praw.readthedocs.org and google). Edit: I'm also trying to figure out if I can retrieve only unread submissions/comments (from a certain user), I can't find that in the documentation either.", "created_utc": 1389109683, "gilded": 0, "name": "t1_cejlkg4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1umg87/how_to_get_all_posts_from_a_user/"}, {"author": "_Daimon_", "body": "Have you looked at the [writing a bot](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) article? It talks about using python introspection to learn more about objects, what arguments methods can be called with and so forth.", "created_utc": 1389124776, "gilded": 0, "name": "t1_cejshuf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1umg87/how_to_get_all_posts_from_a_user/"}, {"author": "SwedishBoatlover", "body": "No, I've missed that article, it seems great! Thank you! I'm going to read through it thoroughly! I'll return here if I still have some questions after that.", "created_utc": 1389134437, "gilded": 0, "name": "t1_cejxc7r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1umg87/how_to_get_all_posts_from_a_user/"}, {"author": "_Daimon_", "body": "In [PRAW 2.0.12](https://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-0-12) functionality was added to decode &,. By default it is disabled, as it may break some clients. To fix your issue simply switch this feature on. See the [configuration files](https://praw.readthedocs.org/en/latest/pages/configuration_files.html) in PRAW's documentation for more details on this.", "created_utc": 1388651877, "gilded": 0, "name": "t1_cefd42r", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "Justinsaccount", "body": "Good to know.. unfortunately reddit apps for android don't have that option :) Reddit sync and reddit news break, baconreader works though.", "created_utc": 1388671839, "gilded": 0, "name": "t1_cefghw7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "_Daimon_", "body": "I assumed you were speaking of PRAW which is a client of reddits API since we have the issue (with default settings) and you used us in your example. Maybe those client should use PRAW then ;)", "created_utc": 1388695251, "gilded": 0, "name": "t1_cefpw05", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "ExceedinglyEdible", "body": "Websafe encoding, as in encoding that you can put directly into a web page. You are supposed to encode ampersands in HTML, whether they are part of text or in attributes only. This is legal HTML: `link` This is *not* legal HTML: `link` Although many browsers will safely ignore missing HTML-entity encoding for ampersands, it is definitely good practice to respect the rule.", "created_utc": 1388633187, "gilded": 0, "name": "t1_cef7ejk", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "Justinsaccount", "body": "Sure, but this is JSON and not HTML. The URLs do not have to be and should not be encoded in JSON responses. Since they are encoded, clients that use the JSON api need to decode them manually.", "created_utc": 1388633732, "gilded": 0, "name": "t1_cef7m9c", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "xcombelle", "body": "I think only the documentation should be updated as it is not a bug but a feature. Bugging client should make the decoding on their side.", "created_utc": 1388696553, "gilded": 0, "name": "t1_cefqila", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u7553/client_developers_i_think_most_all_of_your/"}, {"author": "releasebot", "body": "I just use this: r = praw.Reddit(user_agent='releasebot') pw = getpass(prompt='releasebot login: ') r.login('releasebot', pw) It means I have to enter the password each time, but on the other hand I don't have to keep the credentials on file and can publish the script on GitHub.", "created_utc": 1388447091, "gilded": 0, "name": "t1_cedpkjl", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "cris9696", "body": "You can probably save the login credentials in a local file outside your bot folder. For example you have the bot in c:\\bot\\ and the uname-password in c:\\unamepass.txt. In the program then you open that file and read everything you need. This way you can make the bot folder open source via github while keeping the credentials local. Just add a readme telling which files should be created manually. Edit: you can also make the file in the same folder and don't add it to the git repo. Edit2: you can also pass the uname and password as command line argumentz , these are just variables and you can use them in the login functions.", "created_utc": 1388445091, "gilded": 0, "name": "t1_cedopm7", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "_Daimon_", "body": "It's a bit complicated, so I'll just copy paste from the docstring of `login` > Look for username first in parameter, then praw.ini and finally if both were empty get it from stdin. Look for password in parameter, then praw.ini (but only if username matches that in praw.ini) and finally if they both are empty get it with getpass. Add the variables user (username) and pswd (password) to your praw.ini file to allow for auto- login. I use it without arguments mainly because this makes it harder for me to accidentally post my access credentials. That's the simple truth :) Thanks for your kind word of the tutorials. Really mean a lot.", "created_utc": 1388445642, "gilded": 0, "name": "t1_cedoy8a", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "pachufir", "body": "So, if I could ask some follow up questions, where is the praw.ini located? Should I just make a file in the same directory named praw.ini? And how would I specify the credentials in that file?", "created_utc": 1388490062, "gilded": 0, "name": "t1_cee13d8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "_Daimon_", "body": "We have a page on that. http://praw.readthedocs.org/en/latest/pages/configuration_files.html If you have any questions to the content there then I'll be happy to answer them :)", "created_utc": 1388495882, "gilded": 0, "name": "t1_cee1z0p", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "pachufir", "body": "Sorry about the late response, but I have been traveling so I haven't had consistent internet access. Anyway i tried to make a local configuration file in my working directory, named it praw.ini and saved the credentials in this format: [bboe] domain: www.reddit.com ssl_domain: ssl.reddit.com user: bboe pswd: this_isn't_my_password however, when i run my script it still prompts me for login info in the terminal. I'm not sure what I'm doing wrong? Anyway, thanks for your support, it has been incredibly helpful!", "created_utc": 1389003898, "gilded": 0, "name": "t1_ceim0ee", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "_Daimon_", "body": "The word inside the hard brackets is the name of the reddit instance you want to connect to or DEFAULT for default settings that may be overriden by specific instance settings. If you create a praw.ini file as below (obviously changing to your real pswd). [DEFAULT] user: pachufir pswd: your_password", "created_utc": 1389523755, "gilded": 0, "name": "t1_cenlwny", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "pachufir", "body": "Ah, chaning it to default fixed the issue, thanks again!", "created_utc": 1389617184, "gilded": 0, "name": "t1_ceoeohn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "pachufir", "body": "So, if I could ask some follow up questions, where is the praw.ini located? Should I just make a file in the same directory named praw.ini? And how would I specify the credentials in that file?", "created_utc": 1388489896, "gilded": 0, "name": "t1_cee12fy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "shaggorama", "body": "Just remember if you store your credentials in an external file to include that file in your .gitignore to ensure you don't push your username/password to a public repo like github.", "created_utc": 1388449278, "gilded": 0, "name": "t1_cedqhv2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1u1pwy/open_sourcing_code_with_login_credentials/"}, {"author": "LungFungus", "body": "I'm not sure about controlling the read status of a mention. I would recommend using a db. Even if its just sqlite. This would allow you to store additional information for records keeping. Such as time it replied, the ID of its comment reply, ID of the post etc... that extra information might be useful in the future. Could help with debugging if you find your bot isn't working correctly.", "created_utc": 1388415315, "gilded": 0, "name": "t1_cedcaf1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tzwqd/python_praw_get_mentions_and_old_mentions_after/"}, {"author": null, "body": "I would use Python's [Regex](http://docs.python.org/2/library/re.html) library and [this](http://stackoverflow.com/questions/833469/regular-expression-for-url) resource for determining whether or not it is a URL.", "created_utc": 1388348868, "gilded": 0, "name": "t1_cecqyk0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "pachufir", "body": "Ah, okay, I guess this makes sense. I was just hoping that praw would be able to recognize hyperlinks within a text. This will work though, so thanks!", "created_utc": 1388350388, "gilded": 0, "name": "t1_cecrm7k", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "radd_it", "body": "Use the body_html property and some fancy regex that looks for ``href=\"http`` and ``.gif``. I'm curious what you're trying to do, as my site may already do it.", "created_utc": 1388358618, "gilded": 0, "name": "t1_cecv9wl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "pachufir", "body": "I'm trying to write a bot that will utilize the gyfcat website to automatically covert gifs to html5 video, upon request", "created_utc": 1388394470, "gilded": 0, "name": "t1_ced8q0i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "radd_it", "body": "We [used to have one of those](http://www.reddit.com/r/gifs/comments/1q8nhf/45foot_flying_pushup/cdab1qy) running around, but it didn't go over well with the mods in the subredits it was most useful in. Maybe being only on-demand will help get around that, but many of the image-heavy subs tend to be quick with the banhammer when it comes to bots.", "created_utc": 1388396041, "gilded": 0, "name": "t1_ced8zno", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "pachufir", "body": "Yeah, I saw that... I'm hoping that being on demand will limit the bans on it, since its not just posting everywhere... either way it could still be useful on some smaller reddits just to quickly convert gif to html5 (plus this is mostly just an excercise for me anyway).", "created_utc": 1388399955, "gilded": 0, "name": "t1_ced9l39", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "LungFungus", "body": "You can use beautiful soup to parse html bodies of comments. From their you can extract the href attribute of a tags. Example: links = BeautifulSoup(body_html, parseOnlyThese=SoupStrainer('a')) urls = [link ['href'] for link in links]", "created_utc": 1388604581, "gilded": 0, "name": "t1_ceewcxe", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "pachufir", "body": "Ah, ok, I ended up writing a parser myself using regex, but this is still interesting to me. Is beautiful soup a python library?", "created_utc": 1388693649, "gilded": 0, "name": "t1_cefp59s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "LungFungus", "body": "Yeah it's available via pip. It's really useful for more complex html parsing.", "created_utc": 1388701211, "gilded": 0, "name": "t1_cefsr9s", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tyk0o/anyway_to_extract_links_from_a_comment/"}, {"author": "DarkAutumn", "body": "The reddit API is a simple GET/POST http/json API that doesn't require PRAW to use. The easiest way to do what you are looking to accomplish is to run something like Fiddler to monitor HTTP requests of a PRAW application (like the collect_data.py you linked), then replicate those raw http requests/json parsing into your new language. The reddit API documentation similarly can help resolve questions when you don't exactly know what data is being passed around. You don't have to rewrite praw in R, but a few helper functions to pull and parse the data you need is really not that difficult (assuming R has usable HTTP and JSON libraries...I've never used the language). I've done this same thing in C# when I couldn't find a suitable C# Reddit library for some side projects.", "created_utc": 1388052886, "gilded": 0, "name": "t1_ceaerbu", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tq6kg/any_documentation_or_tutorials_for_using_the_api/"}, {"author": "_Daimon_", "body": "What's the problem with Redditsharp?", "created_utc": 1388061950, "gilded": 0, "name": "t1_ceafts6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tq6kg/any_documentation_or_tutorials_for_using_the_api/"}, {"author": "DarkAutumn", "body": "Well for starters, it didn't exist a year and a half ago when I was doing this work. RedditSharp is a good C# API for writing Reddit bots or performing one off moderating tasks and the like. However (since you asked), RedditSharp would be a bad thing to use in a GUI C# application (Metro or WPF) because it's entirely single threaded. It lacks Async (awaitable) overloads for its methods, and at its core it doesn't support concurrency. To use the API in a GUI application, you would need to write a lot of \"glue\" to spin up a worker thread and pass custom events through a message pump. This isn't difficult code to write, but it would be exceedingly tedious to create request events and response events for every action you need to perform (especially if you used a lot of the surface area of the API). If I were starting a GUI project (like a metro app) and RedditSharp was still the only option, I'd probably just fork the library, hang onto the http request/json parsing logic, and scrap everything else. It would be less tedious to write just the async methods you need than creating a full message pump.", "created_utc": 1388092391, "gilded": 0, "name": "t1_ceaoy6z", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tq6kg/any_documentation_or_tutorials_for_using_the_api/"}, {"author": "shaggorama", "body": "I wrote you a demo but I'm not convinced it fully works at the moment (it looks like it's pulling in more than 1000 items from the user activity page). Anyway, ti should at least help you get started. https://gist.github.com/dmarx/8140428 PS: If anyone can figure out why this code pulls in more than 1000 unique IDs, I'd love to hear about it.", "created_utc": 1388102443, "gilded": 0, "name": "t1_ceash4x", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tq6kg/any_documentation_or_tutorials_for_using_the_api/"}, {"author": "aglidden", "body": "Ummm... why?", "created_utc": 1388047760, "gilded": 0, "name": "t1_ceae128", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tq6kg/any_documentation_or_tutorials_for_using_the_api/"}, {"author": "UWillAlwaysBALoser", "body": "I've written a subreddit recommender in R. I'd like to create a Shiny app that can take a username and spit out recommendations. The only thing I can't do is pull the subreddits that a user has recently commented in.", "created_utc": 1388048194, "gilded": 0, "name": "t1_ceae3js", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tq6kg/any_documentation_or_tutorials_for_using_the_api/"}, {"author": "radd_it", "body": "I get around that limitation by using a subreddit-restricted search. I start with 'a' and cycle through to 'z' and my script ignores anything that it's seen before. If you want to be super-thorough, you can do it multiple times using different sorts. Not perfect, but it's the best scraping option I have outside of starting at /r/all/new and going back.", "created_utc": 1387911171, "gilded": 0, "name": "t1_ce9dbcv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "Ph0X", "body": "Yeah, that's the idea I had too. It really feels silly though, because to me, the whole reason this limit exists is to avoid putting too much load on their servers, but now the only way to do what we need to do is to put even more load than necessary on their servers... Thanks man, keep up the good work on radd_it EDIT: What do you mean by starting in /r/all/new and going back? Can you go infinitely back on that? Although, I only want it for a subreddit, so can I go /r/futurebeats/new and go back?", "created_utc": 1387915740, "gilded": 0, "name": "t1_ce9f0j4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "radd_it", "body": "When you consider how niche-case this kind of mining of the data is, I don't really fault reddit for having the 1,000-count limit. They're trying to maintain millions of lists, it makes their (database, I assume) load much less dealing with a 1,000 things per list than *all the things*. Check out /u/Stuck_In_the_Matrix's site. I'd link ya directly to it but you're better of reading the API guides he's posted. Thanks for radd.it compliment!", "created_utc": 1387917241, "gilded": 0, "name": "t1_ce9fke5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "Ph0X", "body": "Well, definitely for normal requests from reddit users, most definitely almost never need more than 1000 results, I fully agree that this limit makes sense for the database. But on the API side, generating a dump of all the subreddit posts to me makes sense to me as an option. If anything, in many cases (such as yours), it might very well even reduce the load on their servers. Of course they definitely know much more than me about backend optimizations and how their backend is coded, but I'm curious to know if this is possible. Maintaining an archive of all posts in a subreddit and allowing developers to directly download that instead of making huge queries to the database. Hmm, couldn't find said site. Is it searchsubreddits or redditanalytics? Neither game anything useful.", "created_utc": 1387917675, "gilded": 0, "name": "t1_ce9fpzy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "radd_it", "body": "I was referring to redditanalytics.com-- maybe you should just message the dude directly and ask him for a dump of whatever subreddit you want. He has all the posts (but not the comments) in a big, elastic database. Somewhere there's even an API for it, but damn if I can find the URL.", "created_utc": 1387919187, "gilded": 0, "name": "t1_ce9g9r5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "Ph0X", "body": "Awesome, yes I do not need the comments, just the posts. Perfect I messaged him.", "created_utc": 1387919245, "gilded": 0, "name": "t1_ce9gahy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "Stuck_In_the_Matrix", "body": "We aren't going to production until January but send me a pm and I'll give you my google hangouts link and I can get you anything you need.", "created_utc": 1387918082, "gilded": 0, "name": "t1_ce9fvaa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "reseph", "body": "It's capped at 1000.", "created_utc": 1387911092, "gilded": 0, "name": "t1_ce9da8o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tlmrf/list_of_all_submissions_in_a_subreddit/"}, {"author": "gavin19", "body": "There is a relatively easy way to do it but it doesn't change at set interval. Sometimes it can be 5 minutes, or 45 minutes. Depends. You can see it used at /r/RESissues for the tips banner at the top of the page, or the background colour of the header at /r/csstestdump, or the coloured strip at the top of the sidebar at /r/gavin19. They all hook into the same element. All you need is to put a link in the sidebar where you want the image to appear [](/pic) and the base CSS (assuming a uniform 500x300px image size) .side .md [href$=\"/pic\"] { height: 500px; width: 300px; display: block; cursor: default; margin: 3px 0; } Then you have 36 possible values that can each have an image attached .side .usertext[id$=\"a\"] [href$=\"/pic\"] { background: url(%%pic-a%%); } .side .usertext[id$=\"b\"] [href$=\"/pic\"] { background: url(%%pic-b%%); } .side .usertext[id$=\"c\"] [href$=\"/pic\"] { background: url(%%pic-c%%); } etc, covering a-z then 0-9, giving 36 in total. You don't need to assign one pic per value, you can simply combine selectors like .side .usertext[id$=\"4\"] [href$=\"/pic\"], .side .usertext[id$=\"5\"] [href$=\"/pic\"], .side .usertext[id$=\"6\"] [href$=\"/pic\"] { background: url(%%pic-456%%); } That said, I don't think there is anything strictly wrong with your current method.", "created_utc": 1387743793, "gilded": 0, "name": "t1_ce7x0zo", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1th5ar/cycling_through_a_set_of_images_on_the_sidebar_is/"}, {"author": "spladug", "body": "http://www.reddit.com/r/multibeta/comments/1jisr2/new_beta_feature_subreddit_suggestions/ http://www.reddit.com/r/changelog/comments/1skv2t/reddit_change_explore_page_for_discovering/ http://www.reddit.com/explore", "created_utc": 1387519962, "gilded": 0, "name": "t1_ce65wkj", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tay79/noticed_new_api_call_get_subreddit_recommendations/"}, {"author": "_Daimon_", "body": "I'm not quite sure if you're also asking when `get_subreddit_recommendations` was added to PRAW. Just in case you were, it was added in 2.1.8. You can read [it's changelog entry](http://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-1-8) to see more details about it and the other new features/changes that have been made to PRAW since.", "created_utc": 1387655774, "gilded": 0, "name": "t1_ce77tf2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1tay79/noticed_new_api_call_get_subreddit_recommendations/"}, {"author": "markerz", "body": "What part are you stuck on? It sounds like you were able to get it partially working.", "created_utc": 1387483209, "gilded": 0, "name": "t1_ce5r0b5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1t9l74/how_should_i_implement_tracking_karma_by_subreddit/"}, {"author": "AngusDWilliams", "body": "Well right now I know how to track karma by subreddit for the last N submissions. I want to be able to track total, cumulative karma by subreddit and was wondering if there's a better way to do this than pulling every submission the user has ever made and tallying them all up.", "created_utc": 1387483524, "gilded": 0, "name": "t1_ce5r5do", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1t9l74/how_should_i_implement_tracking_karma_by_subreddit/"}, {"author": "judasblue", "body": "created and created_utc are the timestamps for a comment.", "created_utc": 1386873104, "gilded": 0, "name": "t1_ce05b5q", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "_Daimon_", "body": "and you should always use `created_utc` and never `created` since that's the creation time for the server which obviously is completely unreliable.", "created_utc": 1386875295, "gilded": 0, "name": "t1_ce06ber", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "_Daimon_", "body": "What's your version of PRAW? Prior to 2.1.6 you could experience multiple postings if reddit failed after inserting the comment into the database and returning a successful response to PRAW. See the [changelog](http://praw.readthedocs.org/en/latest/pages/changelog.html#praw-2-1-6) for more details.", "created_utc": 1386875248, "gilded": 0, "name": "t1_ce06anq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "pranavrc", "body": "2.1.11 is the version. I've managed to fix most of the double-posting, but the bug isn't gone yet, sometimes the bot seems to create two posts at exactly the same time. Like for instance, [this](http://www.reddit.com/r/Cricket/comments/1i7lh3/hey_rcricket_i_made_us_a_bot_meet_uhowstat/ce3tq7t) and [this](http://www.reddit.com/r/Cricket/comments/1i7lh3/hey_rcricket_i_made_us_a_bot_meet_uhowstat/ce3tq7s) comment were posted by the bot at the same second. Any ideas?", "created_utc": 1387292889, "gilded": 0, "name": "t1_ce3vhjc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "_Daimon_", "body": "I think the error is in your code, maybe in the way you handle exceptions. Either that or it's a uncovered corner case of the bug that was fixed in 2.1.6. If there was a a bug in the latest version of PRAW that consistently caused double postings then there would have been a lot more posts about it. I'll need to see your bots code and dig into that before I can provide any more help.", "created_utc": 1387654705, "gilded": 0, "name": "t1_ce77fv3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "judasblue", "body": "And as a result of looking at this code, I learned that str(comment.author) will produce the user name. Which should have been obvious, but I missed before and was always doing comment.author.name. Thanks.", "created_utc": 1386874916, "gilded": 0, "name": "t1_ce06591", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": null, "body": "Wow, I'm silly for not knowing that. Literally just saw the same thing.", "created_utc": 1386944863, "gilded": 0, "name": "t1_ce0tkps", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "su5", "body": "Kinda related question, but you track completed requests with the comment.id. Does the id change if a comment is edited? Could it show up in a generator twice? If you some how got the comment in your generator twice I would imagine this would happen because you only check against the old list of responded comments. And maybe I am missing something, but is line 122 ever executed? It looks like it is after a while True: statement. If not, then theres your problem, you never check the list of actually responded to questions cus ou look in old_list but are populating new_list edit: also, in your dealt_with() function you look through to see if you have responded, but this wont work if your comment is buried in a MoreComments object I dont think. If you are having your Try snag up on line 64 it is because MoreComments objects dont have an author attribute, they are just generators like a Comments object", "created_utc": 1386885186, "gilded": 0, "name": "t1_ce0ayij", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sq7yl/praw_bot_doubleposts_often/"}, {"author": "gavin19", "body": "Just the usernames? You can use [this](https://dl.dropboxusercontent.com/u/2552046/_scripts/py/cScr.py) and then the file can be imported by Excel, or Google docs etc. No dupe checking, ordering, sorting, it just lists the authors.", "created_utc": 1386599866, "gilded": 0, "name": "t1_cdxefjo", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sg934/help_with_a_praw_script_to_collect_usernames_from/"}, {"author": "Innokin_Paul", "body": "Thank you very much! Worked great.", "created_utc": 1386650816, "gilded": 0, "name": "t1_cdy14u5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sg934/help_with_a_praw_script_to_collect_usernames_from/"}, {"author": "shrayas", "body": "Great script. Maybe you can put it in a gist? I can add the dupe checking part that way.", "created_utc": 1386601042, "gilded": 0, "name": "t1_cdxerul", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sg934/help_with_a_praw_script_to_collect_usernames_from/"}, {"author": "gavin19", "body": "https://gist.github.com/gavin19/7873770 Added a simple dupe check to the loop.", "created_utc": 1386602398, "gilded": 0, "name": "t1_cdxf7ih", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sg934/help_with_a_praw_script_to_collect_usernames_from/"}, {"author": "shrayas", "body": "Super! :)", "created_utc": 1386602824, "gilded": 0, "name": "t1_cdxfcyr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sg934/help_with_a_praw_script_to_collect_usernames_from/"}, {"author": "damontoo", "body": "You'll also want account ages to ensure the accounts weren't created after the announcement was made. That adds a lot more requests though.", "created_utc": 1386621802, "gilded": 0, "name": "t1_cdxngu9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sg934/help_with_a_praw_script_to_collect_usernames_from/"}, {"author": "shrayas", "body": "Thats a good point. I'll see if i can add that in.", "created_utc": 1386647289, "gilded": 0, "name": "t1_cdxzgf5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1sg934/help_with_a_praw_script_to_collect_usernames_from/"}, {"author": "largenocream", "body": "Would something like `list(reddit.get_unread(limit=1000)).reverse()` be acceptable? There's not really any way to make the generator itself run backwards since reddit itself doesn't allow you to choose an ascending sort, You have to go through every page until you get to the end.", "created_utc": 1386410083, "gilded": 0, "name": "t1_cdvr9z1", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1saqs9/i_want_to_process_messages_from_the_inbox_from/"}, {"author": "_Daimon_", "body": "Just to be a bit more precise, you need to do it in two steps as `reverse` updates the object it works on and returns `None`. results = list(reddit.get_unread(limit=1000)) reverse(results) They results are now in the reversed sorting.", "created_utc": 1386414956, "gilded": 0, "name": "t1_cdvru11", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1saqs9/i_want_to_process_messages_from_the_inbox_from/"}, {"author": "largenocream", "body": "Thanks, I was on my phone and couldn't check my code", "created_utc": 1386416901, "gilded": 0, "name": "t1_cdvs2nw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1saqs9/i_want_to_process_messages_from_the_inbox_from/"}, {"author": "im14", "body": "Thanks to you both. The following code works: messages = list(reddit.get_unread(limit=1000)) messages.reverse() for m in messages: // do stuff m.mark_as_read()", "created_utc": 1386469701, "gilded": 0, "name": "t1_cdw9u53", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1saqs9/i_want_to_process_messages_from_the_inbox_from/"}, {"author": "Pathogen-David", "body": "That error means something bad happened on Reddit's end and could mean any number of things. You can keep it from crashing your bot by catching the HTTPError somewhere in your code and make sure you handle the error cleanly (ie: reattempt whatever failed.)", "created_utc": 1385916669, "gilded": 0, "name": "t1_cdqzezb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "Plague_Bot", "body": "So in the context of PRAW, how do I catch that? My googling is coming up with [urllib2](http://docs.python.org/2/howto/urllib2.html), but from what I can tell, since PRAW deals with fetching data, that module is redundant. Sorry I'm a bit new to python and haven't done much error catching before.", "created_utc": 1385917954, "gilded": 0, "name": "t1_cdqzv7f", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "Pathogen-David", "body": "No problem, In a nutshell, you can catch an exception in Python like this: try: r.somePrawCall() r.orMaybeABunchOfThem() except HTTPError: print \"An HTTP error happened! Oh no!\" You might want to read up on exceptions in Python in general: http://docs.python.org/2/tutorial/errors.html Exceptions in general (in Python and otherwise) are thrown/raised to signal that the program is in an inconsistent state, meaning that something bad happened and the program can't recover from it or thinks you should decide how to recover from it. When you make a try-catch block, you are basically giving Python instructions on how to recover from the event.", "created_utc": 1385939720, "gilded": 0, "name": "t1_cdr9bs8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "Plague_Bot", "body": "Okay so what I've got is this: try: comments = r.get_comments('all', limit=None) raise HTTPError except HTTPError: print \"HTTPError caught\" As you can see, I'm trying to raise the error so that I can check that it's working. Except I'm getting a *diffferent* error, a NameError telling me that HTTPError is not defined. Am I responsible for defining it? Sorry I'm confused, if it's being thrown, shouldn't it already exist? Traceback (most recent call last): File \"C:/Python27/programs/redditPlague0.1.py\", line 131, in except HTTPError: NameError: name 'HTTPError' is not defined", "created_utc": 1385945270, "gilded": 0, "name": "t1_cdrboi6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "Pathogen-David", "body": "Oops, sorry. You also need to import HTTPError from the requests library. Just add this to your imports: from requests.exceptions import HTTPError", "created_utc": 1385955371, "gilded": 0, "name": "t1_cdrfxjg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "Plague_Bot", "body": "Okay perfect thanks, I have it working now. Cheers!", "created_utc": 1385992632, "gilded": 0, "name": "t1_cdroujy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ru8jl/httperror_500_server_error_internal_server_error/"}, {"author": "thunder_afternoon", "body": "I tried your code and it works as expected for me. Do you have a config file? Maybe something is configured incorrectly there. Also change your user agent just in case. I don't think that's the case but maybe \"testing_praw\" is too common or something.", "created_utc": 1385831001, "gilded": 0, "name": "t1_cdqabh3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rs00u/praw_retrieving_all_comments_works_once_then/"}, {"author": "echblog", "body": "Config file is default. I tried changing the user agent to something unique, it didn't help. This did give me the idea though that it may make a difference if I was logged in or not. I'm currently testing with it logged in, and have yet to replicate the problem. As it stands, this may have been what was causing it. EDIT: I just realized the code I provided has r.login(), but the code I was running had it commented out.", "created_utc": 1385833580, "gilded": 0, "name": "t1_cdqb8rx", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rs00u/praw_retrieving_all_comments_works_once_then/"}, {"author": "mgrieger", "body": "I run my bot on Heroku and I use a PostgreSQL database to store comment IDs. Whenever it retrieves a new set of comments it first checks the database to make sure it hasn't been replied to. I'm not sure if it is the most efficient way of doing it, but it works. **EDIT:** Felt like I should add why I have opted for a database instead of a simple text file or something. Heroku uses something called an ephemeral file system: > Each dyno gets its own ephemeral filesystem, with a fresh copy of the most recently deployed code. During the dyno\u2019s lifetime its running processes can use the filesystem as a temporary scratchpad, but no files that are written are visible to processes in any other dyno and any files written will be discarded the moment the dyno is stopped or restarted.", "created_utc": 1385676931, "gilded": 0, "name": "t1_cdp6k4b", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rnznl/best_way_for_a_bot_to_keep_track_of_the_replies/"}, {"author": "zeffr", "body": "[deleted]", "created_utc": 1385688132, "gilded": 0, "name": "t1_cdp9z34", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rnznl/best_way_for_a_bot_to_keep_track_of_the_replies/"}, {"author": "SuperSniperGuy", "body": "My coinflipbot and colorcodebot both use a mysql database to store their data. They are pretty fast, check out http://coinflipbot.re/logs.php", "created_utc": 1385707116, "gilded": 0, "name": "t1_cdpft7j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rnznl/best_way_for_a_bot_to_keep_track_of_the_replies/"}, {"author": "shaggorama", "body": "An alternative to checking the comment id against all comments your bot has responded to (it should probably do this at least for a cache-worth of comment ids) is to get the child comments. If your bot is the author of any of the immediate children to the comment you are looking at, you know you don't want to respond again.", "created_utc": 1385710556, "gilded": 0, "name": "t1_cdpgmay", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rnznl/best_way_for_a_bot_to_keep_track_of_the_replies/"}, {"author": "DEADB33F", "body": "Probably the easiest method is to give it gold, then have it only reply when someone specifically asks for it by mentioning it's username (eg /u/DEADB33F). Then you can just monitor www.reddit.com/message/mentions/ and reply to any new messages that appear. All the subreddits I moderate will insta-ban bots that don't do this. In most cases, bots posting auto-generated content without the person they're replying to directly asking for the bot's input is unwanted, especially on larger subreddits. A lot of default subreddits and many larger subs also have this policy. --- Of course if the bot is for use on your own subreddits then you're free to spam up the comments section with auto-generated content as much as you like.", "created_utc": 1385730476, "gilded": 0, "name": "t1_cdpjhsx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rnznl/best_way_for_a_bot_to_keep_track_of_the_replies/"}, {"author": "umop_aplsdn", "body": "Try using this: https://praw.readthedocs.org/en/latest/", "created_utc": 1385587543, "gilded": 0, "name": "t1_cdogeks", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rlhzs/praw_retrieving_the_newest_10_submissions_in_a/"}, {"author": "ttbrahbro", "body": "Are you just looking for someone to do this for you?", "created_utc": 1385599849, "gilded": 0, "name": "t1_cdol4rr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rlhzs/praw_retrieving_the_newest_10_submissions_in_a/"}, {"author": "deten", "body": "Not do it but get me started.", "created_utc": 1385620644, "gilded": 0, "name": "t1_cdos6lg", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rlhzs/praw_retrieving_the_newest_10_submissions_in_a/"}, {"author": "gosslot", "body": "In your Python installation there should be a folder named \"Scripts\". If you have successfully installed pip, there should be a pip.exe. The command pip install praw should be issued from Windows command prompt. To install PIP on Windows is ironically quite complicated itself. I myself just went with [WinPython](http://code.google.com/p/winpython/), a python installation that comes with many useful packages (like PIP) pre-installed.", "created_utc": 1385575383, "gilded": 0, "name": "t1_cdoaxmc", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": "WaitForItTheMongols", "body": "Update: /u/gosslot's suggestion worked. WinPython was much better than what I had.", "created_utc": 1385586060, "gilded": 0, "name": "t1_cdofrko", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": "_Daimon_", "body": "Have you tried following the guides we listed in the installation step? If you did, I'd love to hear what was unclear/confusing to you. Because rather than try to walk you through it personally, I'd prefer figuring out how to improve the installation guide we point to. It just scales better :) [Our installation instructions](https://praw.readthedocs.org/en/latest/#installation) > If you don\u2019t have pip installed, then the Hitchhiker\u2019s Guide to Python has a section for setting it up on Windows, Mac and Linux. There is also a Stack overflow question on installing pip on Windows that might prove helpful.", "created_utc": 1385576653, "gilded": 0, "name": "t1_cdobihg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": "WaitForItTheMongols", "body": "Well, the big thing is that easy_install.py just says something like \"Easy Install didn't work. Something went wrong.\" And I don't know how to fix it.", "created_utc": 1385583978, "gilded": 0, "name": "t1_cdoeuuo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": "_Daimon_", "body": "Sorry, I accidentally didn't include the links in my quoting of our installation instructions. Not very useful then. Glad you found a solution :)", "created_utc": 1385587719, "gilded": 0, "name": "t1_cdogh7c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": null, "body": "the easiest way I found was to download/unzip the files from [github](https://github.com/praw-dev/praw). click the '.zip' download on the right hand side. Unzip it to a directory you can easily navigate to. Open up a Powershell and navigate to that directory once you are in that directory type 'python setup.py' and it should install PRAW", "created_utc": 1385618012, "gilded": 0, "name": "t1_cdorf5b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkv8u/yet_another_praw_installation_issue/"}, {"author": "rcxdude", "body": "I think this can be fixed in PRAW. It looks like the 'r' value needs to be set in the POST request otherwise reddit treats the post as a post to the frontpage for the purposes of validation. Currently it looks like it is only set on certain requests. This is based on reading the code on github, not experimenting because I'm lazy. Relevant code: [validation code](https://github.com/reddit/reddit/blob/f7c2ebb6efbfd172a22a2c249160c2e61293ef05/r2/r2/lib/validator/validator.py#L624) Note that it reads from c.site [part which sets c.site](https://github.com/reddit/reddit/blob/ac25e39dda668e4260e76085fee1b18222ede837/r2/r2/controllers/reddit_base.py#L335) the only [two](https://github.com/praw-dev/praw/blob/19ddbd04541ac372b8c7c3c35d0630ec4da126e3/praw/objects.py#L618) [places](https://github.com/praw-dev/praw/blob/19ddbd04541ac372b8c7c3c35d0630ec4da126e3/praw/internal.py#L89) where PRAW sets 'r' in the POST request that I can find.", "created_utc": 1385685079, "gilded": 0, "name": "t1_cdp925n", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkmao/praw_im_getting_apiexception_too_long_when_trying/"}, {"author": "swollen_pickle", "body": "Thank you! This is definitely the issue. Adding `data['r'] = six.text_type(subreddit)` to the submit request stops the problem from occurring and allows me to post my stupidly long strings. However it seems kind of redundant to have two parameters now set to the subreddit name, both `'sr'` and `'r'` in the submit request.", "created_utc": 1385717655, "gilded": 0, "name": "t1_cdphuve", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rkmao/praw_im_getting_apiexception_too_long_when_trying/"}, {"author": "bboe", "body": "> https://praw.readthedocs.org/en/PRAW-__1.0.9__/praw.html#praw.objects.LoggedInRedditor.my_reddits My guess is you're running a different version of PRAW than the docs you're looking at. Try `r.user.get_my_reddits()`", "created_utc": 1385327524, "gilded": 0, "name": "t1_cdm30wp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1rcl4u/praw_praw_throwing_an_attributeerror_when_trying/"}, {"author": "_Daimon_", "body": "That's weird. 403 errors are caused when trying to access restricted information with insufficient authentication. Now, when I say \"insufficient\" I don't mean insufficient OAuth scope which would result in a `OAuthInsufficientScope` error. Instead I mean cases such as accessing a private subreddit, without being allowed. Getting it for `get_unread`, but not `get_mod_mail` is weird. Like you said, it's the same scope and is very much alike. Does `get_inbox` or `get_sent` result in any errors? Do the same occur if you authenticate via user/password rather than OAuth?", "created_utc": 1385247254, "gilded": 0, "name": "t1_cdlfywv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1r9ci4/get_unread_giving_httperror_403_client_error/"}, {"author": "atomicUpdate", "body": "get_inbox() works, but I haven't tried anything else, or logging in with my user/password, so I can try giving that a shot tomorrow (not sure how big of a change that will be, but hopefully it will be relatively straightforward). Anything else I can try or data I can get while I'm playing around with it?", "created_utc": 1385264881, "gilded": 0, "name": "t1_cdlm4cm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1r9ci4/get_unread_giving_httperror_403_client_error/"}, {"author": "NotSinceYesterday", "body": "Another minor issue: I can't seem to get py2exe to work with praw. Is this a known issue? A search only brought up one other instance of this and it was not resolved in the thread. It's not a major issue, but I wanted to give my other mods an .exe of the bot so they didn't need to install python and praw. Is there another method for producing an executable?", "created_utc": 1384782565, "gilded": 0, "name": "t1_cdh48py", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "_Daimon_", "body": "Unknown issue. What error are you getting that's preventing you from using PRAW with py2exe and what was the other thread where this issue was raised?", "created_utc": 1384812414, "gilded": 0, "name": "t1_cdhgh8o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "NotSinceYesterday", "body": "http://en.reddit.com/r/learnprogramming/comments/1efarx/python_trying_to_create_an_executable_from_a/ I get the same issue as here.", "created_utc": 1384812542, "gilded": 0, "name": "t1_cdhgj9p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "_Daimon_", "body": "Looks like py2exe modifies the path to the system packages, causing PRAW to crash as it cannot find a configuration file. Try copying [praw.ini](https://github.com/praw-dev/praw/blob/master/praw/praw.ini) from the github repo and place it in the same directory as the script you're trying to compile. This should make PRAW use this as the configuration and being unable to find the global configuration file, which is usually used as a basis, won't matter.", "created_utc": 1384813257, "gilded": 0, "name": "t1_cdhgv1e", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "NotSinceYesterday", "body": "I now have a new error: >C:\\Python27\\dist>porygon2.exe >Traceback (most recent call last): > File \"porygon2.py\", line 3, in > File \"praw\\__init__.pyc\", line 42, in > File \"six.pyc\", line 84, in __get__ > File \"six.pyc\", line 103, in _resolve > File \"six.pyc\", line 74, in _import_module >ImportError: No module named htmlentitydefs", "created_utc": 1384852741, "gilded": 0, "name": "t1_cdhuz7d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "_Daimon_", "body": "It seems that ImportError is fairly common with py2exe. Have you read this [relevant entry in their FAQ](http://www.py2exe.org/index.cgi/DealingWithImportError)?", "created_utc": 1384865259, "gilded": 0, "name": "t1_cdhwsbj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "reseph", "body": "Is your praw up to date? This wasn't added until v2.1.5", "created_utc": 1384799907, "gilded": 0, "name": "t1_cdhamfd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "NotSinceYesterday", "body": "Thanks! I even updated it for the py2exe problem in my comment, and just haven't tested the sticky since then. I feel a bit stupid...", "created_utc": 1384800714, "gilded": 0, "name": "t1_cdhazax", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qw2u3/sticky_posts/"}, {"author": "_Daimon_", "body": "Read the [getting started](http://praw.readthedocs.org/en/latest/pages/getting_started.html) page in our documentation. It shows how to get a `Redditor` object, which you can then use to get an overview, a listing of their comments or whatever it is you want.", "created_utc": 1384771288, "gilded": 0, "name": "t1_cdh2czq", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qvzkd/praw_opening_a_user_page/"}, {"author": "DAsSNipez", "body": "...I figured that out minutes after I posted this question, I need a way to stall my questions, there's something about posting that makes the answers pop up. Cheers for the response!", "created_utc": 1384771651, "gilded": 0, "name": "t1_cdh2eru", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qvzkd/praw_opening_a_user_page/"}, {"author": "bboe", "body": "http://en.wikipedia.org/wiki/Rubber_duck_debugging", "created_utc": 1384879360, "gilded": 0, "name": "t1_cdi1alu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qvzkd/praw_opening_a_user_page/"}, {"author": "rreyv", "body": "I wrote the /r/cricket match thread bot. It creates a game thread 1 hour before a match begins and maintains live score as well. It also creates a table in the sidebar with upcoming fixtures and the time left. Take a look-see - /r/cricket. What subreddit are you trying to do this for? [Here's my project on github.](https://github.com/rreyv/r-Cricket-Bot) Take a look and I can help with any questions you have. This is how you post things to subreddits for a self-post: submission = r.submit(subreddit, threadTitle, text=threadText)", "created_utc": 1384383666, "gilded": 0, "name": "t1_cddpqq7", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "NEBRASSKICKER", "body": "i'm 100% new to programing so go easy on me.. I need a bot for my sub /r/huskies that will submit games threads [like this] (http://www.reddit.com/r/huskies/comments/1q9zp4/game_thread_colorado_washington_500pm_pst/) one hour before the game. The task has proven much harder than I anticipated and people I try to get help with assume I already very familiar with programing. I've downloaded python and praw and I need help with getting python to post the thread at a certain time..I've been going at this for about 7 hours... Any Help?? Edit: I posted the question to stack overflow [here's the question] (http://stackoverflow.com/questions/19973888/how-to-schedule-a-python-script-to-run-at-a-certain-time). I haven't really gotten anywhere.", "created_utc": 1384427546, "gilded": 0, "name": "t1_cde3iue", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "Dustin-", "body": "I agree with /u/rreyv. Try making some smaller stuff before diving into a project like this. Codeacadamy as a good beginners course for those who are new at coding.", "created_utc": 1384543709, "gilded": 0, "name": "t1_cdf5afa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "rreyv", "body": "I would suggest you start off with smaller projects before you aim for a bot like this. There are many many ways to schedule tasks using python and once you've done the smaller stuff, I can guarantee you'll figure out scheduling on your own. Once you've got the basics understood, this'll be easier. Creating a bot is easy but not for a beginner.", "created_utc": 1384457542, "gilded": 0, "name": "t1_cdedbiq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "AndrewNeo", "body": "After logging in, you need to call get_subreddit(\"name\") to fetch the sub you want to post to! then call submit(title, text=\"Post text\") to post to the sub. Just use your system scheduler to post. cron if you're on Linux, Task Scheduler if you're on Windows.", "created_utc": 1384339312, "gilded": 0, "name": "t1_cdd9j2b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "NEBRASSKICKER", "body": "Could you explain how i would go about doing this. like in python do i type r.get_subreddit(\"name\") after I get it to log in? what is post. cron on my scheduler?", "created_utc": 1384345613, "gilded": 0, "name": "t1_cddaf4t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "s32", "body": "Google 'cron'", "created_utc": 1384360522, "gilded": 0, "name": "t1_cddez2a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "AndrewNeo", "body": "Here's a quick example: import praw r = praw.Reddit(user_agent=\"UA goes here\") r.login(\"username\", \"password\") sub = r.get_subreddit(\"Subreddit name goes here\") sub.submit(\"Title goes here\", text=\"Post text goes here\") Cron is the name of the scheduler. Check the man page or search for it.", "created_utc": 1384375812, "gilded": 0, "name": "t1_cddm2t6", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "NEBRASSKICKER", "body": "Ok so i'm on windows (kind of a brain fart with cron..haha)...and i'm trying to use the task scheduler. how would i make it so it runs the code above at a certain time? save it as a python file then have task scheduler open it up at a certain time? would this actually run the code or just open the file...thank you for your help!", "created_utc": 1384397420, "gilded": 0, "name": "t1_cddvk8z", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "AndrewNeo", "body": "Sorry, I'm not running Windows right now so I can't help too much. I think you need to save it as a python file, have it run python.exe at the set time and pass it the script name as the argument, and make sure to set the running directory as the directory the script is saved in.", "created_utc": 1384397940, "gilded": 0, "name": "t1_cddvskm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "NEBRASSKICKER", "body": "Yeah..not much luck...i had it run the file test.py which had the code on it and nothin...", "created_utc": 1384399256, "gilded": 0, "name": "t1_cddwd56", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "NEBRASSKICKER", "body": "Ok...I will try this! Thank You", "created_utc": 1384398250, "gilded": 0, "name": "t1_cddvxg7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "NEBRASSKICKER", "body": "Thank You!", "created_utc": 1384396017, "gilded": 0, "name": "t1_cdduyoj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1qiu59/help_with_making_auto_posting_bot/"}, {"author": "_Daimon_", "body": "Don't use the `created` attribute for comparing timestamps. It is the server time and since reddit uses distributed servers this is completely unreliable. Always use `created_utc`. Which is the timestamp of creation on Universal timezone.", "created_utc": 1383781043, "gilded": 0, "name": "t1_cd8ff1b", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1q13hd/praw_using_place_holder_still_returns_posts_older/"}, {"author": "davidystephenson", "body": "Thanks for the note! However, this still does not seem to solve the problem. Changing the code to: import os import praw r = praw.Reddit('aegis 1.0 by /u/davidystephenson') terms = ['scrum', 'fight'] if os.path.exists('last.txt'): with open('last.txt', 'r') as file: last = file.read() last_created_utc = r.get_submission(submission_id=last).created_utc print('last test', last, last_created_utc) else: print('Warning: no last.txt file') last = None subreddits = ['rugbyunion', 'hockey'] submissions = [] for subreddit in subreddits: submissions += list( r.get_subreddit(subreddit).get_new(limit=20, place_holder=last) ) print('length test', len(submissions)) for submission in submissions: text = submission.selftext.lower() title = submission.title.lower() matches = [term for term in terms if term in text or term in title] print( 'after test', submission.created_utc, last_created_utc, submission.created_utc > last_created_utc, submission.short_link, ) if matches and submission.id != last: message = ('matches: ' + str(matches) + ' ' + submission.short_link) print(message) else: pass # no matches if submission.created_utc > last_created_utc: with open('last.txt', 'w') as file: file.write(submission.id) Still repeatedly returns a list full of old values: last test 1q3sd0 1383837889.0 length test 24 after test 1324653662.0 1383837889.0 False http://redd.it/nnz15 after test 1324350637.0 1383837889.0 False http://redd.it/njcky after test 1324339049.0 1383837889.0 False http://redd.it/nj4fz after test 1324258832.0 1383837889.0 False http://redd.it/nhwp0 after test 1323190963.0 1383837889.0 False http://redd.it/n2nyi after test 1323123716.0 1383837889.0 False http://redd.it/n1ne5 after test 1323092218.0 1383837889.0 False http://redd.it/n12ts after test 1322913039.0 1383837889.0 False http://redd.it/mynkf after test 1322750462.0 1383837889.0 False http://redd.it/mw5bs after test 1321621476.0 1383837889.0 False http://redd.it/mgvc5 after test 1321585437.0 1383837889.0 False http://redd.it/mgh5e after test 1321581807.0 1383837889.0 False http://redd.it/mgf12 after test 1321530988.0 1383837889.0 False http://redd.it/mflcn after test 1321446493.0 1383837889.0 False http://redd.it/mebag after test 1321255851.0 1383837889.0 False http://redd.it/mbmhy after test 1320730653.0 1383837889.0 False http://redd.it/m4ich after test 1320705280.0 1383837889.0 False http://redd.it/m41qo after test 1320520377.0 1383837889.0 False http://redd.it/m1mrt after test 1320399238.0 1383837889.0 False http://redd.it/m04bg after test 1319990581.0 1383837889.0 False http://redd.it/lu5au after test 1383838500.0 1383837889.0 True http://redd.it/1q3t6e after test 1383838498.0 1383837889.0 True http://redd.it/1q3t6a after test 1383838040.0 1383837889.0 True http://redd.it/1q3skh after test 1383837889.0 1383837889.0 False http://redd.it/1q3sd0", "created_utc": 1383842090, "gilded": 0, "name": "t1_cd8wl20", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1q13hd/praw_using_place_holder_still_returns_posts_older/"}, {"author": "_Daimon_", "body": "As bboe said, the submissions in the new listing are sorted by when they where added to the new queue not their creation time. If a moderator swings by, mass approves a bunch of submissions that has been stuck in the spam filter then all submissions after the placeholder could well be younger than it.", "created_utc": 1384022757, "gilded": 0, "name": "t1_cdah5qt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1q13hd/praw_using_place_holder_still_returns_posts_older/"}, {"author": "davidystephenson", "body": "What is the best way to reliably get only submissions after a submission then?", "created_utc": 1384222505, "gilded": 0, "name": "t1_cdc7zut", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1q13hd/praw_using_place_holder_still_returns_posts_older/"}, {"author": "bboe", "body": "As I recall submissions that enter the spam queue and then are approved will appear in the new listing at the time of approval, not at the time of creation. It's also possible that your placeholder submission is removed, thus it doesn't show up in the listing on a subsequent request.", "created_utc": 1383753730, "gilded": 0, "name": "t1_cd83bok", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1q13hd/praw_using_place_holder_still_returns_posts_older/"}, {"author": "davidystephenson", "body": "That doesn't seem to be an issue: often all results test as \"False\" (that is, dated before `last_created`). For example, running a search for \"ukraine\" in /r/geopolitics with a limit of 20 returns: last test 1q0zkw 1383778127.0 length test 20 after test 1383743049.0 1383778127.0 False after test 1383738593.0 1383778127.0 False after test 1383720420.0 1383778127.0 False matches test http://redd.it/1pzesl ['ukraine'] after test 1383672394.0 1383778127.0 False after test 1383657110.0 1383778127.0 False after test 1383596146.0 1383778127.0 False after test 1383572155.0 1383778127.0 False after test 1383561922.0 1383778127.0 False after test 1383546313.0 1383778127.0 False after test 1383540940.0 1383778127.0 False after test 1383522016.0 1383778127.0 False after test 1383487187.0 1383778127.0 False after test 1383309728.0 1383778127.0 False after test 1383294259.0 1383778127.0 False after test 1383291938.0 1383778127.0 False after test 1383266995.0 1383778127.0 False after test 1383240858.0 1383778127.0 False after test 1383230463.0 1383778127.0 False after test 1383155985.0 1383778127.0 False after test 1383099494.0 1383778127.0 False Running the script again returns the same results.", "created_utc": 1383755263, "gilded": 0, "name": "t1_cd83ya0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1q13hd/praw_using_place_holder_still_returns_posts_older/"}, {"author": "SOTB-human", "body": "If I recall correctly, `flat_comments` becomes a \"generator object\" that queries reddit every time you iterate through it. The way I usually do it is: flat_comments = [x for x in praw.helpers.flatten_tree(submission.comments)] Then, it will make all of the queries at that point, and store the results in memory.", "created_utc": 1383674240, "gilded": 0, "name": "t1_cd7dyys", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pyczr/store_api_call_and_code_cleanup_help/"}, {"author": "AdamJacobMuller", "body": "This is correct, but, assuming you're doing \"simple\" conversion of generator -> list you can do it much more simply than a comprehension, just flat_comments = list(praw.helpers.flatten_tree(submission.comments)) Will convert a generator to a list. Obviously anything more complex than a totally flat/simple conversion is going to (probably) be well done with a comprehension.", "created_utc": 1383703506, "gilded": 0, "name": "t1_cd7qjbj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pyczr/store_api_call_and_code_cleanup_help/"}, {"author": "_Daimon_", "body": "Post traceback.", "created_utc": 1383598111, "gilded": 0, "name": "t1_cd6q5bs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "The_Gingey", "body": "Can you please tell me what traceback is? This is the full error: Traceback (most recent call last): File \"/Users/The_Gingey/Desktop/praw.py\", line 3, in import praw File \"/Users/The_Gingey/Desktop/praw.py\", line 5, in r = praw.Reddit(user_agent='my_cool_application') AttributeError: 'module' object has no attribute 'Reddit' That is the full error.", "created_utc": 1383598236, "gilded": 0, "name": "t1_cd6q7au", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "_Daimon_", "body": "That's the traceback and it confirmed what I thought. Don't name your files or directories the same as libraries. Local dir is found before system packages when looking through PYTHON_PATH. So it found the file and thought that was what you wanted to import from. Your file obviously doesn't have anything named `Reddit`, so it crashes with that error.", "created_utc": 1383598432, "gilded": 0, "name": "t1_cd6qagi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "The_Gingey", "body": "Thank you! I think I seriously messed something up though... When I try to import praw, idle says there is no module is named praw. But when I use the command line, it works. Would you happen to know what I'm doing wrong there?", "created_utc": 1383598700, "gilded": 0, "name": "t1_cd6qeub", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "_Daimon_", "body": "I don't know. Does IDLE work with other libraries? I don't use IDLE and I can't remember anyone with similar issue, so your best bet will probably be to either google for the error or not use IDLE. For quick testing I use the interactive interpreter, just write `python` on the commandline and it opens.", "created_utc": 1383599181, "gilded": 0, "name": "t1_cd6qmob", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "The_Gingey", "body": "Ok, I will try that. Thanks for your help Daimon. The internet is great. I can run in fine through terminal and unitron, but not IDLE. Whatever. Thanks again!", "created_utc": 1383599424, "gilded": 0, "name": "t1_cd6qqmy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pwda4/problem_using_reddit_api/"}, {"author": "_Daimon_", "body": "Just use a try .. except block inside the for loop?", "created_utc": 1382875844, "gilded": 0, "name": "t1_cd0krus", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1pag1x/get_comments_older_than_placeholder/"}, {"author": "_Daimon_", "body": "If you're learning to use PRAW then IMO the best way is with the official [getting started](http://praw.readthedocs.org/en/latest/pages/getting_started.html) tutorial. I can't see what goes wrong for you with this script, because that line is not part of the linked code and you didn't post a traceback.", "created_utc": 1382873653, "gilded": 0, "name": "t1_cd0ki91", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1paezw/help_with_a_keyerror_using_praw/"}, {"author": "kirbz1692", "body": "Hey this reply is to simply draw you back to the post. My problem is happening again and I added the traceback to the post, do you think you could take a quick look at it?", "created_utc": 1383106571, "gilded": 0, "name": "t1_cd2p1a0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1paezw/help_with_a_keyerror_using_praw/"}, {"author": "_Daimon_", "body": "You are encountering a `RateLimitError` which comes from exceeding reddits API limit. PRAW protects you against the regular limit, but for certain actions such as logging in, messaging or posting content this limit is lower. Since it's also dependent on super secret anti-spamming code, we cannot know in advance whether something will exceed the API limit for those methods. So it's generating a `RateLimitError`. That error contains a 'ratelimit' argument that tells you how long to wait before you can go again. It's the x in the \"you're doing that too much, please wait x time\" you might have seen on webend if you're exceeding the ratelimit there. Somehow though, that argument isn't included in the json returned by reddit. I'm not sure why that is. It could be a temporary thing or that for some cases it doesn't tell us how much. If you go on the webend and try logging in, then it should tell you how long to wait for. In general, when you're writing code that requires logging in. Try postponing adding the log in code until the very last time, as running code that logs in in quick succession will prompt the `RateLimitError`.", "created_utc": 1383865843, "gilded": 0, "name": "t1_cd9792h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1paezw/help_with_a_keyerror_using_praw/"}, {"author": "kirbz1692", "body": "Oh wow are you Daimon the developer, thanks for even looking at my code. The problem fixed itself somehow, idk but it works now so thanks anyway! **EDIT:** Just so you know, here's the traceback: Traceback (most recent call last): File \"C:/Users/ME/PycharmProjects/RedditBot/tutorial\", line 9, in r.login(\"MYUSERNAME\",\"MYPASSWORD\"); File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\__init__.py\", line 1157, in login self.request_json(self.config['login'], data=data) File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\decorators.py\", line 172, in wrapped return_value)) File \"C:\\Users\\ME\\.virtualenvs\\PRAW\\lib\\site-packages\\praw\\errors.py\", line 325, in __init__ self.sleep_time = self.response['ratelimit'] KeyError: 'ratelimit'", "created_utc": 1382905559, "gilded": 0, "name": "t1_cd0tq4c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1paezw/help_with_a_keyerror_using_praw/"}, {"author": "_Daimon_", "body": "Try import pprint import praw r = praw.Reddit(UNIQUQ_AND_DESCRIPTIVE_USERAGENT) s = r.get_subreddit('redditdev', fetch=True) pprint.pprint(vars(s)) See the [writing a bot](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) page in PRAW's documentation for more information on python introspection and [lazy loading](http://praw.readthedocs.org/en/latest/pages/lazy-loading.html) of objects for why the `fetch=True` is included.", "created_utc": 1382739412, "gilded": 0, "name": "t1_cczo8k4", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "Zarqu0n", "body": "thanks for your answer. is there also a way to get multiple subscriber counts?", "created_utc": 1382771960, "gilded": 0, "name": "t1_cczwu8e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "_Daimon_", "body": "There isn't a method via PRAW to get it in bulk. You'll need to request each subreddit separately.", "created_utc": 1382787099, "gilded": 0, "name": "t1_cczyhlt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "gavin19", "body": "There may be a better method but subs = r.get_subreddit('python').subscribers will return an int with the numbers of subscribers.", "created_utc": 1382739071, "gilded": 0, "name": "t1_cczo4ga", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "Zarqu0n", "body": "Thank you! I was reading the docs, searching for anything about subscribers, but didn't find it,for some reason.", "created_utc": 1382739336, "gilded": 0, "name": "t1_cczo7oz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "gavin19", "body": "Usually applying `dir` can be a big help. Try sub = r.get_subreddit('python') dir(sub) for example.", "created_utc": 1382739691, "gilded": 0, "name": "t1_cczobwf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p7zhy/how_do_you_find_out_the_subscriber_count_of_a/"}, {"author": "_Daimon_", "body": "The comment is in Markdown, so you need either \" \\n\" at the end of a line for a short line break or \"\\n\\n\" for a end of paragraph. So something like submission.add_comment('Hello \\nWorld') Read the [Markdown Primer](http://www.reddit.com/r/reddit.com/comments/6ewgt/reddit_markdown_primer_or_how_do_you_do_all_that/c03nik6) for more info on using Markdown with Reddit.", "created_utc": 1382693572, "gilded": 0, "name": "t1_ccz9qu0", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p5z3c/praw_creating_multiline_comment_from_list_of/"}, {"author": "ascetica", "body": "Hi, thanks for the reply. I was able to get it to work using this method. The list has to be formatted like this: jurassic_park_quote_list = [\"Dr. Alan Grant: T-Rex doesn\\'t want to be fed. He wants to hunt. Can\\'t just suppress 65 million years of gut instinct.\", \"Dr. Alan Grant: [finding egg shells] Oh my God. Do you know what this is? This is a dinosaur egg. The dinosaurs are breeding.\\n\\nTim: But Grandpa said all the dinosaurs were girls.\\n\\nDr. Alan Grant: Amphibian DNA.\\nLex: What\\'s that?\\n\\nDr. Alan Grant: Well, on the tour, the film said they used frog DNA to fill in the gene sequence gaps. They mutated the dinosaur genetic code and blended it with that of a frog\\'s. Now, some West African frogs have been known to spontaneously change sex from male to female in a single sex environment. Malcolm was right. Look...\\n\\n[we see a trail of baby dinosaur footprints\\n\\nDr. Alan Grant: Life found a way.\", \"Dr. Ian Malcolm: [while being chased by the T-Rex] Must go faster.\"] Unfortunately I haven't yet found a way to make it work when importing text form an XML. Thanks for the help!", "created_utc": 1382750873, "gilded": 0, "name": "t1_cczrtxq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p5z3c/praw_creating_multiline_comment_from_list_of/"}, {"author": "not_alot_bot", "body": "I'm not sure if you mean ----- >comment.permalink (you can append your own \"?context=x\" string in some manner) or >comment.submission.url", "created_utc": 1382591019, "gilded": 0, "name": "t1_ccygi3f", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "hinayu", "body": "Either of those will work. I just didn't know they existed because the object is lazily loaded I think. Thanks.", "created_utc": 1382615312, "gilded": 0, "name": "t1_ccykdkt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "bboe", "body": "The docs list dynamically generated properties of objects: https://python-reddit-api-wrapper.readthedocs.org/en/latest/praw.html#praw.objects.Comment", "created_utc": 1382638523, "gilded": 0, "name": "t1_ccysabr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "hinayu", "body": "Okay, another question. When doing for comment in comment: I was getting results pretty fast. Now I'd like to also get the permalink: It looks like it significantly slows down the process of getting comments. Is there anything I can do to speed that up? context_url = comment.permalink + \"?context=1\"", "created_utc": 1382658164, "gilded": 0, "name": "t1_ccz05xk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "bboe", "body": "Yes, you can build the comment manually if you don't require the _actual_ permalink. For instance the permalink to your comment is: http://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/ccz05xk which includes the submission title (the submission must be fetched for that which is why `permalink` is slow. You can, however, actually reduce that url just to: http://www.reddit.com/comments/1p3qau/_/ccz05xk You can actually replace the `_` with anything between the two `/`. The first base36 number is the submission id, and the second is the comment id.", "created_utc": 1382658977, "gilded": 0, "name": "t1_ccz0go8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "hinayu", "body": "Hmm. Sorry for another question. When I inspect the comment object, I see 'id', which is the comment id. Where is the submission id located? I see that I can get the submission and then could get the submission ID through that, but that seems like it'd be just as slow as getting the actual permalink. I must be missing something there.", "created_utc": 1382662733, "gilded": 0, "name": "t1_ccz1tww", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "bboe", "body": "I think it's the link_id. I haven't actually verified, however. Try it and see if it works. Edit: Yes that's it, but you have to strip off the `t3_` part.", "created_utc": 1382678099, "gilded": 0, "name": "t1_ccz73o2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "hinayu", "body": "I was wondering about that. Thanks.", "created_utc": 1382702396, "gilded": 0, "name": "t1_cczawh1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "hinayu", "body": "I wondered if I could build it by using the submission_id and comment_id. I'll give it a shot; thanks!", "created_utc": 1382660835, "gilded": 0, "name": "t1_ccz14sj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "_Daimon_", "body": "I think that's just a list of the property decorated methods.", "created_utc": 1382640751, "gilded": 0, "name": "t1_ccyt89d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "bboe", "body": "> dynamically generated properties of objects ;)", "created_utc": 1382641652, "gilded": 0, "name": "t1_ccytldq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "_Daimon_", "body": "IMO all the properties of the objects are dynamically generated, as they are generated dynamically depending on the returned json from reddit.", "created_utc": 1382642739, "gilded": 0, "name": "t1_ccyu1s6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "bboe", "body": "Different terms for different folks. I use `attribute` exclusively to refer to named-values of an object (visible via `vars`), and `properties` to refer to functions decorated with `property`.", "created_utc": 1382658587, "gilded": 0, "name": "t1_ccz0bk2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p3qau/praw_constructing_url_from_a_comment/"}, {"author": "Deimorz", "body": "The problem is that all of the comments are loaded at once, and the flair isn't updated on them to reflect updates made to that user. For example, when you load the list of comments, you get 4 things like this (simplified): #1 (parent): author = rog1122, author_flair_css_class = '', author_flair_text = '' #2 (child): author = rog1121, author_flair_css_class = '', author_flair_text = '' #3 (child): author = rog1121, author_flair_css_class = '', author_flair_text = '' #4 (child): author = rog1121, author_flair_css_class = '', author_flair_text = '' Now, when you process those comments, your code looks at `author_flair_css_class` and `author_flair_text` to decide what to set the user's flair to. So you look at comment #2, update rog1121's flair to class \"1\", but then you look at comment #3, and `author_flair_css_class` and `author_flair_text` on that weren't updated to the new flair, they're still both the blanks that they were at the time of loading. So now it'll set to `1` again, and the same again for comment #4. You'll need to do something like track the flair changes made so far for each user and apply the checks/updates to that *modified* flair, not what their flair was at the time the comments were originally loaded.", "created_utc": 1382517232, "gilded": 0, "name": "t1_ccxqtca", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p0xc9/script_counting_multiple_comments_as_one/"}, {"author": "rog1121", "body": "Shouldn't line 42 fix that though? How is it that the parent comment is being updated with its flair values throughout the script and child comments aren't? comment.author_flair_css_class = child_css and parent.author_flair_css_class = parent_css I'm guessing that only updates the value for the #2 child and doesn't update them for the rest? The problem also isn't just with empty flairs, this is the outcome of running the script for a second time with non-empty values. Before: http://i.imgur.com/1XLHeqK.png After: http://i.imgur.com/6ANlSa1.png", "created_utc": 1382539593, "gilded": 0, "name": "t1_ccxvbrs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p0xc9/script_counting_multiple_comments_as_one/"}, {"author": "Deimorz", "body": "The situations are different because all 3 children have the same parent, so when you update the flair on that parent, it's been updated when the other children look at it later. The children are separate though, so to get equivalent behavior, whenever you made a flair update you'd have to look through the whole list of comments for all other comments made by the same author, and update the flair on all of them.", "created_utc": 1382551417, "gilded": 0, "name": "t1_ccy0968", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p0xc9/script_counting_multiple_comments_as_one/"}, {"author": "rog1121", "body": "That's what I was thinking, I managed to fix it with this for com in flat_comments: if com.author == comment.author: com.author_flair_css_class = child_css Thanks a lot", "created_utc": 1382552717, "gilded": 0, "name": "t1_ccy0tlc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1p0xc9/script_counting_multiple_comments_as_one/"}, {"author": "_Daimon_", "body": "From my reply to your last post that you linked at the top of this one. > A good solution to reduce number of API calls to retrieve a submission with loads of comments is to first buy gold for the account then changing the number of comments loaded with a submission to 1500 in [the user preferences](https://ssl.reddit.com/prefs/). Go to your user preferences. The link is right above. Find the option with how many comments are loaded with a submission. The default value is 200. Change that value to 1500. Click \"Save Options\"", "created_utc": 1382345058, "gilded": 0, "name": "t1_ccw7uxe", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ovi6a/just_bought_reddit_gold_how_do_i_change_default/"}, {"author": "Raerth", "body": "> Change that value to 1500. That will only go up to 500. You can't have reddit display 1500 by default but need to call 1500 by clicking the link in a large thread.", "created_utc": 1382356881, "gilded": 0, "name": "t1_ccw9dz4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ovi6a/just_bought_reddit_gold_how_do_i_change_default/"}, {"author": "nareik15", "body": "So for the purposes of fetching these comments using the API/PRAW, is there any way to request a page with 1500 comments. As I had said, I get an error when adding \"?limit=1500\" to the end of urls (which I think may be something to do with dynamic pages). Given that it is possible to retreive 1500 comments for a page through the browser, surely there should be a working PRAW function that could retrieve that page? Again, thanks for the help.", "created_utc": 1382362140, "gilded": 0, "name": "t1_ccwahvw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ovi6a/just_bought_reddit_gold_how_do_i_change_default/"}, {"author": "bboe", "body": "Have you tried `get_submission('regular url', limit=1500)`? Edit, sorry it's: get_submission('regular url', comment_limit=1500) The documentation is usually pretty helpful for the functions you are using: [get_submission](https://praw.readthedocs.org/en/latest/pages/code_overview.html?highlight=get_submission#praw.__init__.UnauthenticatedReddit.get_submission)", "created_utc": 1382467028, "gilded": 0, "name": "t1_ccx98k3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ovi6a/just_bought_reddit_gold_how_do_i_change_default/"}, {"author": "_Daimon_", "body": "Can you be a bit more specific on what you're trying to do precisely and how the bottom part of the code doesn't work?", "created_utc": 1382206440, "gilded": 0, "name": "t1_ccv32k5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1osc2c/parent_id_of_comment_actions/"}, {"author": "rog1121", "body": "Basically perform the same actions to the parent comment, since parent_id simply returns the id and it doesn't function as an object like \"comment\" on line 15 EDIT: to clarify, this is the part that doesn't work http://i.imgur.com/V45onTj.png The comment.parent_id.author_flair_text part", "created_utc": 1382207007, "gilded": 0, "name": "t1_ccv39hu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1osc2c/parent_id_of_comment_actions/"}, {"author": "_Daimon_", "body": "Ah. Good point, I'll add a `parent` attribute to `Comment`s that are lazy objects of their parents. Which would mean the submission itself for top level and another comment for non-top level. The simplest way for now if you already have a flat list of all comments is to look through that list for a comment with the same id as `parent_id`. Which is a `Comment` for the parent. parent_comment = [com for com in flat_comments if com.fullname == comment.parent_id][0]", "created_utc": 1382212353, "gilded": 0, "name": "t1_ccv52uc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1osc2c/parent_id_of_comment_actions/"}, {"author": "aaron1312", "body": "If you're viewing it in Firefox, Firefox will add an extra TBODY in most cases. Get rid of that in your parsing and you may be good after that.", "created_utc": 1382763591, "gilded": 0, "name": "t1_cczv91x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1osc2c/parent_id_of_comment_actions/"}, {"author": "_Daimon_", "body": "I don't know. This question is about importing and running a 3rd party library, it has nothing to do with PRAW specifically. So you should ask it in r/learnpython.", "created_utc": 1382011907, "gilded": 0, "name": "t1_cctir6g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1omaox/installing_praw_into_python/"}, {"author": "_Daimon_", "body": "A good solution to reduce number of API calls to retrieve a submission with loads of comments is to first buy gold for the account then changing the number of comments loaded with a submission to 1500 in [the user preferences](https://ssl.reddit.com/prefs/). There aren't any way to track the progress of the `replace_more_comments` method. It's also a bit problematic implementing it if you're requesting all `MoreComment`s. As a `MoreComment` may contain other `MoreComment`objects. So it's impossible to know the size of the task before it is completed.", "created_utc": 1381855276, "gilded": 0, "name": "t1_ccs77vg", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oi5fb/a_number_of_questions_about_the_replace_more/"}, {"author": "nareik15", "body": "thanks for the reply, I had thought about this thing of initially getting more comments before and had tried adding \"limit=500\" on the end of fetched URLs but this didn't work. Didn't realise that this could be done properly with preferences. this should help a lot. :)", "created_utc": 1382034064, "gilded": 0, "name": "t1_cctqbr9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oi5fb/a_number_of_questions_about_the_replace_more/"}, {"author": "hiles", "body": "It would help a lot if you posted the crash message.", "created_utc": 1381647100, "gilded": 0, "name": "t1_ccqovce", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "body": "Sorry. Here it is: Traceback (most recent call last): File \"C:\\Python27\\botTest.py\", line 34, in main() File \"C:\\Python27\\botTest.py\", line 27, in main r.send_message(author,subLine,msg) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 303, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 205, in wrapped return function(obj, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 1909, in send_message retry_on_error=False) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 141, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 479, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 348, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 321, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\adapters.py\", line 354, in send raise ConnectionError(e) ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /api/compose/.json (Caused by : [Errno 10054] An existing connection was forcibly closed by the remote host)", "created_utc": 1381670682, "gilded": 0, "name": "t1_ccqs1d6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "_Daimon_", "body": "If the submission has been either deleted or inaccessible. Which in this case can only happen if the post becomes removed and the authenticated account doesn't have access to removed submissions. Then an exception will happen when trying to access the `link_flair_text` as only at this time does PRAW send an API request to reddit to return information about the submission. If the information cannot be returned then an error will occur. EDIT: That's my guess at least. Just like /u/hiles I think a crash message would be most helpful.", "created_utc": 1381659336, "gilded": 0, "name": "t1_ccqqjfk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "body": "The bot is a moderator on the sub, and the posts had not been removed. Here's the crash message: Traceback (most recent call last): File \"C:\\Python27\\botTest.py\", line 34, in main() File \"C:\\Python27\\botTest.py\", line 27, in main r.send_message(author,subLine,msg) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 303, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 205, in wrapped return function(obj, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 1909, in send_message retry_on_error=False) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\decorators.py\", line 141, in wrapped return_value = function(reddit_session, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 479, in request_json retry_on_error=retry_on_error) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 348, in _request response = handle_redirect() File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\__init__.py\", line 321, in handle_redirect timeout=timeout, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 135, in wrapped result = function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 54, in wrapped return function(cls, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw-2.1.9-py2.7.egg\\praw\\handlers.py\", line 90, in request allow_redirects=False) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\sessions.py\", line 460, in send r = adapter.send(request, **kwargs) File \"C:\\Python27\\lib\\site-packages\\requests-2.0.0-py2.7.egg\\requests\\adapters.py\", line 354, in send raise ConnectionError(e) ConnectionError: HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with url: /api/compose/.json (Caused by : [Errno 10054] An existing connection was forcibly closed by the remote host)", "created_utc": 1381670782, "gilded": 0, "name": "t1_ccqs20d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "thunder_afternoon", "body": "It looks like you're losing the connection after sleeping for 3 minutes. Maybe reddit has a quick tcp timeout, I don't know. Can you try to re-establish the connection after 180 seconds? Re-login etc. Edit: better yet, why don't you compare current time to the \"created\" field and deal with submissions that have been submitted more than 3 minutes ago.", "created_utc": 1381688816, "gilded": 0, "name": "t1_ccqx2ii", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "body": "Sorry to bother you again, but do you know exactly what the created field holds? I get a huge double for it when I print it out. I'm not sure what way to get the current time in a way that I can compare the two. ~~**Edit:** Nvm. I think I've got it.~~ I don't got it...", "created_utc": 1381792664, "gilded": 0, "name": "t1_ccrr9zn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "thunder_afternoon", "body": "It's called Epoch time. Google about it. Unix date command or equivalent in Windows will give you the current time in epoch with the proper argument.", "created_utc": 1381795704, "gilded": 0, "name": "t1_ccrsb24", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "body": "Yeah I figured out that it was epoch time and unix uses 1970 as the epoch. I also got that time.time() returns the current epoch time. That's when I made my first edit. However, when comparing the current epoch time to the post time returned by submission.created, I'm finding that the submissions' times are sometimes greater than the current time. I must be doing something wrong, but I can't figure it out. In theory I think my code should look like: if((time.time()-submission.created)>180.0): check the flair But I'm getting negative numbers for the time difference sometimes. I don't understand how that's possible.", "created_utc": 1381796022, "gilded": 0, "name": "t1_ccrsewk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "thunder_afternoon", "body": "Gmt?", "created_utc": 1381797263, "gilded": 0, "name": "t1_ccrsuac", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "body": "How would I specific GMT? I'm in EST so I just tried adding 4 hours (14400 seconds) to the current time and comparing them but the post times are still greater.", "created_utc": 1381797644, "gilded": 0, "name": "t1_ccrsz3y", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "thunder_afternoon", "body": "Ok I'll be at home in a little bit. I can check it there.", "created_utc": 1381800064, "gilded": 0, "name": "t1_ccrtuqt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "body": "Thank you so much for your help. I really appreciate it.", "created_utc": 1381800340, "gilded": 0, "name": "t1_ccrtyfx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "_Daimon_", "body": "Don't use the `created` attribute. That's the current epoch time in the timezone of the server, which isn't reliable. It's depreciated and shouldn't be used. Use `created_utc` instead, which will always be the epoch time in [coordinated universal time](http://en.wikipedia.org/wiki/Coordinated_Universal_Time). [See this #redditdev convo](https://botbot.me/freenode/reddit-dev/msg/3831316/)", "created_utc": 1381828062, "gilded": 0, "name": "t1_ccs0wj7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "body": "I'll try it, but that wouldn't explain why it was working at one point. Also, I initially had it sleep for 5 minutes and there were no problems. Could it also be my internet connection? I'm using my college's wifi and I've had some weird session persistence issues over the three years I've been at this school.", "created_utc": 1381689003, "gilded": 0, "name": "t1_ccqx4tg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "thunder_afternoon", "body": "The last message says connection was forcibly closed by the remote. You can install wire shark (tshark on command line Linux) and watch the traffic and see what's going on. Maybe it's your router or some other factor. I can tell based on the error log.", "created_utc": 1381689196, "gilded": 0, "name": "t1_ccqx790", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "body": "I tried re-logging in after sleeping and that seems to have fixed it. I also just saw your edit and that seems like a great idea. I'll give it a try. Thanks for your help!", "created_utc": 1381691499, "gilded": 0, "name": "t1_ccqy0t3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": null, "body": "[deleted]", "created_utc": 1381690397, "gilded": 0, "name": "t1_ccqxmm8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "JBHUTT09", "body": "I sleep there because it reads new posts almost the instant they are posted. Flair must be added after a post is posted. This gives them a three minute window to set their flair. While your method would work, it would only give the delay to everyone except the first post it reads. If I start it right at someone posts, then they won't have time to set their flair before the bot messages them. I'll check out the *set* idea and the data persistence. It's been a long time since I've worked with python (over two years) so I'm a bit rusty. I basically tried to get this to a \"working\" state. I plan on improving it once I reach that point. It's working again thanks to /u/thunder_afternoon's suggestion, so I'll work on improving it over the next few days.", "created_utc": 1381691555, "gilded": 0, "name": "t1_ccqy1ji", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1obxel/having_issues_with_a_time_delay_in_a_bot/"}, {"author": "Mr_Dionysus", "body": "The reddit API limits requests to 1000 posts, and as far as I know there is no way around it.", "created_utc": 1381583374, "gilded": 0, "name": "t1_ccq73oc", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": "sixteenmiles", "body": "Well what I mean is, by using get_new(limit=None) it gets everything, but only as far back as reddit displays 'new' posts, which seems to be about 28 pages. This isn't exclusive to using PRAW or anything, even just browsing in a browser, the 'new' page (at least of the specific sub I am looking at) goes back 28 pages and then stops. I just tried it manually and it eventually just hits a \"There are no more pages to load\" message. The sub I am looking at has been around for 4 years, but only seems to display about a month worth of content. Is this what you are talking about? Even browsing manually, reddit only displays a certain number of posts and all posts prior to that are... what, lost? Is there a way to use PRAW to look at all of a subs content, rather than what I am doing right now which is only looking at the 'new' content?", "created_utc": 1381595895, "gilded": 0, "name": "t1_ccqa0gi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": "gavin19", "body": "At 100 items shown per page, 10 pages is as far back as you can go. The listings will show 1000 different posts, depending on sort type, but they all max out at 1000. Any other older posts aren't lost, it's just that they can't be found via those listings, only search. > which is only looking at the 'new' content You can use any of the other sort types like get_hot() etc. All found [here](https://praw.readthedocs.org/en/latest/pages/code_overview.html#module-praw.objects).", "created_utc": 1381602203, "gilded": 0, "name": "t1_ccqbuja", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": "sixteenmiles", "body": "Right. Thanks.", "created_utc": 1381602545, "gilded": 0, "name": "t1_ccqby67", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": null, "body": "You might be able to use this, I'm not sure if it even works still: http://www.reddit.com/r/TheoryOfReddit/comments/zcd40/im_in_the_process_of_scraping_every_submission/c63cpil", "created_utc": 1381593564, "gilded": 0, "name": "t1_ccq9dcu", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": "rhiever", "body": "That's the only way I know of doing it, but it's extremely laborious.", "created_utc": 1381593681, "gilded": 0, "name": "t1_ccq9eie", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1oabiy/praw_is_it_possible_to_get_the_content_of_an/"}, {"author": "_Daimon_", "body": "You set flair with the [`set_flair`](http://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModFlairMixin.set_flair) method. Use standard introspection to see the attributes of Submission objects. It should be pretty obvious which ones are flair related. We have a [page about that](http://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) in our documentation if you're unsure on how to use introspection :)", "created_utc": 1381267693, "gilded": 0, "name": "t1_ccnqgac", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "HerpieMcDerpie", "body": "I'm having issues finding thread flair. (Perhaps I'm even using the wrong term). With [this thread](http://www.reddit.com/r/CoolWall/comments/1m3b0p/volkswagen_golf_gti_mk6/) as an example, I'd like to isolate the \"Cool\" flair that appears next to the thread title. So far, I've tried: link_flair_text and link_flair_css_class but both return \"None\". Am I missing something? Thanks!", "created_utc": 1384672676, "gilded": 0, "name": "t1_cdg8l9d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "_Daimon_", "body": "I don't have access to a machine running PRAW atm, but the following should work. submission = r.get_submission('http://www.reddit.com/r/CoolWall/comments/1m3b0p/volkswagen_golf_gti_mk6/.json') submission.link_flair_text If the above doesn't work, post code showing what you're doing.", "created_utc": 1384991951, "gilded": 0, "name": "t1_cdj7sgm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "HerpieMcDerpie", "body": "That worked great and I see where my error was. I was having a moment of the st00pids. Thank you!", "created_utc": 1385017235, "gilded": 0, "name": "t1_cdjhq0p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "Lauren_of_Lore", "body": "Okay thank you :) Sorry I just started learning Python and I've never done this before.", "created_utc": 1381267918, "gilded": 0, "name": "t1_ccnqjgm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "_Daimon_", "body": "You're not alone, so don't think about it. That page is our most linked page precisely because it describes standard introspection. Which IMO should be taught very early in tutorials/classes etc. but for some weird reason isn't.", "created_utc": 1381268027, "gilded": 0, "name": "t1_ccnqkxy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1o0hcg/praw_thread_flair/"}, {"author": "_Daimon_", "body": "The cache on reddit's end uses a longer time out for unauthenticated requests. Log in and the cache timeout (both ends) will be 30secs.", "created_utc": 1381232373, "gilded": 0, "name": "t1_ccneci2", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nyznn/praw_is_caching_comments_for_2m_when_configured/"}, {"author": "Gambit89", "body": "Ah, that explains why it works in the browser! Thanks! I was hoping I wouldn't have to create a bot account for more compatibility with different hosting sites, but I guess I can't avoid it.", "created_utc": 1381277651, "gilded": 0, "name": "t1_ccnu1ku", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nyznn/praw_is_caching_comments_for_2m_when_configured/"}, {"author": "_Daimon_", "body": "The `update_settings` basically runs `get_settings` and then updates the existing settings with those parameters you've given and use that to run `set_settings`. The problem is that the recent addition of [new spam levels](http://www.reddit.com/r/changelog/comments/1krtu4/reddit_change_new_spam_filter/) didn't add them to the json about the page. Go to r/YOUR_SUBREDDIT/about/edit/.json to see what PRAW sees. Notice there is nothing about spam. So when PRAW sends the settings, the spam settings are not included because PRAW doesn't know about them, which means the spam settings become unset. I've made [a comment](http://www.reddit.com/r/changelog/comments/1krtu4/reddit_change_new_spam_filter/cck61hp) on the spam filter post. Since it's more of a forgotten change that belonged with this rather than a bug or a new feature. Ideally this will be fixed upstream by adding said fields to the json version of about/edit. If not, then I'll find a workaround for you.", "created_utc": 1380803639, "gilded": 0, "name": "t1_cck62p3", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "rreyv", "body": "Thank you. So when you say 'unset', does this mean that they're changed back to default of high, high, low? Or are they turned off? I feel like they would be changed back to defaults and I went through the reddit code to see if this is actually true. I thought I found the block of code that changes the spam settings to default if they're ever set to null, but I'm new to Python and not confident in my answer. Could you help me answer this? Thanks again for the prompt response. Really appreciate it and your work with praw.", "created_utc": 1380807101, "gilded": 0, "name": "t1_cck6vyw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "reostra", "body": "Okay, I've just [committed code](https://github.com/reddit/reddit/commit/e041c348cd8951d933df9d76492a358feb53b06d) to fix that, you should be seeing the various spam_* attributes. My apologies for not including that the first time around!", "created_utc": 1380834671, "gilded": 0, "name": "t1_cckh9tq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "_Daimon_", "body": "Don't think about it, it was easy to miss. Thanks for fixing so quickly.", "created_utc": 1380905466, "gilded": 0, "name": "t1_ccl0dn0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "rreyv", "body": "I love you!", "created_utc": 1380834900, "gilded": 0, "name": "t1_cckhczl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "reostra", "body": "Thanks for the heads-up in the comment; I'll start working on that today :)", "created_utc": 1380821122, "gilded": 0, "name": "t1_cckbvm5", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nmwta/potential_bug_with_updating_sidebar_in_praw_or_am/"}, {"author": "michel_v", "body": "Not familiar with Reddit API yet, but I work with OAuth. As you figured it, you need to store the refresh token and use it to get a new access token once the corresponding access token expires (either you keep track of its expiration time, or you do a refresh only when you get a 403 using the token).", "created_utc": 1380749010, "gilded": 0, "name": "t1_ccjsla3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nll3d/saving_oauth_codes_in_db/"}, {"author": "_Daimon_", "body": "The refresh_token is permanent. Store it. No, except obviously ``client_id``, ``client_secret`` and ``redirect_uri`` need to be set before OAuth can be used. See [PRAW's OAuth section 2](http://praw.readthedocs.org/en/latest/pages/oauth.html#step-2-setting-up-praw) on setting that up. Send us a link once your app is done, especially if it's opensource, and we might link to it from our documentation. :)", "created_utc": 1380755058, "gilded": 0, "name": "t1_ccjuoju", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nll3d/saving_oauth_codes_in_db/"}, {"author": "bVector", "body": "Thanks, this is working using redis and just storing the keys as nick:refresh_token (its an irc-> reddit bot), then calling refresh_access...() with the refresh codes to change identities. I actually didnt find a way of switching identities with just the access_token. Tried passing access_token to refresh_access get_access set_access functions.", "created_utc": 1380756029, "gilded": 0, "name": "t1_ccjv08d", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nll3d/saving_oauth_codes_in_db/"}, {"author": "_Daimon_", "body": "Sounds interesting :) The method you're looking for is [refresh_access_token](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.AuthenticatedReddit.refresh_access_information). Pass the new `refresh_token` as an argument and it will refresh the OAuth information based on that.", "created_utc": 1380756962, "gilded": 0, "name": "t1_ccjvbnm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1nll3d/saving_oauth_codes_in_db/"}, {"author": "bboe", "body": "> Through Reddit UI, I can continuously press \"Next\" to get all user's comments. That will only give you the last 1000.", "created_utc": 1380223640, "gilded": 0, "name": "t1_ccfy67f", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "_Daimon_", "body": "Read the [comment parsing](http://praw.readthedocs.org/en/latest/pages/comment_parsing.html) article in PRAW's documentation.", "created_utc": 1380223210, "gilded": 0, "name": "t1_ccfy0k4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "im14", "body": "That's the first thing I did, but it talks about comments from a submission, not specific user's comments.", "created_utc": 1380223574, "gilded": 0, "name": "t1_ccfy5d5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "_Daimon_", "body": "Getting a specific users comments is demonstrated in [getting started](http://praw.readthedocs.org/en/latest/pages/getting_started.html).", "created_utc": 1380224018, "gilded": 0, "name": "t1_ccfyb2f", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "_Daimon_", "body": "I'm having a bit of a bad day. Sorry if I'm being a bit short with you. it has nothing to do with you. It is better if you could find the answer from the documentation, in that way I save time by not giving the same answer repeatedly. If that doesn't help you, then the best outcome would be to get great feedback from you so the documentation can be improved for future users. But here is a demonstration so you don't have to look longer. import praw r = praw.Reddit(UNIQUE_AND_DESCRIPTIVE_USER_AGENT) user = r.get_redditor('im14') for comment in user.get_comments(limit=None): print comment.body **EDIT:** Somehow I totally missed the 1000 limit part. Yeah /u/bboe is correct. The 1000 limit is a max due to Reddits cache, it doesn't matter whether you access it directly, via PRAW or any other wrapper. The max is the same.", "created_utc": 1380224514, "gilded": 0, "name": "t1_ccfyhp8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "im14", "body": "Thanks Daimon. I actually looked at your karma summary script and see that it doesn't do anything like get_comments().next() to retrieve every single comment.. only goes up to limit. So, it's basically impossible for me to retrieve every single comment :/", "created_utc": 1380224924, "gilded": 0, "name": "t1_ccfyn5l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1n7164/can_i_get_all_comments_of_a_user_by_calling/"}, {"author": "_Daimon_", "body": "No (as of PRAW 2.1.7). I think the simplest solution for you would be to patch your version of PRAW to store the json_dict as an attribute upon instantiation. A `reddit_session` is an instance of a `Reddit` object. It is mandatory because it contains configuration information that is needed for the `Comment`object to work. So pass it an instance of `Reddit` with the configuration you need.", "created_utc": 1379874082, "gilded": 0, "name": "t1_ccdagai", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1mtvu2/praw_saving_json_response_and_using_that_to/"}, {"author": "_Daimon_", "body": "That's a bug in IDLE. See [this SO answer](http://stackoverflow.com/a/17143073/1368070) for more information and 2 possible solutions.", "created_utc": 1379758573, "gilded": 0, "name": "t1_cccl169", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1mtup9/praw_code_errors_at_line_20_r_prawredditblahblah/"}, {"author": null, "body": "I'm seeing very slow response times from reddit via my AWS EC2 machine (Oregon). This started on Sep 9 or 10. Many of my scripts are timing out, often with partial (incomplete) json. $ time wget http://www.reddit.com/r/AskReddit/.json?limit=200 real 0m26.862s user 0m0.008s sys 0m0.004s For comparison, from my home machine: $ time wget http://www.reddit.com/r/AskReddit/.json?limit=200 real 0m0.941s user 0m0.004s sys 0m0.004s I don't know if this is AWS related or if Reddit is throttling me for some reason. (I also posted [this comment here](http://www.reddit.com/r/help/comments/1m6k9n/bot_getting_502503_errors_when_posting_comments/), seems relevant.)", "created_utc": 1378967704, "gilded": 0, "name": "t1_cc6qgaz", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "body": "My load times are pretty random. On both Heroku and on my laptop, sometimes the API request works fine, and other times it times out.", "created_utc": 1378970937, "gilded": 0, "name": "t1_cc6r0sl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": null, "body": "I haven't looked at your code closely but i do know that praw does some level of caching. Could that account for some of your sporadic result times? Edit: Perhaps (and this is just a guess) praw goes to grab a new copy of the page after the normal cache expire time, gets a bad response from reddit, the falls back to using the now outdated cached version? That might explain why your bot is replying to the same comment multiple times.", "created_utc": 1378972655, "gilded": 0, "name": "t1_cc6r9lr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "body": "For the people who are having this issue. Can you tell me whether you use PRAW? What methods you use to find out where to comment/submit/respond and what (if anything) you do in case of an exception. I'll use that information to check whether my hunch of a 1 year old bug is correct. If there is a bug, then I won't be able to fix it until I get home which will be in about 8-10 hours.", "created_utc": 1378978034, "gilded": 0, "name": "t1_cc6rx81", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "body": "I use PRAW for my bot. [The code is here on Github](https://github.com/matthieugrieger/versebot), but I have made some changes to the code since my last commit (invalid comment ids aren't stored in the database anymore, some attempts at fixes for this spamming problem, etc). Here's where the comment is made in my current code: botComment = False while True: try: botComment = comment.reply(currentComment).id except: if botComment != False: print('Comment posted on ' + ctime() + '.') return True else: continue I haven't really tested it that much so I'm not 100% sure if it works correctly. I have a hunch that it may not work right, as some others have said that reddit has been returning incomplete JSON data.", "created_utc": 1378997962, "gilded": 0, "name": "t1_cc6vuv1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "Tjstretchalot", "body": "Why did you respond 3 times as different people", "created_utc": 1379009462, "gilded": 0, "name": "t1_cc70862", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "nandhp", "body": "Because there's (at least) three different people having this problem.", "created_utc": 1379020036, "gilded": 0, "name": "t1_cc74a5i", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "Tjstretchalot", "body": "Ohh, re-read this to understand. I thought he was asking for OP's code, and was very confused", "created_utc": 1379026911, "gilded": 0, "name": "t1_cc76quw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "body": "I've made a fix in PRAW that should fix the problem. See [this comment](http://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/cc7kn2q) for more info.", "created_utc": 1379083760, "gilded": 0, "name": "t1_cc7kq87", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "avery_crudeman", "body": "Using PRAW. [Here's](https://github.com/avery-crudeman/Baseball-GDT-Bot/tree/master/src) the github page of the bot I'm using. It posts threads and updates them with statistics. When it posts multiple threads I just delete all but one and switch the sub = r.submit part with sub = r.get and use the ID of the remaining post. Here's the part of the code that does the posting. for d in directories: timecheck.gamecheck(d) title = editor.generatetitle(d) if not timecheck.ppcheck(d): while True: try: print \"Submitting game thread...\" sub = r.submit('SUBREDDITNAME', title, editor.generatecode(d)) #sub = r.get_submission(submission_id='xxxxxx') print \"Game thread submitted...\" break except Exception, err: print err time.sleep(300) while True: str = editor.generatecode(d) while True: try: sub.edit(str) break except Exception, err: print \"Couldn't submit edits, trying again...\" time.sleep(10) if \"|Decisions|\" in str: print \"Submitting postgame thread...\" posttitle = posteditor.generatetitle(d) sub = r.submit('SUBREDDITNAME', posttitle, posteditor.generatecode(d)) print \"Postgame thread submitted...\" break elif \"###POSTPONED\" in str: break time.sleep(10) [**EDIT:** Just to be clear, I have a simplified version of this I use for testing that doesn't have any \"try except\" bits and should just submit a post without retrying or anything, but that also posts a random amount of multiple submissions before it finally realizes it's posted something and stops running.]", "created_utc": 1378996995, "gilded": 0, "name": "t1_cc6vivl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "body": "I've made a fix in PRAW that should fix the problem. See [this comment](http://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/cc7kn2q) for more info.", "created_utc": 1379083755, "gilded": 0, "name": "t1_cc7kq5j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "nandhp", "body": "I'm using PRAW. Here's my code: post = praw.objects.Submission.from_id(self.reddit, postid) try: comment = post.add_comment(comment_text) comment_id = comment.id except praw.errors.APIException, exception: if exception.error_type in ('TOO_OLD','DELETED_LINK'): print \"[Can't post comment: archived by reddit]\" else: raise # Try posting again later [The full code is on Github](https://github.com/nandhp/movieguide/blob/master/movieguide.py#L261). I'm using PRAW 2.0.15, not 2.1; but I can try upgrading if it will help. (I tried back when it came out and had trouble, so I've still been using 2.0.15.)", "created_utc": 1378994409, "gilded": 0, "name": "t1_cc6urom", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "body": "I've made a fix in PRAW that should fix the problem. See [this comment](http://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/cc7kn2q) for more info.", "created_utc": 1379083738, "gilded": 0, "name": "t1_cc7kpxr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "body": "PRAW automatically retries requests that fails, if the error is a temporary one. Like a 502 status. This makes you see fewer exceptions and ensures that exceptions are actually exceptions rather than temporary glitches in Reddit or the internet infastructure. In the past few days Reddit has been under heavier stress than normal and been throwing more errors than normal. I wasn't running any bot at the time of the problem, but according to /u/offtherocks a lot of requests were timing out impartial json. This mean a 504, which PRAW retries for. So what happened was that the request was made, the data was added to Reddit's database and Reddit then tried to return a succesful response. But ended up timing out so PRAW received incomplete data. Which then prompted it to resend the request causing the data to be added multiple times to the database. I've fixed the bug in PRAW, so now methods that may spam reddit will not retry failed requests. This bugfix is included in PRAW 2.1.6. So I recommend upgrading asap. $ pip install praw -U Remember that PRAW has a [changelog](https://praw.readthedocs.org/en/latest/pages/changelog.html) so you can easily see what changes have been made that affect your applications. Finally if you still have the problem after upgrading then let me know as quickly as possible. I've done the best I can with this solution, but testing handling of errors on a machine you don't control is quite hard.", "created_utc": 1379083503, "gilded": 0, "name": "t1_cc7kn2q", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "avery_crudeman", "body": "This seems to do the trick. Thanks!", "created_utc": 1379084818, "gilded": 0, "name": "t1_cc7l313", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "body": "Great to hear the effort was appreciated :)", "created_utc": 1379110337, "gilded": 0, "name": "t1_cc7u9b1", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "body": "Awesome, thanks! Once Heroku is back online I will test this out. **EDIT:** It seems to work just fine! Thank you!", "created_utc": 1379087477, "gilded": 0, "name": "t1_cc7m0dq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "body": "Awesome. Great to hear that! :)", "created_utc": 1379110303, "gilded": 0, "name": "t1_cc7u8xb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "avery_crudeman", "body": "I'm having this same issue. It just started for me a few days ago.", "created_utc": 1378959755, "gilded": 0, "name": "t1_cc6odst", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "body": "So I'm not the only one... It started for me on September 9th, so it was a couple days ago for me too.", "created_utc": 1378959861, "gilded": 0, "name": "t1_cc6oezt", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "avery_crudeman", "body": "I know of one other user that's got the same problem as well. I've been working on it for a few days (like you I thought it was error handling or something) and I can't figure it out for the life of me.", "created_utc": 1378960059, "gilded": 0, "name": "t1_cc6ohas", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "im14", "body": "See my response top comment for solution :)", "created_utc": 1378962560, "gilded": 0, "name": "t1_cc6p8nm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "body": "Yeah, I can't figure it out either. The behavior is pretty strange and unpredictable. Maybe it's just something that we have to wait for reddit/PRAW to fix.", "created_utc": 1378960158, "gilded": 0, "name": "t1_cc6oiep", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "avery_crudeman", "body": "I think so too.", "created_utc": 1378960292, "gilded": 0, "name": "t1_cc6ojw7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "im14", "body": "I've been having the same issue from time to time with my /u/altcointip bot. My solution was to store each message ID in database when processed and check for duplicates each time I check inbox. It seems that during heavy loads the 'mark as read' feature on Reddit isn't reliable.", "created_utc": 1378962508, "gilded": 0, "name": "t1_cc6p84w", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "body": "My bot isn't triggered by messages in it's inbox, but by comments in specific subreddits with a certain syntax inside. I do something similar though, by storing the comment ids of the comments the bot has already replied to. It was working just fine for a few weeks, but now it doesn't work reliably because of the time out issues.", "created_utc": 1378971045, "gilded": 0, "name": "t1_cc6r1db", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "shaggorama", "body": "same thing. You should cache comments you've responded to recently so the bot doesn't duplicate work.", "created_utc": 1378992127, "gilded": 0, "name": "t1_cc6u5df", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "nandhp", "body": "I think I found a workaround. --------------------- Using http://api.reddit.com seems to help, both with 502/503 errors and with [page load times](http://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/cc6qgaz). You can do this in PRAW by setting the environment variable `REDDIT_SITE=reddit_bypass_cdn`or by using the constructor `praw.Reddit(site_name='reddit_bypass_cdn')`. I suggest you don't leave this set on a long-term basis, it probably causes more load on their servers.", "created_utc": 1379038951, "gilded": 0, "name": "t1_cc7b17t", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": null, "body": "This works. Thanks.", "created_utc": 1379052650, "gilded": 0, "name": "t1_cc7f7xo", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "body": "My bot still seems to spam. I tried both the constructor method and the environment variable method. I'm running my bot on Heroku, so I'm assuming the Heroku environment variables would work fine? I use them to load my config anyway. When using the constructor method I would just get exceptions (I'm not exactly sure which ones because I have a try/except block around the code that connects to reddit). Thanks though!", "created_utc": 1379048138, "gilded": 0, "name": "t1_cc7e4jc", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "nandhp", "body": "I'm having this problem too. [Here's my post in /r/help](/r/help/comments/1m6k9n/bot_getting_502503_errors_when_posting_comments/).", "created_utc": 1378963245, "gilded": 0, "name": "t1_cc6pffp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "_Daimon_", "body": "You should always first store the ID of the thing you're replying to, before actually commenting on it. Otherwise commenting on the same thing more than once is bound to happen eventually. There's simply too many places between adding it to Reddits database and your end where it can go wrong. Using PRAW an error can happen in 5 different libraries `Reddit`, `Urllib3`, `requests`, `PRAW` and your own. And in the internet infastructure as well obviously. Currently it seems Reddit is under heavier stress than normal and throwing more errors than usual. Which obviously would make this problem happen more frequently. Store the id first, then make the comment. If the comment-making process fails, then first check whether a comment has been made. Either manually or via PRAW. Then make the comment again if it hasn't. Remember that there is a 30 second cache in both Reddit and PRAW. The above also applies if you're responding to a message or similar processes.", "created_utc": 1378976269, "gilded": 0, "name": "t1_cc6rpza", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "mgrieger", "body": "I've altered my code to store the comment id in a set before commenting on it, and I still have the same issue.", "created_utc": 1379009606, "gilded": 0, "name": "t1_cc70a4u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": null, "body": "Happening to me, too, with /u/handy_related_sub. I'm going to try storing the IDs before commenting and doing a sanity check before each comment, but it sounds like you already tried that.", "created_utc": 1379019168, "gilded": 0, "name": "t1_cc73xrj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1m85d0/bot_randomly_started_spamming_with_no_changes_to/"}, {"author": "m1ss1ontomars2k4", "body": "Maybe it's just a bug in PRAW then?", "created_utc": 1377581909, "gilded": 0, "name": "t1_cbw49pk", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "_Daimon_", "body": "Fetching contributors always work when you are a moderator of the subreddit, which is why the method is currently restricted to moderators. I've been looking at a subreddit I mod, and I cannot find any way of making the contributor list public. If there isn't an option, then I can only assume that users having access to the list of contributors of accesable subreddits are what is intended. And that the 404s are the bugs. In which case I'll file a bug report on Reddits end and remove the requirement on PRAW's end when Reddit fixes the bug. So is there a way of making the contributor list public?", "created_utc": 1377601016, "gilded": 0, "name": "t1_cbw74og", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "qviri", "body": "> So is there a way of making the contributor list public? Maybe it's public automatically when submissions are restricted? (As they are in the example subreddit.) Or the option of making it public/private appears when you restrict them? I don't have a test subreddit so can't test that, sorry.", "created_utc": 1377613586, "gilded": 0, "name": "t1_cbw9tsb", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "_Daimon_", "body": "I've tried it with several public and default subreddits that should have maximum openess and I've been unable to find another subreddit where the list of contributors is accessible. So I don't know whether the bug is that some subreddits return 404 or that polandballs is accessible. Either way I've [filed an bug report](https://github.com/reddit/reddit/issues/891), so we'll see what's up when I get a response from Reddit.", "created_utc": 1377621500, "gilded": 0, "name": "t1_cbwcpka", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "qviri", "body": "Thanks, I'll watch that issue to see if anything develops. I was able to work around the modonly for my purposes, so this isn't crucial for me by any means - most just asking out of curiosity, and in case I'd need to get a contributors list again.", "created_utc": 1377621758, "gilded": 0, "name": "t1_cbwct4v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "Deimorz", "body": "The contributors list is public if the subreddit is restricted or private, but mod-only if it's a public subreddit: https://github.com/reddit/reddit/blob/master/r2/r2/controllers/front.py#L623-L631 I'm not sure exactly why that is, but that's the way it's currently set up.", "created_utc": 1377626899, "gilded": 0, "name": "t1_cbwet55", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l5tsa/praw_why_is_subredditget_contributors_modonly/"}, {"author": "_Daimon_", "body": "That's a bug in IDLE. See [this SO answer](http://stackoverflow.com/a/17143073/1368070) for more information and 2 possible solutions.", "created_utc": 1377505747, "gilded": 0, "name": "t1_cbvigzp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l3hkp/praw_help_nonetype_has_no_attribute_write/"}, {"author": "Eliminioa", "body": "Thanks for the help! I'm just downloading a new IDE. Might try out a few.", "created_utc": 1377528776, "gilded": 0, "name": "t1_cbvmecx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l3hkp/praw_help_nonetype_has_no_attribute_write/"}, {"author": "_Daimon_", "body": "https://praw.readthedocs.org/en/latest/pages/faq.html#when-i-print-a-comment-only-part-of-it-is-printed-how-can-i-get-the-rest", "created_utc": 1377453922, "gilded": 0, "name": "t1_cbv377q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1l185p/printing_submission_comments_doesnt_print_entire/"}, {"author": "_Daimon_", "body": "`get_all_comments` was just a convenience method for `get_comments` with the subreddit argument set to \"all\". The two methods are equally efficient.", "created_utc": 1377454749, "gilded": 0, "name": "t1_cbv3gr3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kz820/having_an_issue_using_get_all_comments_in_praw/"}, {"author": "SuperSniperGuy", "body": "I do that with my bots every 30 seconds. I'm not having any performance trouble at all. But i'm also curious. Edit: not 5000 but 500 per 30 seconds.", "created_utc": 1377349913, "gilded": 0, "name": "t1_cbudv2x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kz820/having_an_issue_using_get_all_comments_in_praw/"}, {"author": "OnceAndForever", "body": "comments = r.get_comments('all', limit=500) That is the correct way to do it, and it is as efficient as possible. Keep in mind that reddit can only send you comments in batches of 100, so asking for 500 comments will take about 10 seconds (5 API calls). Also, each page is cached for 30 seconds, so you'll have to wait longer than that to get fresh data.", "created_utc": 1377377873, "gilded": 0, "name": "t1_cbulk54", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kz820/having_an_issue_using_get_all_comments_in_praw/"}, {"author": "SuperSniperGuy", "body": "That's exactly what i'm doing.", "created_utc": 1377419415, "gilded": 0, "name": "t1_cbuw9qf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kz820/having_an_issue_using_get_all_comments_in_praw/"}, {"author": "bboe", "body": "When you fetch an individual comment object using `get_info`, reddit does not return the replies along with it thus the empty list of replies. Here's a work around: comment = r.get_submission(comment.permalink).comments[0] It's a costly operation, however.", "created_utc": 1377459045, "gilded": 0, "name": "t1_cbv4usl", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kxd1n/how_can_i_get_the_replies_to_a_comment_with_praw/"}, {"author": "isolani", "body": "Use `sticky()` and `unsticky()` on a Submission object. Source: https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission.sticky", "created_utc": 1377103779, "gilded": 0, "name": "t1_cbsd5ym", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kt95a/using_praw_to_set_subreddit_sticky/"}, {"author": "Squidifier", "body": "Whoops, I feel dumb now. Searching ['sticky' in readthedocs](https://readthedocs.org/search/project/?q=sticky&selected_facets=project_exact%3Apraw) didn't turn up with anything, but it seems [Google does](https://www.google.com.ph/search?q=site%3Ahttps%3A%2F%2Fpraw.readthedocs.org+sticky&oq=site%3Ahttps%3A%2F%2Fpraw.readthedocs.org+sticky&aqs=chrome..69i57j69i58.3155j0&sourceid=chrome&ie=UTF-8). Thanks again for your help!", "created_utc": 1377103910, "gilded": 0, "name": "t1_cbsd7x5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kt95a/using_praw_to_set_subreddit_sticky/"}, {"author": "_Daimon_", "body": "A [recent commit](https://github.com/praw-dev/praw/commit/cd9f5ce005dd59cbee85301c2d15c46001cc27c8) added the functionality. Which you can also see in [PRAW's changelog](https://praw.readthedocs.org/en/latest/pages/changelog.html). But what you can also see there is that it's part of a version that hasn't yet been released. It is up to bboe to decide when the next version of PRAW is deployed, but I think it will be soon.", "created_utc": 1377107687, "gilded": 0, "name": "t1_cbserch", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kt95a/using_praw_to_set_subreddit_sticky/"}, {"author": "OnceAndForever", "body": "What do you mean your code is making too many requests? Reddit only lets you make 30 api calls per minute, and praw handles all of that for you. I don't think get_submitted() and get_comments() are slower, but praw will only let you call that method every 2 seconds.", "created_utc": 1377053308, "gilded": 0, "name": "t1_cbs10jf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "MrFanzyPanz", "body": "get_submitted() and get_comments() both return get_content() objects through PRAW. get_content() objects return batches of at the most 100 items within 1 call. For example: for post in r.get_subreddit('pics').get_hot(limit=500): This code would get posts from the 'pics' subreddit in batches of 100 posts, sorted by the get_hot() algorithm. If you asked for attributes of posts, say post.score or post.fullname, those attributes would be placed in the call along with the 99 other posts and their attributes. There is an attribute, however, that requires its own call: the post.comments attribute. It returns a list of the root comments of the given post. If you're trying to get the comments of all the posts, instead of getting your posts in batches of 100, post.replies will slow your code down to one post every two seconds. I want to know if this is inherent or if I've simply messed something up. **Edit:** This is also a problem for when pulling comments. comment.replies requires its own call as well.", "created_utc": 1377057040, "gilded": 0, "name": "t1_cbs2b7n", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "OnceAndForever", "body": ">There is an attribute, however, that requires its own call: the post.comments attribute. It returns a list of the root comments of the given post. > >If you're trying to get the comments of all the posts, instead of getting your posts in batches of 100, post.replies will slow your code down to one post every two seconds. I want to know if this is inherent or if I've simply messed something up. I see what you mean. No you haven't messed anything up. The two second delay is inherent. When you use praw, you can't make requests any faster than once every two seconds. It is part of reddit's API limitations. reddit only allows 30 api requests per minute, which ends up being one every two seconds. It is explained in more detail in the praw documentation: >Another thing you may have noticed is that retrieving a lot of elements take time. reddit allows requests of up to 100 items at once. So if you request", "created_utc": 1377066489, "gilded": 0, "name": "t1_cbs50gi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "Bruin116", "body": "Right, but I think his problem is that while most of the praw calls get data in batches of 100, this particular post.comments call only gets 1 entry at a time, making it run 100x slower than the rest of his code. It is effectively making 100 times more requests to get the same amount of information as all of the other batch pulls.", "created_utc": 1377067433, "gilded": 0, "name": "t1_cbs57v4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "bboe", "body": "The listing page will allow you fetch info for 100 submissions at once. However, in order to get up to 2500 comments for each submission that requires fetching the individual submissions. There's no way around that if you want to work with the comments.", "created_utc": 1377459610, "gilded": 0, "name": "t1_cbv51gj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "MrFanzyPanz", "body": "Okay. Good to know. Thanks.", "created_utc": 1377067225, "gilded": 0, "name": "t1_cbs568l", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kpuzo/my_code_is_making_too_many_requests/"}, {"author": "_Daimon_", "body": "Talk to either /u/Deimorz or /u/Stuck_In_the_Matrix I believe both have a full scraping of Reddit's submissions. Getting a copy from one of them should be much faster and a lot simpler. What's your research about?", "created_utc": 1376925432, "gilded": 0, "name": "t1_cbqw4qj", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "CMThF", "body": "thank you very much! my research is about a categorization and the development of reddit's submissions over time, and maybe later a definition of what reddit is all about - similar to the paper [What is Twitter](http://venus.sociedadhumana.com/archivos/download/2010-www-twitterlh49129.pdf). I am still gathering literature, information and data to get an understanding and some ideas what and how to analyse.", "created_utc": 1376928419, "gilded": 0, "name": "t1_cbqx6dn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "_Daimon_", "body": "That sounds interesting. Be sure to post it here to /r/redditdev when you're done. Would love to read it :) Btw, it would be really great if you would add PRAW to your citations. Like for [IPython](http://ipython.org/citing.html). [Bryce](http://cs.ucsb.edu/~bboe/), the main committer, is getting a Ph.D. in Computer Science so citations matter for him. It doesn't much for me professionally, I'd just think it would be awesome :)", "created_utc": 1376933145, "gilded": 0, "name": "t1_cbqyyeh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "CMThF", "body": "thank you for mentioning this, I will do that! A colleague of mine also writes his master thesis, and I will recommend him to cite PRAW too. Also, I am going to post the resulting paper; I hope it will result in a worthwhile read... we will see about that. :)", "created_utc": 1376953337, "gilded": 0, "name": "t1_cbr5x8n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "bboe", "body": "Yes, it would be awesome if you cite PRAW. I suppose something like the following would be a good citation: > PRAW Development Team (2013). Python Reddit API Wrapper (PRAW), GNU General Public License. https://github.com/praw-dev/praw Edit: Please sent us a link to whatever work you create using PRAW as we'll add a section of PRAW-related publications. Thanks!", "created_utc": 1377121363, "gilded": 0, "name": "t1_cbski9x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "Stuck_In_the_Matrix", "body": "I'm helping him.", "created_utc": 1376953991, "gilded": 0, "name": "t1_cbr65le", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ko256/using_praw_i_get_a_gateway_timeout_when_crawling/"}, {"author": "MrFanzyPanz", "body": "Also, I have code that looks like this: r = praw.Reddit(user_agent=dict['user_agent']) h_index_list = list() for comment in r.get_redditor(account).get_comments(limit = 300): comment_citations = 0 for reply in comment.replies: comment_citations = comment_citations + 1 h_index_list.append(comment_citations) print(comment_citations) This counts each comment in comment.replies as a request. This means that counting the replies to each comment takes at least 2 seconds. However, I have other code like this: for post in r.get_subreddit(subreddit).get_hot(limit=500): # write to database # data listed as needed is in order: unique_id = post.name #reddit_id subred = post.subreddit.display_name that processes 100 posts at once, along with their post.name and post.subreddit.display_name requests. When I built the account code, I was told it should pull data in 100 blocks like the second set of code. Is this true? Have I done something wrong?", "created_utc": 1376639325, "gilded": 0, "name": "t1_cbovrdr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kgauh/in_praw_is_there_a_way_to_request_an_entire/"}, {"author": "_Daimon_", "body": "Are you interested in all the comments in a Submission or only a small subset? EDIT: PRAWs ``MoreComments`` represent stubbed out comments. The same you see on the webend, when you use the \"see more comments\" button. For comments that are hidden due to being on a low layer, there's no method of preventing them from being ``MoreComments`` AFAIK. My testing says that it's the 11 layer that becomes a ``MoreComment``. Do you have a link to where it happens with 5?", "created_utc": 1376656288, "gilded": 0, "name": "t1_cboyak6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kgauh/in_praw_is_there_a_way_to_request_an_entire/"}, {"author": "MrFanzyPanz", "body": "Update: Sorry, my partner moved on from the project since the MoreComments can't be sped up. Unfortunately, he never got back to me on whether the MoreComment layers started at the 5th level or whether he was exaggerating. Thank you for your help, though! It's always appreciated.", "created_utc": 1376975929, "gilded": 0, "name": "t1_cbrdsbf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kgauh/in_praw_is_there_a_way_to_request_an_entire/"}, {"author": "_Daimon_", "body": "Captchas are disabled if you have a bit of positive karma in the subreddit. So if the subreddit wants your bot and upvotes it', then capchas won't be a problem after you've posted the first few images manually.", "created_utc": 1376603168, "gilded": 0, "name": "t1_cbokfxp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kfy6s/how_do_you_post_images_using_praw/"}, {"author": "cosileone", "body": "Ah I see, what's the lower limit?", "created_utc": 1376606128, "gilded": 0, "name": "t1_cbolirj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kfy6s/how_do_you_post_images_using_praw/"}, {"author": "_Daimon_", "body": "Very little. Can't remember if it's 2 or 4 link karma.", "created_utc": 1376640417, "gilded": 0, "name": "t1_cbovxyd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kfy6s/how_do_you_post_images_using_praw/"}, {"author": "Falmarri", "body": "No. Why would reddit let bots post links?", "created_utc": 1376600276, "gilded": 0, "name": "t1_cbojbzy", "num_comments": null, "score": -6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kfy6s/how_do_you_post_images_using_praw/"}, {"author": "radd_it", "body": "/r/FullMoviesOnline /r/listentocurated /r/listentonew 3 of my fully-automated subreddits.", "created_utc": 1376671888, "gilded": 0, "name": "t1_cbp37uu", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1kfy6s/how_do_you_post_images_using_praw/"}, {"author": "_Daimon_", "body": "Seems ``update_checker`` tries to create a temporrary directory and that functionality isn't implemented by GAE's version of Python 2.7. You can try purging references to update_checker from PRAW, which won't have any effect on the functioning of PRAW. Except obviously update_checker will no longer work.", "created_utc": 1376376558, "gilded": 0, "name": "t1_cbmpfow", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "Lick_A_Brick", "body": "I know his problem is already solved, but can you maybe help me with setting up my bot at GAE? I'm a total noob and never have done anything like this before. I can add you as a developer at the project though.", "created_utc": 1377792520, "gilded": 0, "name": "t1_cbxqtw5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "_Daimon_", "body": "I don't use GAE, so I wouldn't be much help. I would either contact OP of this thread, who got PRAW running on GAE, or checkout the [Pull Request](https://github.com/praw-dev/praw/pull/239) he sent us to add GAE compatibility.", "created_utc": 1377794728, "gilded": 0, "name": "t1_cbxrpp5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "pipsqueaker117", "body": "Followed your advice, and managed to get the script working. Thanks! Also, I want to post a .zip containing the dependencies for praw and the modded version I had to use (help other newbs avoid the trouble I went through), any idea where I would post such a thing?", "created_utc": 1376411642, "gilded": 0, "name": "t1_cbmwu5f", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "_Daimon_", "body": "If you can write a guide to getting PRAW running on GAE, then I will happily include it as a page in [PRAW's documentation](https://praw.readthedocs.org/en/latest/). But I don't want zip files, because it is static format. Meaning that any bugfixes, new features or other improvements made to PRAW or any of it's dependencies will not be included in that zip Worst case, a backwards incompatible change to the API would make it impossible for users of that zip folder to follow the tutorials or run any of the example code. I don't want any PRAW user to experience that, and especially not newcomers.", "created_utc": 1376433196, "gilded": 0, "name": "t1_cbn5cs9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "pipsqueaker117", "body": "Good point. Do you maintain praw, btw? If you do, I've put in a pull request for a change which would allow stickying of posts through the library Also, I changed some things around in init.py to make it compatible with Google App Engine: would you want me to submit a pull request for those changes as well- if they're accepted then users should be able to use praw w/ GAE w/o having to mod it (after they downloaded all its dependencies, of course)", "created_utc": 1376501866, "gilded": 0, "name": "t1_cbnnzna", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "coderanger", "body": "Even if you get things working, remember you will be limited to 657000 URL fetches per day. Depending on what your bot does you could blow through this easily with just a few API operations a second. Use Heroku and save yourself a bundle of trouble on all fronts.", "created_utc": 1376372036, "gilded": 0, "name": "t1_cbmoe7p", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "pipeep", "body": "At 30 requests/minute (Reddit's API limit), that's only `24*60*30 = 43200` requests (or URL fetches) per day.", "created_utc": 1376421369, "gilded": 0, "name": "t1_cbn0nmk", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1k8ocm/is_there_any_way_to_easily_host_a_reddit_bot/"}, {"author": "vaetrus", "body": "The json data of a comment permalink will give you all children of that comment along with their authors. Here's json for [cbi6ato](http://www.reddit.com/r/linux/comments/1jtkkn/fulltime_linux_users_why_did_you_make_the_switch/cbi6ato.json): [{ SUBMISSION JSON DATA }, {\"data\": {\"children\": [{\"data\": {\"replies\": {\"data\": {\"children\": [{\"data\": {\"replies\": {\"data\": {\"children\": [{\"data\": {\"replies\": {\"data\": {\"children\": [{\"data\": {\"replies\": {\"data\": {\"children\": [{\"data\": {\"replies\": \"\", \"parent_id\": \"t1_cbicai2\", \"name\": \"t1_cbid2y3\",}}], }}, \"parent_id\": \"t1_cbiab9q\", \"name\": \"t1_cbicai2\"}}], }}, \"parent_id\": \"t1_cbi8smq\", \"name\": \"t1_cbiab9q\"}}], }}, \"parent_id\": \"t1_cbi6ato\", \"name\": \"t1_cbi8smq\",}}], }}, \"author\": \"TheNoodlyOne\", \"parent_id\": \"t3_1jtkkn\", \"name\": \"t1_cbi6ato\", }}], }}]", "created_utc": 1375830010, "gilded": 0, "name": "t1_cbidto1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jsuht/get_comment_and_replies_in_one_request/"}, {"author": "TheNoodlyOne", "body": "so instead of using \"comment.replies\", I should use \"comment.children?\"", "created_utc": 1375847913, "gilded": 0, "name": "t1_cbik05b", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jsuht/get_comment_and_replies_in_one_request/"}, {"author": "vaetrus", "body": "I don't think comment objects have a .children attribute. If you wanted to use the json instead of pulling reddit objects you'd have to parse out the children and replies. You wouldn't be able to just change one line and keep running.", "created_utc": 1376605675, "gilded": 0, "name": "t1_cbolczc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jsuht/get_comment_and_replies_in_one_request/"}, {"author": "pgl", "body": "I don't have an answer, but, I'm curious: what do you want a bot to give gold for?", "created_utc": 1375564099, "gilded": 0, "name": "t1_cbgc5or", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "IAmAnAnonymousCoward", "body": "Rewarding people who make successful submissions to my subreddit :)", "created_utc": 1375564585, "gilded": 0, "name": "t1_cbgcaq2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "pgl", "body": "Are you not worried that your bot could be gamed, causing you financial grief?", "created_utc": 1375567393, "gilded": 0, "name": "t1_cbgd39m", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "IAmAnAnonymousCoward", "body": "Not really, no. Worst thing that could happen is I lose the creddits because I suck at programming.", "created_utc": 1375567475, "gilded": 0, "name": "t1_cbgd42g", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "pgl", "body": "But don't you have to pay for them?", "created_utc": 1375567521, "gilded": 0, "name": "t1_cbgd4j8", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "IAmAnAnonymousCoward", "body": "Yes of course. But it's just a silly experiment. And if people use vote cheating to meet the requirement then they'll be shadow banned, so that seems rather pointless.", "created_utc": 1375567696, "gilded": 0, "name": "t1_cbgd6au", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "Steven_with_a_PH", "body": "You could also set a hourly/daily/monthly limit that you can override whenever. It would help in case someone actually starts gaming it while you're asleep.", "created_utc": 1375583459, "gilded": 0, "name": "t1_cbghidv", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "pgl", "body": "OK, cool. Just curious. Good luck finding an answer, and with your subreddit!", "created_utc": 1375568167, "gilded": 0, "name": "t1_cbgdayw", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "jomkr", "body": "I don't see why not.", "created_utc": 1375566238, "gilded": 0, "name": "t1_cbgcro0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "IAmAnAnonymousCoward", "body": "How?", "created_utc": 1375566293, "gilded": 0, "name": "t1_cbgcs92", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": null, "body": "[deleted]", "created_utc": 1375566627, "gilded": 0, "name": "t1_cbgcvlw", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "IAmAnAnonymousCoward", "body": "Thanks, but I'd just charge it up with [creddits](http://www.reddit.com/wiki/creddits), manually. My question is how I can make the bot give someone a \"creddit\".", "created_utc": 1375567049, "gilded": 0, "name": "t1_cbgczty", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "Stuck_In_the_Matrix", "body": "You don't give people \"creddits,\" you use your creddits to purchase reddit gold for someone else.", "created_utc": 1375569719, "gilded": 0, "name": "t1_cbgdqbt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "IAmAnAnonymousCoward", "body": "Yeah, I could have worded that better.", "created_utc": 1375570053, "gilded": 0, "name": "t1_cbgdtr2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "jomkr", "body": "Sorry, I didn't know about reddit creddits. You send a GET request to this URL. Using the python requests library: https://ssl.reddit.com/gold?goldtype=gift&months=1&recipient=USER&signed=yes&giftmessage=MESSAGE_HERE This URL comes from, https://ssl.reddit.com/gold?goldtype=gift you can play around with different options etc.. (Don't worry, pressing Give just takes you to a confirmation screen). You then extract the required parameters from the \"sendcreddits\" form in the HTML of the page, and build up a POST request and send it; again you could use the python Requests library. I'm going to bed now, in the morning I'll post some example code.", "created_utc": 1375570930, "gilded": 0, "name": "t1_cbge2oy", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "IAmAnAnonymousCoward", "body": "Thanks. I was hoping I could use the API / PRAW, so this will be new territory for me.", "created_utc": 1375571274, "gilded": 0, "name": "t1_cbge66a", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jmzm7/praw_can_a_bot_give_gold/"}, {"author": "_Daimon_", "body": "Possible yes, implemented in PRAW no. I have some other coding matters to attend to first, but I should be able to add this functionality within the week. I'll be sure to add another comment to this post when I do.", "created_utc": 1375661486, "gilded": 0, "name": "t1_cbgzzx4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jklbu/praw_is_there_a_function_for_getting_unread_mod/"}, {"author": "_Daimon_", "body": "That method was intentionally broken in version 2.0.9. Because it uses a SSL endpoint and Reddits SSl endpoints are not meant for heavy traffic. See [Issue 175 on github.](https://github.com/praw-dev/praw/issues/175)", "created_utc": 1375232697, "gilded": 0, "name": "t1_cbdq9mh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jcgsw/unexpected_redirect_getting_friends_list/"}, {"author": "naffarama", "body": "Thanks that was driving me nuts, I thought I was doing something stupid wrong.", "created_utc": 1375281681, "gilded": 0, "name": "t1_cbe20uw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1jcgsw/unexpected_redirect_getting_friends_list/"}, {"author": "isolani", "body": "I think that would be the only way. You can just compare the submission id's for ones that are in both (if you specifically onlywant ones from the top 10) or just check the subreddits of the submissions from `.get_liked()`.", "created_utc": 1375071429, "gilded": 0, "name": "t1_cbcdjm3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "MonkeyNin", "body": "I can't seem to find a function named `get_liked` anywhere? Normally I'm good at reading documentation but I'm having trouble with PRAW. And it doesn't match the built in docstrings, which is weird.", "created_utc": 1375122901, "gilded": 0, "name": "t1_cbcq43x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "nemec", "body": "It [should](https://github.com/praw-dev/praw/blob/master/praw/objects.py#L684). Maybe you're using an older version or misspelling the name?", "created_utc": 1375129920, "gilded": 0, "name": "t1_cbcszzi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "isolani", "body": "Maybe it has to do with the `make my votes public (let everyone see /user/isolani/liked and /user/isolani/disliked)` option in preferences? Though I don't think it should matter if it's through API and you're logged int\u2026", "created_utc": 1375175144, "gilded": 0, "name": "t1_cbd7b10", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "isolani", "body": "Then it might not exist. I know `get_saved` exists.", "created_utc": 1375123058, "gilded": 0, "name": "t1_cbcq6f4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "MonkeyNin", "body": "I don't specifically want 10, but some of the subs have low traffic. > or just check the subreddits of the submissions from .get_liked() Oh that sounds like it removes step one. Since the goal is images I personally liked.", "created_utc": 1375108461, "gilded": 0, "name": "t1_cbckqqs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "isolani", "body": "Then you shouldn't even need to retrieve submissions from the subreddit.", "created_utc": 1375108565, "gilded": 0, "name": "t1_cbckrvc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "xTristan", "body": "It is simpler than that... Once you've logged in, you can get your likes with `r.user.get_liked()` which returns a [RedditContentObject](http://python-reddit-api-wrapper.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.RedditContentObject) that you can iterate through to get your liked posts as [Submission](http://python-reddit-api-wrapper.readthedocs.org/en/latest/pages/code_overview.html#praw.objects.Submission) objects. Example: import praw r = praw.Reddit(user_agent='example') r.login('userfoo', 'passwordbar') for post in r.user.get_liked(): if post.subreddit.display_name == 'EarthPorn' or post.subreddit.display_name == 'spaceporn': print post.url", "created_utc": 1375229367, "gilded": 0, "name": "t1_cbdp4ms", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "MonkeyNin", "body": "I finally got it working. Thanks guys.", "created_utc": 1375289491, "gilded": 0, "name": "t1_cbe51lr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j8wfe/praw_download_images_in_subs_if_you_upvote_them/"}, {"author": "bboe", "body": "I [filed a bug](https://github.com/praw-dev/praw/issues/235) for this issue. You should be able to extract the url from the exception object, and then call `r.get_submission` on that URL, but you shouldn't have to. Not sure when the issue will be fixed though.", "created_utc": 1375123674, "gilded": 0, "name": "t1_cbcqfkz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j1j6q/praw_redirectexception_when_searching_for_url_if/"}, {"author": "IAmAnAnonymousCoward", "body": "Thanks! Yeah it's annoying but manageable. Any idea how to handle [my other issue](http://www.reddit.com/r/redditdev/comments/1hvgjf/praw_actually_getting_the_top_200_submissions/)?", "created_utc": 1375124334, "gilded": 0, "name": "t1_cbcqpfu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1j1j6q/praw_redirectexception_when_searching_for_url_if/"}, {"author": null, "body": "I literally had this problem 15 minutes ago :) What operating system are you using? I know this is a problem, but personally I didn't even use virtualenv and just used pip to install praw easily. http://www.tylerbutler.com/2012/05/how-to-install-python-pip-and-virtualenv-on-windows-with-powershell/ This was a miracle. I don't use PowerShell, but if you ignore those sections of the tutorial you can easily get pip. Even if you use a Unix based OS or somethng else, this tutorial also works because it just relies on python scripts to install everything.", "created_utc": 1374723077, "gilded": 0, "name": "t1_cb9rakn", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "Which parts of the tutorial should I skip specifically?", "created_utc": 1374723659, "gilded": 0, "name": "t1_cb9rhve", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": null, "body": "I would say anything you don't immediately need. I'm not familiar with virtualenv, but I realize I'll need to learn it eventually. However I pretty much just did everything up to and including installing pip and virtualenv through it, then just installed praw. Feel free to explore and try things out though!", "created_utc": 1374727612, "gilded": 0, "name": "t1_cb9ssjm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "Wow. I *just* found a shortcut. All I had to do to install PRAW was download distribute_setup.py and get-pip.py, then type: python distribute-setup.py python get-pip.py pip install praw", "created_utc": 1374727760, "gilded": 0, "name": "t1_cb9su71", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "mrwazsx", "body": "Hey i just found this through a google search and it looks like you have successfully managed to install PRAW; anyway i'm up to having installed pip and in your shortcut you say type \"pip install praw\" would you mind expanding on this i'm confused about how to direct this to the .zip file from github", "created_utc": 1377188920, "gilded": 0, "name": "t1_cbt2v6e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "PRAW installation is included in pip. Literally just execute \"pip install praw\" in cmd.", "created_utc": 1377193885, "gilded": 0, "name": "t1_cbt4xcz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "mrwazsx", "body": "i think i may have installed pip incorrectly or i'm running the command incorrectly - in cmd do you just run the \"pip install now\" from system 32 or do you do something like [this](http://stackoverflow.com/questions/4621255/how-do-i-run-a-python-program-in-the-command-prompt-in-windows-7). thanks", "created_utc": 1377195701, "gilded": 0, "name": "t1_cbt5p00", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "1. Open cmd (Start button > \"cmd\" > Hit Enter) 2. Type \"pip install praw\" 3. ???? 4. PROFIT!!", "created_utc": 1377212344, "gilded": 0, "name": "t1_cbtci8m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "mrwazsx", "body": "yeah i think i may have not installed PIP properly because cmd doesn't recognize any of the pip commands- thanks for the help though", "created_utc": 1377240925, "gilded": 0, "name": "t1_cbtllnt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "mrwazsx", "body": "In the end i just used [this](https://sites.google.com/site/pydatalog/python/pip-for-windows) and it seems to be working really well", "created_utc": 1377263192, "gilded": 0, "name": "t1_cbtp4cp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "Good job!", "created_utc": 1377271168, "gilded": 0, "name": "t1_cbtrp7c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "mrwazsx", "body": "Thanks - side note. how is it going 28 days later from this thread. Have you made a bot :O have you managed with python", "created_utc": 1377271406, "gilded": 0, "name": "t1_cbtrsjl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "Yes, it almost runs 24/7. But lately it's been off because I have to get a new laptop. /u/LinkFixerBot2", "created_utc": 1377272251, "gilded": 0, "name": "t1_cbts4d5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "mrwazsx", "body": "woahdude you made linkfixerbot that is really amazing i love linkfixerbot", "created_utc": 1377272650, "gilded": 0, "name": "t1_cbtsa5o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "Thanks!", "created_utc": 1377276240, "gilded": 0, "name": "t1_cbttq4d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "PM_ME_YOUR_TITS_GIRL", "body": "Sorry to butt in on your thread but I found this via google search as I am looking to install PRAW. Your solution seems very easy to follow but I am curious what folder do you put the distribute_setup.py and get-pip.py files? I too am using Windows.", "created_utc": 1375883963, "gilded": 0, "name": "t1_cbirbki", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "Just put them in your downloads folder, and do `python \"C:/Users/PM_ME/Downloads/distribute-setup.py\"`, then do the same for get-pip.py.", "created_utc": 1375895972, "gilded": 0, "name": "t1_cbivssi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": null, "body": "That's what I did too actually. I wanted to share those, but I figured I would show what actually lead to my discovery in the first place.", "created_utc": 1374772967, "gilded": 0, "name": "t1_cba413t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "Okay. I have Win7 on this computer. Thanks, a lot!", "created_utc": 1374723158, "gilded": 0, "name": "t1_cb9rbl4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Dominoed", "body": "Hello Edit: Shit! It worked! YES!", "created_utc": 1374730373, "gilded": 0, "name": "t1_cb9tlen", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "Ordered_Chaos", "body": "Can you help me? I am still having issues. I tried following what's been said, and I cannot for the life of me do it. Please help.", "created_utc": 1382494743, "gilded": 0, "name": "t1_ccxkqsq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1izqfy/help_with_installing_praw/"}, {"author": "ElliotSpeck", "body": "My [RamNode](http://ramnode.com/) VPS. :)", "created_utc": 1374646183, "gilded": 0, "name": "t1_cb93s79", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "mamaBiskothu", "body": "I'm curious; how are these VPSes different from a EC2 instance? Which one is faster?", "created_utc": 1374877521, "gilded": 0, "name": "t1_cbb0uf0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "isolani", "body": "It can't really be said which is faster because you can get different configurations of VPSes. Most VPSes you pay for on a monthly, quarterly, or yearly basis, while EC2 instances are sort of a pay-as-you-go sort of thing.", "created_utc": 1375032562, "gilded": 0, "name": "t1_cbc1c7m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "IAmAnAnonymousCoward", "body": "Thanks again for pointing me to the right direction, both bots are running there now for $16.56 a year and I'm only using 44.5 MB of 128 MB RAM yet :D", "created_utc": 1374781823, "gilded": 0, "name": "t1_cba7v0j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "isolani", "body": "Thanks for mentioning the memory usage. I was wondering about that myself :)", "created_utc": 1374807961, "gilded": 0, "name": "t1_cbahagw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "IAmAnAnonymousCoward", "body": "Holy shit, how can this be so cheap? (My bots currently run on a dying notebook...)", "created_utc": 1374688990, "gilded": 0, "name": "t1_cb9ehbt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "isolani", "body": "Unmanaged VPSes are cheap, but you have to\u2026 well, manage them. You get very little help from the provider with setting it up and maintaining it (they only handle problems that are their fault) but they're much cheaper than managed VPSes. Also, a VPS isn't a machine by itself. They're usually on dedicated servers that run many VPS instances (so you're sharing the machine with other people). They're completely separated (pretty much virutal machines), but it allows them to be cheaper than dedicated servers.", "created_utc": 1374807806, "gilded": 0, "name": "t1_cbah8m1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "IAmAnAnonymousCoward", "body": "Thanks. Yeah, I understand all this, but still...", "created_utc": 1374868783, "gilded": 0, "name": "t1_cbaxnlt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "isolani", "body": "Are you hosting other stuff on your VPS? If so, do Reddit bots take a considerable amount of resources to run and affect them?", "created_utc": 1374676500, "gilded": 0, "name": "t1_cb99ob6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "slyf", "body": "Depending on what your bot is doing, your bot will spend most of the time waiting around to make requests, or rate limiting until it can make a request. I would not worry too much about being cpu or IO bound.", "created_utc": 1374701484, "gilded": 0, "name": "t1_cb9jne0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "untrusted_wifi", "body": "I'm using a bottom tier VPS to run multiple projects. Praw's included praw-multiprocess has been a lifesaver. Resource consumption is minimal, but will depend on what you're doing. Other projects on the same VPS include a LAMP stack and various bits and pieces.", "created_utc": 1374684742, "gilded": 0, "name": "t1_cb9cqix", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "isolani", "body": "What VPS provider/host are you using?", "created_utc": 1375032635, "gilded": 0, "name": "t1_cbc1d0g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "shaggorama", "body": "I have an old laptop that won't run if it's unplugged so I can't take it anywhere. VideoLinkBot runs on that. I might move it into heroku as an experiment, but she's been running locally for quite some time. It's just easy.", "created_utc": 1374671208, "gilded": 0, "name": "t1_cb984ny", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "isolani", "body": "What operating system are you running on it?", "created_utc": 1375032578, "gilded": 0, "name": "t1_cbc1cdy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "shaggorama", "body": "It really doesn't make any difference, but Ubuntu 10.04 LTS. Why does it matter? It's just a python script. I could run it in a toaster.", "created_utc": 1375032791, "gilded": 0, "name": "t1_cbc1esk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "isolani", "body": "Could be running a really stripped down distro.", "created_utc": 1375033086, "gilded": 0, "name": "t1_cbc1i3s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "shaggorama", "body": "Nah, just an old laptop that happens to be running my script.", "created_utc": 1375033142, "gilded": 0, "name": "t1_cbc1iqk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": null, "body": "A cheap VPS from DigitalOcean.", "created_utc": 1374675830, "gilded": 0, "name": "t1_cb99g90", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "Bornhuetter", "body": "I use a VPS for long running bots, short term adhoc stuff on my main laptop, and more longer duration scraping stuff on an old laptop.", "created_utc": 1374681018, "gilded": 0, "name": "t1_cb9babu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "bennythomson", "body": "Raspberry Pi", "created_utc": 1374669047, "gilded": 0, "name": "t1_cb97nh9", "num_comments": null, "score": -1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "isolani", "body": "How do you run the Python on the Arduino? Edit: bennytomson edited his comment. It originally said \"Raspberry Pi or Arduino\"; hence my comment and untrusted_wifi's.", "created_utc": 1374676434, "gilded": 0, "name": "t1_cb99nji", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "honestbleeps", "body": "Raspberry Pi is not an Arduino.", "created_utc": 1374683113, "gilded": 0, "name": "t1_cb9c3gb", "num_comments": null, "score": -3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "isolani", "body": "> Raspberry Pi is not an Arduino. bennythomson edited his comment. It originally said \"Raspberry Pi or Arduino\".", "created_utc": 1374683801, "gilded": 0, "name": "t1_cb9cd78", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "untrusted_wifi", "body": "Arduino? While PyMite will run on an Arduino Mega the work it'd take to get PRAW to work makes it an illogical choice. **Edit**: Instead of replying to u/isolani or myself, u/bennythomson edited their response. The original text was \"Raspberry ip [sp] or arduino\"", "created_utc": 1374674654, "gilded": 0, "name": "t1_cb992ov", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ixqu0/praw_where_do_you_all_host_your_pythonbased_bots/"}, {"author": "SuperSeriouslyUGuys", "body": "It looks like if your image comes from somewhere other than imgur or is already in used it's going to leave the current one there for another sleep cycle. You could use python's built in `filter` or a list comprehension to strip all the non-imgur links from submissions before going into the loop. And use else continue If it's already in used. Also, I'm not experienced with praw, so this might not be the case, but I wouldn't expect to see the same submission id multiple times so you could probably take the whole used array and check out.", "created_utc": 1374434551, "gilded": 0, "name": "t1_cb7bv7z", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "xiggy", "body": "Like I said, I am quite the novice when it comes to Python and PRAW. This is literally the first thing I've written. I do think I understand what you're saying about leaving the current image for another cycle. I believe I experienced that when I was testing with only grabbing 5 submissions. In that case, only 3 of the 5 images where from i.imgur.com. After it got to the last image, it just kept repeating it over and over. Although, one time once it looped though all the submissions, it just started writing blank CSS files, which, in turn, displayed a blank HTML page. I wrote in the part with the used list thinking that once it's grabbed that submission, it will add it to a list and skip over it every time afterwards. It, however, doesn't do that, to my knowledge. I still have a lot to learn about Python and programming in general. I will definitely research what you've mentioned in your reply.", "created_utc": 1374437674, "gilded": 0, "name": "t1_cb7culb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "stqism", "body": "Any reason you filter our i.imgur.com urls?", "created_utc": 1374435685, "gilded": 0, "name": "t1_cb7c7xs", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "xiggy", "body": "My reasoning behind that is that I only want direct image links because I'm embedding them. Am I wrong in thinking this is the best solution to do so?", "created_utc": 1374436556, "gilded": 0, "name": "t1_cb7chrs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "stqism", "body": "Give me a day or so, I've got a couple changes to make :) Mostly the imgur gifs and making python serve the html itself.", "created_utc": 1374449313, "gilded": 0, "name": "t1_cb7gk0u", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "MonkeyNin", "body": "If you want to serve a website, not just static HTML, flask is a good lightweight library.", "created_utc": 1375058477, "gilded": 0, "name": "t1_cbc9hcp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "phire", "body": "Well, it's a solution. But many links can be trivially detected as or transformed into a direct image link: * You can safely assume any url ending in .gif is a gif (a safer way would be to fetch the http headers and check the MIME type is \"image/gif\", which should always be correct.) * imgur.com/XYZ can be transformed into i.imgur.com/XYZ.gif (extension doesn't matter, .jpg and .png will return the same image. Again check the MIME type) * You might want to handle imgur albums. Just fetch the page and look for all tags pointing to i.imgur.com * There are many more you could do. Looks like the first 2 rules should get you 98% of /r/gifs traffic. If you really wanted more, you could check out the [Reddit Enhancement Suite source code](https://github.com/honestbleeps/Reddit-Enhancement-Suite/blob/master/lib/reddit_enhancement_suite.user.js#L10804)", "created_utc": 1374445843, "gilded": 0, "name": "t1_cb7fgr1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "MonkeyNin", "body": "note: imgur doesn't care what file extensions are. It can send a .jpg with the proper mime type for a gif.", "created_utc": 1375058390, "gilded": 0, "name": "t1_cbc9gd7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": null, "body": "I found this when i was looking for easy ways too learn how to code my own reddit bot, and i modified your code a bit and put it on my(otherwise shit) website! http://firecan.org/omgifs/", "created_utc": 1384899716, "gilded": 0, "name": "t1_cdiaxuy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iredp/its_not_much_but_its_my_first_bot_i_made_it_to/"}, {"author": "ddxxdd", "body": "I'm not gonna lie, I could not get the MoreComments object to work in any way, shape, or form. So I basically used a workaround: the _request() function. Daimon, one of the praw developers, told me not to use such low-level functions, but I figured \"why not? It works, and it abides by the 2-second API call limit\". So basically, my code looks like: r=praw.Reddit(user_agent='u/ddxxdd') after='' q=r._request('http://www.reddit.com/r/test/comments.json?after=\" + after) after=q['data']['after'] q=r._request('http://www.reddit.com/r/test/comments.json?after=\" + after) And that's effectively how I get more comments.", "created_utc": 1374211272, "gilded": 0, "name": "t1_cb5s0zv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ijb3m/error_when_running_a_praw_script/"}, {"author": "yesorknow", "body": "You don't need to worry about MoreComments as you come across them. There is a simple method you can call at the beginning of your script which automatically replaces all MoreComment objects with Comment objects. r = praw.Reddit('foo') submission = r.get_submission(url=url_you_want) submission.replace_more_comments(limit=None, threshold=0) This last line will do what you want. `limit` is the maximum number of MoreComment objects to replace. `threshold` is the minimum number of replies a comment must have for it to be replaced. `(None, 0)` will replace everything. Note that if you chance `limit` and/or `threshold` to avoid certain MoreComment objects, these obecjts are simply removed, not turned into Comments.", "created_utc": 1374258049, "gilded": 0, "name": "t1_cb63w5f", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ijb3m/error_when_running_a_praw_script/"}, {"author": "_Daimon_", "body": "That's weird. What's the code you're using? Are you running the script from your local machine? Does the following work? import praw r = praw.Reddit(user_agent='experimenting with PRAW by u/ _Skrillex_') submissions = r.get_subreddit('opensource').get_hot(limit=5) [str(x) for x in submissions]", "created_utc": 1374147660, "gilded": 0, "name": "t1_cb55m3e", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iirrw/just_installed_praw_keep_getting_error_when/"}, {"author": "_Skrillex_", "body": "Well, I just updated to 2.1.4 and it seems to have fixed the problem. I was able to run the code you provided with no problems, which I wasn't able to do yesterday. I'm at work so I don't really have time to do any extensive testing at the moment, but I will this afternoon. Thanks!", "created_utc": 1374163200, "gilded": 0, "name": "t1_cb5a5as", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1iirrw/just_installed_praw_keep_getting_error_when/"}, {"author": "vaetrus", "body": "[[runs praw module]] >>> ===== RESTART ===== >>> 2013-07-15 18:16:19,111 - Assigning user_agent. 2013-07-15 18:16:19,114 - Creating reddit object. >>> bot >>> bot.logIn() 2013-07-15 18:16:28,128 - Logging in. >>> bot.getUnread() 2013-07-15 18:16:32,250 - Getting unread messages. >>> bot.unread >>> unread = [i for i in bot.unread] >>> unread [] >>> unread = unread[0] >>> unread >>> dir(unread) ['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattr__', '__getattribute__', '__hash__', '__init__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_get_json_dict', '_info_url', '_populate', '_populated', '_underscore_names', 'author', 'body', 'body_html', 'context', 'created', 'created_utc', 'dest', 'first_message', 'first_message_name', 'from_api_response', 'fullname', 'id', 'mark_as_read', 'mark_as_unread', 'name', 'new', 'parent_id', 'reddit_session', 'replies', 'reply', 'subject', 'subreddit', 'was_comment'] >>> unread.body u'test' This is typically how I deal with all of reddit: I dir() it. Hope it helps.", "created_utc": 1373926978, "gilded": 0, "name": "t1_cb3c5t1", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "Falmarri", "body": "Also you should use IPython which has autocomplete.", "created_utc": 1373939337, "gilded": 0, "name": "t1_cb3geou", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "NH4ClO4", "body": "Ok, thanks. I was sorta frustrated that PRAW's documentation doesn't include variables, more me used to non-python languages.", "created_utc": 1373932062, "gilded": 0, "name": "t1_cb3dvuo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "bboe", "body": "It seems like it may be time to add some of these common attributes as you're not the only one that has these problems. The issue, and the reason I have previously suggested avoiding adding the attributes to the documentation, is that they are completely dynamic. That is the attributes are whatever reddit returns for that API endpoint. I admit while reddit will add attributes from time to time, it is incredibly rare that they change existing ones.", "created_utc": 1374026806, "gilded": 0, "name": "t1_cb47e35", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "_Daimon_", "body": "Use the standard Python tools for introspection. [See this page in PRAWs documentation for more details](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html)", "created_utc": 1373928194, "gilded": 0, "name": "t1_cb3cl50", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1id53y/praw_how_to_get_the_full_text_of_a_message/"}, {"author": "epsy", "body": "Seems like reddit sent you an incomplete json feed. Also what's wrong with copy-pasting?", "created_utc": 1373645846, "gilded": 0, "name": "t1_cb18o3e", "num_comments": null, "score": 8, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "MrFanzyPanz", "body": "Lol, my partner is the one who sent me the picture. He had it on a Linux VM and didn't know how to copy/paste the text to send it to me.", "created_utc": 1373666113, "gilded": 0, "name": "t1_cb1gl30", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "MonkeyNin", "body": "Probably set to console, left click drag to hilight text, MMB to paste.", "created_utc": 1374782201, "gilded": 0, "name": "t1_cba8114", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "epsy", "body": "I'm pretty sure I learned how to copy-paste before being a sysadmin.", "created_utc": 1373666460, "gilded": 0, "name": "t1_cb1gpb1", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "MrFanzyPanz", "body": "Haha! I'll tell him you said that! I thought it was funny too.", "created_utc": 1373666739, "gilded": 0, "name": "t1_cb1gsqk", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "slyf", "body": "Looks like you recieved a bad or incomplete response from reddit....", "created_utc": 1373649607, "gilded": 0, "name": "t1_cb1a4rl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "killver", "body": "catch some exceptions :)", "created_utc": 1373655479, "gilded": 0, "name": "t1_cb1ci8w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "MrFanzyPanz", "body": "How exactly would I go about this? Would I just try/except my entire connection to Reddit.com? Or just the .get() portion?", "created_utc": 1373666298, "gilded": 0, "name": "t1_cb1gnbs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "killver", "body": "just the specific praw call that causes the problems, you can then repeat the call or ignore it if it is not important", "created_utc": 1373667593, "gilded": 0, "name": "t1_cb1h2w6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1i51by/my_praw_code_ran_for_3_days_then_died/"}, {"author": "alienth", "body": "I noticed that it appears you're trying to iterate through listings. I should give you a heads-up that every listing on reddit is limited to a maximum of 1000 items. Also, the limit parameter only allows values up to 100 (you're currently passing 500).", "created_utc": 1373079620, "gilded": 0, "name": "t1_cawsoo7", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "MrFanzyPanz", "body": "That's okay! We only wanted to peruse the first 500 submissions under the 'hot' section of each subreddit we analyze. We figured we'd rather get less submissions and more subreddits, especially because some of the subreddits we will scrape have less than 10,000 subscribers, but they offer the most interesting communities. For the limit parameter, we just eat the time delay. It takes roughly 9.3 seconds to scrape a subreddit of its first 500 submissions using praw. We're fine with this. Thank you, though! :D", "created_utc": 1373155926, "gilded": 0, "name": "t1_caxa45f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "bboe", "body": "As long as the VMs have different IPs you shouldn't run into any problems with the 30 requests a minute rule. If they do not, then you'll probably have some issues and might as well run only a single instance (or use praw's multiprocessing feature) to handle the rate-limiting.", "created_utc": 1373065052, "gilded": 0, "name": "t1_cawog1j", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "MrFanzyPanz", "body": "We are using dedicated proxies for each VM. I may be wrong, but it seemed to me that dedicated proxies are more stable for a month-long unbroken datascrape across multiple VMs, considering that multi-processing is still rather new. Besides, I already wrote the code to handle the data without multi-processing. Each VM is respecting the 30 requests/minute rule that is a default for praw.", "created_utc": 1373065784, "gilded": 0, "name": "t1_cawonxi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "bboe", "body": "> We are using dedicated proxies for each VM. Reads like you're good to go then.", "created_utc": 1373067619, "gilded": 0, "name": "t1_cawp77h", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "spladug", "body": "> have different IPs you shouldn't run into any problems with the 30 requests a minute rule The letter of the law, not the spirit. :(", "created_utc": 1373078287, "gilded": 0, "name": "t1_cawsa2l", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "killver", "body": "Sounds interesting. Could you give us some insights what you are planning to do and which data you actually want to gather? Also keep in mind that you can not use the voting data if you are gathering data live. For this purpose it is better to crawl the past.", "created_utc": 1373138935, "gilded": 0, "name": "t1_cax59h4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "MrFanzyPanz", "body": "We wanted to study communities and how leaders develop within them. Since each subreddit is basically its own community with its own rules and culture, we thought they would serve as an amazing source of information regarding this. Plus, with the vast amount of data and the diversity subreddits, we have the opportunity to ask interesting questions like \"what constitutes a leader?\" and \"what kinds of goals within a community foster certain types of leaders?\". The voting data is just to track how a post's net score changes with time. We can take this data and analyze how post titles affect posts' success across all the subreddits. The defaults we expect to largely not contain leaders and simply be a jumbled aggregate, but for smaller subreddits we expect to find very interesting things!", "created_utc": 1373155696, "gilded": 0, "name": "t1_caxa1we", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": "killver", "body": "Cool, thounds interesting. What I meant about voting data is that you actually have to recrawl the voting data in several timesteps for the submissions / comments you gathered. Anyhow, keep us updated!", "created_utc": 1373289401, "gilded": 0, "name": "t1_cay6mqo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hptt3/large_reddit_research_project/"}, {"author": null, "body": "The reason it asks you for a captcha is because your bot hasn't got enough karma to post without doing it. The best thing to do would be to post an image in the subreddit asking people to upvote it for the new bot. As for the sidebar thing, I don't think so. I am also new to PRAW and nothing came up after a long, hard google search. Sorry about that, you'll have to do it manually.", "created_utc": 1372700325, "gilded": 0, "name": "t1_catvray", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/"}, {"author": "zzpza", "body": "Ok, that all makes sense. It's been so long since I signed up, I had forgotten about the captcha / karma limit. Thank you for the reply. :)", "created_utc": 1372701861, "gilded": 0, "name": "t1_catwbmr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/"}, {"author": "_Daimon_", "body": "The method you're looking for is [update_settings](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModConfigMixin.update_settings). It takes the same arguments as [set_settings](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.__init__.ModConfigMixin.set_settings) the diference is that with ``update_settings`` only the arguments explicitly given are changed. With ``set_settings`` it's all. I *believe* the argument you want to change is description. But I don't have access to a Python terminal with PRAW atm, so I cannot confirm.", "created_utc": 1372703714, "gilded": 0, "name": "t1_catx0tc", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/"}, {"author": "zzpza", "body": "Awesome, thank you so much! :)", "created_utc": 1372704346, "gilded": 0, "name": "t1_catx9ag", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1hf6fi/praw_can_it_change_the_sidebar_description_text/"}, {"author": "bboe", "body": "That feature is not currently supported. You can request it on [PRAW's github](https://github.com/praw-dev/praw) page if you'd like.", "created_utc": 1372650142, "gilded": 0, "name": "t1_catkgts", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1has5q/praw_how_do_you_add_moderators_with_nonfull/"}, {"author": null, "body": "[deleted]", "created_utc": 1372456680, "gilded": 0, "name": "t1_cas9byd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": "tgdm", "body": "Will I have to put the login inside the `While` loop?", "created_utc": 1372457337, "gilded": 0, "name": "t1_cas9kdw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": null, "body": "[deleted]", "created_utc": 1372457610, "gilded": 0, "name": "t1_cas9nxj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": "tgdm", "body": "Thank you so much. Is there a way I can print a time stamp at the very end of each loop? Would be useful for me to check in on it every once in a while. When I tried `print time` that didn't seem to work out >.>", "created_utc": 1372457899, "gilded": 0, "name": "t1_cas9riy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": "Quarkitude", "body": "Unrelated, but you don't need to call .get_comments() every loop?", "created_utc": 1372451506, "gilded": 0, "name": "t1_cas7gh6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": "tgdm", "body": "`subreddit_comments = subreddit.get_comments(limit = 10)` handles that", "created_utc": 1372453585, "gilded": 0, "name": "t1_cas880g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": "Quarkitude", "body": "And it gets the latest comments when you iterate over it?", "created_utc": 1372454555, "gilded": 0, "name": "t1_cas8kdu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": "tgdm", "body": "I don't follow what you're asking", "created_utc": 1372455398, "gilded": 0, "name": "t1_cas8v9x", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h9rda/need_help_getting_bot_to_keep_running/"}, {"author": "vacantmentality", "body": "In your example, \"t\" is a list of submission objects. In order to see the kinds of things you can get from those objects visit the URL of a submission, like the one in your example, except add \".json\" to the end, Like [this](http://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/.json). That will show you the JSON data for the page that can be accessed with the reddit API. It's a pretty good bet that PRAWs variable names are the same. If you wanted to get the actual text of the submission, 'selftext' seems to be what you need (from looking at the JSON link). This would mean the way to get that information would be: >>> t = list(r.get_content('http://www.reddit.com/r/redditdev', limit = 10)) >>> print(t) [, , , , , , , , , ] >>> t[0].selftext \"Hi, i'm very interested in using PRAW and i think its great, but i can't figure out how to get the actual text out of a submission. For example, if i do \\n\\n >>>t = list(r.get_content('http://www.reddit.com/r/redditdev', limit = 10))\\n >>>str(t[0])\\n '1:: [Praw] [Request] Use timestamps in submission query'\\n\\nhowever, i don't want the title of the post, i want the text which is \\n\\n> I recently had good luck using timestamps in an undocumented manner. It would be awesome to be able to use them in Praw and it might even get around the 1k assuming 'new' sorts chronologically (?).\\n\\nI've searched through the documentation and have read through a good portion of it, but i still haven't figured out how to do this, if it's possible. Can anyone help? Thank You!\\n\\nexample post from here: http://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/\\n\\nIf it makes any difference i'm trying using this with a subreddit where all posts are text posts\\n \" As you can see, it's not a matter of 'turning the objects into strings', each submission object has a range of variables like 'subreddit', 'banned_by' etc. that can be accessed by object(dot)variable_name, eg. \"t[0].selftext\". Hope that helps :) I'm still learning to use reddit bots myself but feel free to ask me any questions.", "created_utc": 1372389500, "gilded": 0, "name": "t1_carqwpw", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7yno/how_to_get_text_of_a_submission/"}, {"author": "osmotischen", "body": "ah-ha! selftext was what i was looking for, thanks!.", "created_utc": 1372413658, "gilded": 0, "name": "t1_carw4f9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7yno/how_to_get_text_of_a_submission/"}, {"author": "_Daimon_", "body": "Read [this tutorial in PRAWs documentation](https://praw.readthedocs.org/en/v2.0.15/pages/writing_a_bot.html) that talks about introspecting. It contains your answer and how to figure it out for yourself.", "created_utc": 1372409621, "gilded": 0, "name": "t1_carvlxq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7yno/how_to_get_text_of_a_submission/"}, {"author": "bboe", "body": "Requests for features should be made on [github](https://github.com/praw-dev/praw). If you do add a request, I suggest filling in a little more detail about what you want specifically along with how to accomplish those things outside of PRAW.", "created_utc": 1372649918, "gilded": 0, "name": "t1_catke88", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7hbr/praw_request_use_timestamps_in_submission_query/"}, {"author": "bboe", "body": "You can tell PRAW to decode them. See: https://github.com/praw-dev/praw/issues/186", "created_utc": 1372649740, "gilded": 0, "name": "t1_catkc5u", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7722/praw_html_entities_and_special_characters_in/"}, {"author": "SOTB-human", "body": "I noticed that special characters are sometimes handled inconsistently in PRAW; they appear as HTML entities in some places and as normal Python characters in other places. To demonstrate this, I'll execute some code to use PRAW to repost the selftext of this submission as a comment on the submission. > quoted text \u0ca0_\u0ca0 < >", "created_utc": 1372363173, "gilded": 0, "name": "t1_carhqz6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7722/praw_html_entities_and_special_characters_in/"}, {"author": "SOTB-human", "body": "Notice how the signs are changed, and the \"quoted text\" indicator becomes a literal > sign. Here is the code used to generate the comment: >>> import praw >>> r = praw.Reddit(\"commenting tester by /u/SOTB-human\") >>> r.login(\"SOTB-human\",\"\") >>> submission = r.get_submission(submission_id = \"1h7722\") >>> submission.selftext u\"I noticed that special characters are sometimes handled inconsistently in PRAW; they appear as HTML entities in some places and as normal Python characters in other places. To demonstrate this, I'll execute some code to use PRAW to repost the selftext of this submission as a comment on the submission.\\n\\n> quoted text\\n\\n \\u0ca0_\\u0ca0\\n < >\" >>> comment = submission.add_comment(submission.selftext) >>> comment.body u\"I noticed that special characters are sometimes handled inconsistently in PRAW; they appear as HTML entities in some places and as normal Python characters in other places. To demonstrate this, I'll execute some code to use PRAW to repost the selftext of this submission as a comment on the submission.\\n\\n&gt; quoted text\\n\\n \\u0ca0_\\u0ca0\\n &lt; &gt;\" Is there any way to prevent this, short of manually replacing all `&` entities with their equivalents?", "created_utc": 1372363359, "gilded": 0, "name": "t1_carhtnh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7722/praw_html_entities_and_special_characters_in/"}, {"author": "slyf", "body": "> short of manually replacing all & entities with their equivalents? There is your answer", "created_utc": 1372372200, "gilded": 0, "name": "t1_carl6d2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h7722/praw_html_entities_and_special_characters_in/"}, {"author": "tgdm", "body": "##Oh I learned how to do this today! *** Let's say you wanted to turn **my post** into the format you use. Well here is *how*: submission.addcomment('''##Oh I learned how to do this today! *** Let's say you wanted to turn **my post** into the format you use. Well here is *how*:''') The trick is to use three quotation marks before and after to mark where you want the multi-line content. Single quotation marks work fine, too.", "created_utc": 1372313219, "gilded": 0, "name": "t1_car4p6j", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "cgillett", "body": "That's actually perfect. I tried the other persons solution, but it didn't seem to work. This did. Thanks!", "created_utc": 1372313297, "gilded": 0, "name": "t1_car4pv3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "AndrasKrigare", "body": "EDIT: Realized I'm an idiot and someone already mentioned this farther down. Sorry about that. I realize this is a month old, but I figured I'd make a correction in case anyone else wanders in here. The issue is with reddit, since reddit needs 2 newline characters in order to show one. Your original would have been fine if it were: username = \"cgillett\" submission.add_comment(\"Username: \" + username + \"\\n\\n hey\") His fix probably worked if you used the ''' newline and left your \\n in there.", "created_utc": 1375075885, "gilded": 0, "name": "t1_cbcessg", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "ContemptuousCat", "body": "Thanks for correcting it, I had the same issue today but now have the formatting right!", "created_utc": 1379016127, "gilded": 0, "name": "t1_cc72qho", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "_Daimon_", "body": "Reddit uses Markdown to parse the text uses post into pretty looking text. Making a comment via PRAW i.e. the Reddit API is no different. The text gets formatted in the same way. Go to /r/test and try posting that line. You'll notice that it doesn't have a newline either. For more info on Markdown (and how to do what you want) check out this [older post](http://www.reddit.com/r/reddit.com/comments/6ewgt/reddit_markdown_primer_or_how_do_you_do_all_that/c03nik6), which IIRC is still the most accurate. If you're feeling adventurous, then you can look at the [source code for Reddits markdown parser](https://github.com/reddit/snudown) on github.", "created_utc": 1372326310, "gilded": 0, "name": "t1_car6xfy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "aidanhs", "body": "This is nothing to do with python and everything to do with markdown. In particular, markdown requires you either 1. Have two successive newlines to generate a new paragraph (i.e. a space between the previous line and this one) or 2. Have two successive spaces and then a newline to just start writing on the next line. So in your example it would be one of submission.add_comment(\"Username: \" + username + \"\\n\\n hey\") submission.add_comment(\"Username: \" + username + \"\\n hey\") The multiline comments work because they're literally taking the newlines from the python source file. This works, but it's handy to know what's going on behind the scenes.", "created_utc": 1372376239, "gilded": 0, "name": "t1_carmj1m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "dakta", "body": "This is a Python question, not a PRAW question. In python, string literals including line break can be enclosed in double or single quote strings. Quotes are interchangeable, and thus a matter of style. You want the string literal for new line: `\\n` AFAIK that pseudocode is totally valid, at least it's valid Python. Unless you're asking if you can spread that string concatenation over multiple lines, in which case the answer is 'yes, you can'. To split pretty much arbitrary lines, use this: http://stackoverflow.com/questions/53162/how-can-i-do-a-line-break-line-continuation-in-python", "created_utc": 1372312679, "gilded": 0, "name": "t1_car4khp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "cgillett", "body": "Awesome, thanks. Sorry for the stupid question.", "created_utc": 1372312938, "gilded": 0, "name": "t1_car4ms5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "dakta", "body": "No worries. Happy to help.", "created_utc": 1372313499, "gilded": 0, "name": "t1_car4rl5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h5u3a/praw_creating_multiline_comments/"}, {"author": "_Daimon_", "body": "Yes it does. Try using the `vars` method on a Comment object. For more details see [this tutorial on introspection in PRAW's documentation](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html).", "created_utc": 1372206785, "gilded": 0, "name": "t1_caq8qev", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h2p60/does_praw_provide_access_to_traits_of_comments/"}, {"author": "Lh2You", "body": "1. Yes, you can write the code in Notepad and save it as a .py file. Notepad though is not designed for programmers compared to something like Notepad++ which is designed for that purpose and features support for many languages such as Python, Lua, and Perl. Notepad++ can easily mark out things like imports and defining functions and can help handle other things related to supported languages. Notepad++ is Windows exclusive however but Linux features gedit as a decent alternative (no clue on OSX having never really used it). 2. You can run the program by clicking on the file however this is sometimes not the best way. You can use your command line (cmd.exe on Windows, Terminal on OSX) to run the program by typing \"python yourprogramname.py\". The advantage of this is that it allows you to see what errors are instead of clicking repeatedly. There is a downside however in that Windows does not learn the path to Python by default and you would have to learn how to set it to work. If you do want to set it up, [this guide](http://stackoverflow.com/questions/6318156/adding-python-path-on-windows-7) has valid instructions for Windows 7 (and possibly XP and Vista). Note: To use cmd.exe or any command line, you will need to browse to the right path. Use cd to change what directory you are in. Eg. If it was on my desktop, I would type \"cd C:\\Users\\Lh2You\\Desktop\" and then type \"python myprogramname.py\"", "created_utc": 1372173041, "gilded": 0, "name": "t1_capw0dx", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "_deffer_", "body": "Okay - so I downloaded Notepad++ and everything looks great. All I have to do is save the file as a `.py` file? Then in the cmd.exe, I just type `python filename.py' and it will run, or do I have to put the file somewhere specific? I've already added python to the environment variables, so when I open the cmd prompt I just have to type python and it's ready. Would I have to manually run it every so often, or is there something I can put in the code to tell it to run itself every 10 minutes (600 seconds)? I've been reading [this page](https://praw.readthedocs.org/en/latest/#a-few-short-examples) and think I've found the few lines that I would need to put in to grab the information I want. I want to try it out before moving on to the (assumingly) more difficult part of posting that information in a new subreddit, unless you know how and can explain like I'm an idiot.", "created_utc": 1372174210, "gilded": 0, "name": "t1_capwfnx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "Lh2You", "body": "No you do not have to put it anywhere specific. Yes it will run if you just type in \"python filename.py\" when in the same directory. You would need to manually run itself after you make a change to the code but if you wanted it to loop infinitely after running it you could do something like this: import praw import time while 1: time.sleep(seconds) Note: This requires importing time. Doing this would infinitely loop forever and you could change the variable to something else in order to allow the program to stop itself if a certain condition is met such as import praw import time runcount = 0 runrep = 1 while runrep == 1: time.sleep(seconds) runcount = runcount + 1 if runcount >= 20: runrep = 0 What this will do is it will run the code 20 times, gradually counting the amount of times it has been run. When it reaches 20 runs it will stop. I haven't really experimented with posting posts to a specific subreddit (I can take a guess at it but I have no clue on how to format the text posted) but just a warning. If you try to do something such as message someone with less than ~10 link karma it will give you a CAPTCHA to fill out (it will give you a URL to enter which will link you to a CAPTCHA). In the docs it says that the code to post a SELF post to a specific subreddit is r.submit('reddit_api_test', 'submission title', text='body'). As far as I can tell, the first one is the subreddit, the second the title, and the third the text in the body. This will need a login to complete though which is also mentioned on the page at #2.", "created_utc": 1372175384, "gilded": 0, "name": "t1_capwv9u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "_deffer_", "body": "Can I avoid the captcha if I make the bot an approved submitter or moderator to the subreddit? So, I can't figure out how to just open cmd.exe and type `python defferbot.py` to make it run, it says errno 2 cannot find file or directory or something similar. I can change the directory to C:\\python27 and then type `defferbot.py` and it will accept it, but it won't do anything. The cursor moves to the next link, the underscore blinks, but nothing ever runs. Here is my code: import praw import time while 1: r = praw.Reddit(user_agent='_deffer_') submissions = r.get_subreddit('gameswap').get_new(limit=5) [str(x) for x in submissions] Am I messing something up?", "created_utc": 1372177475, "gilded": 0, "name": "t1_capxns1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "Lh2You", "body": "I don't know if you can avoid the CAPTCHA by being a moderator (I am not one). You may not have been in the right directory at the time when you tried to run it. Your code has a flaw as far as I can tell. You put your str(x) code outside of the while loop which means that it will not print until it exits the while loop. Fixed code is below. Also, you can leave your user agent before the while loop as you only need to declare it once. import praw import time while 1: r = praw.Reddit(user_agent='_deffer_') submissions = r.get_subreddit('gameswap').get_new(limit=5) [str(x) for x in submissions]", "created_utc": 1372178203, "gilded": 0, "name": "t1_capxxiz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "_deffer_", "body": "I used your code, but am getting the same result. I don't know what it is, but when I run it using my N++ file, I get nothing. When I type it out manually, it works just fine. My N++ file is exactly the same as what I type into python, just without the `>>>`. Am I missing something?", "created_utc": 1372179313, "gilded": 0, "name": "t1_capycqz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "Lh2You", "body": "I have absolutely no idea why it is not working. Based on the result that I got by typing it in in Python, I assumed that you were going for the title. This rewritten code achieves basically the same result. Sorry, I have literally no idea why it is not working when it is put into a file. import praw import time while 1: r = praw.Reddit(user_agent='_deffer_') submissions = r.get_subreddit('gameswap').get_new(limit=5) for submission in submissions: print(submission.title) Since the only difference is when it is typed in (and the >>> is just the signal to tell you that you can type something in) I really have no clue. My only guess is that for some reason, x is defined as having no value when you run the program but does gain a value when you type it in. I assume this because when I run the code without the while loop, there is a line without any information on it and then it ends. Once again, sorry.", "created_utc": 1372180489, "gilded": 0, "name": "t1_capyt3p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "_deffer_", "body": ">Once again, sorry. No worries - I have no idea what I'm doing anyway, haha. I'll try this out in a bit (on mobile for the time being.)", "created_utc": 1372180950, "gilded": 0, "name": "t1_capyzit", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "luuk2305", "body": "Just my 2 cents on 1.: Try Sublime Text. It's free in the evaluation, is pretty cheap compared to other editors, is lightning fast, and the evaluation is unlimited.", "created_utc": 1372175218, "gilded": 0, "name": "t1_capwsyv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1h1fi9/okay_ive_installed_python_pip_and_praw_if_its/"}, {"author": "nemec", "body": "Did you even try debugging? By the stack trace, `author` is null. If you print that submission's url, you are directed [here](http://www.reddit.com/r/pics/comments/1gsckf/a_brain_preserved_in_paraffin_wax_being_sliced/) where it's plain to see the author of the post deleted his/her account (thus the empty author attribute).", "created_utc": 1371876375, "gilded": 0, "name": "t1_canveni", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/"}, {"author": "peteyMIT", "body": "I see. I thought that might be the case, but I did a quick visual inspection of /top/ and didn't see any [deleted] names. I should have just printed the URL and checked that way, of course, but didn't think of it (I'm somewhat new to Python). Can you recommend a way to set name = to either submission.author.name or [deleted]? I was thinking of writing an if statement but I'm not sure how to assess submission.author.name in the conditional and can't test right now because there's not a [deleted] author anywhere I can easily find in /top/ Apologies, and thanks to you and others for the help.", "created_utc": 1371901457, "gilded": 0, "name": "t1_canzfmh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/"}, {"author": "_Daimon_", "body": "name = submission.author.name or \"[deleted]\" Python is great.", "created_utc": 1371909935, "gilded": 0, "name": "t1_cao0ryp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/"}, {"author": "peteyMIT", "body": "awesome, that's what i had written, but i couldn't verify. thank you!", "created_utc": 1371910899, "gilded": 0, "name": "t1_cao0z4j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/"}, {"author": "bboe", "body": "That's not going to work for the same reason. You want: name = submission.author.name if submission.author else '[deleted]'", "created_utc": 1371933564, "gilded": 0, "name": "t1_cao7517", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1gu6y5/did_something_just_break_with_praws_get_top_method/"}, {"author": "vacantmentality", "body": "I think what you're looking for is: if isinstance(item, praw.objects.Submission): # stuffs elif isinstance(item, praw.objects.Comment): # other stuffs I got that from the source code for [AutoModerator](https://github.com/Deimos/AutoModerator/blob/master/modbot.py), written by /u/Deimorz. I've never used [isinstance](http://docs.python.org/2/library/functions.html#isinstance) before (I'm still learning python myself) but I think this is the way you would do it.", "created_utc": 1371967201, "gilded": 0, "name": "t1_caog6qi", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "brucemo", "body": "No, that's not the problem. The problem is when you resolve the link, it gives you the submission even if it's a comment. So the first test would return True even if the link is a link to a comment.", "created_utc": 1371980173, "gilded": 0, "name": "t1_caoia19", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "vacantmentality", "body": "Is there something in the way that reddit URLs are formatted that will tell you if it's a comment or not? How well is your way working at the moment?", "created_utc": 1371981116, "gilded": 0, "name": "t1_caoidsy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "brucemo", "body": "I haven't explored this, in part since I didn't really get an answer. They are different, but this would be another inelegant way to do any of this stuff. A submission: http://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/ A comment: http://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/caoidsy If you do `rh.get_submission` on either URL, obj.permalink will equal the first URL.", "created_utc": 1371981561, "gilded": 0, "name": "t1_caoiflk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "RandomIndianGuy", "body": "I was wondering about the same thing and found an answer. If you haven't found it yet, then, [here](http://stackoverflow.com/a/12738719/2711873) is a stack overflow answer by /u/bboe who is (i think) one of the devs working on praw.", "created_utc": 1377278039, "gilded": 0, "name": "t1_cbtugnh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g98a4/praw_looking_for_the_proper_way_to_get_a/"}, {"author": "_Daimon_", "body": "You can use the `place_holder` attribute to only have reddit return results until it meets that submission. import praw r = praw.Reddit('placeholder example by u/_daimon_') subreddit = r.get_subreddit('redditdev') for submission in subreddit.get_new(limit=100, place_holder='1bu7ak'): print submission.title This will return all results younger than the id **on the assumption that a submission with that id can be found**. This is important. If it cannot be found, say because the submission was deleted, then this script will return the last 100 submissions. So you need an additional test to see when you've reached an \"old\" submisison. There are three different ways of doing this. * Keep the id of the of the last 5 submissions you've handled and test if the id matches one of them. * Use the ``created_utc`` value to see when you reach posts as old as your last processed post or older, watch out for submissions at exactly the same time. * All Reddit ids are in base36 and sequential after creation time. So you've reached an already processed post when you encounter a id with a lower or equal base36 value to your last processed post. Hope this helped.", "created_utc": 1370976304, "gilded": 0, "name": "t1_cagruqw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/"}, {"author": "brucemo", "body": "Wouldn't using the base36 ID have a problem if something that was stuck in the spam filter is released into the new queue?", "created_utc": 1371164750, "gilded": 0, "name": "t1_caif9pa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/"}, {"author": "_Daimon_", "body": "Yes. Just like with the other two methods.", "created_utc": 1371167002, "gilded": 0, "name": "t1_caig0xl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/"}, {"author": "peteyMIT", "body": "daimon - i have the same need / question. what do you think the best / most effective test to implement is? and the 100 you've put above is an arbitrary limit, right? it could be anything up to 1000?", "created_utc": 1371079567, "gilded": 0, "name": "t1_cahpfim", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/"}, {"author": "_Daimon_", "body": "The first is the simplest. The base36 method is what I believe to be least likely to accidentally include more results than you wanted. The 100 is an arbitrary limit yes, it can be up anything up to 1000.", "created_utc": 1371168179, "gilded": 0, "name": "t1_caigffc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/"}, {"author": "bboe", "body": "> All Reddit ids are in base36 and sequential after creation time. So you've reached an already processed post when you encounter a id with a lower or equal base36 value to your last processed post. That's not 100% true, but it's an accurate enough assumption [[ref](http://www.reddit.com/r/redditdev/comments/n624n/submission_ids_question/)]. Edit: a -> an", "created_utc": 1370983805, "gilded": 0, "name": "t1_cagv0ck", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g3p8y/getting_only_new_submissions_since_last_request/"}, {"author": "stickytruth", "body": "Edit: Now using this comment and submission in the example Getting the submission of a comment >>> import praw >>> r = praw.Reddit(user_agent=YourUserAgentHere) >>> comment = r.get_info(thing_id='t1_cagd5wo') retrieving: http://www.reddit.com/api/info/.json params: {'id': 't1_cagd5wo'} data: None >>> comment.name u't1_cagd5wo' >>> comment.parent_id u't3_1g308t' >>> submission = comment.submission retrieving: http://www.reddit.com/comments/1g308t.json params: None data: None >>> submission.name u't3_1g308t'", "created_utc": 1370921408, "gilded": 0, "name": "t1_cagd5wo", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/"}, {"author": "stickytruth", "body": "Getting the parent of a comment >>> child = r.get_info(thing_id='t1_cagdb9a') retrieving: http://www.reddit.com/api/info/.json params: {'id': 't1_cagdb9a'} data: None >>> child.parent_id u't1_cagd5wo' >>> parent = r.get_info(thing_id=child.parent_id) retrieving: http://www.reddit.com/api/info/.json params: {'id': u't1_cagd5wo'} data: None >>> parent.name u't1_cagd5wo' >>> parent.is_root retrieving: http://www.reddit.com/comments/1g308t.json params: None data: None True >>> grandparent = r.get_info(thing_id=parent.parent_id) retrieving: http://www.reddit.com/api/info/.json params: {'id': u't3_1g308t'} data: None >>> grandparent", "created_utc": 1370921759, "gilded": 0, "name": "t1_cagdb9a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/"}, {"author": "brucemo", "body": "Mine then becomes: def ParentObj(self, obj): assert type(obj) == praw.objects.Comment if obj.is_root: return obj.submission return self.rh.get_info(thing_id=obj.parent_id) This code works, and addresses the concerns I had. Thank you very much.", "created_utc": 1370922700, "gilded": 0, "name": "t1_cagdp0p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1g308t/in_praw_getting_the_object_that_is_the_parent_of/"}, {"author": "bboe", "body": "You either need to call `replace_more_comments` first (before flattening the tree), or test to ensure the object you're fetching is a `Comment` object and not a `MoreComments` object as the latter does not have a `body` attribute.", "created_utc": 1370815556, "gilded": 0, "name": "t1_cafhe43", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fyz52/problem_viewing_comments_with_praw/"}, {"author": "BittyTang", "body": "Thanks that fixed it!", "created_utc": 1370819699, "gilded": 0, "name": "t1_cafiq11", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fyz52/problem_viewing_comments_with_praw/"}, {"author": "radd_it", "body": "Any bot that provides a function and *only shows up when requested* is a fine bot by me.", "created_utc": 1370791419, "gilded": 0, "name": "t1_caf9t4r", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/"}, {"author": "halfourname", "body": "fun stuff, dude.", "created_utc": 1370822884, "gilded": 0, "name": "t1_cafjpt2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/"}, {"author": "jerenept", "body": "When you get more karma in a particular subreddit your timeout will go down. Just hang in there, and maybe use the message from the praw.RateLimitExceeded exception to wait for the correct amount of time?", "created_utc": 1370834690, "gilded": 0, "name": "t1_cafnqbw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/"}, {"author": null, "body": "That's really cool! It would be good if you could call if from anywhere too. So say it could look through new /r/adviceanimals comments, and appear when someone requests it, like how Jiffybot does.", "created_utc": 1370894520, "gilded": 0, "name": "t1_cag3409", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fyxv7/am_i_an_evil_bot_generates_memes_from_comments/"}, {"author": "shaggorama", "body": "Exception handling. If it gets an error signifying that reddit is down, tell it to back off. Increase the time it backs off with each sequential error.", "created_utc": 1370703741, "gilded": 0, "name": "t1_caeq3wo", "num_comments": null, "score": 9, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fx8ws/how_can_i_keep_a_bot_running_continuously_when/"}, {"author": "im14", "body": "Here's a code that does just that (except the exponential backoff): https://github.com/vindimy/altcointip/blob/master/src/ctb/ctb_misc.py#L53", "created_utc": 1371627368, "gilded": 0, "name": "t1_caltchd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fx8ws/how_can_i_keep_a_bot_running_continuously_when/"}, {"author": "gavin19", "body": "Don't have it run continuously. Just schedule it to run every *x* minutes.", "created_utc": 1370695801, "gilded": 0, "name": "t1_caeokfv", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fx8ws/how_can_i_keep_a_bot_running_continuously_when/"}, {"author": "Medowar", "body": "for example, running on linux, set up a crontab restarting the bot every 24 hours and add a simple echo check, running every 5 mins.", "created_utc": 1370701024, "gilded": 0, "name": "t1_caepi8j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fx8ws/how_can_i_keep_a_bot_running_continuously_when/"}, {"author": "brucemo", "body": "Some more civilized variant of this: import traceback wait = 60 while True: try: time.sleep(wait) wait = 60 except Exception: traceback.print_exc() wait = 300 That \"except\" statement will catch everything including errors in your code. The call into the traceback module will print a stack trace as if you had crashed, but the program will keep going. I haven't tried to figure out what derived class of exception covers all of the exceptions Reddit can raise, but that would solve the problem of your program continuing even if you call methods that don't exist.", "created_utc": 1370861739, "gilded": 0, "name": "t1_caftajv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fx8ws/how_can_i_keep_a_bot_running_continuously_when/"}, {"author": "202halffound", "body": "Huh. I never thought about using a router. I was about to go buy a Raspberry Pi to use as a 24/7 server, but I think I'll try the router thing. If I have any success I'll update this post.", "created_utc": 1370695129, "gilded": 0, "name": "t1_caeogn1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fx1qr/running_a_reddit_bot_on_my_router/"}, {"author": "JimmyRecard", "body": "You won't have much success unless you have a supported router. If you need any help, let me know, I've done my fair share of not bricking my routers with DD-WRT and Tomato.", "created_utc": 1370696761, "gilded": 0, "name": "t1_caeoq4t", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fx1qr/running_a_reddit_bot_on_my_router/"}, {"author": "Azdle", "body": "As long as your libraries are pure python you can just download tar, untar it, cd into the dir and run `python setup.py install` and it will install it to the proper location. Actually, even easier than that you can just copy the folder into the same folder as your source file and python will just use it from there without cluttering up your router's file system. If they aren't pure python (as in they have some code that needs to be compiled first) it gets a lot more complicated. You'll need to get a dev environment setup on your router, but I really don't suggest that, or you'll need to setup a cross compiler to compile it to ARM binaries. (But I doubt you need to do any of that.)", "created_utc": 1370706971, "gilded": 0, "name": "t1_caeqwzo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fx1qr/running_a_reddit_bot_on_my_router/"}, {"author": "JimmyRecard", "body": "Okay, I'm obviously even more behind than I thought. I can't do shit. Anywhere I can learn about running Python scripts in a Linux environment?", "created_utc": 1370791031, "gilded": 0, "name": "t1_caf9pph", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fx1qr/running_a_reddit_bot_on_my_router/"}, {"author": "bboe", "body": "Looks like your dumping and loading of the session should work fine. In the end it only saves you one or two requests to authenticate which isn't a big deal unless you're either intentionally switching between a number of accounts in a single program, or re-executing the program over-and-over again rather than implementing looping in the program itself.", "created_utc": 1370542980, "gilded": 0, "name": "t1_cadhwz1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fsz2m/storing_praw_objects/"}, {"author": "stickytruth", "body": "It's the latter, currently. I'm trying to utilize Upstart, which may or may not be a good idea. I plan on moving the loop into the program once I figure some other stuff out.", "created_utc": 1370543602, "gilded": 0, "name": "t1_cadi63m", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fsz2m/storing_praw_objects/"}, {"author": "bboe", "body": "Just FYI -- if you're using the multiprocessing handler, the authentication will be cached, so essentially you'll only have to re-authenticate (with reddit's servers) once every 30 seconds. But saving the session is obviously more efficient ;) Edit: Spelling", "created_utc": 1370544083, "gilded": 0, "name": "t1_cadid4c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fsz2m/storing_praw_objects/"}, {"author": "stickytruth", "body": "That's interesting to know, thanks :) Do you know of any open source projects using the multiprocess handler?", "created_utc": 1370545145, "gilded": 0, "name": "t1_cadisss", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fsz2m/storing_praw_objects/"}, {"author": "bboe", "body": "Nope. I think this is the first question I've seen that references it.", "created_utc": 1370546005, "gilded": 0, "name": "t1_cadj4nt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fsz2m/storing_praw_objects/"}, {"author": "bboe", "body": "PRAW's test suite is usually a good place to look for how to use something: https://github.com/praw-dev/praw/blob/master/praw/tests/__init__.py#L1239", "created_utc": 1370482842, "gilded": 0, "name": "t1_cad1uqp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "brucemo", "body": "Okay, I believe that I understand it fully. Our CSS contains this line: body.res:lang(np) > div When I get that line from the API, it comes back as: body.res:lang(np) > div This is obviously a syntax error. The data that is retrieved has \"\\n\" in it and I was already converting this to ascii linefeed, so the data is already not in sufficient state to be sent back, so I'm not munging it sufficiently.", "created_utc": 1370500473, "gilded": 0, "name": "t1_cad7cmf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "brucemo", "body": "In case anyone comes along and sees this, here is a useful link: https://github.com/praw-dev/praw/issues/186", "created_utc": 1370501281, "gilded": 0, "name": "t1_cad7iq7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "bboe", "body": "The '\\n' is not a problem -- that's how you represent a newline in a string. It's the '&gt' which is an issue because reddit returns the results that way. You either have to replace those with their actual values yourself, or enable PRAW's decoding of HTML entities. Edit: `r.config.decode_html_entities = True` should be how you enable the decoding of entities. That should allow you to call `sub.set_stylesheet(sub.get_stylesheet()['stylesheet'])`.", "created_utc": 1370501158, "gilded": 0, "name": "t1_cad7hum", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "brucemo", "body": "Yes, thank you. I meant that the string has escape sequences in it, so I already have to do something to it if I want to print it, etc. That config option works like a champ, and I think you for your time.", "created_utc": 1370502020, "gilded": 0, "name": "t1_cad7nu5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "bboe", "body": "> I meant that the string has escape sequences in it, so I already have to do something to it if I want to print it, etc. Any string you give to `print`, or the underlying `write` function will convert '\\n' into a newline in the output (this is handled by the operating system), so I don't really see what you mean. The '\\n' is only displayed if you output the `repr`, or representation, of the string.", "created_utc": 1370543171, "gilded": 0, "name": "t1_cadhzqd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "brucemo", "body": "You are right. I was mistaken about what was going on in the data. I thought that you had `r\"\\n\"` embedded in some strings and `\"\\n\"` in others, but I was wrong.", "created_utc": 1371980601, "gilded": 0, "name": "t1_caoibr9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "brucemo", "body": "Thank you for answering. I had seen that code and didn't know what to make of it. The problem as I understand it now is that /r/Christianity's stylesheet is invalid. If I go look at it, and press the \"save\" button, it updates properly, but if I try to do it via the API, it returns the indicated error. I don't know if tracking this is of interest to you, but I'll save a copy of our stylesheet before I dick around with it to see what aspect of it is causing the problem, if you want it.", "created_utc": 1370499349, "gilded": 0, "name": "t1_cad73qx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "bboe", "body": "Is there a , or & in the CSS? If so then those are probably causing the problem as reddit encodes those characters. One of the PRAW settings enables decoding of all html entities (I don't remember what it's called) but enabling that may fix your problem.", "created_utc": 1370500125, "gilded": 0, "name": "t1_cad79x1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "brucemo", "body": "Yes, please pardon me, I edited my thing to remove the reference to a bug on your end, since there is a \">\" in the CSS and I'm already having to munge that stuff. What comes back to me is stuff like: \"some stuff \\n some other stuff\" ... meaning that there is a \"\\n\" in the code, which is already obviously wrong, so I have to do something to fix that anyway.", "created_utc": 1370500873, "gilded": 0, "name": "t1_cad7fo8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fplrx/how_do_you_use_praws_set_stylesheet/"}, {"author": "bboe", "body": "Maybe PythonAnywhere has a limit to the number of sockets you can open. In that case, perhaps you need to limit the size of the connection pool (part of requests). That's my only speculation.", "created_utc": 1370214557, "gilded": 0, "name": "t1_caawbjq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "stickytruth", "body": "Is that something I can do in my application, or would I need to edit PRAW directly?", "created_utc": 1370215326, "gilded": 0, "name": "t1_caawjoy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "bboe", "body": "I haven't looked directly into it. Nevertheless, you _should_ be able to do it through your application by either changing a class attribute on the request connection pool class (whatever it's called), or by altering the request session object through the `r` attribute. You'll have to figure out exactly what works for you (assuming this is actually the problem).", "created_utc": 1370215630, "gilded": 0, "name": "t1_caawmvt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "stickytruth", "body": "Hmm.. That's a bit over my head. Any suggestion on where I might go to learn about this? Thanks", "created_utc": 1370216376, "gilded": 0, "name": "t1_caawur2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "bboe", "body": "Maybe ask in /r/learnpython, or on stackoverflow with a #requests and #python tag. You might verify that you can reproduce the issue just by using requests and making multiple connections.", "created_utc": 1370218189, "gilded": 0, "name": "t1_caaxds3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "stickytruth", "body": "I think I've stumbled on a potential cause. Using just requests.get(url) the logging shows: INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): proxy.server But PRAW's connections show: INFO:requests.packages.urllib3.connectionpool:Starting new HTTP connection (1): www.reddit.com I edited the selftext with some more information. How can I have PRAW use PythonAnywhere's proxy? AFAIK, there's no username or password for it.", "created_utc": 1370228015, "gilded": 0, "name": "t1_cab0lvb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "bboe", "body": "Just follow the proxy directions and omit the `username:password` part.", "created_utc": 1370229448, "gilded": 0, "name": "t1_cab13hu", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "stickytruth", "body": "I'll try reproducing with Requests. The script I'm trying to use PRAW with fails at the first request, though. Every time.", "created_utc": 1370218531, "gilded": 0, "name": "t1_caaxhh5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fjl11/praw_exception_connection_refused/"}, {"author": "Buffer_Underflow", "body": "try changing submission.comments_flat to submission.comments", "created_utc": 1370127794, "gilded": 0, "name": "t1_caabpvh", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fhjbt/praw_error_when_using_flat_comments/"}, {"author": "bboe", "body": "~~Where do you see a suggestion to use `comments_flat`?~~ Nevermind, I see that the [page](https://praw.readthedocs.org/en/latest/pages/comment_parsing.html) is out of date (at least the \"complete\" example at the end). ~~Updating...~~ It's fixed now. Edit: /u/Buffer_Underflow has the correct answer.", "created_utc": 1370210833, "gilded": 0, "name": "t1_caav7io", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fhjbt/praw_error_when_using_flat_comments/"}, {"author": "radd_it", "body": "Why would you search the web for something when you already know where the docs are? Using the wiki for reddit-side state-storage is your best option-- it's how the new version of /u/AutoModerator keeps its settings per subreddit.", "created_utc": 1370010409, "gilded": 0, "name": "t1_ca9hgkg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fe7pj/does_praw_support_wiki_stuff/"}, {"author": "brucemo", "body": "Because a text search on the API page for wiki returns no relevant hits, and typing \"wiki\" in the search bar there returns nothing at all. Deimorz has also not made his new code public yet, so my question remains unanswered.", "created_utc": 1370022430, "gilded": 0, "name": "t1_ca9ll1r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fe7pj/does_praw_support_wiki_stuff/"}, {"author": "bboe", "body": "The index is usually a pretty good place to look: https://praw.readthedocs.org/en/latest/genindex.html Interesting that readthedocs doesn't do matching within function names. Edit: And the answer is yes.", "created_utc": 1370045055, "gilded": 0, "name": "t1_ca9tdmg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fe7pj/does_praw_support_wiki_stuff/"}, {"author": "brucemo", "body": "This is exactly what I was looking for. Thank you.", "created_utc": 1370227938, "gilded": 0, "name": "t1_cab0kx0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fe7pj/does_praw_support_wiki_stuff/"}, {"author": "Pathogen-David", "body": "In my experience, mark_as_read can malfunction if Reddit is being slow or is in read-only mode. /u/Searchbar_Trixie would sometimes spam people continuously if Reddit was having issues marking things as read.", "created_utc": 1369946873, "gilded": 0, "name": "t1_ca91fyg", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "Thanks, that's what I suspected. +alttip 4 namecoins", "created_utc": 1370028980, "gilded": 0, "name": "t1_ca9o1z6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "Pathogen-David", "body": "Another thing to note, is that you do not want to use timestamps to validate whether you've seen a message/post/whatever or not. Spam filter and similar Reddit oddities can make posts show up out of order. The only workaround is to keep a list of processed messages somewhere.", "created_utc": 1370054633, "gilded": 0, "name": "t1_ca9w547", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "Well, when I scrape comments from Reddit, I stop when I reach a timestamp smaller or equal to one I've seen on last run. But for actual tips I store message ID as well as timestamp and username. Do you think I should be identifying messages with just message ID and username?", "created_utc": 1370056975, "gilded": 0, "name": "t1_ca9wsl3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "Pathogen-David", "body": "Eh, I would not bother anything beyond the message ID. Its never going to change. Storing the timestamp and username might be good for diagnostics / tracking purposes though. Since your bot processes /r/all/comments, it might not be a bad idea to process by timestamp since storing all those ids would generate a lot of data really fast. (If I didn't answer your question, let me know. I might be reading you wrong there.)", "created_utc": 1370058580, "gilded": 0, "name": "t1_ca9x8qt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "No, you got the right idea. For parsing comments, I know I reached old one when I encounter old timestamp. My concern is, should I be using username-messageID-timestamp as the unique key in identifying an action? Or should I be worried that timestamp can change and/or username can go away (user deleted, comment deleted). I don't want to end up having duplicate actions because timestamp changed and bot thinks there's been no such action before.", "created_utc": 1370063272, "gilded": 0, "name": "t1_ca9yfm5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "Pathogen-David", "body": "The thing ID is eternal, its all you need to identify the unique comment/subreddit/user/message/whatever. For instance, [your comment](http://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/ca9yfm5) is comment number ca9yfm5 in submission number 1fcolg. There will never be another submission on Reddit with ID 1fcolg. Likewise, there will never be another comment with ID ca9yfm5. Even if this entire thread is removed and our users are banned. The only exception to this is when database corruption occurs, which [has happened before](http://blog.reddit.com/2011/03/why-reddit-was-down-for-6-of-last-24.html). However, you'll still never have the same ID twice at the same time from the same Reddit server. Worst case is you would miss a message entirely, which would not be too bad of a failure case when the entire website is tearing at the seams. Like you said, users and comments can be deleted. (Although a comment removed by a moderator will still be visible outside the subreddit it was posted in unless an admin removes it.) I can't think of a reason why timestamps would change, but I also can't think of a reason to add it to your unique key. **TL;DR**: The message ID offers all the uniqueness you need.", "created_utc": 1370066700, "gilded": 0, "name": "t1_ca9z6zm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "Thanks, your feedback is very helpful!", "created_utc": 1370131543, "gilded": 0, "name": "t1_caacrg2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "Pathogen-David", "body": "No problem, best of luck on the bot!", "created_utc": 1370143479, "gilded": 0, "name": "t1_caag8d0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "ALTcointip", "body": "^(__[Verified]__:) ^/u/im14 ^-> ^/u/Pathogen-David, __^4 ^Namecoin(s)__ ^__($2.691)__ ^[[help]](http://www.reddit.com/r/ALTcointip/wiki/index)", "created_utc": 1370029035, "gilded": 0, "name": "t1_ca9o2qd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": null, "body": "This submission has been linked to in 1 subreddit (at the time of comment generation): * /r/ALTcointip: [ALTcointip bot hit a likely bug in Reddit API which caused it to send out multiple duplicate messages to some users (no duplicate transactions occurred). I've put a fix in place by storing inbox messages like +info, +register, etc. in database as well. Let me know if you see any other issues.](/r/ALTcointip/comments/1fcp8t/altcointip_bot_hit_a_likely_bug_in_reddit_api/) ---- This comment was posted by a bot, see /r/Meta_Bot for more info.", "created_utc": 1369939328, "gilded": 0, "name": "t1_ca8yirn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "Here's relevant code: messages = reddit.get_unread(limit=99999) for m in messages: if m.was_comment: m.mark_as_read() continue if bool(m.author) and m.author.name.lower() == _config['reddit']['user'].lower(): m.mark_as_read() continue action = eval_message(m, self) if action != None: action.do() m.mark_as_read()", "created_utc": 1369942412, "gilded": 0, "name": "t1_ca8zppj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "bboe", "body": "I cannot reproduce your issue. What is the return value from `m.mark_as_read()`? It should just be `{}` but if it's anything else it could mean it failed. Edit: My guess is that your logic to retrieve the \"new\" set of unread messages resulted in a cached request and pulled the set of messages you've already processed. You should explicitly verify you don't process message duplicates.", "created_utc": 1369944253, "gilded": 0, "name": "t1_ca90fi1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "Here's example of inconsistent behavior. As you can see, after a few runs of my check_inbox() messages finally become \"read\" but the number of runs needed seems to be going up. INFO 2013-05-30 16:01:45,436 _check_inbox(): executed action register from message_id vnqkx INFO 2013-05-30 16:07:12,845 _check_inbox(): executed action register from message_id vnqzk INFO 2013-05-30 16:07:51,101 _check_inbox(): executed action register from message_id vnqzk INFO 2013-05-30 16:12:31,167 _check_inbox(): executed action info from message_id vnrfn INFO 2013-05-30 16:12:49,628 _check_inbox(): executed action info from message_id vnrfn INFO 2013-05-30 16:13:34,994 _check_inbox(): executed action info from message_id vnrfn INFO 2013-05-30 16:41:44,199 _check_inbox(): executed action accept from message_id vntkc INFO 2013-05-30 16:42:21,793 _check_inbox(): executed action accept from message_id vntkc INFO 2013-05-30 16:42:59,820 _check_inbox(): executed action accept from message_id vntkc INFO 2013-05-30 16:43:39,016 _check_inbox(): executed action accept from message_id vntkc INFO 2013-05-30 17:01:17,356 _check_inbox(): executed action info from message_id vnv3w INFO 2013-05-30 17:01:51,772 _check_inbox(): executed action info from message_id vnv3w INFO 2013-05-30 17:02:23,750 _check_inbox(): executed action info from message_id vnv3w INFO 2013-05-30 17:02:42,477 _check_inbox(): executed action info from message_id vnv3w INFO 2013-05-30 17:03:17,475 _check_inbox(): executed action info from message_id vnv3w", "created_utc": 1369945189, "gilded": 0, "name": "t1_ca90sio", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "bboe", "body": "Try sleeping 31 seconds before calling `get_unread` and see if that helps. If so then it's a caching problem.", "created_utc": 1369945508, "gilded": 0, "name": "t1_ca90x14", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "Unfortunately I can't afford the luxury of sleeping that long (my current sleep time is 15 sec) because I have to scrape comments from /r/all in the same loop. If I sleep too long, I won't be able to scrape 'em all - reddit.subreddits.get_comments(limit=99999) seems to be giving me at most ~700-800 comments per call; if I sleep for 30 seconds and meanwhile 1,000 comments get posted, I'll miss ~200-300 of them. One solution would be to run check_inbox() and check_comments() in separate processes, but then I'll be messing with API calls/second limit that's handled by PRAW. So what I can do is have two different bot accounts, one to check inbox, and one to scrape comments, running from two different hosts.", "created_utc": 1369945762, "gilded": 0, "name": "t1_ca910ic", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "bboe", "body": "Your example code doesn't have `get_comments` in it at all. I just mean between requests to `get_unread`. Surely you don't get 1000+ messages in a 30-second period. Do what you have to do to make sure `get_unread` doesn't occur within any 31 second window.", "created_utc": 1369945875, "gilded": 0, "name": "t1_ca91221", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "Right, I didn't include the comment scraping function in example, but it runs right after check_inbox() function. Thanks for the advice, though, I'll limit how often check_inbox() runs.", "created_utc": 1369946071, "gilded": 0, "name": "t1_ca914qs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "bboe", "body": "I meant that you should try it to see if it helps. If so that likely means it's just a caching issue. You can bypass the caching by other means if that's indeed what is causing the issue. Thanks for the \"tip\".", "created_utc": 1369946511, "gilded": 0, "name": "t1_ca91avp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "Thanks for quality advice! +alttip 5 namecoins", "created_utc": 1369946149, "gilded": 0, "name": "t1_ca915sk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "ALTcointip", "body": "^__[Verified]__: ^/u/im14 ^-> ^/u/bboe, __^5 ^Namecoin(s)__ ^__($3.748)__ ^[[help]](http://www.reddit.com/r/ALTcointip/wiki/index)", "created_utc": 1369946179, "gilded": 0, "name": "t1_ca91677", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "bboe", "body": "Also -- you might be interested in using [comment_stream](https://praw.readthedocs.org/en/latest/pages/code_overview.html#praw.helpers.comment_stream).", "created_utc": 1369945936, "gilded": 0, "name": "t1_ca912vn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "im14", "body": "At this point I don't check the return value because I rely on PRAW to throw an exception if anything goes wrong (which have worked well up to this point). Since no exception is thrown, I assume Reddit has reported that message has been marked as read successfully. My fix now is to store every incoming message in database and check for duplicates, which solves my problem, but it's still not ideal. The odd thing is that after a few runs, the \"unread\" message will finally become \"read\", so the inconsistency in behavior is unsettling.", "created_utc": 1369945064, "gilded": 0, "name": "t1_ca90qq6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1fcolg/mmark_as_read_is_broken_since_this_morning/"}, {"author": "bboe", "body": "PRAW automatically handles the normal delay required between API requests. The RateLimitExceeded exception occurs when you try to do something too frequently such as post comments, or post submissions to subreddits that you are not an approved submitter. I contemplated automatically handling such errors in PRAW, however, I thought it would be odd to sleep upwards of 4 minutes thus it's up to you to properly handle it. Here's an example you can follow to automatically handle RateLimitExceeded exceptions yourself: https://gist.github.com/bboe/1860715", "created_utc": 1369841187, "gilded": 0, "name": "t1_ca85r60", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/"}, {"author": "jdp407", "body": "Thanks!", "created_utc": 1369841437, "gilded": 0, "name": "t1_ca85u90", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/"}, {"author": "shaggorama", "body": "bboe is correct as always, but he's not telling you the whole story: your problem isn't the bot, it's the bot's reddit account. If you were using the bot's account through the website, you'd probably be seeing \"you're doing that too often\" messages. I recommend using the bot account as your main account for a few days to accumulate karma. Once reddit sees that your bot's account is a reasonably well behaved account, you shouldn't see those errors anymore and praw will handle the minimal rate limiting your bot will require.", "created_utc": 1369860933, "gilded": 0, "name": "t1_ca8d3g1", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/"}, {"author": "bboe", "body": "I may be wrong, but I believe even with an account that has decent karma, you can still hit \"you're doing that too often\" messages when posting to / commenting on new (to your account) subreddits.", "created_utc": 1369861249, "gilded": 0, "name": "t1_ca8d7sz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f9t5s/when_is_prawerrorsratelimitexceeded_thrown/"}, {"author": "slyf", "body": "It looks like you need http://", "created_utc": 1369781483, "gilded": 0, "name": "t1_ca7r41i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f6xw4/not_able_to_configure_proxy_settings_in_prawini/"}, {"author": "radd_it", "body": "There's a way to do a reddit search by UTC time (seconds since whenever). I'd suggest \"randomly\" picking a time and then trying to find your \"random\" post that way. Roll a random time, grab the top 25 posts, and then see which of those first matches your criteria. No idea how you'd do that in PRAW though.", "created_utc": 1369715084, "gilded": 0, "name": "t1_ca79s5i", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f61mp/is_it_possible_to_roll_random_threads_with_given/"}, {"author": "jonas747", "body": "Thing id's are base 36 encoded, so you could find a thing id from 7 days ago, decode it into base 10 and then get a thing_id from a pretty recent thread, decode and pick a random number between those two, then encode it in base 36 again and you can get a thread from that id.", "created_utc": 1369725128, "gilded": 0, "name": "t1_ca7c3l0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f61mp/is_it_possible_to_roll_random_threads_with_given/"}, {"author": "I_SLEEP_NORMALLY", "body": "Somewhat related: reddit.com/random will pull up a recent random thread. The dev blog did not define recency here, but they all appear to be less than 24 hours old.", "created_utc": 1369795497, "gilded": 0, "name": "t1_ca7vv2t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f61mp/is_it_possible_to_roll_random_threads_with_given/"}, {"author": "bboe", "body": "https://praw.readthedocs.org/en/latest/pages/configuration_files.html Edit: Just providing the link so other people can reference it.", "created_utc": 1369704125, "gilded": 0, "name": "t1_ca765o0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "emilvikstrom", "body": "\"/home/foobar\" is meant to be your own home directory. \"foobar\" is just a placeholder phrase, like \"example\". If your username is \"whodunit28\" you should look in \"/home/whodunit28/.config/\". You can create the praw.ini file yourself. Praw should have a default praw.ini somewhere that you can copy.", "created_utc": 1369650204, "gilded": 0, "name": "t1_ca6s6r6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "lawenforcerbot", "body": "Thanks. Actually I was able to locate praw.ini in \"/usr/local/lib/python2.7/dist-packages/praw\" and added >[reddit.com] >http_proxy:user:password@host:port to the file replacing username, pwd, etc. But it still isn't working. I am getting this error: >e 327, in send > raise ConnectionError(e) >requests.exceptions.ConnectionError: >HTTPConnectionPool(host='www.reddit.com', port=80): Max retries exceeded with >url: /r/opensource/.json?limit=5 (Caused by : [Errno 111] >Connection refused) What I am doing wrong? Edit: OP here. Sorry, I posted with my another account.", "created_utc": 1369656231, "gilded": 0, "name": "t1_ca6swpg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "aperson", "body": "If you created one and lost it (or whatever), use `find`. `find` is your friend, learning it will serve you well. find /home -iname praw.ini", "created_utc": 1369651425, "gilded": 0, "name": "t1_ca6sbse", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1f4mtc/where_do_i_find_my_prawini_in_ubuntu_1204/"}, {"author": "slyf", "body": "Whats the error?", "created_utc": 1369441156, "gilded": 0, "name": "t1_ca5ha7z", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": "Morphiac", "body": "EDIT: I posted it on the wrong account, but this is the error message I get. Traceback (most recent call last): File \"C:\\Python33\\Scripts\\MeeshuBot.py\", line 3, in r = praw.Reddit(user_agent = 'An automated reddit project - Meeshu') File \"C:\\Python33\\lib\\site-packages\\praw-2.1.3-py3.3.egg\\praw\\__init__.py\", line 993, in __init__ super(AuthenticatedReddit, self).__init__(*args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw-2.1.3-py3.3.egg\\praw\\__init__.py\", line 495, in __init__ super(OAuth2Reddit, self).__init__(*args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw-2.1.3-py3.3.egg\\praw\\__init__.py\", line 611, in __init__ super(UnauthenticatedReddit, self).__init__(*args, **kwargs) File \"C:\\Python33\\lib\\site-packages\\praw-2.1.3-py3.3.egg\\praw\\__init__.py\", line 286, in __init__ update_check(__name__, __version__) File \"C:\\Python33\\lib\\site-packages\\update_checker-0.5-py3.3.egg\\update_checker.py\", line 146, in update_check result = checker.check(package_name, package_version, **extra_data) File \"C:\\Python33\\lib\\site-packages\\update_checker-0.5-py3.3.egg\\update_checker.py\", line 54, in wrapped retval = function(obj, package_name, package_version, **extra_data) File \"C:\\Python33\\lib\\site-packages\\update_checker-0.5-py3.3.egg\\update_checker.py\", line 105, in check headers=headers) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\api.py\", line 99, in put return request('put', url, data=data, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\api.py\", line 44, in request return session.request(method=method, url=url, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\sessions.py\", line 335, in request resp = self.send(prep, **send_kwargs) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\sessions.py\", line 438, in send r = adapter.send(request, **kwargs) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\adapters.py\", line 292, in send timeout=timeout File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\packages\\urllib3\\connectionpool.py\", line 423, in urlopen conn = self._get_conn(timeout=pool_timeout) File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\packages\\urllib3\\connectionpool.py\", line 238, in _get_conn return conn or self._new_conn() File \"C:\\Python33\\lib\\site-packages\\requests-1.2.2-py3.3.egg\\requests\\packages\\urllib3\\connectionpool.py\", line 205, in _new_conn strict=self.strict) File \"C:\\Python33\\lib\\http\\client.py\", line 737, in __init__ DeprecationWarning, 2) File \"C:\\Python33\\lib\\idlelib\\PyShell.py\", line 59, in idle_showwarning file.write(warnings.formatwarning(message, category, filename, AttributeError: 'NoneType' object has no attribute 'write'", "created_utc": 1369464164, "gilded": 0, "name": "t1_ca5n8tr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": null, "body": "It looks like you're running your script in IDLE; try running it outside IDLE and if that works my guess is it's doing something PRAW doesn't like. I know IDLE (especially on Windows) has had issues in the past from doing things like setting stderr to None.", "created_utc": 1369497615, "gilded": 0, "name": "t1_ca5sfwc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": "Morphiac", "body": "Thanks, it worked. Mind if I ask you how you could tell I was running it in idle based off that error message? I'm not very good a reading python error messages.", "created_utc": 1369501447, "gilded": 0, "name": "t1_ca5tfwi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": null, "body": "File \"C:\\Python33\\lib\\idlelib\\PyShell.py\", line 59, in idle_showwarning file.write(warnings.formatwarning(message, category, filename,", "created_utc": 1369503394, "gilded": 0, "name": "t1_ca5tz4j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": "bboe", "body": "Notice the \"idlelib\" in the traceback.", "created_utc": 1369503191, "gilded": 0, "name": "t1_ca5tx1z", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ezssi/error_with_rlogin/"}, {"author": "bboe", "body": "`requests.exceptions.HTTPError` is what you want. Here's an example: https://github.com/praw-dev/praw/blob/master/praw/helpers.py#L25", "created_utc": 1369357057, "gilded": 0, "name": "t1_ca4u1so", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "im14", "body": "After some trial and error I think I have got all the cases handled. Here's the code for anyone looking in the future. import logging from requests.exceptions import HTTPError from praw.errors import ExceptionList, APIException, InvalidCaptcha, InvalidUser, RateLimitExceeded from socket import timeout def _reddit_reply(msg, txt): \"\"\" Reply to a comment/message on Reddit Retry if Reddit is down \"\"\" lg = logging.getLogger() while True: try: msg.reply(txt) break except APIException as e: lg.warning(\"_reddit_reply(): failed (%s)\", str(e)) return False except ExceptionList as el: for e in el: lg.warning(\"_reddit_reply(): failed (%s)\", str(e)) return False except (HTTPError, RateLimitExceeded) as e: if str(e) == \"403 Client Error: Forbidden\": lg.warning(\"_reddit_reply(): banned to reply to %s\", msg.permalink) return False lg.warning(\"_reddit_reply(): Reddit is down (%s), sleeping...\", str(e)) time.sleep(30) pass except timeout: lg.warning(\"_reddit_reply(): Reddit is down (timeout), sleeping...\") time.sleep(30) pass except Exception as e: raise", "created_utc": 1369709217, "gilded": 0, "name": "t1_ca77vww", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "rreyv", "body": "Really helpful mate. Does this catch 503 as well?", "created_utc": 1378090355, "gilded": 0, "name": "t1_cbzzb5f", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "im14", "body": "Check out this function, the code has been working pretty well for me, catching any Reddit downtime errors while still breaking on other errors: https://github.com/vindimy/altcointip/blob/master/src/ctb/ctb_user.py#L167 def tell(self, subj=None, msg=None): \"\"\" Send a Reddit message to user \"\"\" lg.debug(\"> CtbUser::tell(%s)\", self._NAME) if not bool(subj) or not bool(msg): raise Exception(\"CtbUser::tell(%s): subj or msg not set\", self._NAME) if not self.is_on_reddit(): raise Exception(\"CtbUser::tell(%s): not a Reddit user\", self._NAME) while True: # This loop retries sending message if Reddit is down try: lg.debug(\"CtbUser::tell(%s): sending message\", self._NAME) self._REDDITOBJ.send_message(subj, msg) break except APIException as e: lg.warning(\"CtbUser::tell(%s): failed (%s)\", self._NAME, str(e)) return False except ExceptionList as el: for e in el: lg.warning(\"CtbUser::tell(%s): failed (%s)\", self._NAME, str(e)) return False except (HTTPError, RateLimitExceeded) as e: lg.warning(\"CtbUser::tell(%s): Reddit is down (%s), sleeping...\", self._NAME, str(e)) time.sleep(self._CTB._DEFAULT_SLEEP_TIME) pass except timeout: lg.warning(\"CtbUser::tell(%s): Reddit is down (timeout), sleeping...\", self._NAME) time.sleep(self._CTB._DEFAULT_SLEEP_TIME) pass except Exception, e: raise", "created_utc": 1378230379, "gilded": 0, "name": "t1_cc0zed9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "im14", "body": "Thanks! I've modified the bot, so hopefully next time Reddit is down it'll work. +alttip 10 namecoins", "created_utc": 1369362742, "gilded": 0, "name": "t1_ca4vqph", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "bboe", "body": "Woah ~$8.7 for that answer. Thanks!", "created_utc": 1369378272, "gilded": 0, "name": "t1_ca502st", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "ALTcointip", "body": "^__[Verified]__: ^/u/im14 ^-> ^/u/bboe, __^10 ^Namecoin(s)__ ^__($8.7)__ ^[[help]](http://www.reddit.com/r/ALTcointip/wiki/index)", "created_utc": 1369376643, "gilded": 0, "name": "t1_ca4zr88", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "im14", "body": "I can see that AutoModerator bot is also trying to catch urllib2.HTTPError: https://github.com/Deimos/AutoModerator/blob/master/modbot.py#L606", "created_utc": 1369356781, "gilded": 0, "name": "t1_ca4tyl7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1exvqb/after_weeks_of_trial_and_error_i_still_cant_catch/"}, {"author": "jerenept", "body": "It seems like, uh, the SSL module is missing. Or something. Try this StackOverflow question http://stackoverflow.com/questions/10197948/python-requests-library-timing-out-under-linux Try the things in the question out and see the output, eg. `import ssl; ssl.get_server_certificate(('google.com',443)) `", "created_utc": 1369313245, "gilded": 0, "name": "t1_ca4ete4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ew2gk/rlogin_failing/"}, {"author": "RampagingKoala", "body": "I fixed it. Turns out python just needed a reboot to start working again. Sometimes python makes me so mad.", "created_utc": 1369324078, "gilded": 0, "name": "t1_ca4i5kj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ew2gk/rlogin_failing/"}, {"author": "jerenept", "body": "haha, well, that's always the first thing to try. Glad you got it to work!", "created_utc": 1369332181, "gilded": 0, "name": "t1_ca4lao0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ew2gk/rlogin_failing/"}, {"author": "bboe", "body": "My speculation is you have \"bot\" in your user-agent string. Remove that and you'll be good to go. That's the only reason I've seen that the login succeeds and later you get errors about not being logged in.", "created_utc": 1369270378, "gilded": 0, "name": "t1_ca4527u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ev2ex/unexplained_login_errors_using_praw/"}, {"author": "feblehober123", "body": "I changed the user agent and it works fine now. Thanks.", "created_utc": 1369336500, "gilded": 0, "name": "t1_ca4mzk2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ev2ex/unexplained_login_errors_using_praw/"}, {"author": "slyf", "body": "We don't and never have used HTML for the sidebar..not sure what the person who wrote that bot was thinking.", "created_utc": 1369169772, "gilded": 0, "name": "t1_ca39qo6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ese01/seeking_a_very_simple_countdownvalue_decrement/"}, {"author": "alphanovember", "body": "Yeah, must have been an oversight. Good news is that he updated it for MD now, so it works.", "created_utc": 1369178067, "gilded": 0, "name": "t1_ca3ctom", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ese01/seeking_a_very_simple_countdownvalue_decrement/"}, {"author": "Falmarri", "body": ">I just need someone to write the script for me. That is not how you ask for help. No one is going to write code for you. We are not your personal developers.", "created_utc": 1369181913, "gilded": 0, "name": "t1_ca3e5kl", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1ese01/seeking_a_very_simple_countdownvalue_decrement/"}, {"author": "bboe", "body": "It might be possible through search... how are you doing it?", "created_utc": 1369158338, "gilded": 0, "name": "t1_ca353ix", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1erqio/get_submissions_by_date_and_subreddit_with_praw/"}, {"author": "bboe", "body": "It looks like it was a python 3.3.2 issue with requests [[ref](https://github.com/kennethreitz/requests/pull/1370)]. Update requests to version 1.2.1 and you should be go to go.", "created_utc": 1369089783, "gilded": 0, "name": "t1_ca2muf3", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/"}, {"author": "Biggerontheinside", "body": "Brilliant! Problem solved! Thanks!", "created_utc": 1369144005, "gilded": 0, "name": "t1_ca304ye", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/"}, {"author": "Prcrstntr", "body": "Thanks! It seems to work now!", "created_utc": 1369091150, "gilded": 0, "name": "t1_ca2nccp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/"}, {"author": "bboe", "body": "I am at a loss too as the error is occurring in the python standard library. Did you use `pip` or `easy_install` to install PRAW? If not, you may have installed the wrong version of packages. Step one would be to make sure the environment is setup correctly. If that doesn't work, you may want to see if you can replicate the issue using requests directly and if so file a bug there: https://github.com/kennethreitz/requests/issues?state=open", "created_utc": 1369076309, "gilded": 0, "name": "t1_ca2huwz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/"}, {"author": "Prcrstntr", "body": "I am having this same problem http://www.reddit.com/r/learnpython/comments/1epc9y/praw_i_thought_i_installed_it_with_pip_but/", "created_utc": 1369084145, "gilded": 0, "name": "t1_ca2ksqb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1epfrj/cant_sort_this_one_out_trying_to_log_in_using/"}, {"author": "jerenept", "body": "Is the account fairly new, little karma? Reddit will limit you until you get some link karma, in order to reduce spamming. Get some karma on your bot account, or maybe use an older/higher karma account, that should help.", "created_utc": 1369000172, "gilded": 0, "name": "t1_ca1y4dp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1enepj/ratelimitexceeded_problem/"}, {"author": "Buffer_Underflow", "body": "Yeah it's new. It was just kinda a testbot to make sure I'm using the API correctly. Thanks a lot, I'll try and get some karma.", "created_utc": 1369001382, "gilded": 0, "name": "t1_ca1yino", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1enepj/ratelimitexceeded_problem/"}, {"author": "shaggorama", "body": "I recommend building a test subreddit for your bot and playing around there. You'll be able to control what your bot is doing and who and how it's responding better, and all the while slowly build up its karma to a point where it can provide a more general service.", "created_utc": 1369067920, "gilded": 0, "name": "t1_ca2evlw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1enepj/ratelimitexceeded_problem/"}, {"author": "djimbob", "body": "Let's simplify things. First can you get praw to work? Go to the interactive python (python.exe) terminal and try >>> import praw >>> r = praw.Reddit(user_agent = 'praw test - undergroundmonorail') >>> r.login() Username: undergroundmonorail Password for undergroundmonorail: >>> r.send_message('undergroundmonorail', 'hello', 'body') {u'errors': []} If you get that far it works (should send a message to undergroundmonorail with hello body -- you have to check your inbox to see). Otherwise, something is going wrong, likely with PyScripter (your IDE) implementing stdin wrong which interacts poorly with praw. Reading through the traceback, it does not seem to be a reddit, praw, or python issue, but a \"PyScripter\" issue, where pyscripter (replaced `sys.stdin` with a `DebugOutput` instance) that doesn't have an attribute `closed` that is there in the normal [sys.stdin](http://docs.python.org/2/library/sys.html#sys.stdin) (a [file object](http://docs.python.org/2/library/stdtypes.html#file-objects) that takes standard input from the terminal and should have a closed attribute). So when praw checks that the stdin hasn't been closed yet, it raises an exception, as the DebugOutput overwriting the normal sys.stdin implemented by your version of PyScripter doesn't have it. This possibly is due to running an old version of pyscripter; so if you upgrade it could work. I'm a linuxer, so don't know about windows IDEs (personally, I like `ipython` and a standard text editor (run it with `%run file.py`) and pycharm for large projects, but here's a list of windows [IDEs](http://stackoverflow.com/questions/126753/is-there-a-good-free-python-ide-for-windows).)", "created_utc": 1368743710, "gilded": 0, "name": "t1_ca07qp0", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "undergroundmonorail", "body": "Running it on the command line works. I guess it's a PyScripter problem, then. I could get another editor but in all honesty I'm not sure it's worth it when I do the vast majority of my python programming on my Linux box anyway. Thank you!", "created_utc": 1368744179, "gilded": 0, "name": "t1_ca07wj9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "bboe", "body": "Are you doing something funny with sys.stdin? PRAW assumes sys.stdin is a file stream, I don't know where this `DebugOutput` object came from, but I suppose PRAW should make sure sys.stdin is of the expected type. Can you provide me insight on how you are running your code so I can best handle the it? It seems like even without this bug, you'd still get a CaptchaException as I would have to assume you cannot type in captchas.", "created_utc": 1368743675, "gilded": 0, "name": "t1_ca07qal", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "undergroundmonorail", "body": "I'm not doing anything funny anywhere. The one thing I thought of that might cause a problem is that I'm on my Windows machine today (instead of my usual Linux one) and he PRAW install wasn't quite as foolproof. When I get a chance I'll try the same code on there and see if I get a better result.", "created_utc": 1368743786, "gilded": 0, "name": "t1_ca07rla", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "bboe", "body": "This PyScripter ID must be redirecting the input and output streams. Try running your code outside of the IDE (you can still use it to code) and you shouldn't have any issues. I imagine PyScripter allows \"raw_input\" to work properly, however, if I am unable to test whether or not the input stream is closed, then I must assume input is not possible, thus using PyScripter, you will not be able to input captchas unless you handle such exceptions yourself.", "created_utc": 1368744681, "gilded": 0, "name": "t1_ca082pv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eh4ih/im_about_60_sure_that_this_is_a_praw_problem_but/"}, {"author": "ExceedinglyEdible", "body": "$ pip install --upgrade praw or else $ pip install praw==", "created_utc": 1368517708, "gilded": 0, "name": "t1_c9yg5du", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "noun_exchanger", "body": "At what point should I be typing that code into the command prompt? Do i type it directly into the command prompt or with python.exe? Do I have to import PIP and/or PRAW before I type that? I've tried most of those combinations but it never seems to recognize the \"$\".", "created_utc": 1368519240, "gilded": 0, "name": "t1_c9ygchz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "ExceedinglyEdible", "body": "Oh, I skipped-out on the whole Windows part. (that $ symbol is Linux boilerplate stuff, btw) PIP is an executable, which means you'll probably find it in `C:\\Python\\bin\\pip.exe`. Open a command prompt (`cmd.exe` in Start>Run) then type: cd C:\\Python\\bin pip install --upgrade praw For easier access, you can add the Python bin folder to your PATH through Properties of \"My Computer\". Google \"windows PATH variable\" for help. Doing so will let you type \"pip \" from any folder while in the command prompt.", "created_utc": 1368520282, "gilded": 0, "name": "t1_c9ygh5l", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "noun_exchanger", "body": "That seemed to work but now there's other errors that have popped up. I'm sure I'll figure them out. Thanks for the help", "created_utc": 1368521451, "gilded": 0, "name": "t1_c9ygm6g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "ExceedinglyEdible", "body": "What are they? Maybe I can help you out.", "created_utc": 1368554346, "gilded": 0, "name": "t1_c9yogpo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "noun_exchanger", "body": "I figured it out, but thanks for the offer", "created_utc": 1368594493, "gilded": 0, "name": "t1_c9z2k7h", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1eauwc/how_to_update_praw_with_pip/"}, {"author": "radd_it", "body": "If a format could be agreed upon, this could easily be done with the wiki.", "created_utc": 1368480631, "gilded": 0, "name": "t1_c9y4p4h", "num_comments": null, "score": 8, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "Zook", "body": "Could technically be done with the CSS too. Have the bot look for specific tags in a subreddit through the /about/stylesheet page.", "created_utc": 1369561257, "gilded": 0, "name": "t1_ca682u3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "shaggorama", "body": "My end game here is for this feature to be an attribute on the subreddit object in praw. That way if a bot is scraping comments in /r/all, it could quickly see if the subreddit the comment came from is attributed to a subreddit the bot is welcome in. This doesn't necessarily preclude your idea (I'm guessing you're thinking something along the lines of a page titled \"nobots\" or something). But I imagine this being delivered in reddit's JSON response. The problem with implementing it on the wiki is that this would mean an additional GET request to the subreddit's wiki each time praw builds a new subreddit instance. I really don't want the implementation of something as trivial as this flag to have any potential to slow people (or reddit) down.", "created_utc": 1368482718, "gilded": 0, "name": "t1_c9y5gxz", "num_comments": null, "score": 7, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "AlLnAtuRalX", "body": "Or if you wanted a fast solution and the reddit admins are unwilling to help a Wiki page on an r/nobots subreddit where moderators could add their sub name that would then be downloaded as a plaintext file and cached. That could easily and quickly be queried.", "created_utc": 1368543000, "gilded": 0, "name": "t1_c9ykeko", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "shaggorama", "body": "That could definitely work, but the problem with that is it would only work for people \"in-the-know,\" as opposed to a flag in reddit's JSON/XML response that would make a developer go \"hmm, what's that? Better check the docs!\" Nothing preventing us from trying it out, of course. If we did this, I would contact the moderators of subreddits that are known to have banned bots. Both VideoLinkBot and LinKFixerBot have published the subreddits that they're banned from. This could be a good starting point.", "created_utc": 1368543322, "gilded": 0, "name": "t1_c9ykic6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "AlLnAtuRalX", "body": "Very true. A flag in the response is definitely optimal, but worst case scenario we can just spam all the known API docs with info on this. Then if developers choose to ignore it they're opting out.", "created_utc": 1368545990, "gilded": 0, "name": "t1_c9ylfb5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "radd_it", "body": "Makes sense. I think a better spin would be a combination of your idea and ketralnis's-- add another checkbox on the subreddit settings page that says \"we're cool with bots\". No need to complicate it, it's just a boolean.", "created_utc": 1368482867, "gilded": 0, "name": "t1_c9y5iuk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "bboe", "body": "Yeah to be effective (and efficient) such a flag would need to be added to the json data for every submission and comment (when not part of an existing subtree) as by default PRAW only receives the subreddit name when fetching the listings most bots are interested in.", "created_utc": 1368484741, "gilded": 0, "name": "t1_c9y66jf", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "saralk", "body": "Nice idea, but maybe there should be some more fine grained control. For example, a subreddit might want \"helpful\" bots like the one that collates links to videos, while not having the \"novelty\" bots like the risky click bot", "created_utc": 1368487768, "gilded": 0, "name": "t1_c9y7864", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "shaggorama", "body": "haha, VideoLinkBot just happens to be my baby! Thanks for ranking it as one of the helpful bots. I get that I'm sort of shooting myself in the foot with this proposition, but it seems like there's demand for it. A lot of people seem to appreciate my bot, and plenty don't. The bot gets banned from subs where it's not appreciated, and maybe it would be easier if the subs could just tell the bot to go away.", "created_utc": 1368532580, "gilded": 0, "name": "t1_c9yhy59", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "AndrewNeo", "body": "As both a bot writer and a sub moderator, this sounds like a pain.", "created_utc": 1368507291, "gilded": 0, "name": "t1_c9ye07z", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "shaggorama", "body": "Yeah, this might be more trouble than it's worth. We don't have to implement it. I'm enjoying this discussion.", "created_utc": 1368532855, "gilded": 0, "name": "t1_c9yhzpx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "AsterJ", "body": "If a subreddit is looking at bots by a case-by-case basis then banning is fine.", "created_utc": 1368490596, "gilded": 0, "name": "t1_c9y878q", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": null, "body": "This submission has been linked to in 1 subreddit (at the time of comment generation): * /r/botwatch: [/r/redditdev proposal: robots.txt equivalent for reddit bots.](/r/botwatch/comments/1ebgy5/rredditdev_proposal_robotstxt_equivalent_for/) ---- This comment was posted by a bot, see /r/Meta_Bot for more info.", "created_utc": 1368545076, "gilded": 0, "name": "t1_c9yl3m5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "ketralnis", "body": "> why not just modify the ecosystem so the bots already get the message without evening needing the mods to ban them? In my personal opinion subreddits and users should have to opt *into* bots, not be forced to opt out of bots individually (or even the idea of bots, as in this proposal). You want to run a robot? Get the okay from the subreddit moderator *first*. Otherwise stay the hell away. Frankly I have yet to see a commenting bot I liked, the majority of them being glorified shitty novelty accounts. They detract from every thread they enter.", "created_utc": 1368476933, "gilded": 0, "name": "t1_c9y3avf", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "shaggorama", "body": "Fair enough. I think setting the api wrappers to default ignore everyone is a close middle ground, but I'd be OK with making it so people/subreddits needed to opt-in for bots. My concern is mainly that if the feeling is that this flag is too broadly applied, it might not get respected at all which would defeat the purpose of having such a flag to begin with.", "created_utc": 1368477158, "gilded": 0, "name": "t1_c9y3dyg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "itsnotlupus", "body": "Assuming bots would generally obey, that'd threaten innovation, or something. Bots would be relegated to empty test subreddits with no good way to prove their worth. There's more to bots than novelty accounts.. For example, there are bots that attempt to make overused meme images accessible by extracting the text from them and putting it in comments. Or there's that bot that lets you tip other users with bitcoins. Anyway, I'd vote for somewhat conservative defaults, which would still let new bots have some minimum level of activity, hopefully enough to prove their worth and convince mods to raise up the allowable post rate for the good ones.", "created_utc": 1368507134, "gilded": 0, "name": "t1_c9ydyni", "num_comments": null, "score": 13, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "ketralnis", "body": "> Or there's that bot that lets you tip other users with bitcoins Yeah I definitely want my conversations polluted with `/tipthisguy 0.005`. It's right up there with \"this\" and \"came here to say this\" on my list of favourite things", "created_utc": 1368507688, "gilded": 0, "name": "t1_c9ye47r", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "pipeep", "body": "There are minimum tip requirements, and tips below a certain amount are discouraged on the bot's faq.", "created_utc": 1371227231, "gilded": 0, "name": "t1_caiv3fs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "AsterJ", "body": "I agree you should be able to override the flag. Lots of mobile and web applications interface with reddit with the same API that bots use. Users using those services need to have the same permissions as a user logged into the site. I still think this is a good idea though. To flesh it out maybe these are some other bot permissions a subreddit can be set. * Post comments (default to yes) * Vote on comments (default to no) * Post submissions (default to no) * Vote on submissions (default to yes) * Posting frequency (ie once a day, once an hour, etc.)", "created_utc": 1368491071, "gilded": 0, "name": "t1_c9y8d43", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "ketralnis", "body": "Is there *ever* a good reason to have a robot voting?", "created_utc": 1368507850, "gilded": 0, "name": "t1_c9ye5sl", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "AsterJ", "body": "Hmm, the only reasons I can think of would be for bots operated by moderators of that subreddit (like bots that downvote disallowed comments / topics). Still if you wanted to do that you might as well just have the bot delete the comment. Perhaps the only real variable permission is the ability to post comments then? I guess I only see bots make submissions to subreddits when they are run by a moderator...", "created_utc": 1368511337, "gilded": 0, "name": "t1_c9yf09n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "MatrixFrog", "body": "You may want to x-post this to http://www.reddit.com/r/botwatch/", "created_utc": 1368505927, "gilded": 0, "name": "t1_c9ydlue", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "shaggorama", "body": "we seem to have a pretty good discussion going here already, but you're welcome to x-post there if you feel so inclined", "created_utc": 1368532751, "gilded": 0, "name": "t1_c9yhz56", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e9lwp/proposal_robotstxt_equivalent_for_reddit_bots/"}, {"author": "pkamb", "body": "Any chance you've changed the user? http://www.reddit.com/r/redditdev/comments/17oer0/api_change_login_requests_containing_a_session/ I thought I remembered that all logins now require ssl, but I can't find anything and could be mistaken.", "created_utc": 1368126177, "gilded": 0, "name": "t1_c9vn6jv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e0ffk/did_something_in_the_reddit_api_change_last_night/"}, {"author": "testuser12345678", "body": "Nevermind, it was a PythonAnywhere issue. They've resolved it.", "created_utc": 1368127727, "gilded": 0, "name": "t1_c9vnqpf", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1e0ffk/did_something_in_the_reddit_api_change_last_night/"}, {"author": "bboe", "body": "Unless you want to do something that PRAW doesn't provide yet (e.g., fetching listings by domain) you shouldn't use `get_content` directly. It looks like what you actually want to use is `r.get_submission(url=)`. [This answer](http://stackoverflow.com/a/12738719/176978) I provided on SO may also be useful.", "created_utc": 1367808586, "gilded": 0, "name": "t1_c9t6xa9", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dro2b/problems_with_the_praw_get_content_method/"}, {"author": "rapture_survivor", "body": "Ah, ok, I didn't know get_submission could link to an individual comment EDIT: Thank you, this works perfectly! my code only needs a little bit more polishing now", "created_utc": 1367810611, "gilded": 0, "name": "t1_c9t7lws", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dro2b/problems_with_the_praw_get_content_method/"}, {"author": "nemec", "body": "> kept track of using the submission ID 1. This is the most obvious solution. It's the same technique that email clients use to keep track of which messages are new (see the UIDL paragraph [here](http://www.everything-mdaemon.com/mdaemon/understanding-pop3-mail-client-duplicates)). 2. You could keep track of the *timestamp* of the last checked message. Any messages newer than that timestamp are new. I'm not sure that messages coming \"later\" are guaranteed to have a timestamp greater than (or equal to) any previous message, though (think latency: [A submits, B submits, B processes, A processes; A has an earlier timestamp than B because it was submitted first] or timezone differences). 3. As far as I'm aware the submission ID is monotonically increasing, i.e. any submissions processed *after* a given submission will have a higher ID. The IDs are just numbers in Base 36, but if you compare the ID lexicographically then `a", "created_utc": 1367694111, "gilded": 0, "name": "t1_c9se0vg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "johnflim", "body": "Thanks a bunch nemec!", "created_utc": 1367701813, "gilded": 0, "name": "t1_c9sg52i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "Rotten194", "body": "You should check [http://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/c9shb9u](this) before implementing somehting based on successive ids.", "created_utc": 1367745339, "gilded": 0, "name": "t1_c9sqkxv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "JordanTheBrobot", "body": "# **Fixed your link** I hope I didn't jump the gun, but you got your link syntax backward! Don't worry bro, I fixed it, have an upvote! - [this](http://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/c9shb9u) ^Bot ^Comment ^- ^[ [^Stats ^& ^Feeds](http://jordanthebrobot.com) ^] ^- ^[ [^Charts](http://jordanthebrobot.com/charts) ^] ^- ^[ [^Information ^for ^Moderators](http://jordanthebrobot.com/moderators) ^] ^- ^[ [^Live ^Image ^Feed](http://jordanthebrobot.com/links) ^]", "created_utc": 1367745588, "gilded": 0, "name": "t1_c9sqm13", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "radd_it", "body": "> monotonically Thank you for showing me this word.", "created_utc": 1367695132, "gilded": 0, "name": "t1_c9seb1w", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "bboe", "body": "> As far as I'm aware the submission ID is monotonically increasing For the most part that is true, however, there are some exceptions with respect to rolled back transactions (as I understand) and thus it's not a guarantee.", "created_utc": 1367706173, "gilded": 0, "name": "t1_c9shb9u", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "johnflim", "body": "I see. Can you suggest a better way of managing PMs that have been processed by the bot already? Thanks!", "created_utc": 1367772926, "gilded": 0, "name": "t1_c9svijn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "bboe", "body": "Keep a set of the base36 ids of already processed PMs. If you find one you've already processed then you can stop. You'll note you can optimize by only storing a cache of the \"newest\" processed ids, but if the number is small enough you might as well just store them all.", "created_utc": 1367805379, "gilded": 0, "name": "t1_c9t5tio", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1doto3/reddit_bot_reading_inbox/"}, {"author": "_Daimon_", "body": "Do you mean link flair as on /r/askscience? EDIT: I.e. [this hot askscience post](http://www.reddit.com/r/askscience/comments/1dazzt/how_far_back_in_time_would_i_have_to_go_for_my/) has the link flair \"Computing\" which is what I suppose you mean by tag right?", "created_utc": 1367238153, "gilded": 0, "name": "t1_c9ouxpi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/"}, {"author": "killver", "body": "I think that's what I mean. Thanks! So can users only choose from given flairs or can they assign new ones?", "created_utc": 1367238770, "gilded": 0, "name": "t1_c9ov1qd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/"}, {"author": "_Daimon_", "body": "I would recommend you create your own private test subreddit, this is usually the quickest way of finding out how stuff like this works. There is a setting that doesn't allow link flair, makes it mod only or allow mods + post creators to set it. Only one of the created css templates can be used, but the text can be anything. What link flair a submission has is exposed via the API yes. Try doing the following and see if you can find something interesting in the output. import praw from pprint import pprint r = praw.Reddit('link flair testing by u/_Daimon_') subreddit = r.get_subreddit('askscience') submission = next(subreddit.get_hot()) pprint(vars(submission)) There is AFAIK, no way to search Reddit for all submissions with a specific link_flair.", "created_utc": 1367239616, "gilded": 0, "name": "t1_c9ov7oa", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/"}, {"author": "LinkFixerBot", "body": "/u/_daimon_", "created_utc": 1367239646, "gilded": 0, "name": "t1_c9ov7w0", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/"}, {"author": "kemitche", "body": "> There is AFAIK, no way to search Reddit for all submissions with a specific link_flair. You can actually search for things like: flair:approved subreddit:i18n [Example](http://www.reddit.com/r/i18n/search?q=flair%3Aapproved+subreddit%3Ai18n&sort=relevance&t=all)", "created_utc": 1367245942, "gilded": 0, "name": "t1_c9owsak", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/"}, {"author": "_Daimon_", "body": "AWESOME, I was hoping someone would turn up and prove me wrong. I had been trying it out with stuff like \"link_flair\", which didnt work.", "created_utc": 1367246228, "gilded": 0, "name": "t1_c9owvhm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1dbzvd/tags_on_reddit/"}, {"author": "rudelude", "body": "I'm not completely clear on what your doing but I would say your first big logical problem is this: Line 14 will add the submission to already_done. Line 18 will then check if the submission is in already_done (how can it not be at this point?). You may want line 18 to be an else.", "created_utc": 1367158856, "gilded": 0, "name": "t1_c9o9ypy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d9ops/trouble_with_an_if_loopwhile_loop_combination/"}, {"author": "acidzest", "body": "Could I store 25 submissions in an array and iterate through them, while removing them? Array. Post array[1] to Twitter Remove array[1] from Array Post array[2] to Twitter Remove array[1] from Array If Array is empty Create a new array..", "created_utc": 1367159557, "gilded": 0, "name": "t1_c9oa45r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d9ops/trouble_with_an_if_loopwhile_loop_combination/"}, {"author": "rudelude", "body": "[Queues](http://docs.python.org/2/tutorial/datastructures.html#using-lists-as-queues)", "created_utc": 1367160213, "gilded": 0, "name": "t1_c9oa9d5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d9ops/trouble_with_an_if_loopwhile_loop_combination/"}, {"author": "acidzest", "body": "Basically, I want to get the top post on Reddit and post it to say, Twitter, which I've sorted out, it's just this part of the code I'm struggling with. I want to get the top post, post it to Twitter, then, an hour later, get the top post again, if it's the same as it was previously, get the second top post, and so on.", "created_utc": 1367159071, "gilded": 0, "name": "t1_c9oa0bx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d9ops/trouble_with_an_if_loopwhile_loop_combination/"}, {"author": "rudelude", "body": "Well fix that and see how it does. You may want to consider grabbing more of the front page. As it's written, you'll request one at a time until you find a post you haven't done. If you requested a bunch of them, you can search through them locally, which should be more efficient.", "created_utc": 1367159624, "gilded": 0, "name": "t1_c9oa4od", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d9ops/trouble_with_an_if_loopwhile_loop_combination/"}, {"author": "acidzest", "body": "Well, I'm not sure how I would store them in an array and then remove them from that array as I post them... I'm pretty new to Python.", "created_utc": 1367159859, "gilded": 0, "name": "t1_c9oa6ic", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d9ops/trouble_with_an_if_loopwhile_loop_combination/"}, {"author": "bboe", "body": "Just added the documentation page: https://praw.readthedocs.org/en/latest/pages/multiprocess.html", "created_utc": 1367010342, "gilded": 0, "name": "t1_c9nc6n5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d2nr4/multiprocess_praw_testing_needed/"}, {"author": "killver", "body": "If you have the PRAW Comment Object, there is a field \"link_id\" that specifies the corresponding submission.", "created_utc": 1367237304, "gilded": 0, "name": "t1_c9ousdz", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/"}, {"author": "radd_it", "body": "> http://www.reddit.com/by_id/t1_c9mcwag Just kidding, but damn that'd be nice. I've not used PRAW, but how do you have a comment ID without its post ID? Do you not get a link_id with it? What about fries? Do you get fries with that?", "created_utc": 1366902594, "gilded": 0, "name": "t1_c9mef3l", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/"}, {"author": "SOTB-human", "body": "Yeah, so, I made a bunch of comments, but made the mistake of recording only the comment ID thinking that that'd be enough to find it later. Now I know to record the link_id too.", "created_utc": 1367392748, "gilded": 0, "name": "t1_c9q6oe4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/"}, {"author": "radd_it", "body": "Heh, good to know my -1 comment had the answer you were looking for. :) So, what about the fries?", "created_utc": 1367393528, "gilded": 0, "name": "t1_c9q6swv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d2ino/is_it_possible_to_fetch_a_comment_using_only_its/"}, {"author": "bboe", "body": "What's the issue with using pickle on the submissions? I haven't tried it but it _should_ work. If the actual pickle command takes longer than an instant for a single submission, then there is an issue with fetching all the dynamic attributes such as comments. If you want all the comments, then there isn't much you can do about speed as fetching comments takes quite some time.", "created_utc": 1366853505, "gilded": 0, "name": "t1_c9m2yie", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d0cln/praw_store_submission_data/"}, {"author": "killver", "body": "I know that comment fetching takes some time. So I tried to take only the static values out of the submission object and pickle it via vars(submissions). Still it took a very long time per object which is really strange.", "created_utc": 1366875085, "gilded": 0, "name": "t1_c9m9bbg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d0cln/praw_store_submission_data/"}, {"author": "bboe", "body": "Ah I just tried pickling a submission and what ends up happening is it makes 2 additional requests. One to fetch the subreddit and one to fetch the user. It seems overwriting `submission.author` and `submission.subreddit` with just the string values isn't sufficient to avoid the requests. The only thing that works is to set those values to `None`.", "created_utc": 1366881137, "gilded": 0, "name": "t1_c9ma4jb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d0cln/praw_store_submission_data/"}, {"author": "killver", "body": "Ah nice catch. Well, that's pretty bad because I need those string fields. But well I think I'll work with a database.", "created_utc": 1366886269, "gilded": 0, "name": "t1_c9mapsh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1d0cln/praw_store_submission_data/"}, {"author": "ketralnis", "body": "reddit's API is probably just temporarily a touch slow. Try again in an hour?", "created_utc": 1366758517, "gilded": 0, "name": "t1_c9la31z", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "alexleavitt", "body": "This has been happening consistently for the past 3 days though... Was wondering initially if it was because of the DDoS, but it's still dropping. Maybe it'll be more like wait a few days...?", "created_utc": 1366758697, "gilded": 0, "name": "t1_c9la5bx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "ketralnis", "body": "Are you hitting reddit too hard? e.g. by disabling PRAW's built-in rate limiting", "created_utc": 1366758991, "gilded": 0, "name": "t1_c9la8z5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "alexleavitt", "body": "Nope.", "created_utc": 1366764735, "gilded": 0, "name": "t1_c9lcbfh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "shaggorama", "body": "Are you never able to get comments using this code, or do you hit this error occasionally? This could just be reddit going down temporarily and your code just needs to handle the exception and wait a few minutes.", "created_utc": 1366772413, "gilded": 0, "name": "t1_c9lf72w", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "alexleavitt", "body": "I was able to get it in the past (eg., back in February). I was able to get about 600 users' data last week, and since they I pretty much get a timeout every initial user upon rerunning the script (or maybe a few users and then it timeouts).", "created_utc": 1366772847, "gilded": 0, "name": "t1_c9lfcz9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "shaggorama", "body": "Maybe reddit is specifcally limiting/blocking your user agent string? Try changing that and see what happens. Probably won't make a difference, but if your code used to work and hasn't changed, the problem probably isn't (directly) inherent in the code.", "created_utc": 1366773083, "gilded": 0, "name": "t1_c9lfg6l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "alexleavitt", "body": "Just tried changing it: got through about 3 dozen users and it crapped out again...", "created_utc": 1366778597, "gilded": 0, "name": "t1_c9lhd6i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "radd_it", "body": "reddit has had some DDoS issues in the last week. It's possible you/ your IP got caught in the crossfire.", "created_utc": 1366833703, "gilded": 0, "name": "t1_c9lvik7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "alexleavitt", "body": "Update: tried another script that has worked perfectly in the past to get comments, but it also returned timeout errors...", "created_utc": 1366781084, "gilded": 0, "name": "t1_c9li445", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "bboe", "body": "What version of PRAW are you running? How long does it take to timeout? The setting should be at 45 seconds.", "created_utc": 1366825449, "gilded": 0, "name": "t1_c9ls7is", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cyxjr/praw_timeouts/"}, {"author": "bboe", "body": "What version of PRAW are you using as I cannot reproduce. Also to remove your own posting you should use `delete` not `remove`.", "created_utc": 1366654159, "gilded": 0, "name": "t1_c9kdtrp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "lamerx", "body": "Same thing... NotLoggedIn: `please login to do that` on field `None` Should i be using python 3? i have 2.7 now", "created_utc": 1366672010, "gilded": 0, "name": "t1_c9kkfov", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "bboe", "body": "Either version of python is fine. import praw user='redditnick' passwd='password' reddit.login(user,passwd) That code example doesn't make sense. Where is `reddit` defined there? Assuming you did `reddit = praw.Reddit('user-agent string')` at some point, then either you're neglecting to share some of your code, or your PRAW installation is not as it should be.", "created_utc": 1366852712, "gilded": 0, "name": "t1_c9m2o1y", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "lamerx", "body": "sorry, its in a test scrippt with a bunch of commented lines tha ti uncomment for reading stuff, i didnt want to paste that crap in here import praw user='redditnick' passwd='password' reddit = praw.Reddit(user_agent='BotScript 1.0, by u/LamerX') print \"Logging in...\" reddit.login(user,passwd) print \"Logged In\" target = reddit.get_submission(submission_id='1cwepg') target.upvote()", "created_utc": 1366935662, "gilded": 0, "name": "t1_c9mqx24", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "LinkFixerBot", "body": "/u/lamerx", "created_utc": 1366935693, "gilded": 0, "name": "t1_c9mqxfp", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "bboe", "body": "Heh. Remove \"Bot\" from your user-agent script and it'll work. Apparently that's a _taboo_ word in a user-agent string.", "created_utc": 1366939421, "gilded": 0, "name": "t1_c9msbgs", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "lamerx", "body": "no way... *checks code* Yup, that was it allright, thanks a ton", "created_utc": 1366958837, "gilded": 0, "name": "t1_c9myign", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "bboe", "body": "Yup, shocked me too as I couldn't for the life of me reproduce your problem until I started with the exact example you provided above.", "created_utc": 1366959252, "gilded": 0, "name": "t1_c9myla1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "lamerx", "body": "Well thanks man, I appreciate you looking into it for me", "created_utc": 1366959846, "gilded": 0, "name": "t1_c9myp0u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "lamerx", "body": "its the most current version 2.7.3 i uninstalled and reinstalled right before i tried this script..will try delete now", "created_utc": 1366671745, "gilded": 0, "name": "t1_c9kkcfe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "_Daimon_", "body": "He meant version of PRAW, not python. Try doing python -c \"import praw; print praw.__version__\" When you're deleting your own posts, you use `delete` not `remove`. `remove` is what moderators use when they delete somebody else's post. Could you post a full program causing the upvote / downvote error like in your first example?", "created_utc": 1366672813, "gilded": 0, "name": "t1_c9kkpxp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": "lamerx", "body": "2.0.15 is the praw version ##CODE import praw user='redditnick' passwd='password' reddit.login(user,passwd) print \"Logged In\" target=reddit.get_submission(submission_id='1cwepg') target.upvote() ##OUTPUT Traceback (most recent call last): File \"C:\\Python27\\Scripts\\testB0T2.py\", line 62, in target.upvote() File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 447, in upvote return self.vote(direction=1) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 341, in wrapped return function(cls, *args, **kwargs) File \"C:\\Python27\\lib\\site-packages\\praw\\objects.py\", line 464, in vote return self.reddit_session.request_json(url, data=data) File \"C:\\Python27\\lib\\site-packages\\praw\\decorators.py\", line 239, in error_checked_function raise error_list[0] NotLoggedIn: `please login to do that` on field `None`", "created_utc": 1366700344, "gilded": 0, "name": "t1_c9ktiak", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cumng/praw_voting_removing/"}, {"author": null, "body": "[deleted]", "created_utc": 1366599901, "gilded": 0, "name": "t1_c9k0z3d", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "frumious", "body": "That's not a very appropriate sub for this issue.", "created_utc": 1366671077, "gilded": 0, "name": "t1_c9kk3py", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": null, "body": "[deleted]", "created_utc": 1366672262, "gilded": 0, "name": "t1_c9kkiwp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "frumious", "body": "I guess I'm asking you for a little reading comprehension. I'm asking if anyone has insinuated a persistent store into praw.", "created_utc": 1366672864, "gilded": 0, "name": "t1_c9kkqkv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "_Daimon_", "body": "The simplest solution is to simply change [cache_timeout in the configuration files](https://praw.readthedocs.org/en/latest/pages/configuration_files.html) to a very large number to prevent items in the cache ever timing out.", "created_utc": 1366558918, "gilded": 0, "name": "t1_c9jnm3u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "frumious", "body": "Thanks. I'll look at this. I want a store that will persist between executions of Python (I edited the post to hopefully make that more clear). Do you know if the internal cache would persist like this or is it just in memory?", "created_utc": 1366565878, "gilded": 0, "name": "t1_c9jppj0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "bboe", "body": "Just FYI -- in the current development version (soon to be 2.1.0) I've redone how requests are handled by adding a concept of a request handler that must provide two methods, `request` and `evict`. With this addition, it is trivial to provide your own handler that provides a persistent cache: https://github.com/praw-dev/praw/blob/master/praw/handlers.py#L90 In fact, if you can write it in a generic way that works on Windows, OSX, and Linux without additional dependencies I would be happy to add it as one of the \"official\" PRAW handlers. As I imagine, loading from disk should only occur when PRAW is loaded, and writing to disk need only occur when PRAW terminates ([atexit](http://docs.python.org/2/library/atexit.html)). If you need multiprocess support, then something similar should be added to the [multiprocess server](https://github.com/praw-dev/praw/blob/master/praw/multiprocess.py#L17) as it is responsible for the cache in that case.", "created_utc": 1366853194, "gilded": 0, "name": "t1_c9m2uff", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "frumious", "body": "Very cool, bboe. Thanks for the notice. It sounds exactly what I was wanting. I'll check it out. BTW, I saw on github that you are at UCSB. That's my alma mater!", "created_utc": 1366853693, "gilded": 0, "name": "t1_c9m313t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "bboe", "body": "> It sounds exactly what I was wanting. I'll check it out. Awesome, let me know what you come up with. And go Gauchos!", "created_utc": 1366853834, "gilded": 0, "name": "t1_c9m32zf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "ketralnis", "body": "It is in-memory only. Your best bet sounds like something like SQLite (which ships with Python) You could replace praw's own cache with SQLite reasonably straight-forwardly", "created_utc": 1366586451, "gilded": 0, "name": "t1_c9jwep5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "frumious", "body": "Thanks. Yeah, that is the direction I was thinking. I'm hopping someone beat me to it. :)", "created_utc": 1366586955, "gilded": 0, "name": "t1_c9jwkb7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1csvar/persistent_store_for_praw/"}, {"author": "bboe", "body": "Catch the 404 exception and continue your crawling. PRAW can raise a number of exceptions that your code should be able to handle.", "created_utc": 1366440095, "gilded": 0, "name": "t1_c9iyvjk", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": null, "body": "try: #code except Exception as e: print(e)", "created_utc": 1366460315, "gilded": 0, "name": "t1_c9j1a6n", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": null, "body": "[deleted]", "created_utc": 1366485119, "gilded": 0, "name": "t1_c9j7h6g", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": null, "body": "Where can I find all the different exceptions that PRAW has? I know when posting it sometime requires captcha's and you can also get the \"You're posting too much. Wait a couple minutes.\".", "created_utc": 1366487677, "gilded": 0, "name": "t1_c9j87d2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": "nemec", "body": "`ClientException`, `OAuthException`, `APIException` are the base classes. https://github.com/praw-dev/praw/blob/master/praw/errors.py You could also inspect the code through reflection and filter out what doesn't belong: >>> import praw >>> dir(praw.errors) ['APIException', 'BadCaptcha', 'ClientException', 'ERROR_MAPPING', 'ExceptionList', 'InvalidUserPass', 'LoginRequired', 'ModeratorRequired', 'NonExistentUser', 'NotLoggedIn', 'RateLimitExceeded', '__builtins__', '__doc__', '__file__', '__name__', '__package__', '_build_error_mapping', 'inspect', 'six', 'sys']", "created_utc": 1366490803, "gilded": 0, "name": "t1_c9j9222", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": null, "body": "Interesting. I know this is kinda unrelated to the main topic, but if you're posting with a bot is there a way to tell if it's requiring a captcha via code?", "created_utc": 1366491151, "gilded": 0, "name": "t1_c9j95gj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": "bboe", "body": "My response to [this other question](http://www.reddit.com/r/redditdev/comments/1crgln/how_can_i_grab_a_captcha_image_to_present_to_a/c9jt1ig) should help a bit.", "created_utc": 1366592965, "gilded": 0, "name": "t1_c9jyizd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": "nemec", "body": "Looks like you'll have to monitor `sys.stdout` for a [prompt](https://github.com/praw-dev/praw/blob/master/praw/decorators.py#L178). It'll give you a .png for you to display, then send the response to `sys.stdin` terminated by a newline. Anything with the `requires_captcha` decorator in the [init](https://github.com/praw-dev/praw/blob/master/praw/__init__.py) file could potentially prompt you for a captcha.", "created_utc": 1366492195, "gilded": 0, "name": "t1_c9j9fmk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1cq7g1/how_to_know_if_subreddit_exists/"}, {"author": "nemec", "body": "Have you actually tried inspecting a comment in the shell? >>> import praw >>> r = praw.Reddit(user_agent=\"sdlfkn /u/nemec\") >>> s = r.get_submission(submission_id=\"1c7og2\") >>> c = s.comments[0] >>> dir(c) [..., 'approve', 'approved_by', 'author', 'author_flair_css_class', 'author_flair_text', ...] >>> c.author Redditor(user_name='ProtoKun7')", "created_utc": 1365827891, "gilded": 0, "name": "t1_c9e8puu", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "wtf_are_my_initials", "body": "I figured it out. Thanks though.", "created_utc": 1365828360, "gilded": 0, "name": "t1_c9e8u6o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "APPARENTLY_HITLER", "body": "What was the solution? Sorry I was able to get the redditor object from it but still can't seem to compare the redditor.author with a string. I did this (in an if statement): comment.author.lower() != \"apparently_hitler\"", "created_utc": 1369194478, "gilded": 0, "name": "t1_ca3ir40", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "wtf_are_my_initials", "body": "I don't remember exactly what I did. I'll look through my tomorrow and PM you when I remember/figure it out.", "created_utc": 1369194585, "gilded": 0, "name": "t1_ca3isjg", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "APPARENTLY_HITLER", "body": "thanks!", "created_utc": 1369194677, "gilded": 0, "name": "t1_ca3itro", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "APPARENTLY_HITLER", "body": "Figured it out! and str(comment.author).lower() != 'apparently_hitler'", "created_utc": 1369198070, "gilded": 0, "name": "t1_ca3jy31", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "_Daimon_", "body": "I see you've already got your answer, but for any fututre questions like this the [PRAW tutorial about introspecting PRAW and reddit](https://praw.readthedocs.org/en/latest/pages/writing_a_bot.html) should prove helpful :)", "created_utc": 1365833585, "gilded": 0, "name": "t1_c9ea2r2", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c931l/praw_and_comments/"}, {"author": "_Daimon_", "body": "import praw r = praw.Reddit('user reply example by u/_Daimon_') user = r.get_redditor('nannal') comment_generator = user.get_comments() first_comment = next(comment_generator) print first_comment.body r.login('BOT USERNAME', 'BOT PASSWORD') r.get_inbox() # Logged in users inbox. Hope this helped :) If you have any more questions, then feel free to ask.", "created_utc": 1365697245, "gilded": 0, "name": "t1_c9d3l9m", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "nannal", "body": "oh hey it's you, that's Daimon! I'll use the tools you've provided for good (probably) I'll give things a shot when I get home, catfactsbot should be working by the end of the day.", "created_utc": 1365697364, "gilded": 0, "name": "t1_c9d3mxb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "_Daimon_", "body": "> oh hey it's you, that's Daimon! Hi :) You know me?", "created_utc": 1365699171, "gilded": 0, "name": "t1_c9d4bs3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": null, "body": "[deleted]", "created_utc": 1365701028, "gilded": 0, "name": "t1_c9d51bm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": null, "body": "[deleted]", "created_utc": 1365701356, "gilded": 0, "name": "t1_c9d55v0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "kemitche", "body": "Whatever you're planning on using this for sounds very spammy and has an approximately 87% chance of getting you banned. Please consider the [rules](/rules) of reddit before continuing.", "created_utc": 1365709253, "gilded": 0, "name": "t1_c9d8cvp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": null, "body": "87%. How did you go about calculating that?", "created_utc": 1366338402, "gilded": 0, "name": "t1_c9i6qe3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "kemitche", "body": "Magic 8 Ball", "created_utc": 1366340963, "gilded": 0, "name": "t1_c9i7pff", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": null, "body": "That's a fancy Magic 8 Ball you got there.", "created_utc": 1366341582, "gilded": 0, "name": "t1_c9i7y1g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "nannal", "body": "Yeah I know the rules, I'm in the clear as far as I can tell, it's an admin decision though if I wind up banned on catfactsbot that's fine it was an educational exercise it's going to be different content (I've got a heap of cat facts) and it's not promoting my website or agenda", "created_utc": 1365713357, "gilded": 0, "name": "t1_c9da19r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "kemitche", "body": "Allow me to be more clear: based on your description and actions so far, this would be considered spamming. Don't do it.", "created_utc": 1365715602, "gilded": 0, "name": "t1_c9dax8b", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "nemec", "body": "OP, since you mentioned you also need inbox access are you trying to receive a private message from a user and then reply \"publicly\" to them using their latest comment? That's probably not the best solution for whatever your use case is. Could you explain what you're trying to do here? Maybe we can help you avoid getting marked as a spam bot ;)", "created_utc": 1365711911, "gilded": 0, "name": "t1_c9d9frc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "nannal", "body": "alright it's pretty simple, receive username from inbox, send that user cat facts publically, keep doing that until they fall off the submitted users chain (at the minute it's 50 users long). if that user is added again then they are added again, else they don't hear from catfactsbot.", "created_utc": 1365713562, "gilded": 0, "name": "t1_c9da4ej", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "nemec", "body": "Remember, although the *user* may have elected to subscribe to cat facts, other Redditors reading the comments section have not and will probably report you as a spammer. I highly doubt this adventure will last long :( Maybe, instead, you could watch for comments containing \"+catfacts\" (like the bitcointip bot) and reply to *only that comment* with a random fact. That would be more explicit and I suspect would allow your bot a longer lifespan.", "created_utc": 1365713867, "gilded": 0, "name": "t1_c9da8vv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "nannal", "body": "Yeah I was planning that at first, but I couldn't quite get it to work and I liked the idea of people signing each other up to receive cat facts.", "created_utc": 1365714584, "gilded": 0, "name": "t1_c9daj38", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "nemec", "body": "> signing each other up to receive cat facts That will *definitely* get you banned once people get fed up with it. Nice idea though :)", "created_utc": 1365714816, "gilded": 0, "name": "t1_c9damdm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "nannal", "body": "yeah you're right, plus if the subscribed list stops moving then the same 50 people get cat facts. I'll have a play around when I get back on monday and see what happens", "created_utc": 1365715063, "gilded": 0, "name": "t1_c9dapwz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1c52gt/submit_reply_to_users_most_recent_comment/"}, {"author": "_Daimon_", "body": "You can use the `place_holder` attribute to only have reddit return return results until it meets that submission. import praw r = praw.Reddit('placeholder example by u/_daimon_') subreddit = r.get_subreddit('redditdev') for submission in subreddit.get_new(limit=100, place_holder='1bu7ak'): print submission.title This will return all results younger than the id **on the assumption that a submission with that id can be found**. This is importment. If it cannot be found, say because the submission was deleted, then this script will return the last 100 submissions. If you're doing something where doing the same thing twice would be bad, say running a script that does statistics on submissions to reddit. Then you need additional security to ensure you don't process the same item twice. Keeping the id of the youngest submission you've handled is a good idea, but one id is not enough. You need more. I would recommend at least the youngest 5 submissions. Hope this helped :)", "created_utc": 1365358126, "gilded": 0, "name": "t1_c9ac2wq", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bv06t/stuck_at_processing_each_submission_on_a/"}, {"author": "damontoo", "body": "What functionality do you need? I had the same problem so I just wrote a small class that fetches JSON and handles the request limits like PRAW. It's very, very basic but that's all most of my projects need. Also, are you taking Steve Huffman's Udacity course or is this just coincidence?", "created_utc": 1365325149, "gilded": 0, "name": "t1_c9a5ksm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/"}, {"author": "naive_babes", "body": "Just going to update the post. I did manage to get it working, but only after getting rid of dependencies on update_checker. I unzipped all the egg files of the praw installation and put them in the local directory of my app. There is some trouble with the update_checker dependency, and i got rid of it completely. There's an additional problem with obtaining the platform of google app engine while constructing the user agent string. I got rid of that bit as well. Now i have it working fine. No, im not taking the udacity course you speak of. What is it about?", "created_utc": 1365341141, "gilded": 0, "name": "t1_c9a7i99", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/"}, {"author": "Jjonahjamesonjunior", "body": "Hey, can you elaborate a little bit more about what you did to get this working? I'm new to both App Engine and PRAW.", "created_utc": 1365373618, "gilded": 0, "name": "t1_c9ah7zp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bu7ak/how_do_i_run_praw_on_google_appengine/"}, {"author": "dakta", "body": "One could easily write an IRC bot to do this. Have users authenticate by PM to the bot which then logs into the API as that user and reads their inbox. The interface for selecting a message to reply to, composing the message, and sending it would need a bit of testing. Of course, I don't see much use for this sort of thing.", "created_utc": 1365186879, "gilded": 0, "name": "t1_c9976dy", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/"}, {"author": "utopiah", "body": "well the need is to have most of communication (IRC, IM, Twitter, ...) in one place that is easy to script and running on a server, e.g. a CLI IRC client", "created_utc": 1365359995, "gilded": 0, "name": "t1_c9acoon", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/"}, {"author": "Raerth", "body": "You wouldn't even need that. You can get RSS feeds of your inbox, just need a good RSS-IRC script.", "created_utc": 1365192376, "gilded": 0, "name": "t1_c9998j9", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/"}, {"author": "dakta", "body": "You'd still have to write an interface for composing replies, if you wanted to do it all from IRC.", "created_utc": 1365442318, "gilded": 0, "name": "t1_c9azh1c", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/"}, {"author": "utopiah", "body": "yes that's good to get notifications but that's not enough to reply", "created_utc": 1365359875, "gilded": 0, "name": "t1_c9acn9t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1bqlqv/is_there_a_solution_eg_irssi_script_or_bitblee/"}, {"author": "AndrewNeo", "body": "This is technically a python question, not a Reddit question, but. 1. Try `easy_install --user praw` first 1. Can you use [pip](https://pypi.python.org/pypi/pip) instead? (`pip install --user praw`) A lot of people would likely recommend it over easy_install, which is considered outdated. You also can't remove packages with easy_install. 2. Can you use [virtualenv](https://pypi.python.org/pypi/virtualenv)? This creates an environment specifically to run your scripts in with whatever packages you want without affecting the rest of the system.", "created_utc": 1364110541, "gilded": 0, "name": "t1_c91ed25", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1awkvu/permissions_issue_when_trying_to_easyinstall_praw/"}, {"author": "ipitythefoobar", "body": "Sorry, for hitting the wrong sub. I'll try /r/python the next time. I wasn't able to get 1 or 2 working but then I tried again using literally \"--user\" instead of substituting in my account user name (yes, I'm a n00b) and that worked! Thanks for the assistance!", "created_utc": 1364110987, "gilded": 0, "name": "t1_c91efbe", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1awkvu/permissions_issue_when_trying_to_easyinstall_praw/"}, {"author": "shrayas", "body": "I just tried this out with the raw API over at apigee and [this](https://snap.apigee.com/11yaRVf) is what i got. Looks to me like the API only doesn't return any information for that url. Hmm. Maybe our understanding of /info is wrong. Let me read into it a little and get back", "created_utc": 1364103523, "gilded": 0, "name": "t1_c91d366", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1aw7jc/praw_get_info_not_working/"}, {"author": "StealsTopComments", "body": "At first I thought that it just takes a while for the information to be available, but this post is way older than the mental limit I had imagined.", "created_utc": 1364150069, "gilded": 0, "name": "t1_c91l6f9", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1aw7jc/praw_get_info_not_working/"}, {"author": "notanasshole53", "body": "The problem is the imgur link you guys are looking up. You're pointing at the full-res image, which wasn't what was submitted. Try this link instead: http://imgur.com/jOG9a That works for me: >>> import praw >>> r = praw.Reddit(user_agent='testing /u/StealsTopComments') >>> info = r.get_info(url='http://imgur.com/jOG9a') >>> pprint.pprint(vars(info[0]) ... ) {'_comment_sort': None, '_comments': None, '_comments_by_id': {}, '_info_url': 'http://www.reddit.com/api/info/', '_orphaned': {}, '_populated': True, '_replaced_more': False, '_underscore_names': None, 'approved_by': None, 'author': Redditor(user_name='CocoaBeans'), 'author_flair_css_class': None, 'author_flair_text': None, 'banned_by': None, 'clicked': False, 'created': 1348899894.0, 'created_utc': 1348896294.0, 'distinguished': None, 'domain': u'imgur.com', 'downs': 10, 'edited': False, 'hidden': False, 'id': u'10no3v', 'is_self': False, 'likes': None, 'link_flair_css_class': None, 'link_flair_text': None, 'media': None, 'media_embed': {}, 'name': u't3_10no3v', 'num_comments': 12, 'num_reports': None, 'over_18': False, 'permalink': u'http://www.reddit.com/r/gaming/comments/10no3v/the_pokemon_holy_grail/', 'reddit_session': , 'saved': False, 'score': 14, 'selftext': u'', 'selftext_html': None, 'subreddit': , 'subreddit_id': u't5_2qh03', 'thumbnail': u'http://e.thumbs.redditmedia.com/UG34OE9OjiDQyXid.jpg', 'title': u'The Pokemon holy grail', 'ups': 24, 'url': u'http://imgur.com/jOG9a'}", "created_utc": 1364229753, "gilded": 0, "name": "t1_c924fq3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1aw7jc/praw_get_info_not_working/"}, {"author": "bboe", "body": "Flask (or another lightweight framework) + PRAW and you can do this very simply.", "created_utc": 1363979246, "gilded": 0, "name": "t1_c90kcqh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1atckm/a_seemingly_simple_task_but_i_have_no_clue_any/"}, {"author": "bboe", "body": "Yes, it is kind of strange that re-authorization is required each time for the same application and the same scope. I haven't played with the OAuth stuff much, other than the PRAW implementation, but I don't remember specifically if the same behavior occurs when obtaining permanent grants. Have you tried that?", "created_utc": 1363540625, "gilded": 0, "name": "t1_c8xb4p4", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1agkmm/praw_and_oauth/"}, {"author": "shrayas", "body": "Yes, i found it weird too. And i am picking up permanent grants as shown [here](https://github.com/shrayas/slashsub/blob/master/slashsub.py#L22). Do you have any other thoughts?", "created_utc": 1363542788, "gilded": 0, "name": "t1_c8xbrdx", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1agkmm/praw_and_oauth/"}, {"author": "roger_", "body": "Awesome, thanks!", "created_utc": 1363369968, "gilded": 0, "name": "t1_c8w8gc2", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/"}, {"author": "_Daimon_", "body": "Also, [PRAW's description on the cheeshop is now beautiful :)](https://pypi.python.org/pypi/praw) It's not a fact that's easy to find, but PyPi cannot parse markdown. If you want it to be pretty, then you need reStructured text.", "created_utc": 1363385511, "gilded": 0, "name": "t1_c8wdywn", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/"}, {"author": "catmoon", "body": "This opens a lot of doors for /r/nba. Thanks for doing this! We really appreciate it.", "created_utc": 1363721986, "gilded": 0, "name": "t1_c8ymqwe", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/"}, {"author": "bboe", "body": "You're welcome! It'd be awesome to see how you are using it once you have everything implemented.", "created_utc": 1363732577, "gilded": 0, "name": "t1_c8yqms8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1aczcr/praw_2013_adds_basic_wiki_editing_support/"}, {"author": "_Daimon_", "body": "I'm not entirely sure what you're asking. If it's how to use PRAWs `search` method to search for posts matching a title keyword and/or other requirements then you can get more information about the method with. import praw r = praw.Reddit('Search post info example by u/_Daimon') help(r.search) If you didn't know that you needed to use the `search` method to search reddit then you can search for \"search\" on [the search page](https://praw.readthedocs.org/en/latest/search.html?q=search) of PRAWs documentation on ReadTheDocs and it will tell you what methods to use. I'll add a link to the search page from index today or so. If I misunderstood you, or you have more questions then feel free to ask.", "created_utc": 1362669254, "gilded": 0, "name": "t1_c8reg8s", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/"}, {"author": "mrg3_2013", "body": "I was asking more on overcoming the response limit from reddit backend (1000 response). For now, I am okay as I can live with less results. However, one question you can help is..how do I avoid results with \"nsfw\" tag. For example s = r.search(tag,subreddit='pics',limit=100) is my search. Is there a way to specify nsfw:False for the search above. Thanks!", "created_utc": 1362690291, "gilded": 0, "name": "t1_c8rliev", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/"}, {"author": "_Daimon_", "body": "You can't go further back than a 1000 instances, it's an upstream cache limitation. See [PRAW's wiki](https://praw.readthedocs.org/en/latest/pages/faq.html). Maybe you could just run your script once in a while and get the newest results rather than doing one request going really far back? If you want to know more about how to use PRAWs `search` method to search for posts matching a title keyword and/or other requirements then you can get more information about the method with. import praw r = praw.Reddit('Search post info example by u/_Daimon') help(r.search)", "created_utc": 1362691864, "gilded": 0, "name": "t1_c8rm2u1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/"}, {"author": "mrg3_2013", "body": "OK. Thanks, _Daimon_. BTW it was a breeze to get praw running - so kudos!", "created_utc": 1362693256, "gilded": 0, "name": "t1_c8rmkqw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19tiyz/using_praw_to_search_posts_with_a_title_keyword/"}, {"author": "Pathogen-David", "body": "The error basically means you're getting rate-limited. ([See this thread for more info](http://www.reddit.com/r/redditdev/comments/witfv/redmproductions_humble_flairbot_so_many_429s/)). Have you changed the user agent in your bots settings.json? Make sure it is not a browser's UA and it is unique to your bot. I would make it something like \"`Groompbot/1.0 (Operated by /u/SN4T14)`\" Also, if you changed the api_request_delay value in the [praw config file](https://github.com/praw-dev/praw/blob/master/praw/praw.ini) to something less than 2.0, you need to change it back.", "created_utc": 1362576652, "gilded": 0, "name": "t1_c8qqmwq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "body": "The bots haven't been run for days, I already completely changed the user agent, and I haven't messed with the praw config file at all, although I'll take a look at it.", "created_utc": 1362581131, "gilded": 0, "name": "t1_c8qrjw5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "Pathogen-David", "body": "Can you still log in as the bot manually? (Preferably through the computer it is running on.)", "created_utc": 1362585815, "gilded": 0, "name": "t1_c8qstki", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "body": "The bot is running through a dedicated server which I only have SSH access to, but I can log in to the bot through my own browser fine, though.", "created_utc": 1362586873, "gilded": 0, "name": "t1_c8qt548", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "bboe", "body": "Is it possible you are running your script multiple times in parallel? PRAW does not ratelimit properly across multiple processes.", "created_utc": 1362591408, "gilded": 0, "name": "t1_c8qul82", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "body": "Nope, I even deleted the cron jobs entirely to be sure, then I just run python groompbot.py, and it freaks out like that.", "created_utc": 1362594820, "gilded": 0, "name": "t1_c8qvq9v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "_Daimon_", "body": "Do you know if other reddit scripts / apss (praw or otherwise) are running on the dedicated server?", "created_utc": 1362606545, "gilded": 0, "name": "t1_c8qzsms", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "body": "Nope, nothing, the only thing that's running are my game servers, should I try uninstalling praw just to be safe?", "created_utc": 1362643282, "gilded": 0, "name": "t1_c8rat2g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "catmoon", "body": "I just started getting a similar error on /r/NBA's NBA_MOD bot. It hadn't experienced any down time in weeks and that was just for maintenance. I'm still running the older version of PRAW. This makes me think that the problem is probably on Reddit's end since we both started getting the error at the same time. I wonder if the server issues Reddit has had today has anything to do with it. Could you let me know if anyone figures this out?", "created_utc": 1362616183, "gilded": 0, "name": "t1_c8r304h", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "body": "Sure, I'll let you know.", "created_utc": 1362643231, "gilded": 0, "name": "t1_c8rasrv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "catmoon", "body": "I updated praw to the latest and greatest and my bot works again. [[LINK]](https://github.com/praw-dev/praw)", "created_utc": 1362701322, "gilded": 0, "name": "t1_c8rp86a", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "body": "Still broken for me, going to try uninstalling all the praw stuff and reinstalling it.", "created_utc": 1362764101, "gilded": 0, "name": "t1_c8s3sja", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "catmoon", "body": "I also rebooted my server. Not sure why that would change anything though.", "created_utc": 1362764221, "gilded": 0, "name": "t1_c8s3tz0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "SN4T14", "body": "I'll try that, I had old praw 2.0.10 and praw 2.0.11 stuff, so that might be it, too.", "created_utc": 1362764315, "gilded": 0, "name": "t1_c8s3v2w", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19rubs/issues_with_youtube_bot/"}, {"author": "bboe", "body": "I'm not quite sure what you're asking. Using a link you can use `r.get_submission(link)` to get a Submission object. From that you have access to its comment tree through `submission.comments`.", "created_utc": 1362121906, "gilded": 0, "name": "t1_c8nqwx6", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19g943/any_way_to_glean_comments_from_a_link/"}, {"author": "shaggorama", "body": "even better, r.get_submission(submission_id = thing_id) This way you only have to pass in a short identifier instead of the whole url.", "created_utc": 1362147260, "gilded": 0, "name": "t1_c8nu84k", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19g943/any_way_to_glean_comments_from_a_link/"}, {"author": "bboe", "body": "The `message` variable in your example holds an object of type `Message`, which contains a number of attributes such as `author`, and `created_utc` and `body`. Try `print message.body`. In python if you want to see what attributes are available along with their values for an object, you can use the function `vars` like: `print(vars(message))`.", "created_utc": 1361948509, "gilded": 0, "name": "t1_c8mhe8t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19b8mp/praw_get_inbox_printing_whole_message_from_inbox/"}, {"author": "johnflim", "body": "Thanks! I'll give this a try later today.", "created_utc": 1362013567, "gilded": 0, "name": "t1_c8mxuxf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/19b8mp/praw_get_inbox_printing_whole_message_from_inbox/"}, {"author": "bboe", "body": "/u/Deimorz is mostly correct. If you specify any limit via `limit=XXX` PRAW will implicitly use that value to make each request hitting reddit's 100 item maximum. If you omit the limit, then the limit parameter is omitted in each request as well, thus defaulting to 25. Here's the [source](https://github.com/praw-dev/praw/blob/master/praw/__init__.py#L311) if you're interested which also explains the possible keyword arguments these functions take. If you want to explicitly override the url parameter `limit` the `url_data` parameter to `get_hot` (and other similar functions) has been renamed to `params` for consistency. Edit: Added source link.", "created_utc": 1361832394, "gilded": 0, "name": "t1_c8lle9p", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/"}, {"author": "Deimorz", "body": "The new version of praw defaults to using a limit of 100, so you shouldn't need that any more anyway.", "created_utc": 1361831899, "gilded": 0, "name": "t1_c8ll89j", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/"}, {"author": "MrFanzyPanz", "body": "yep! I just deleted it and it worked fine! Thanks!", "created_utc": 1361834385, "gilded": 0, "name": "t1_c8lm1fh", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197uz8/praw_url_data_removed_from_get_content/"}, {"author": "bboe", "body": "What version are you running?", "created_utc": 1361821042, "gilded": 0, "name": "t1_c8lh8nd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "lamerx", "body": "praw-1.0.16-py2.7.", "created_utc": 1361824725, "gilded": 0, "name": "t1_c8likbb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "bboe", "body": "I'm not sure why that's not working as I don't believe that functionality changed. Nevertheless, I don't support previous versions so I recommend you upgrade to the latest release, 2.0.11. That should resolve any such issues, but if not feel free to follow up.", "created_utc": 1361827341, "gilded": 0, "name": "t1_c8ljjy2", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "lamerx", "body": "I upgraded one of my python installs with praw-2.0.11-py2.7. the same code block is now giving me this ClientException: Unexpected redirect from http://www.reddit.com/reddits/mine/moderator/.json to http://www.reddit.com/reddits/login.json?dest=%2Freddits%2Fmine%2Fmoderator%2F.json%3Flimit%3D1024", "created_utc": 1361915059, "gilded": 0, "name": "t1_c8m6utz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "bboe", "body": "If PRAW didn't think you were logged-in in 2.0+ then you _should_ see a LoginRequired exception. This unexpected redirect to the login page seems to indicate that your session is no longer valid. If you actually are running the above code, I really don't see how that's possible without the login failing with an InvalidUserPass exception.", "created_utc": 1361915825, "gilded": 0, "name": "t1_c8m75fj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/197erw/praw_moderator_bot_remove_method/"}, {"author": "_Daimon_", "body": "You've hit a bug with PRAW's cache. I've sent bboe [a bugfix](https://github.com/praw-dev/praw/pull/184). BBoe is the maintainer and in charge of versions, but there should be a version increase with this commit. So wait until he pulls and upgrade your version of PRAW. ps. sorry for the terse reply. Bit pressed for time atm.", "created_utc": 1361534607, "gilded": 0, "name": "t1_c8jolt7", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "atomicUpdate", "body": "Thanks for the quick reply. And don't worry about the terse reply, was all I'm looking for.", "created_utc": 1361548233, "gilded": 0, "name": "t1_c8jraw6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "bboe", "body": "I just released version 2.0.11 which adds /u/_Daimon_'s patch. Thanks for reporting!", "created_utc": 1361592879, "gilded": 0, "name": "t1_c8k4yi3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "atomicUpdate", "body": "I downloaded the update, and while the r.get_me() is updating correctly now, the r.get_unread() seems to still be using the cached values from the previous invocation for a different user. Any ideas?", "created_utc": 1361646763, "gilded": 0, "name": "t1_c8kdxvl", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "bboe", "body": "I just quickly pushed a fix to github. If you are so inclined, could you check out the latest version and see that it resolves all your issues? I don't yet have the time to test the fix, and won't deploy a new version until such a time.", "created_utc": 1361655076, "gilded": 0, "name": "t1_c8kg3qd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "atomicUpdate", "body": "Sorry about the delay there, was out or a walk with the dog. Just tried it out, and it looks good to me. I was able to make requests for 2 different users and got the correct information each time. Thanks a lot for your help and the very quick fixes.", "created_utc": 1361660480, "gilded": 0, "name": "t1_c8khipe", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "bboe", "body": "Oh right. The patch only handled the one case. I should probably invalidate the entire cache. I'll let you know when there's a fix.", "created_utc": 1361650944, "gilded": 0, "name": "t1_c8kf1g3", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "atomicUpdate", "body": "Great. I'll download it and give it a shot tomorrow and let you guys know how it goes. Thanks much.", "created_utc": 1361596559, "gilded": 0, "name": "t1_c8k5u9l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1905ed/how_to_reuse_oauth_tokens_with_praw/"}, {"author": "shaggorama", "body": "I'm guessing you're a kind of new (python) programmer. One of the beautiful things about python is that it's set up to document itself. To ask the environment to help you figure out how to use a particular object or what attributes/methods it has attached to it, pass the object name to dir() and/or help(). type() is also useful: submission.selftext is a string, and all strings have a .lower() method. In addition to the wiki \\_\\_Daimon__ pointed you towards, you might find [this praw documentation page](http://python-reddit-api-wrapper.readthedocs.org/en/latest/) useful. I'm not sure if this is completley up-to-date, but the page says 2.0 so it can't be too far behind.", "created_utc": 1361202788, "gilded": 0, "name": "t1_c8h89yk", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "_Daimon_", "body": "I didn't intend it. But I can tell if someone knows their reddit if they can write my name :) > In addition to the wiki \\_\\_Daimon\\_\\_ pointed you towards, you might find this praw documentation page[1] useful. I'm not sure if this is completley up-to-date, but the page says 2.0 so it can't be too far behind. I'm pretty sure PRAW uses [service hooks](https://read-the-docs.readthedocs.org/en/latest/webhooks.html) to update the documentation on readthedocs. So it's *always* up-to-date. (readthedocs is pretty awesome).", "created_utc": 1361203839, "gilded": 0, "name": "t1_c8h8k2d", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "JRDubstepcom", "body": "ok so ive been messing around but lets say i use the dir() method. i get all these items that return \" ['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattr__', '__getattribute__', '__hash__', '__init__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_comments', '_get_json_dict', '_info_url', '_populate', '_populated', '_underscore_names', '_update_submission', 'children', 'comments', 'count', 'from_api_response', 'fullname', 'id', 'name', 'parent_id', 'reddit_session', 'submission'] \" for all the objects it returns i know i can use something like grab.fullname because it tells me i have a directory fullname available.(from what i understand) but why cant i use something like grab.__new__ or better yet how would i use any of the objects towards the top with the \"__\" in them. thanks again. you all are very helpful. And yes im new to python ;) edit: it seems that reddit has changed anything thats inclosed in \"__\" to bold.", "created_utc": 1361227223, "gilded": 0, "name": "t1_c8hgdd5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "shaggorama", "body": "new is probably a method, so you need to call it with parens to make it actually do something, like this: grab.new() you can figure out what's a method and what's just an attribute like this: object = grab for attr in dir(object): print attr, callable(getattr(object, attr)) Anything that reports back \"true\" is a method. Anything that comes back \"false\" is an attribute. Also, these questions aren't reddit specific: I suggest you post future questions of this kind to /r/learnprogramming or /r/learnpython since they're outside the scope of this subreddit.", "created_utc": 1361292797, "gilded": 0, "name": "t1_c8hvife", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "sakkaku", "body": "Usually stuff prefixed with an underscore are \"private\" methods/attributes and you might not want to be messing with them.", "created_utc": 1361237593, "gilded": 0, "name": "t1_c8hjiqi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "_Daimon_", "body": "We have an article called [writing a bot](https://github.com/praw-dev/praw/wiki/Writing-A-Bot) on the wiki, which talks about using introspection to discover attributes, methods etc of praw using `help`, `dir`, `vars` and the webway via `json` version of pages. There is also a [code overview](https://python-reddit-api-wrapper.readthedocs.org/en/latest/praw.html) page, which you might also find useful. If you have any other questions, then feel free to ask. :)", "created_utc": 1361177306, "gilded": 0, "name": "t1_c8h4o49", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "JRDubstepcom", "body": "awesome, this was exactly what i was looking for. thank you very much :)", "created_utc": 1361178268, "gilded": 0, "name": "t1_c8h4snw", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "judokick8910", "body": ":-)", "created_utc": 1361219395, "gilded": 0, "name": "t1_c8hdpgu", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18qq0d/is_there_a_praw_library_of_some_sort/"}, {"author": "shaggorama", "body": "I understand your concerns, but reddit is just a text based site, and you're just going to be snapping up short XML/JSON packets. I have trouble believing you will suck up significant bandwidth on whatever server you use just with a single reddit bot. As far as best practices, PRAW will control the rate limiting for you, so I recommend that whatever scraping you are doing you only run a single praw.Reddit instance from your box: this way, all calls to reddit are governed to no more than 1 GET every 2 seconds. I don't know how cheap your hosting is, but I suspect you'll be fine. You could always try testing your bot locally to see how much it affects your bandwidth, or just go for it on your external server: worst case, you use your quota for the month.", "created_utc": 1361116463, "gilded": 0, "name": "t1_c8govhm", "num_comments": null, "score": 4, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "ipitythefoobar", "body": "OK, so assuming that resources aren't an issue which sounds like the case, then is there a good tutorial someone can point me toward for setting up Python scripts on a shared server? I can run locally fine but I don't know how to do it from my server. Do I just upload my script and run a cron job to trigger it?", "created_utc": 1361137788, "gilded": 0, "name": "t1_c8gueq5", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "Rotten194", "body": "What kind of hosting do you have? Is it a full Nix VM? In that case, you should be fine just grabbing all the dependencies (python, PRAW) and setting it up in a cron job, yeah.", "created_utc": 1361143117, "gilded": 0, "name": "t1_c8gvwpn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "ipitythefoobar", "body": "Standard LAMP stack.", "created_utc": 1361167184, "gilded": 0, "name": "t1_c8h2tm8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "shaggorama", "body": "Why should running it on a server be any different from running it on your computer? I feel like the main hurdle is setting up the server. Once you've got your environment in place with all your dependencies, if you have execute permissions on the box I don't see how running the script externally really differs from running it locally. I guess it really depends on what kind of server you have.", "created_utc": 1361139554, "gilded": 0, "name": "t1_c8guwl3", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "ipitythefoobar", "body": ">Why should running it on a server be any different from running it on your computer? Dunno. Just not used to it. I don't have a console like I do on my local. I don't know if Python is even supported for sure. My server is a web hosting server and originating traffic from the server instead of responding to requests just is unfamiliar to me.", "created_utc": 1361167407, "gilded": 0, "name": "t1_c8h2vkc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "Bornhuetter", "body": "> Do I just upload my script and run a cron job to trigger it? Yes.", "created_utc": 1361139469, "gilded": 0, "name": "t1_c8guvpk", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "ipitythefoobar", "body": "OK, thanks. Will give it a shot.", "created_utc": 1361167436, "gilded": 0, "name": "t1_c8h2vsu", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": null, "body": "[deleted]", "created_utc": 1361117336, "gilded": 0, "name": "t1_c8gp28a", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "ipitythefoobar", "body": "It's only a few bucks a month, I'm not worried about it. I'll look into AWS though. I've been curious so this would be a good opp to get off my ass and try it.", "created_utc": 1361200093, "gilded": 0, "name": "t1_c8h7mpr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "shaggorama", "body": "Dedicated? Not virtual/shared? Man, I've been out of the loop. Any links on good cheap hosting?", "created_utc": 1361120769, "gilded": 0, "name": "t1_c8gpu7v", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "nandhp", "body": "Free for one year. After that a micro instance costs 0.02\u00a2/hour.", "created_utc": 1361120764, "gilded": 0, "name": "t1_c8gpu6u", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "bboe", "body": "You should be able to do this for free on [Heroku](http://www.heroku.com/).", "created_utc": 1361242241, "gilded": 0, "name": "t1_c8hkzzy", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "ipitythefoobar", "body": "Interesting. I'll take a look. Thanks for the lead!", "created_utc": 1361280311, "gilded": 0, "name": "t1_c8hseb9", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "nemec", "body": "The hosting company can easily kill your bot process if it's gulping down resources, so if you make it to the end of the first month and the bot's still running, you're probably in good shape.", "created_utc": 1361134862, "gilded": 0, "name": "t1_c8gtkdi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "grendelt", "body": "I'm with dreamhost and was going to run a bot, but can't get PRAW to install. :( Any ideas?", "created_utc": 1371585727, "gilded": 0, "name": "t1_calforj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "ipitythefoobar", "body": "Sorry, I'm not the expert. I assume that hosts aren't all that excited about bending over backwards to help with non-web stuff. You could try asking their tech support. Could also try asking on here which hosts people have had luck running bots on. If you have a local box to test on you can make sure that you are making the right calls to install it in the first place just to eliminate that as a possible cause. Good luck!", "created_utc": 1375672295, "gilded": 0, "name": "t1_cbh3ith", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18ov4m/advice_on_running_a_reddit_bot_written_in_python/"}, {"author": "bboe", "body": "Thanks for posting! > In this case that will work (even though I am not actually trying to get json data)... So, I'd like to ask bboe to consider putting some sort of _request functionality into supported \"user space\" Every request PRAW makes to reddit's API _should_ receive json data in response. Within PRAW there are only two requests that don't expect json as a response, one for getting a random subreddit, and one for uploading images. Aside from those two cases (which PRAW already handles internally) you should always get json back and that's why `request_json` is part of the _public_ interface and `_request` is not. If you find something that actually requires using `_request` through a supported API endpoint, please [file a bug](https://github.com/praw-dev/praw/issues/new).", "created_utc": 1360712860, "gilded": 0, "name": "t1_c8e4o16", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/"}, {"author": "shaggorama", "body": "You should consider adding [how to replace praw.Submission.all_comments_flat](http://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/)", "created_utc": 1360737304, "gilded": 0, "name": "t1_c8ecpgt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/18eeep/resolved_issues_porting_to_praw_2/"}, {"author": "shaggorama", "body": "Going through the recent commit messages I found helpers.flatten_tree, but if I use this repeatedly the number of items returned just grows and grows apparently without limit. from praw.helpers import flatten_tree as f subm = r.get_submission(submission_id = '178ki0' ) subm.num_comments # 34 len(subm.comments) # 20 len(f(subm.comments)) # 30 len(f(f(subm.comments))) # 82 len(f(f(f(subm.comments))) # 275 wtf?", "created_utc": 1359924284, "gilded": 0, "name": "t1_c88qddo", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/"}, {"author": "bboe", "body": "`flatten_tree` is the correct function in combination with an explicit `replace_more_comments` function call. The reason `flatten_tree` is not idempotent is because it doesn't destroy the `replies` attribute of the comments it flattens. This is intentional so that whenever you work with a comment in the flattened tree, you can still obtain its descendants.", "created_utc": 1359955442, "gilded": 0, "name": "t1_c88zdkj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/"}, {"author": "shaggorama", "body": "I'm not accustomed to working with comment trees, just flat comments. Can you maybe modify my code snipped to show me how to properly use the technique you're describing? Do you only ever need to call replace_more_comments() once, or would I need to call it repeatedly and then flatten the tree? EDIT: Is this all I have to do? all_comments_flat = flatten_tree( replace_more_comments( subm.comments ))", "created_utc": 1359990592, "gilded": 0, "name": "t1_c894r70", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/"}, {"author": "bboe", "body": "You want to do: subm.replace_more_comments() all_comments_flat = flatten_tree(subm.comments) Just FYI `replace_more_comments` returns a list of MoreComments objects that were not replaced. By default it only replaces 32 instances and will return the remainder.", "created_utc": 1360025547, "gilded": 0, "name": "t1_c89fx3t", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/"}, {"author": "_Daimon_", "body": "> Going through the recent commit messages I found helpers.flatten_tree PRAW has a [Changelog](https://github.com/praw-dev/praw/wiki/Changelog) which is usually a lot better place to find api changes than digging through the commit messages. Two excerpts from it * **[CHANGE]** Remove `comments_flat` property of Submission objects. The new `praw.helpers.flatten_tree` can be used to flatten comment trees. * **[CHANGE]** Remove `all_comments` and `all_comments_flat` properties of Submission objects. The now public method `replace_more_comments` must now be explicitly called to replace instances of `MoreComments` within the comment tree. `help(submission.replace_more_commets)` and `help(praw.helpers.flatten_tree)` provide more information about the usage on these functions, alternatively you can look at the PRAW package on [readthedocs.org](https://python-reddit-api-wrapper.readthedocs.org/en/latest/praw.html) which is also useful as a nice reference. I'll try looking at `flatten_tree` returning more and more results. It shouldn't do that. Hope that helped :)", "created_utc": 1359930727, "gilded": 0, "name": "t1_c88s9kn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17tn7r/replacement_for_all_comments_flat/"}, {"author": "Buttscicles", "body": "What are you using to check if an account exists?", "created_utc": 1359809666, "gilded": 0, "name": "t1_c881wt8", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "perezdev", "body": "Have you updated PRAW to the latest version? There was a recent change to the API that caused my libraries and apps to break. But PRAW was just recently updated to comply with the new changes.", "created_utc": 1359777844, "gilded": 0, "name": "t1_c87x8s6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "bboe", "body": "It's always good to be up-to-date, though the recent fix only addresses 409 errors when performing authentication switching.", "created_utc": 1359828935, "gilded": 0, "name": "t1_c885kxy", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "MrFanzyPanz", "body": "It did need updating, but the update did not fix the problem :/", "created_utc": 1360304649, "gilded": 0, "name": "t1_c8bgpsc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "bboe", "body": "We need more information. Does it fail on the same accounts? Do you get the same request failures using your browser (turn on request logging in praw.ini)? How are you checking to see if an account exists?", "created_utc": 1359828814, "gilded": 0, "name": "t1_c885jsd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "MrFanzyPanz", "body": "Sorry for not responding sooner, I just waded through midterm hell. I'm checking the accounts by using if user is None: break I'm getting the data through: for link in r.get_redditor(usertemp).get_submitted(limit = 200): ...csvwrite(link.***stuff***) ... I'm also gleaning comments using the get_comments() call. It fails after about 8 seconds, which is enough to go through about 3 accounts. It isn't terminating on any specific item or at any specific time. Some runs it will terminate on one account halfway through the data for a link, and another run it will record that data, but fail two lines later on a different account. I've run the code dozens of times now, and have found no consistency to the error other than that it takes roughly 8 seconds, which leads me to believe I'm being kicked from my connection to the server. It just says \"HTTPError: 404 Client Error: Not Found\"", "created_utc": 1360297997, "gilded": 0, "name": "t1_c8bf0ge", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "bboe", "body": "Oh I see. It sounds like you might be hitting shadow-banned users. That would represent a 404 error on a user's page. You'll just have to handle the 404 exception and skip the user in question. You can confirm the results by visting the user's page in your browser. You should also receive a 404 page.", "created_utc": 1360312198, "gilded": 0, "name": "t1_c8bi15j", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "MrFanzyPanz", "body": "No, the accounts are there. I'm using the same list of accounts to test the code over and over, and it will stop on either the 3rd or 4th account, and neither account returns a 404 when I go to the user's page. I really think I'm getting booted from the server, but I can't figure out why.", "created_utc": 1360456423, "gilded": 0, "name": "t1_c8ce7vk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "bboe", "body": "Post a simple version of your code that reproduces the issue and we can probably figure it out.", "created_utc": 1360464339, "gilded": 0, "name": "t1_c8cg7yk", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "MrFanzyPanz", "body": "def accountscrape(): #initialize necessary imports import praw import time r = praw.Reddit(user_agent='MrFanzyPanz Account Scraper :D') #open necessary files. txt format for now, CSV or ORM database once out of testing. name = open('dataset_accountlist.txt', 'r') userdata = open('accountdata.txt', 'a') userdata.write('username, submission type, submission number, subreddit, submission_created_utc , current_utc , score , usercommentK , userlinkK , is_mod , account_created_utc , text , num_comments , is_self , over_18 , ups, downs , url , is_imgur \\n') #pull a username from the namelist document. this must be a loop which #iterates until there are no more names in the file, since we do not know #how many are going to be in the file once data collection is finished. while True: usertemp = name.readline() #", "created_utc": 1360801851, "gilded": 0, "name": "t1_c8eswbj", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "bboe", "body": "You still haven't mentioned which usernames it actually fails on. My only thought is it is a whitespace issue. I would suggest this replacement: usertemp = usertemp.strip() if not usertemp: break", "created_utc": 1360803631, "gilded": 0, "name": "t1_c8etgza", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "MrFanzyPanz", "body": "Here are the accounts I'm scraping to test my code: sleepauger smashes11 Tony255 Based_Walrus crunchy_socks Gooky_chan jlew24asu Kwasbeb IAmBaconsReckoning kappi8 skyhit rec207 Proteon willmuirhead open_minded_canadian JHTBO NSFW_PORN_ONLY CharlieDarwin2 I changed the usertemp = ... according to your suggestion. Unfortunately, I still receive a 404 error after about 8 seconds. It keeps stopping in either Tony255's history or in the history of Based_Walrus. It isn't failing on a specific account or submission, it just raises a 404 error after about 8 seconds. **EDIT:** So I removed the entries before Tony255 and ran the code a few more times. Now it is consistently stopping on the post by Based_Walrus titled \"Ballin!\". It won't take the title of the post. Perhaps this has been the problem all along, despite that it didn't fail with this consistency before.", "created_utc": 1360823324, "gilded": 0, "name": "t1_c8ez932", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "bboe", "body": "Well it sounds like you're narrowing down the issue. Keep at it and you'll figure out what exactly is causing the issue.", "created_utc": 1360824004, "gilded": 0, "name": "t1_c8eze4q", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17q7b3/404_error_during_account_scraping/"}, {"author": "_Daimon_", "body": "There is a server-side cache that store most stuff for 30 secs, in addition the API Rules say you shouldn't request the same resource more than once every 30 secs. PRAW handles this by creating an internal cache, so if you request the same resource within a 30 sec timespan then you'll be served the cached results rather than sending a new request. It is this internal cache timeout you reduced, but obviously this does nothing to the server-side cache. **API Rules** > Most pages are cached for 30 seconds, so you won't get fresh data if you request the same page that often. Don't hit the same page more than once per 30 seconds. So just make a request every 30 seconds and ensure the limit is high enough that you get all the new results. But don't request far more than you need to reduce the load on reddit.", "created_utc": 1359165459, "gilded": 0, "name": "t1_c83pw96", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "IcyRespawn", "body": "Thanks, that clears up one of the points I brought up. The disappointing this is that as you say it, there is currently no way of getting reliably getting a submission within 10 seconds from its creation time. With that said, are you able to think of a reason for new threads not to appear even after a minute or two has passed? I could somehow manage with a 30-second delay, but more than that would be too much. EDIT: Would cycling my user-agent between three or four different ones do the trick? Is that an acceptable thing to do if I absolutely need it?", "created_utc": 1359165800, "gilded": 0, "name": "t1_c83pzn9", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "bboe", "body": "> With that said, are you able to think of a reason for new threads not to appear even after a minute or two has passed? I could somehow manage with a 30-second delay, but more than that would be too much. This very likely has to do with the Akamai cache. reddit uses Akamai to help reduce load on their servers, and so when you request a page as a non-authenticated user that request goes through Akamai which is set to cache the page for 1 minute (if I'm not mistaken). If you make a request just before a new item is posted it will take 1 minute before the cache updates and you see the new listing. One option is to login, and you _should_ be able to bypass the Akamai cache. If you do that, I'm going to strongly suggest only making one request every 30 seconds for the same URL. Breaking reddit's API rules (that PRAW enforces) is strongly discouraged. > Would cycling my user-agent between three or four different ones do the trick? Is that an acceptable thing to do if I absolutely need it? That likely won't accomplish anything other than getting your IP banned. Don't mess around with user-agent strings; there's nothing regarding the reddit API that you \"absolutely need\".", "created_utc": 1359167854, "gilded": 0, "name": "t1_c83qjyp", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "IcyRespawn", "body": "Oh, cool, I didn't know about the Akamai cache. That explains everything, as I was in fact using an UnauthenticatedReddit. As for user-agent cycling, I knew it was probably a bad idea. Couldn't hurt to ask though. Do you mind sharing your knowledge as to why these limitations are in place? More than just this specific script, I'm curious as to what user-agent strings actually mean, why they're used, and so on. Keep in mind that I'm completely new to web API usage. My specific script relies on getting the relevant data as soon as it's posted, otherwise it's probably too late for me to do anything with it - to give you more context, I'm trying to crawl a game's subreddit for codes which can be then redeemed to in-game items. It's a rather popular subreddit, though - so within 30 seconds the code would probably already have been used. I guess I can just scrap the idea if there is, in fact, no legitimate way to do what I am trying to do. Thanks for replying!", "created_utc": 1359172911, "gilded": 0, "name": "t1_c83rxep", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "bboe", "body": "> Do you mind sharing your knowledge as to why these limitations are in place? It's simply a resource issue. reddit has been designed to scale well to thousands (maybe even millions) of concurrent human users. It's awesome that reddit supports an API, but they make no money off of it (how do you serve ads to a non-interactive program)? So rather than worry too much about the load generated by API users, reddit simply places restrictions on their API. If they had an easy way to monetize the API, I bet you these restrictions would be gone and reddit's API would operate similarly to twitter's API. > More than just this specific script, I'm curious as to what user-agent strings actually mean, why they're used, and so on. It's simply an identifier for something that makes an HTTP request. There's a good wikipedia [article](http://en.wikipedia.org/wiki/User_agent) on the user agent. With respect to reddit, a proper user-agent string could allow the _admins_ to block misbehaving API clients without resorting to IP level blocks, and in the event the developer's username is provided the developer may be informed of the poor behavior. Additionally, statics can be collected that compares usage between different user-agents. Such information could be useful to help reddit eventually monetize their API. I can't say with any certainty that this is actually what the user agents are being used for, but those are definitely some of the things they can be used for. > I'm trying to crawl a game's subreddit for codes which can be then redeemed to in-game items. It's a rather popular subreddit, though - so within 30 seconds the code would probably already have been used. I want to briefly question the ethics of your stated goal. Do you think whoever is posting these redeemable codes intends them all to go to someone who is passively checking for them all the time? It's fun to be able to do such things, sure, but then you leave basically no chance for someone without the resources you have to participate in these give-a-ways. Ponder that for a bit.", "created_utc": 1359176577, "gilded": 0, "name": "t1_c83sugf", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "IcyRespawn", "body": "I can see where you're coming from in your last paragraph. To clarify, I don't intend to make use of this script to grab all the codes in a given thread - that would indeed be a very selfish and unethical thing to do. They're usually posted in bulk, but I only want to get one in this way, simply because I never managed to snag any of them before, and I think it would be a very cool thing to pull off. This is also the reason I didn't try anything in the form of automatically inputting it in the appropriate place - that's still my job. Basically, instead of hitting F5 every 5-10 seconds, I'm just letting a script do that every 30 seconds - thus still leaving a pretty good window of opportunity to those who are manually refreshing. ^(And of course consuming fewer resources.) With all of that said, I'd like to thank you for your extended guidance - you managed to shed light upon things I'm positive nobody has ever heard of (notably, the Akamai cache - using an Authenticated session did the trick as far as reducing delay to 30 seconds and below!), as well as explain some more basic concepts very clearly.", "created_utc": 1359200148, "gilded": 0, "name": "t1_c83wdil", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/17aiml/praw_getting_newest_posts_to_show_up_as_quickly/"}, {"author": "_Daimon_", "body": "This was handled via irc on #reddit-dev.", "created_utc": 1359137863, "gilded": 0, "name": "t1_c83gzrn", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/"}, {"author": "jokoon", "body": "now I need to query from date to date", "created_utc": 1359141702, "gilded": 0, "name": "t1_c83ibow", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/"}, {"author": "_Daimon_", "body": "Do you mean within a specific timeframe? That's not possible with the API. But you can use the created_utc timestamp of objects to divide them up into various time segments after retrieving them, this works perfectly for", "created_utc": 1359196645, "gilded": 0, "name": "t1_c83w28l", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/"}, {"author": "jokoon", "body": "I wanted to use time frame to be able to retrieve all my likes or comments.", "created_utc": 1359197890, "gilded": 0, "name": "t1_c83w66r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/"}, {"author": null, "body": "Care to state what the solution was so if people stumble upon this post in the future, they won't have a conniption fit?", "created_utc": 1359157148, "gilded": 0, "name": "t1_c83njmd", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/"}, {"author": "_Daimon_", "body": "It was a keyword argument error. `set_oauth_app_info` doesn't take redirect_url as an argument it takes redirect_ur**i**. This is the same name reddit uses on the [app page](https://ssl.reddit.com/prefs/apps/). But there is no reason to use OAuth if it's for your own account. Login in with the username/password combination and retrieve the things you want. There is a very in-depth tutorial about [OAuth with PRAW](https://github.com/praw-dev/praw/wiki/OAuth) on the wiki.", "created_utc": 1359194007, "gilded": 0, "name": "t1_c83vtxy", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/179di4/trying_to_get_my_comments_and_likes/"}, {"author": "bboe", "body": "/r/redditdev is not the place to request programs; for that try /r/forhire. If you're interested in writing it yourself, I recommend you check out PRAW and visit /r/learnpython or the associated learning community for whatever language you want to use.", "created_utc": 1358895449, "gilded": 0, "name": "t1_c81qlo7", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1730dr/a_request_for_saved_links/"}, {"author": "takitesi", "body": "Gotcha ok thanks!", "created_utc": 1358895510, "gilded": 0, "name": "t1_c81qmii", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1730dr/a_request_for_saved_links/"}, {"author": "Pi31415926", "body": "You could also try suggesting it as an idea in /r/ideasfortheadmins, or you could try posting your request in /r/greasemonkey. This said, if you know some PHP + MySQL it's pretty simple to grab the XML and parse it into a table. Although I am admittedly saving the XML to disk manually. Then I just feed the saved file to my PHP script and voila.", "created_utc": 1358895901, "gilded": 0, "name": "t1_c81qrec", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1730dr/a_request_for_saved_links/"}, {"author": "_Daimon_", "body": "It's usually a good idea to post the traceback with a bug report. That makes debugging far easier. It appears the error is not in PRAW, but in update_checker (a PRAW dependency) see the traceback included at the bottom. Looking at update_checkers source, it's probably because it uses the url \"'http://csil.cs.ucsb.edu:65429/check'\" and that url is blocked by PythonAnywhere. See bboe's response. EDIT: Just confirmed it. This is the origin of the error. What you need to do is simply disable update_checker in your programs. So instead of doing r = praw.Reddit(\"Descriptive and unique useragent here. See API Rules\") You do r = praw.Reddit(\"Descriptive and unique useragent here. See API Rules\", disable_update_check=True) **TRACEBACK:** import praw r = praw.Reddit('Descriptive and unique useragent here. See API Rules') Traceback (most recent call last): File \"\", line 1, in File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/praw/__init__.py\", line 699, in __init__ super(AuthenticatedReddit, self).__init__(*args, **kwargs) File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/praw/__init__.py\", line 393, in __init__ super(OAuth2Reddit, self).__init__(*args, **kwargs) File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/praw/__init__.py\", line 242, in __init__ update_check(__name__, __version__) File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/update_checker.py\", line 94, in update_check result = checker.check(package_name, package_version, **extra_data) File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/update_checker.py\", line 33, in check data = response.json() File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/requests/models.py\", line 604, in json return json.loads(self.text or self.content) File \"/usr/local/lib/python2.7/json/__init__.py\", line 326, in loads return _default_decoder.decode(s) File \"/usr/local/lib/python2.7/json/decoder.py\", line 366, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File \"/usr/local/lib/python2.7/json/decoder.py\", line 384, in raw_decode raise ValueError(\"No JSON object could be decoded\") ValueError: No JSON object could be decoded", "created_utc": 1358716151, "gilded": 0, "name": "t1_c80fnjn", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "red_foot", "body": "Thanks! Amazing work, I'll remember to put the trace back in there next time. Can you explain more about what lead you to believe update_checker was the culprit? I'd like to learn how to sort through tracebacks myself better. **EDIT:** This is probably a really stupid question, I'm just so zonked right now. Unbelievably grateful though.", "created_utc": 1358739783, "gilded": 0, "name": "t1_c80n6fl", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "_Daimon_", "body": "It's not a stupid question. Debugging is really MIA in CS / programming / Python tutorials education. I'll start with a simpler example to discuss reading tracebacks, then how that is relevant to the example. Try saving the following code snippet as 'tracebacktest.py' on your pc and running it. def crash(): a = all_good() b = this_is_bad() def all_good(): return 5 def this_is_bad(): something_something = 20 something_else = 30 5 / 0 crash() This program will crash and create the following traceback when run. Traceback (most recent call last): File \"tracebacktest.py\", line 11, in crash() File \"tracebacktest.py\", line 3, in crash b = this_is_bad() File \"tracebacktest.py\", line 11, in this_is_bad 5 / 0 ZeroDivisionError: integer division or modulo by zero The last line tells us the program crashed because of a ZeroDivisionError and gives us the error message. The last line is the final line of code that was executed `5 / 0`, the line above is the filename, line number and function name where the crashing line is located. The line above is the line that called this function `b = this_is_bad` and the line above is the filename, line number and function number of where that line is located. It continues like this all the way back up to the very first function call we made. Notice that the call to `this_is_good` is not part of the traceback. This is because the traceback contains the direct path your program took from initialization. Detours are not part of it. Back to how this relates to your traceback. If you read the filenames, you'll see that they are part of packages. And that they call each other in the following order PRAW -> update_checker -> requests -> json. So the error is either internal in json, which is extremely unlikely since it's an old built-in library, or one call somewhere in the chain is made with invalid arguments. I see the error message is \"No JSON object could be decoded\", I know this error is raised when you call `json.loads` with a string that cannot be decoded into a json object, such as empty string. So I make an educated guess that this is the cause of the crash and go upwards. A few lines up are these lines File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/update_checker.py\", line 33, in check data = response.json() File \"/home/Damgaard/virt_env/praw/lib/python2.7/site-packages/requests/models.py\", line 604, in json return json.loads(self.text or self.content) So if `self.text or self.content` evaluates to a string that cannot be decoded into a json object .At this point I read bboe's reply which state that certain urls are blocked by PythonAnywhere and return a 403 error. This makes me think that what PythonAnywhere returns instead of the correct result is either an empty string or some HTMl i.e. not a JSON object. I didn't do it at the time, but if you do it then you\u00b4ll get the following. '\\n \\n Access Denied\\n \\n\\nAccess Denied\\n\\nAccess to arbitrary websites is onl y available to Premium users.\\nYou can sign up for a premium account at http://www.pythonanywhere.com/account/\\nAlternativ ely, if you want to suggest something to add to our whitelist \\n(http://www.pythonanywhere.com/whitelist) drop us a line a t \\nsupport@pythonanywhere.com \\n\\n\\n\\n\\n\\nGenerated Mon, 21 Jan 2013 04:51:43 GMT by hansel-liveproxy (squid/2.7.STABLE9)\\n\\n\\n' So I now know that in the function `check` in `update_checker` there is a line that tries to do `json.loads` with an invalid string and this cause a crash. Looking at the code on github and you see the url sounds like something that wouldn't be automatically whitelisted. A manual test later with `requests.get('http://csil.cs.ucsb.edu:65429/check')` confirms it. Then all that's left is disabling it update_checker and see if we still get a crash. We don't, so everything is good :) tl;dr I should write a blog or something....", "created_utc": 1358744387, "gilded": 0, "name": "t1_c80oiq2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "red_foot", "body": "You should write several blogs about debugging with python. That was awesome, but is that csil.cs.ucsb.edu url some insider information that you know about?", "created_utc": 1358780274, "gilded": 0, "name": "t1_c80ub9o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "bboe", "body": "If you're using a free account, it appears as if it's a PythonAnywhere restriction. > Why do I get a \"403 Forbidden\" error when accessing a website from PythonAnywhere? > We have implemented blocking for Free accounts to reduce abuse of PythonAnywhere. We do allow access to some sites that we think are useful to a large number of our users (all sites that host PyPI modules, for instance). If there is a site that you think we should add to the list of allowed sites, let us know. Source: https://www.pythonanywhere.com/help/", "created_utc": 1358714758, "gilded": 0, "name": "t1_c80f7nr", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "_Daimon_", "body": "See my response, it appears the url you uses in `update_checker` is not allowed by PythonAnywhere. I'm not familiar with `update_checker`s codebase. But it seems you can fix it by adding this code snippet after [catching Requests exceptions](https://github.com/bboe/update_checker/blob/master/update_checker.py#L32) if not response.ok: return None # Raise an exception or whatever you want.", "created_utc": 1358717162, "gilded": 0, "name": "t1_c80fypp", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "_Daimon_", "body": "As for your second error. It appears PythonAnywhere does not support SSL at least not for free accounts. Reddit requires SSL for login, both normally or with OAuth, it's a security thing. So without SSL PRAW cannot login. You'll have to ask PythonAnywhere staff on how to fix this. Just to demonstrate that this is a PythonAnywhere thing, not PRAW or Reddit. This gives the same error requests.get('https://www.google.dk')", "created_utc": 1358745968, "gilded": 0, "name": "t1_c80oyc8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "bboe", "body": "reddit does not require ssl to login, however, it's strongly encouraged. If you are okay with sending your username and password in plaintext, then you can overwrite the `reddit` site definition in `praw.ini` such that it does not define `ssl_host` (copy everything but that line when re-defining the `reddit` site). Alternatively you can use a different site-name altogether. The config file [wiki](https://github.com/praw-dev/praw/wiki/The-Configuration-Files) page may be of use. Edit: grammar", "created_utc": 1358747583, "gilded": 0, "name": "t1_c80pdr6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "_Daimon_", "body": "Cool. Thanks for clearing that up :)", "created_utc": 1358747780, "gilded": 0, "name": "t1_c80pfig", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16y13b/pythonanywherecom_and_praw/"}, {"author": "bboe", "body": "Giving people access to run arbitrary python code that can open network connections is not a great idea. PRAW will run on something like heroku, however.", "created_utc": 1358442909, "gilded": 0, "name": "t1_c7ymrdv", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16r0m0/has_anyone_built_a_web_app_that_works_with_praw/"}, {"author": "takitesi", "body": "Thanks, I'll try out Heroku after work!", "created_utc": 1358448073, "gilded": 0, "name": "t1_c7yoka8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16r0m0/has_anyone_built_a_web_app_that_works_with_praw/"}, {"author": "handmadeby", "body": "Pythonanywhere might do it too", "created_utc": 1358450944, "gilded": 0, "name": "t1_c7yplh5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16r0m0/has_anyone_built_a_web_app_that_works_with_praw/"}, {"author": "_Daimon_", "body": "Don't feel bad about asking questions. I've tried writing about discovering more information about methods, functions etc in [writing a bot](https://github.com/praw-dev/praw/wiki/Writing-A-Bot). Stupid title, I know. I'm going to rename it, but I'm going to wait a bit more on that until I'm more sure about the future structure of the documentation. I don't want to rename it twice. I would like to hear any critisism of the documentation and suggestions on how to make it better. Feedback on the documentation is something I never get, which makes it hard to make good documentation because people don't tell me what they want or what the current documentation problems are. Anyway, try doing this help(comment) This will print the following Help on Comment in module praw.objects object: class Comment(Approvable, Deletable, Distinguishable, Editable, Inboxable, Report able, Voteable) | A class for comments. | | Method resolution order: | Comment | Approvable | Deletable | Distinguishable | Editable | Inboxable | Reportable | Voteable | RedditContentObject | __builtin__.object | | Methods defined here: | | __init__(self, reddit_session, json_dict) | | __unicode__(self, *args, **kwargs) | | ---------------------------------------------------------------------- | Data descriptors defined here: | | is_root | Indicate if the comment is a top level comment. | | permalink | Return a permalink to the comment. | | replies | Return a list of the comment replies to this comment. | | score | Return the comment's score. | | submission | Return the submission object this comment belongs to. Looking at that, score sounds like what you want. Alternatively, in situations like this both `vars` and `dir` are quite helpful. This information is specific to this comment. For this example I choose [a recent r/python comment](http://www.reddit.com/r/Python/comments/16p63p/tackling_graphical_programming_in_python_i_had_to/c7y5f1d). Have it open on another tab, while reading the following. from pprint import pprint # Just makes the output more readable pprint(vars(comment)) {'_info_url': 'http://www.reddit.com/button_info/', '_populated': true, '_replies': [], '_submission': , '_underscore_names': ['replies'], 'approved_by': none, 'author': redditor(user_name='phaedrusalt'), 'author_flair_css_class': none, 'author_flair_text': none, 'banned_by': none, 'body': u'i bet they get a little suspicious when you have a copy of *work*~~pla y~~boy on your desk.', 'body_html': u'<div class=\"md\"><p>i bet they get a little suspicious when you have a copy of <em>work</em><del>play</del>boy on your desk.</p>\\n</div>', 'created': 1358402223.0, 'created_utc': 1358373423.0, 'downs': 0, 'edited': false, 'gilded': 0, 'id': u'c7y5f1d', 'likes': none, 'link_id': u't3_16p63p', 'name': u't1_c7y5f1d', 'num_reports': none, 'parent_id': u't3_16p63p', 'reddit_session': , 'subreddit': , 'subreddit_id': u't5_2qh0y', 'ups': 15} Comparing the output with what we can see on the webend, it seems that `ups` is almost identical to the comments upvote. Considering reddits obfuscation of upvotes/downvotes `ups` is very likely to be the comments upvotes. `downs` seem clear to be the downvotes. So from this we could get the score (what you called karma) by doing `karma = comment.ups - comment.downs`. There is also the `dir` builtin function, which gives us a list of all methods and attributes in the object. This is especially interesting as PRAW has multiple property-decorated methods. Eg, methods that pretend to be attributes. They will be listed with `dir`, not with `vars` but be callable just like an attribute. I removed class methods and private methods from the output to keep it a bit shorter. [ 'approve', 'approved_by', 'author', 'author_flair_css_class', 'author_flair_text', 'banned_by', 'body', 'body_html', 'clear_vote', 'content_id', 'created', 'created_utc', 'delete', 'distinguish', 'downs', 'downvote', 'edit', 'edited', 'from_api_response', 'gilded', 'id', 'is_root', 'likes', 'link_id', 'mark_as_read', 'mark_as_unread', 'name', 'num_reports', 'parent_id', 'permalink', 'reddit_session', 'remove', 'replies', 'reply', 'report', 'score', 'submission', 'subreddit', 'subreddit_id', 'undistinguish', 'ups', 'upvote', 'vote'] Looking this list over, most can be easily discarded as having anything to do with a comments karma. The score sounds like a possibility, so we try `s.score` and it returns 15. Which matches the score of the comment we used as an example. Again, if you have any feedback whatsoever on the current documentation then please tell us so we can improve it. This goes for any user of PRAW, not just OP.", "created_utc": 1358387341, "gilded": 0, "name": "t1_c7ya5km", "num_comments": null, "score": 8, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16q02d/praw_karma/"}, {"author": "lamerx", "body": "Wow , thank you very much. Thats JUST want i needed sir. >Again, if you have any feedback whatsoever on the current documentation then please tell us so we can improve it. This goes for any user of PRAW, not just OP. I will daimon", "created_utc": 1358388565, "gilded": 0, "name": "t1_c7yak7q", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16q02d/praw_karma/"}, {"author": "slyf", "body": "Will you accept a pull request for wiki support (Against 2)? Kind of feel like doing that in my free time. (HAH! What free time?).", "created_utc": 1358261696, "gilded": 0, "name": "t1_c7xazwl", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/"}, {"author": "bboe", "body": "Absolutely! I encourage anyone to implement missing API functionality and make pull requests. For tracking purposes it would be nice if you file an issue about what you intend to add. Even if you don't have the time to add features, if there is something you wish PRAW supported, please create an issue on github.", "created_utc": 1358274241, "gilded": 0, "name": "t1_c7xeppm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/"}, {"author": "Bornhuetter", "body": "Nice! I appreciate the effort you put into maintaining this.", "created_utc": 1358267654, "gilded": 0, "name": "t1_c7xckk2", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/"}, {"author": "bboe", "body": ":-)", "created_utc": 1358274287, "gilded": 0, "name": "t1_c7xeqbq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16m0uu/praw_20_is_coming_release_in_2_days/"}, {"author": "dfnkt", "body": "Plain and simple, nice job.", "created_utc": 1357836342, "gilded": 0, "name": "t1_c7uhqh3", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/"}, {"author": "mpheus", "body": "Thanks! I was going to use the Reddit API but was really amazed to discover PRAW that made everything easier. Really loving Python and the vast number of libraries.", "created_utc": 1357862049, "gilded": 0, "name": "t1_c7uqryr", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/"}, {"author": "dfnkt", "body": "Wrappers generally make things more idiomatic to do in the language they support. I'm not a pythonista but I think the parlance is 'pythonic' in this case.", "created_utc": 1357868735, "gilded": 0, "name": "t1_c7ussnr", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/"}, {"author": "bboe", "body": "> # reddit doesn't shows new posts made to a subreddit without logging in That's not entirely correct. By default reddit uses the 'rising' sort which never appears to work. When you login you get whatever default view your user has set which is whatever you viewed last. If you don't want to login then you should use the `get_new_by_date` function instead of `get_new`.", "created_utc": 1357866569, "gilded": 0, "name": "t1_c7us5ol", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/"}, {"author": "mpheus", "body": "Thanks for correcting. I wanted to be sure so I opened the new tabs of a few sub-reddits without logging in and as you said, it showed 'rising' sort which gave no results. I didn't notice the other 'new' sort, so I thought that it required logging in. I will update the script to use `get_new_by_date` function.", "created_utc": 1357899008, "gilded": 0, "name": "t1_c7uzr0o", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/16bh8j/wrote_my_first_python_script_using_praw_to_check/"}, {"author": "bboe", "body": "If a user is deleted then the author value is `None`. Your code needs to check for that before attempting to fetch the name of the author. This is currently inconsistent in a few places because reddit will return `[deleted]` in-place of None. In the future that should not be the case. You have two options. (1) You can ignore deleted user submissions/comments by doing: if not post.author: continue Or (2) you can group all the deleted user info into a single account: if not post.author: name = '[deleted]' else: name = post.author.name", "created_utc": 1357513241, "gilded": 0, "name": "t1_c7s8zx9", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/"}, {"author": "MrFanzyPanz", "body": "Thanks! That solved it. On a related note, what will praw return if there aren't any posts left in a subreddit? Say the subreddit is new and only has 50 posts on it, but I still tell praw to pull the first 500 posts? Does it return [none] like the deleted accounts do?", "created_utc": 1357778032, "gilded": 0, "name": "t1_c7u4rqg", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/"}, {"author": "bboe", "body": "All of the listings return a generator. That means it'll automatically end when the last one is reached and thus if you ask for 50 item you will get at __most__ 50 items, but maybe fewer.", "created_utc": 1357780652, "gilded": 0, "name": "t1_c7u5kee", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/"}, {"author": "MrFanzyPanz", "body": "Thank you! I'll just check if there's less than 500 after the iteration is complete and fill the empty spaces in with null entries. Redditdev, you have been amazingly helpful! Thanks to Daimon and bboe!", "created_utc": 1357788406, "gilded": 0, "name": "t1_c7u7zw4", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/"}, {"author": "Falmarri", "body": "Use python's csv module instead of rolling your own. http://docs.python.org/2/library/csv.html", "created_utc": 1357515154, "gilded": 0, "name": "t1_c7s9me0", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/"}, {"author": "Deimorz", "body": "That usually happens if the author deleted the post or their account, which shows on reddit as \"[deleted]\". You could just change that line to something like: if post.author: name = post.author.name else: name = '[deleted]' # or whatever value you want to use", "created_utc": 1357513230, "gilded": 0, "name": "t1_c7s8zsd", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/1630jj/praw_is_returning_postauthorname_errors/"}, {"author": "Deimorz", "body": "The praw wiki has a number of examples of how to do common tasks with it: https://github.com/praw-dev/praw/wiki", "created_utc": 1357226265, "gilded": 0, "name": "t1_c7qbd1b", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15w2cs/help_with_submitting_with_praw/"}, {"author": "bheklilr", "body": "I don't know how I missed that... I had looked through that page about 4 times now. Looks like I need to get my eyes checked", "created_utc": 1357226439, "gilded": 0, "name": "t1_c7qbegv", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15w2cs/help_with_submitting_with_praw/"}, {"author": "_Daimon_", "body": "At the bottom of that page are a bunch of example applications/scripts using PRAW. One is [newsfrbot](https://github.com/gardaud/newsfrbot) which *\"parses RSS feeds from some major french publications and posts them to relevant subreddits.\"*. That might be useful to you :) You're more than welcome to add your own application to that list when you are done writing it.", "created_utc": 1357253081, "gilded": 0, "name": "t1_c7qjzcm", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15w2cs/help_with_submitting_with_praw/"}, {"author": "Deimorz", "body": "Nope, not really. Almost all listings on the site have a cap of 1000 items.", "created_utc": 1357186302, "gilded": 0, "name": "t1_c7q3ykg", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/"}, {"author": "dakta", "body": "All lists of submissions and comments like these are capped at 1000. It's part of the configuration for red it's sophisticated ;) caching system. As of right now, there is no way to get around it. Maybe in the future the admins will implement an unlimited listing feature for a small fee per use (a couple cents would cover actual costs), or a time restricted unlimited listing feature, or even just a way for a user to download a complete copy of their own profile, but until then you're out of luck.", "created_utc": 1357200553, "gilded": 0, "name": "t1_c7q7lcy", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/"}, {"author": "burntsushi", "body": "Dang. Thanks!", "created_utc": 1357232627, "gilded": 0, "name": "t1_c7qd5xd", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15v7l4/retrieving_a_users_comments_seems_to_be_limited/"}, {"author": "_Daimon_", "body": "I can provide you a short example of how to do it :) You need moderator credentials in the subreddit to run this script. Also it will raise an exception if the CSS is invalid. Speaking of documentation, I'm sure you already know of our exisiting documentation on the [Github wiki page](https://github.com/praw-dev/praw/wiki)? import praw r = praw.Reddit(USERAGENT) r.login() subreddit = r.get_subreddit('NAME OF YOUR SUBREDDIT') current_css = subreddit.get_stylesheet()['stylesheet'] subreddit.set_stylesheet(FANCY_NEW_CSS) # Overrides existing CSS.", "created_utc": 1356871561, "gilded": 0, "name": "t1_c7o8fns", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15nuau/praw_css/"}, {"author": "lamerx", "body": "I have been trying all day to get even this to work but im getting no luck at all. I know its something simple but cant for the life of me figgure out what it is import praw r = praw.Reddit(USERAGENT) r.login() subreddit = r.get_subreddit('NAME OF YOUR SUBREDDIT') current_css = subreddit.get_stylesheet()['stylesheet'] subreddit.set_stylesheet(current_css) # Overrides existing CSS. That code when ran against my testing sub gives me the following error APIException: (BAD_CSS) `invalid css` on field `stylesheet_contents` All i did was read the style sheet and try to put it back in.", "created_utc": 1356915877, "gilded": 0, "name": "t1_c7oiepc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15nuau/praw_css/"}, {"author": "_Daimon_", "body": "It sounds like your existing css may be invalid. I'm not really that good with css, so I can't confirm that manually. But try setting the css to '\\n', I know that's valid css and it works on my end.", "created_utc": 1356917352, "gilded": 0, "name": "t1_c7oisyq", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15nuau/praw_css/"}, {"author": "lamerx", "body": "I am now..:) Thank you very much.", "created_utc": 1356901826, "gilded": 0, "name": "t1_c7oej70", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15nuau/praw_css/"}, {"author": "bboe", "body": "The latest release of PRAW does not yet support reddit's OAuth. I actually have been working today on fully integrating a pull request /u/intortus made a while back. Soon, I will get the last bit working, then I'll push to master as one of the final release candidates for version 1.1 of PRAW. To answer your question: once you have an access token, you need to add the following header to all requests: Authorization: bearer Edit: The latest git version of PRAW now should have all the necessary OAuth2.0 stuff integrated. Specifically: * r.get_authorize_url [[src](https://github.com/praw-dev/praw/blob/89e31cb39cb0a66d4c79f7c8e235b2835b3f0d6a/praw/__init__.py#L392)] * r.get_access_token [[src](https://github.com/praw-dev/praw/blob/89e31cb39cb0a66d4c79f7c8e235b2835b3f0d6a/praw/__init__.py#L374)] * r.refresh_access_token [[src](https://github.com/praw-dev/praw/blob/89e31cb39cb0a66d4c79f7c8e235b2835b3f0d6a/praw/__init__.py#L402)]", "created_utc": 1356689109, "gilded": 0, "name": "t1_c7n8qcr", "num_comments": null, "score": 6, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/"}, {"author": "downbound", "body": "Yeah, I've got my token but I'm trying you headers. . no love yet [code](http://pastie.org/5588142)", "created_utc": 1356693711, "gilded": 0, "name": "t1_c7n95ad", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/"}, {"author": "downbound", "body": "ug, got it finally but I have to use v1 =( resp = requests.get(\"https://oauth.reddit.com/api/v1/me.json\", headers={\"Authorization\":\"bearer \"+ key})", "created_utc": 1356695131, "gilded": 0, "name": "t1_c7n9a87", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15kc2j/embarrased_i_need_the_hello_world_of_api/"}, {"author": "rram", "body": "That is not the ban message. You're accessing ssl.reddit.com without a secure connection (i.e. You're not using https://ssl.reddit.com/). What do you mean by \"I cannot get any information with PRAW\"? A the least PRAW should provide you with the HTTP status code and response body.", "created_utc": 1356584140, "gilded": 0, "name": "t1_c7mn8hc", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "expiredtofu", "body": ">A the least PRAW should provide you with the HTTP status code and response body. How would I get that? Sorry, but I'm very new to using PRAW.", "created_utc": 1356584242, "gilded": 0, "name": "t1_c7mn9c9", "num_comments": null, "score": -2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "rram", "body": "PRAW show throw an exception if you get a page with a bad status code. You can catch that exception and get the data from there: import praw r = praw.Reddit('test by /u/rram') sr = r.get_subreddit('rram') try: for l in sr.get_new(): print l except Exception as e: print \"Code: %s\" % e.code print \"Reason: %s\" % e.reason print \"URL: %s\" % e.url print \"Body: \\n%s\" % e.read() That should print something like: Code: 403 Reason: Forbidden URL: http://www.reddit.com/r/rram/new.json Body: {\"error\": 403} Note: My knowledge of PRAW is limited, so this might not work 100% of the time.", "created_utc": 1356599271, "gilded": 0, "name": "t1_c7mq392", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "bboe", "body": "> I think I ran my script a bit too fast. Unless you manually changed the rate limit settings, you needn't worry about such things with PRAW.", "created_utc": 1356584663, "gilded": 0, "name": "t1_c7mncq8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "expiredtofu", "body": "Yea, I just realized that my script was being problematic. Thanks for the timely response!", "created_utc": 1356584795, "gilded": 0, "name": "t1_c7mndsd", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": null, "body": "[deleted]", "created_utc": 1356588612, "gilded": 0, "name": "t1_c7mo8ui", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "expiredtofu", "body": "Sorry, it was poor programming on my part.", "created_utc": 1356589249, "gilded": 0, "name": "t1_c7modxj", "num_comments": null, "score": 0, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/15i5c8/is_it_possible_for_my_bot_to_be_unbanned/"}, {"author": "bboe", "body": "If you want the submissions from /r/pics you should do for submission in r.get_subreddit('pics').get_top(limit=10): print submission.title If you want front page submissions (equivalent to www.reddit.com/.json) then you should do: for submission in r.get_front_page(limit=10): print submission.title", "created_utc": 1355859312, "gilded": 0, "name": "t1_c7in4ae", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152d2w/praw_url_parameters/"}, {"author": "_Daimon_", "body": "Hey, I see that Bboe has already answered your specik question. If you have any more questions, then you might want to check out the [wiki]( https://github.com/praw-dev/praw/wiki). It has a bunch of examples and some tutorials, that should help you getting started with PRAW. :)", "created_utc": 1355865327, "gilded": 0, "name": "t1_c7ip09z", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152d2w/praw_url_parameters/"}, {"author": "bboe", "body": "Edit: Oops I had it backwards. You have an older version of PRAW where the name is `get_saved_links`. Either update, or use `get_saved_links`. In this particular case, I have not made a release including this change hence the confusion between the documentation and the \"stable\" version. Some of this information is contained in the [changelog](https://github.com/praw-dev/praw/wiki/Changelog). The method was renamed from `get_saved_links` to `get_saved` in a recent version. The wiki documentation will always reflect the latest stable version. Sorry for the confusion. When in doubt, python's `dir` command is your friend. If you ran: from pprint import pprint pprint(dir(r.user)) you will see a list of available functions on the `r.user` (LoggedInRedditor) object.", "created_utc": 1355858464, "gilded": 0, "name": "t1_c7imuej", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "LudoA", "body": "Of course, thanks... I did consider the name could've changed, but looking at https://github.com/praw-dev/praw/blob/master/praw/objects.py#L518 I saw *get_saved = _get_section('saved')* so I thought that wasn't the issue. I'm now looking at LoggedInExtension to understand a bit better how praw works :) Thanks Bryce.", "created_utc": 1355858902, "gilded": 0, "name": "t1_c7imzig", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "bboe", "body": "PRAW currently has two branches 1.0 and 1.1 which is making things complicated. I'm hopefully going to release 1.1 soon so I no longer have to maintain the separate releases.", "created_utc": 1355859153, "gilded": 0, "name": "t1_c7in2fh", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "LudoA", "body": "I installed PRAW using pip earlier today. I have 1.0.16 installed. However, I have neither get_saved() nor get_saved_links() available: >>> dir(r.user) ['__class__', '__delattr__', '__dict__', '__doc__', '__eq__', '__format__', '__getattr__', '__getattribute__', '__hash__', '__init__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__unicode__', '__weakref__', '_get_json_dict', '_info_url', '_mod_subs', '_populate', '_populated', '_underscore_names', '_url', 'comment_karma', 'compose_message', 'content_id', 'created', 'created_utc', 'friend', 'from_api_response', 'get_comments', 'get_disliked', 'get_hidden', 'get_inbox', 'get_liked', 'get_modmail', 'get_overview', 'get_sent', 'get_submitted', 'get_unread', 'has_mail', 'has_mod_mail', 'id', 'is_friend', 'is_gold', 'is_mod', 'link_karma', 'mark_as_read', 'modhash', 'my_contributions', 'my_moderation', 'my_reddits', 'name', 'reddit_session', 'refresh', 'send_message', 'unfriend']", "created_utc": 1355863082, "gilded": 0, "name": "t1_c7iobvi", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "_Daimon_", "body": "`get_saved_links` is in the `Reddit` object. So you do import praw r = praw.Reddit(user_agent='example') r.login('LudoA', 'foobar') r.get_saved_links() Sorry for the confusion, I'll add a comment to the wiki about the difference between the last stable edition and the last released verision tomorrow.", "created_utc": 1355865909, "gilded": 0, "name": "t1_c7ip6ot", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "LudoA", "body": "Great, that explains it! It works now. Thanks a lot for the help.", "created_utc": 1355872391, "gilded": 0, "name": "t1_c7ir2q1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "bboe", "body": "Doh! Right. I probably should not have backported these changes to the 1.0 branch. Oh well.", "created_utc": 1355866790, "gilded": 0, "name": "t1_c7ipgup", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "_Daimon_", "body": "I'm certainly looking forward to when we only have 1 branch again, makes everything much easier.", "created_utc": 1355867086, "gilded": 0, "name": "t1_c7ipk8s", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "LudoA", "body": "By the way, other methods, such as *my_contributions()* and *get_liked()* are working correctly: >>> r.user.get_liked() By passing that to list() and passing limits=100, I can see that the number of praw.objects.Submission objects contained by the list changes as I would expect. Which leads me to think that I'm overlooking something obvious...", "created_utc": 1355858267, "gilded": 0, "name": "t1_c7ims5j", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/152c5b/praw_cant_access_saved_links_no_attribute_get/"}, {"author": "bboe", "body": "`user.has_mail` is an attribute that is set upon login and is only updated when the user object is updated. None of the PRAW actions, aside from directly fetching the user object update the user object. You can manually update the user object by running `r.user.refresh()` before checking the `has_mail` attribute. That should solve the problem. However, it's more efficient to simply check for the presence of new messages, than to first check for messages (refresh the user info), and then conditionally retrieve the messages.", "created_utc": 1355086506, "gilded": 0, "name": "t1_c7du3u1", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "esacteksab", "body": "You're recommending something like r.user.get_unread? Or am I missing another way via PRAW to get unread messages? Before, I'd touch /api/me.json and 'has_mail' http://www.reddit.com/dev/api#GET_api_me.json Which I'm guessing is what r.user.has_mail is doing.", "created_utc": 1355086857, "gilded": 0, "name": "t1_c7du7uf", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "bboe", "body": "Yes, you would use `r.user.get_unread` to retrieve the new message. My suggestion depends on what you really want to do. If you only want to know if there is new messages, than using `refresh` + `has_messages` works just as well as doing `bool(list(r.user.get_unread(limit=1)))`. However, if you inevitably want to retrieve the new messages, then there is no point in checking first if there are new messages. `r.user.has_mail` simply returns the value of the `has_mail` attribute associated with the user object (returned from /api/me.json). This value is only populated when the user logs in (when the `LoggedInRedditor` object is created), or upon manually calling `refresh`. Also note, that simply retrieving unread messages (1) does not automatically mark them as read. You can do that by calling the `mark_as_read` function on the individual message. (2) Marking all messages as read through the API does not automatically clear the orange-red icon which is the same as the `has_messages` attribute. Thus it is possible to have no unread messages, but still see the orange-red icon on the website (or see that `has_mail` is True on a new session or after calling refresh).", "created_utc": 1355087560, "gilded": 0, "name": "t1_c7dufmt", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "esacteksab", "body": "All I'm doing is seeing if a message exists. I'm playing with an Arduino, and I want to light an LED if a message exists. I don't (right now) care how many messages exists, but simply if a message exists. bool(list(r.user.get_unread(limit=1))) is perfect! Thanks!", "created_utc": 1355087833, "gilded": 0, "name": "t1_c7duiok", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "bboe", "body": "Cool. Note that the result of this query is cached for 30 seconds as per reddit's API rules. The same is true when refreshing the user object.", "created_utc": 1355087909, "gilded": 0, "name": "t1_c7dujik", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "esacteksab", "body": "yeah, read the \"what's wrong\" section...I have no desire to bang the shit out of the API for a \"toy\". Even as much as I want to go all nuts on \"F5\". Thanks for all your help and quick response! Thanks!", "created_utc": 1355088198, "gilded": 0, "name": "t1_c7dumrb", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/14kakv/does_praw_cache_results_with_ipython/"}, {"author": "bboe", "body": "/u/Deimorz was looking into doing something similar. If you want to effectively count the requests, add a wrapper to `helpers._request`. With PRAW's caching and automatic handling of listing pages you can't easily tell how many requests are actually being made. However, what I think you really want is to make a PRAW request manager that runs as a single process, and has many associated clients that make PRAW requests through the manager. It'd take a little bit of work to make transparent to a single-process model, but I think it's worth it if you're going to be running multiple bots in separate processes.", "created_utc": 1348956885, "gilded": 0, "name": "t1_c6f9zzm", "num_comments": null, "score": 5, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/"}, {"author": "Deimorz", "body": "My current method of handling this by having a single \"scheduler\" program that runs all of my various scripts in succession to make sure only one is running at a time. For example, I define AutoModerator in it to run \"as often as possible\", another script to run \"every 5 minutes\", another \"every 30 minutes\", etc. Then the program has an infinite loop, and each time the loop starts it looks up which scripts are due to run, and picks whichever one of those hasn't been run for the longest time. So this will have various scripts run on their scheduled intervals, with AutoModerator filling all the time between that.", "created_utc": 1348960513, "gilded": 0, "name": "t1_c6fasy1", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/"}, {"author": "rhiever", "body": "That's what I've resorted to. It sounds like running multiple bots simultaneously and properly managing all of the requests will take a significant amount of work.", "created_utc": 1348964019, "gilded": 0, "name": "t1_c6fbjus", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/10omtd/praw_when_are_requests_being_made_code_included/"}, {"author": "Deimorz", "body": "Yes, you have to be a mod to see the values of approved_by, banned_by, and num_reports.", "created_utc": 1348461245, "gilded": 0, "name": "t1_c6ck3pw", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/10dizi/banned_by_and_num_reports_fields/"}, {"author": "binaryechoes", "body": "Thanks Deimorz. So, do you have to be a mod of that sub or any sub?", "created_utc": 1348462613, "gilded": 0, "name": "t1_c6cke2i", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/10dizi/banned_by_and_num_reports_fields/"}, {"author": "Deimorz", "body": "You have to be a mod of the subreddit that the submission/comment is in.", "created_utc": 1348464731, "gilded": 0, "name": "t1_c6cksg6", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/10dizi/banned_by_and_num_reports_fields/"}, {"author": "binaryechoes", "body": "Right, makes sense ... else you could just create/mod your own sub and access. Thanks again.", "created_utc": 1348465570, "gilded": 0, "name": "t1_c6ckxjt", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/10dizi/banned_by_and_num_reports_fields/"}, {"author": "markerz", "body": "apparently, I deleted this comment when I tried editing. use POST, not GET", "created_utc": 1341643097, "gilded": 0, "name": "t1_c5akqs1", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "bboe", "body": "The parameters need not be part of the URL. I use them in the POST data.", "created_utc": 1341796852, "gilded": 0, "name": "t1_c5b9z4g", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "TankorSmash", "body": "Still getting the 404, even with passing in that URL as a POST request.", "created_utc": 1341668131, "gilded": 0, "name": "t1_c5anb3r", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "markerz", "body": "In my example URL, I set the id parameter to the fullname (t1_...).", "created_utc": 1341672461, "gilded": 0, "name": "t1_c5anxsp", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "TankorSmash", "body": "In Python 2.7 on Win7x64 import requests url=r'http://www.reddit.com/api/morechildren?link_id=t3_tgpzx&where=top&children=c4mjwp6,c4mjw16&id=t1_c4mjwp6&api_type=json' r= requests.post(url) print r.text #Bad Request The post is being made from a session with the appropriate cookies, mind you.", "created_utc": 1341673542, "gilded": 0, "name": "t1_c5ao47d", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "markerz", "body": "Sorry I couldn't get back to you earlier. As I said, I'm not a Python person but I think I figured it out after playing around with your code a little. I, too, got a 411 status code when I tried what you did. However, that seems to be the [standard error code](http://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html#sec10.4.12) for requests rejected for not having the content length specified. Anyways, you need to specify your 'Content-Length' header. I did this by creating a dictionary of extra headers and specified the length there and passing it to the post method of the request thingy (module?). >>> import requests >>> url=r'http://www.reddit.com/api/morechildren?link_id=t3_tgpzx&where=top&children=c4mjwp6,c4mjw16&id=t1_c4mjwp6&api_type=json' >>> payload = {'Content-Length' : '0'} >>> r = requests.post(url, payload) >>> r.status_code 200 >>> r.text '{\"json\": {\"errors\": [], \"data\": {\"things\": [{\"kind\": \"t1\", \"data\": {\"parent\": ... Also, cookies shouldn't matter in this case since the thread I used is public and whatnot. I used no cookies (and wouldn't know how in python-requests)", "created_utc": 1341777011, "gilded": 0, "name": "t1_c5b5rpx", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "TankorSmash", "body": "Thanks for working all that out for me! That'll be a great help. You probably only need the modhash for the API calls that need to be logged in for, like making comments and submitting etc.", "created_utc": 1341777558, "gilded": 0, "name": "t1_c5b5vxw", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "bboe", "body": "I'm not certain, nevertheless, there may be an issue if you have session parameters set and are not sending the mod-hash. PRAW [implicitly adds](https://github.com/praw-dev/praw/blob/master/praw/helpers.py#L94) the `uh` parameter to all POST requests associated with a user-session.", "created_utc": 1341797278, "gilded": 0, "name": "t1_c5ba2h7", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "TankorSmash", "body": "It turns out that I wasn't setting the parameters properly; I'm not exactly sure which ones were wrong though, but it certainly seems to be working now. The modhash not the problem though, as far as I can tell, because I could post comments and submit stories just fine. I think it's just the matter of building the URL correctly. How long have you been buidling the PRAW? It's quite a handy tool, from the looks of the source.", "created_utc": 1341797648, "gilded": 0, "name": "t1_c5ba5cm", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "bboe", "body": "I took over as the PRAW package maintainer around last November if I recall correctly.", "created_utc": 1341804375, "gilded": 0, "name": "t1_c5bbpu8", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "slyf", "body": "https://github.com/reddit/reddit/issues/423 describes the request hint: POST not GET", "created_utc": 1341718188, "gilded": 0, "name": "t1_c5ax2n0", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/w60cs/using_morechildren_without_praw/"}, {"author": "WolfSnarI", "body": "What's the reason for the change?", "created_utc": 1341181641, "gilded": 0, "name": "t1_c586b9p", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/vv4tg/python_reddit_api_wrapper_package_rename_and/"}, {"author": "bboe", "body": "Confusion with the name of the python package. I have been using the name PRAW for a few months and I usually have to clarify what I am referring to.", "created_utc": 1341182733, "gilded": 0, "name": "t1_c586jcq", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/vv4tg/python_reddit_api_wrapper_package_rename_and/"}, {"author": "bboe", "body": "Curious, do browsers handle the cookie successfully even though the cookie was assigned to a different domain? If so, then it's probably an issue with cookielib, nevertheless, if the browsers handle it, then it's worth looking into.", "created_utc": 1337098772, "gilded": 0, "name": "t1_c4obl8c", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/"}, {"author": "RedditSrc4Research", "body": "Actually what happens on the browser is that the login will redirect you to whatever domain is in the ini file (though normal browsing won't). So likely you would have to detect the redirect to avoid the issue in PRAW.", "created_utc": 1337101390, "gilded": 0, "name": "t1_c4oc6e0", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/"}, {"author": "bboe", "body": "That's what I figured regarding the browser behavior. However, there is no redirect sent to PRAW as it is completely push based. The reddit2 server in this case allows you to login for its domain, yet, all the links returned I guess appear for the wrong domain. Since the browser implicitly redirects to the correct domain, I'm going to say the reddit_api.cfg should have the \"correct\" domain as well.", "created_utc": 1337105412, "gilded": 0, "name": "t1_c4od431", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/"}, {"author": "RedditSrc4Research", "body": "Sounds reasonable; it was just a PITA to figure out ;). Perhaps just a simple note in the Wiki would suffice? Presumably most people are using the API with reddit.com, and those who aren't likely aren't using multiple domains, but still, it may save a couple people the trouble of trying to debug it.", "created_utc": 1337106659, "gilded": 0, "name": "t1_c4odf6o", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/"}, {"author": "bboe", "body": "Yeah it's totally worth a comment in either the wiki or the default `reddit_api.cfg` file.", "created_utc": 1337108698, "gilded": 0, "name": "t1_c4odxww", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/tnxsj/a_couple_praw_notes_domain_issue_and_make/"}, {"author": "aagavin", "body": "probably did something wrong but: python -c 'import reddit; print(reddit.VERSION)' Traceback (most recent call last): File \"\", line 1, in File \"reddit/__init__.py\", line 16, in import reddit.backport # pylint: disable-msg=W0611 File \"reddit/backport.py\", line 16, in from six import MovedAttribute, add_move ImportError: No module named six", "created_utc": 1332352485, "gilded": 0, "name": "t1_c43eti5", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/"}, {"author": "bboe", "body": "You need to run: `pip install six` Unfortunately this hybrid version has an external dependency. I could include it in the project as it's pretty small, though that's not typically a best-practice.", "created_utc": 1332352934, "gilded": 0, "name": "t1_c43exc0", "num_comments": null, "score": 3, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/"}, {"author": "aagavin", "body": "No worries, would it work if I just dropped the dependency in the reddit directory?", "created_utc": 1332361664, "gilded": 0, "name": "t1_c43gwkz", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/"}, {"author": "bboe", "body": "If you add six.py to the same level as your CWD or to a location on the PYTHONPATH that should work. Adding it into the folder containing the reddit package I don't think will work.", "created_utc": 1332374420, "gilded": 0, "name": "t1_c43jepn", "num_comments": null, "score": 2, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/"}, {"author": "aagavin", "body": "Ah ok thanks. I needed it self contained because other non-technical mods on /r/shittyaskscience would not be able to use the command line. This was all they have to do is install python and download the [script](https://github.com/aagavin/Friday-Flair-Python-reddit-api)[thanks for your help on it] and just double click to run it.", "created_utc": 1332377781, "gilded": 0, "name": "t1_c43k0l6", "num_comments": null, "score": 1, "title": null, "url": "https://www.reddit.com/r/redditdev/comments/r5e5l/python_reddit_api_developers_python_23_hybrid/"}]